--- Emails for Year 2018 ---

=== Thread: [No Subject] ===

From: Unknown
To: Unknown
Subject: 
Date: 
Message-ID: 
--------------------
trusting others enough to just apply patches and take pull requests.
But it's important.

The maintainer does need to be a source of quality control, but at the
same time, it does very much require "trust others to just do the
right thing" too. The quality control may be about finding a quality
person, not about each patch.

Otherwise maintenance ends up being a huge bottleneck.

For the kernel, I obviously trust my maintainers - there's no way I'd
have any time at all if I didn't.

But part of that trust isn't even "is this right", but "will it be
fixed if it is wrong"?

And that trust generally just comes from "X has been around for a
while and shown him/herself to be reliable".  Once you get to that
point, you don't have to worry about the code changes as much, because
you can feel fairly confident that any problems found down the line
won't be _your_ problems ;)

In fact, the thing that drives *me* to swearing and being an asshole
top-level maintainer is generally not "a bug happened", but "a bug
happened and the source of the problem didn't take responsibility".
That will absolutely make me swear at a maintainer.

Bugs and mis-designed happen. They can be annoying and irritating and
embarrassing. But they are also just reality of any development.

So I do think Chris should take patches from Luc in particular more
aggressively - and preferably just pull his tree. Or even have shared
maintenance of the whole tree.

Because Luc has definitely been around long enough that we know he
fixes any issues he has introduced.

               Linus
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Unknown
To: Unknown
Subject: 
Date: 
Message-ID: 
--------------------
  nonstring

    The nonstring variable attribute specifies that an object or member
    declaration with type array of char, signed char, or unsigned char,
    or pointer to such a type is intended to store character arrays that
    do not necessarily contain a terminating NUL. This is useful in detecting
    uses of such arrays or pointers with functions that expect NUL-terminated
    strings, and to avoid warnings when such an array or pointer is used as
    an argument to a bounded string manipulation function such as strncpy.

  https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html

This attribute can be used for documentation purposes (i.e. replacing
comments), but it is most helpful when the following warnings are enabled:

  -Wstringop-overflow

    Warn for calls to string manipulation functions such as memcpy and
    strcpy that are determined to overflow the destination buffer.

    [...]

  -Wstringop-truncation

    Warn for calls to bounded string manipulation functions such as
    strncat, strncpy, and stpncpy that may either truncate the copied
    string or leave the destination unchanged.

    [...]

    In situations where a character array is intended to store a sequence
    of bytes with no terminating NUL such an array may be annotated with
    attribute nonstring to avoid this warning. Such arrays, however,
    are not suitable arguments to functions that expect NUL-terminated
    strings. To help detect accidental misuses of such arrays GCC issues
    warnings unless it can prove that the use is safe.

  https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html

Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
---
 include/linux/compiler_attributes.h | 14 ++++++++++++++
 1 file changed, 14 insertions(+)

diff --git a/include/linux/compiler_attributes.h b/include/linux/compiler_attributes.h
index f0f9fc398440..6b28c1b7310c 100644
--- a/include/linux/compiler_attributes.h
+++ b/include/linux/compiler_attributes.h
@@ -34,6 +34,7 @@
 # define __GCC4_has_attribute___externally_visible__  1
 # define __GCC4_has_attribute___noclone__             1
 # define __GCC4_has_attribute___optimize__            1
+# define __GCC4_has_attribute___nonstring__           0
 # define __GCC4_has_attribute___no_sanitize_address__ (__GNUC_MINOR__ >= 8)
 #endif
 
@@ -181,6 +182,19 @@
  */
 #define   noinline                      __attribute__((__noinline__))
 
+/*
+ * Optional: only supported since gcc >= 8
+ * Optional: not supported by clang
+ * Optional: not supported by icc
+ *
+ *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-nonstring-variable-attribute
+ */
+#if __has_attribute(__nonstring__)
+# define __nonstring                    __attribute__((__nonstring__))
+#else
+# define __nonstring
+#endif
+
 /*
  *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-noreturn-function-attribute
  * clang: https://clang.llvm.org/docs/AttributeReference.html#noreturn
-- 
2.17.1

================================================================================

From: Unknown
To: Unknown
Subject: 
Date: 
Message-ID: 
--------------------
  nonstring

    The nonstring variable attribute specifies that an object or member
    declaration with type array of char, signed char, or unsigned char,
    or pointer to such a type is intended to store character arrays that
    do not necessarily contain a terminating NUL. This is useful in detecting
    uses of such arrays or pointers with functions that expect NUL-terminated
    strings, and to avoid warnings when such an array or pointer is used as
    an argument to a bounded string manipulation function such as strncpy.

  https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html

This attribute can be used for documentation purposes (i.e. replacing
comments), but it is most helpful when the following warnings are enabled:

  -Wstringop-overflow

    Warn for calls to string manipulation functions such as memcpy and
    strcpy that are determined to overflow the destination buffer.

    [...]

  -Wstringop-truncation

    Warn for calls to bounded string manipulation functions such as
    strncat, strncpy, and stpncpy that may either truncate the copied
    string or leave the destination unchanged.

    [...]

    In situations where a character array is intended to store a sequence
    of bytes with no terminating NUL such an array may be annotated with
    attribute nonstring to avoid this warning. Such arrays, however,
    are not suitable arguments to functions that expect NUL-terminated
    strings. To help detect accidental misuses of such arrays GCC issues
    warnings unless it can prove that the use is safe.

  https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html

Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
---
 include/linux/compiler_attributes.h | 14 ++++++++++++++
 1 file changed, 14 insertions(+)

diff --git a/include/linux/compiler_attributes.h b/include/linux/compiler_attributes.h
index f0f9fc398440..6b28c1b7310c 100644
--- a/include/linux/compiler_attributes.h
+++ b/include/linux/compiler_attributes.h
@@ -34,6 +34,7 @@
 # define __GCC4_has_attribute___externally_visible__  1
 # define __GCC4_has_attribute___noclone__             1
 # define __GCC4_has_attribute___optimize__            1
+# define __GCC4_has_attribute___nonstring__           0
 # define __GCC4_has_attribute___no_sanitize_address__ (__GNUC_MINOR__ >= 8)
 #endif
 
@@ -181,6 +182,19 @@
  */
 #define   noinline                      __attribute__((__noinline__))
 
+/*
+ * Optional: only supported since gcc >= 8
+ * Optional: not supported by clang
+ * Optional: not supported by icc
+ *
+ *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-nonstring-variable-attribute
+ */
+#if __has_attribute(__nonstring__)
+# define __nonstring                    __attribute__((__nonstring__))
+#else
+# define __nonstring
+#endif
+
 /*
  *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-noreturn-function-attribute
  * clang: https://clang.llvm.org/docs/AttributeReference.html#noreturn
-- 
2.17.1

================================================================================

From: Unknown
To: Unknown
Subject: 
Date: 
Message-ID: 
--------------------
  nonstring

    The nonstring variable attribute specifies that an object or member
    declaration with type array of char, signed char, or unsigned char,
    or pointer to such a type is intended to store character arrays that
    do not necessarily contain a terminating NUL. This is useful in detecting
    uses of such arrays or pointers with functions that expect NUL-terminated
    strings, and to avoid warnings when such an array or pointer is used as
    an argument to a bounded string manipulation function such as strncpy.

  https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html

This attribute can be used for documentation purposes (i.e. replacing
comments), but it is most helpful when the following warnings are enabled:

  -Wstringop-overflow

    Warn for calls to string manipulation functions such as memcpy and
    strcpy that are determined to overflow the destination buffer.

    [...]

  -Wstringop-truncation

    Warn for calls to bounded string manipulation functions such as
    strncat, strncpy, and stpncpy that may either truncate the copied
    string or leave the destination unchanged.

    [...]

    In situations where a character array is intended to store a sequence
    of bytes with no terminating NUL such an array may be annotated with
    attribute nonstring to avoid this warning. Such arrays, however,
    are not suitable arguments to functions that expect NUL-terminated
    strings. To help detect accidental misuses of such arrays GCC issues
    warnings unless it can prove that the use is safe.

  https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html

Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
---
 include/linux/compiler_attributes.h | 14 ++++++++++++++
 1 file changed, 14 insertions(+)

diff --git a/include/linux/compiler_attributes.h b/include/linux/compiler_attributes.h
index f0f9fc398440..6b28c1b7310c 100644
--- a/include/linux/compiler_attributes.h
+++ b/include/linux/compiler_attributes.h
@@ -34,6 +34,7 @@
 # define __GCC4_has_attribute___externally_visible__  1
 # define __GCC4_has_attribute___noclone__             1
 # define __GCC4_has_attribute___optimize__            1
+# define __GCC4_has_attribute___nonstring__           0
 # define __GCC4_has_attribute___no_sanitize_address__ (__GNUC_MINOR__ >= 8)
 #endif
 
@@ -181,6 +182,19 @@
  */
 #define   noinline                      __attribute__((__noinline__))
 
+/*
+ * Optional: only supported since gcc >= 8
+ * Optional: not supported by clang
+ * Optional: not supported by icc
+ *
+ *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-nonstring-variable-attribute
+ */
+#if __has_attribute(__nonstring__)
+# define __nonstring                    __attribute__((__nonstring__))
+#else
+# define __nonstring
+#endif
+
 /*
  *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-noreturn-function-attribute
  * clang: https://clang.llvm.org/docs/AttributeReference.html#noreturn
-- 
2.17.1

================================================================================


################################################################################

=== Thread: <<<Good Day To You Sir>> ===

From: "Mr Owen Peter" <aturnerr1976 () gmail ! com>
To: linux-sparse
Subject: <<<Good Day To You Sir>>
Date: Mon, 17 Dec 2018 21:09:24 +0000
Message-ID: <201812172109.wBHL98Fw012574-wBHL98G0012574 () antispam ! interfashion ! it>
--------------------
Good Day,
Hope you are doing great Today.I have a proposed BUSINESS DEAL that will benefit both parties. This is legitimate,legal and your personality will not be compromised.Please Reply to me ONLY if you are interested and consider your self capable for details.

Sincerely,

Peter OWEN
================================================================================


################################################################################

=== Thread: =?utf-8?b?V29obHTDpHRpZ2tlaXRzc3BlbmRlIGluIEjDtmhlIHZvbiDigqwgMi4wMDAu?= =?utf-8?q?000=2C00?= ===

From: cinthia_reyes () hmha ! gob ! ec
To: linux-sparse
Subject: =?utf-8?b?V29obHTDpHRpZ2tlaXRzc3BlbmRlIGluIEjDtmhlIHZvbiDigqwgMi4wMDAu?= =?utf-8?q?000=2C00?=
Date: Mon, 24 Sep 2018 07:40:27 +0000
Message-ID: <20180924074036.F2A50259617D () mail ! hmha ! gob ! ec>
--------------------
Lieber Freund,
 
Ich bin Herr Tayeb Souami, New Jersey, Vereinigte Staaten von Amerika, der Mega-Gewinner von $ 315million In Mega Millions Jackpot, spende ich an 5 zufÃ¤llige Personen, wenn Sie diese E-Mail erhalten, dann wurde Ihre E-Mail nach einem Spinball ausgewÃ¤hlt.Ich habe den grÃ¶Ãten Teil meines VermÃ¶gens auf eine Reihe von WohltÃ¤tigkeitsorganisationen und Organisationen verteilt.Ich habe mich freiwillig dazu entschieden, die Summe von â¬ 2.000.000,00 an Sie als eine der ausgewÃ¤hlten 5 zu spenden, um meine Gewinne zu Ã¼berprÃ¼fen, sehen Sie bitte meine You Tube Seite unten.
 
UHR MICH HIER: https://www.youtube.com/watch?v=Z6ui8ZDQ6Ks
 
Das ist dein Spendencode: [DFC530342018]
 
Antworten Sie mit dem SPENDE-CODE an diese E-Mail: financialfundingcompany09@gmail.com
 
Ich hoffe, Sie und Ihre Familie glÃ¼cklich zu machen.
 
GrÃ¼Ãe
Herr Tayeb Souami
================================================================================


################################################################################

=== Thread: Bug#906472: horst: FTBFS in buster/sid (unable to open 'stdarg.h') ===

From: Uwe =?iso-8859-1?Q?Kleine-K=F6nig?= <uwe () kleine-koenig ! org>
To: linux-sparse
Subject: Re: Bug#906472: horst: FTBFS in buster/sid (unable to open 'stdarg.h')
Date: Fri, 14 Sep 2018 19:45:44 +0000
Message-ID: <20180914194543.GA22476 () taurus ! defre ! kleine-koenig ! org>
--------------------

--TB36FDmn/VVEgNH/
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
Content-Transfer-Encoding: quoted-printable

Hello,

[Cc +=3D sparse mailing list]

On Fri, Sep 14, 2018 at 05:04:09PM +0300, Adrian Bunk wrote:
> Control: reassign -1 sparse 0.5.2-1
> Control: affects -1 src:horst
>=20
> On Sat, Aug 25, 2018 at 10:09:47PM +0200, Christoph Biedl wrote:
> > Santiago Vila wrote...
> >=20
> > > 	make -j1 check
> > > make[1]: Entering directory '/<<PKGBUILDDIR>>'
> > > sparse -g -O2 -fdebug-prefix-map=3D/<<PKGBUILDDIR>>=3D. -fstack-prote=
ctor-strong -Wformat -Werror=3Dformat-security -std=3Dgnu99 -Wall -Wextra -=
g -I. -DDO_DEBUG -I/usr/include/libnl3 *.[ch]
> > > /usr/include/err.h:25:11: error: unable to open 'stdarg.h'
> >=20
> > To reproduce this it's important to remove gcc-7 from the build chroot
> > (apt purge libgcc-7-dev ; apt --purge autoremove).
> >=20
> > Problem is, sparse appearently uses hardcoded paths and looks for
> > stdarg.h in (among other places)
> >=20
> > | /usr/lib/gcc/x86_64-linux-gnu/7//include/stdarg.h
> > |                               ^
> >=20
> > ... which fails.
> >=20
> > Solution is to rebuild sparse, building horst was successful then.
> > If this is true (please check!), the interesting question is why this
> > wasn't a problem in earlier gcc version bumps.

I think this is a known limitation of sparse and there are three
ways to fix this (in my order of preference):

 a) let horst use cgcc -no-compile instead of sparse; or
 b) let sparse depend on libgcc-7-dev (or whatever provides the
    necessary files); or
 c) use autodetection which gcc is used and pick its files.

I'm not sure if a) fixes the problem. It fixed another problem we had
with horst's usage of sparse in the past though[1].

The downside of c) is that running this autodetection on every call to
sparse is probably slowing down sparse a bit which isn't nice.

Best regards
Uwe

[1] https://bugs.debian.org/873508

--TB36FDmn/VVEgNH/
Content-Type: application/pgp-signature; name="signature.asc"

-----BEGIN PGP SIGNATURE-----

iQEzBAABCgAdFiEEfnIqFpAYrP8+dKQLwfwUeK3K7AkFAlucD+QACgkQwfwUeK3K
7AmwdQf+PuAzYgQgyvmQQ4VmxhkEjM5DT5qfn4mvrFoyE/vp3Vute6s3FivpIroW
G73W7gUyio9+aKAduWQnBPqXEnMwtnju7IC7NbMY9D13+pn0TAo00U2N5M2z1NqB
hCrc0rGSsr7ftTTiTqTNlIbIULhmPNHAFWjM1KM7vZ8S6zJCiCPtR1o+AnrTQpy0
UgEHWrJle4o6KtuWGcPJV6ASEDgeP1duRmgQIP47cDfRRSDFxiLa1wxmiGomiBpf
SwbBN5kpeAYRdSAQ0lLl7wGQQ9pNXzDrRYhAbbZZNQuUfkD2h4koksg/OzYiLXRr
YzswkknBnJ33POSk4uNEMLbmJwoaRQ==
=4NBx
-----END PGP SIGNATURE-----

--TB36FDmn/VVEgNH/--
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: Bug#906472: horst: FTBFS in buster/sid (unable to open 'stdarg.h')
Date: Mon, 24 Sep 2018 16:23:41 +0000
Message-ID: <20180924162339.wfs6pmy33ujvuyem () ltop ! local>
--------------------
On Fri, Sep 14, 2018 at 09:45:44PM +0200, Uwe Kleine-König wrote:
> Hello,
> 
> [Cc += sparse mailing list]

Sorry for the late answer.
 
> > > Santiago Vila wrote...
> > > 
> > > > 	make -j1 check
> > > > make[1]: Entering directory '/<<PKGBUILDDIR>>'
> > > > sparse -g -O2 -fdebug-prefix-map=/<<PKGBUILDDIR>>=. -fstack-protector-strong -Wformat -Werror=format-security -std=gnu99 -Wall -Wextra -g -I. -DDO_DEBUG -I/usr/include/libnl3 *.[ch]
> > > > /usr/include/err.h:25:11: error: unable to open 'stdarg.h'
> > > 
> > > To reproduce this it's important to remove gcc-7 from the build chroot
> > > (apt purge libgcc-7-dev ; apt --purge autoremove).
> > > 
> > > Problem is, sparse appearently uses hardcoded paths and looks for
> > > stdarg.h in (among other places)

Well, sparse needs to know where it can find the system header files.
There is an option for this: -gcc-base-dir (and -multiarch-dir).
Usually you will want to use either some that are installed by your
distro or the one that match the exact GCC version you're using.

By *default*, if no -gcc-base-dir is used, sparse use the same dir
as the one used by the GCC used to compile sparse itself. It's only
this default that is hardcoded.

> > > Solution is to rebuild sparse, building horst was successful then.
> > > If this is true (please check!), the interesting question is why this
> > > wasn't a problem in earlier gcc version bumps.
> 
> I think this is a known limitation of sparse and there are three
> ways to fix this (in my order of preference):
> 
>  a) let horst use cgcc -no-compile instead of sparse; or
>  b) let sparse depend on libgcc-7-dev (or whatever provides the
>     necessary files); or
>  c) use autodetection which gcc is used and pick its files.
>
> I'm not sure if a) fixes the problem. It fixed another problem we had
> with horst's usage of sparse in the past though[1].
> 
> The downside of c) is that running this autodetection on every call to
> sparse is probably slowing down sparse a bit which isn't nice.

In case the default can't be used, like here, I think the best is
to add in your Makefile something like:
	GCC_BASE_DIR=$(shell $(CC) -print-file-name=)
	SPARSE_FLAGS= -gcc-base-dir $(GCC_BASE_DIR)
	...
		sparse $(SPARSE_FLAGS) ...

This doesn't need to be done at each invocation of sparse (but will be done
at each top-level make invocation).

Regards,
-- Luc
================================================================================

From: =?UTF-8?Q?Uwe_Kleine-K=c3=b6nig?= <uwe () kleine-koenig ! org>
To: linux-sparse
Subject: Re: Bug#906472: horst: FTBFS in buster/sid (unable to open 'stdarg.h')
Date: Tue, 25 Sep 2018 14:38:01 +0000
Message-ID: <8160e494-8548-761c-07c0-83aa0b97b075 () kleine-koenig ! org>
--------------------
This is an OpenPGP/MIME signed message (RFC 4880 and 3156)
--ePLg7kytdGuKgd4YlzLu4vpYFmgLL7Qvd
Content-Type: multipart/mixed; boundary="5V9MNujSbOcyrhDMIhNrnojzyfAe8KrQV";
 protected-headers="v1"
From: =?UTF-8?Q?Uwe_Kleine-K=c3=b6nig?= <uwe@kleine-koenig.org>
To: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
Cc: Christoph Biedl <debian.axhn@manchmal.in-ulm.de>,
 Santiago Vila <sanvila@debian.org>, 906472@bugs.debian.org,
 linux-sparse@vger.kernel.org, Adrian Bunk <bunk@debian.org>
Message-ID: <8160e494-8548-761c-07c0-83aa0b97b075@kleine-koenig.org>
Subject: Re: Bug#906472: horst: FTBFS in buster/sid (unable to open
 'stdarg.h')
References: <E1fqkJB-0004Wc-55@paradis.debian.org>
 <E1fqkJB-0004Wc-55@paradis.debian.org> <1535227396@msgid.manchmal.in-ulm.de>
 <20180914140409.GA25593@localhost>
 <20180914194543.GA22476@taurus.defre.kleine-koenig.org>
 <20180924162339.wfs6pmy33ujvuyem@ltop.local>
In-Reply-To: <20180924162339.wfs6pmy33ujvuyem@ltop.local>

--5V9MNujSbOcyrhDMIhNrnojzyfAe8KrQV
Content-Type: text/plain; charset=utf-8
Content-Language: en-US
Content-Transfer-Encoding: quoted-printable

Hello Luc,

On 09/24/2018 06:23 PM, Luc Van Oostenryck wrote:
> On Fri, Sep 14, 2018 at 09:45:44PM +0200, Uwe Kleine-K=C3=B6nig wrote:
>>>> Santiago Vila wrote...
>>>>
>>>>> 	make -j1 check
>>>>> make[1]: Entering directory '/<<PKGBUILDDIR>>'
>>>>> sparse -g -O2 -fdebug-prefix-map=3D/<<PKGBUILDDIR>>=3D. -fstack-pro=
tector-strong -Wformat -Werror=3Dformat-security -std=3Dgnu99 -Wall -Wext=
ra -g -I. -DDO_DEBUG -I/usr/include/libnl3 *.[ch]
>>>>> /usr/include/err.h:25:11: error: unable to open 'stdarg.h'
>>>>
>>>> To reproduce this it's important to remove gcc-7 from the build chro=
ot
>>>> (apt purge libgcc-7-dev ; apt --purge autoremove).
>>>>
>>>> Problem is, sparse appearently uses hardcoded paths and looks for
>>>> stdarg.h in (among other places)
>=20
> Well, sparse needs to know where it can find the system header files.
> There is an option for this: -gcc-base-dir (and -multiarch-dir).
> Usually you will want to use either some that are installed by your
> distro or the one that match the exact GCC version you're using.

I think I don't want to impose on sparse users to know the system header
location.

> By *default*, if no -gcc-base-dir is used, sparse use the same dir
> as the one used by the GCC used to compile sparse itself. It's only
> this default that is hardcoded.

I wasn't sure if this is expected to work. I had in mind, that the
version of gcc is used somewhere but couldn't find proof for that.

> In case the default can't be used, like here, I think the best is
> to add in your Makefile something like:
> 	GCC_BASE_DIR=3D$(shell $(CC) -print-file-name=3D)
> 	SPARSE_FLAGS=3D -gcc-base-dir $(GCC_BASE_DIR)
> 	...
> 		sparse $(SPARSE_FLAGS) ...

IMHO it is sensible to let sparse depend on (or at least recommend) gcc.
And then the default value should be right.

> This doesn't need to be done at each invocation of sparse (but will be =
done
> at each top-level make invocation).

I'd say you'd need :=3D for this effect instead of =3D. But as I intend t=
o
make sure that with sparse the default headers are also available, this
is a non-issue. (And this only happened because sparse was built with
gcc-7 while horst was built with gcc-8.)

Best regards and thanks for your valuable input,
Uwe


--5V9MNujSbOcyrhDMIhNrnojzyfAe8KrQV--

--ePLg7kytdGuKgd4YlzLu4vpYFmgLL7Qvd
Content-Type: application/pgp-signature; name="signature.asc"
Content-Description: OpenPGP digital signature
Content-Disposition: attachment; filename="signature.asc"

-----BEGIN PGP SIGNATURE-----

iQEzBAEBCgAdFiEEfnIqFpAYrP8+dKQLwfwUeK3K7AkFAluqSEkACgkQwfwUeK3K
7AkoXwf/aJqeKvP8NoDFBwyUsSVzpwvwIFjH3n0APqofQX6VffNaikxM0Ug5NrN5
gMH5Dja8ZvgEKSortt0KNsXNAoiQl9oyjSGIrWTR5m4XblDJqakSwwr85uz4KZAQ
9MTlU63ISYHgNVtqtpIVK20XmRKz012jq5RGndzX16gxrIOUHr0yrOpHZVW71PbQ
4i7inVmemIptG2JxzO/Ptshn9Yh+69GLXJPHTdfyKJOWMiLOwkDzW0KfBnbYMlAH
BhEGXMk6ARhImS1i/ZJ63z1FsuJc6gE42iynEIm1I+dyh+GtxlyZNXY2P4sab4Kw
empIGeOXm179+e7+5NMG+jxiacIZPg==
=9ZRO
-----END PGP SIGNATURE-----

--ePLg7kytdGuKgd4YlzLu4vpYFmgLL7Qvd--
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: Bug#906472: horst: FTBFS in buster/sid (unable to open 'stdarg.h')
Date: Tue, 25 Sep 2018 17:06:02 +0000
Message-ID: <20180925170601.gonqbd6eatgd5rzj () ltop ! local>
--------------------
On Tue, Sep 25, 2018 at 04:38:01PM +0200, Uwe Kleine-König wrote:
> > By *default*, if no -gcc-base-dir is used, sparse use the same dir
> > as the one used by the GCC used to compile sparse itself. It's only
> > this default that is hardcoded.
> 
> I wasn't sure if this is expected to work. I had in mind, that the
> version of gcc is used somewhere but couldn't find proof for that.

Several other flags have their default value corresponding to the
GCC used to compile sparse: for example the architecture (x86, x86-64)
or the endianness. The one you're thinking about is probably the macros
for the GCC version: __GNUC__, __GNUC_MINOR__ & __GNUC_PATCHLEVEL__.

As far as I know, everything that should matter can be set explicitly
either by defining the macro via -D...=... or an explicit flag like
-m{32,64}, -m{little,big}-endian, -funsigned-char, ...

When using sparse for userspace code, it may be best to use the cgcc
wrapper since it take care to set (some of) these values. I'm saying
'may' because personnaly, I have no experience of using sparse via
cgcc (as an user, I've only used sparse for OS or bare-metal code)
but I know others use it like so (for example for git).

> > In case the default can't be used, like here, I think the best is
> > to add in your Makefile something like:
> > 	GCC_BASE_DIR=$(shell $(CC) -print-file-name=)
> > 	SPARSE_FLAGS= -gcc-base-dir $(GCC_BASE_DIR)
> > 	...
> > 		sparse $(SPARSE_FLAGS) ...
> 
> IMHO it is sensible to let sparse depend on (or at least recommend) gcc.
> And then the default value should be right.
> 
> > This doesn't need to be done at each invocation of sparse (but will be done
> > at each top-level make invocation).
> 
> I'd say you'd need := for this effect instead of =.

Yes, indeed.

> Best regards and thanks for your valuable input,
> Uwe

You're very welcome,
-- Luc
================================================================================


################################################################################

=== Thread: For your attention ===

From: FRANK CHRISTOPHER <mr.peteruderika () yahoo ! pt>
To: linux-sparse
Subject: For your attention
Date: Wed, 24 Oct 2018 20:38:40 +0000
Message-ID: <456493476.24145967.1540413520643 () mail ! yahoo ! com>
--------------------


My name is Mr Frank Christopher, the est. managing director of the Banque Internationale du Benin. I am contacting you in a benevolent spirit;and with utmost confidence and trust to enable us provide a solution to a money transfer of $18Million to your account.

 Do get back to me for more details.

 Best Regards,

Mr Frank Christopher.
================================================================================


################################################################################

=== Thread: Fwd: Noisy Sparse warnings ===

From: Steve French <smfrench () gmail ! com>
To: linux-sparse
Subject: Re: Fwd: Noisy Sparse warnings
Date: Thu, 19 Apr 2018 17:13:20 +0000
Message-ID: <CAH2r5mv6tXcxOEqURsd4kZcstfJPDsXaNWWHX7rK4O0YzL5e5w () mail ! gmail ! com>
--------------------
On Wed, Apr 18, 2018 at 4:45 AM, Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
> On Wed, Apr 18, 2018 at 11:26:48AM +0200, Aur=C3=A9lien Aptel wrote:
>> Just forwarding a question I saw on linux-fsdevel:
>>
>> | From:   Steve French <smfrench@gmail.com>
>> | Date:   Wed, 4 Apr 2018 18:05:04 -0500
>> | Subject: Noisy Sparse warnings
>> | To:     LKML <linux-kernel@vger.kernel.org>,
>> |         linux-fsdevel <linux-fsdevel@vger.kernel.org>
>> | Sender: linux-fsdevel-owner@vger.kernel.org
>> | Precedence: bulk
>> | List-ID: <linux-fsdevel.vger.kernel.org>
>> | X-Mailing-List: linux-fsdevel@vger.kernel.org
>> |
>> | I have been getting the following sparse warnings repeatedly on my
>> | current Ubutu (17.10) when compiling the current mainline kernel:
>> |
>> | ./arch/x86/include/asm/nospec-branch.h:144:38: warning: Unknown escape=
 '@'
>> | ./include/linux/string.h:239:1: error: attribute 'gnu_inline': unknown=
 attribute
>> |
>> | I get the same sparse error whether I use the default sparse that
>> | Ubuntu ships (0.5.0) or the current sparse (0.5.2-rc1)
>> |
>> | Ideas?
>
> Thanks for the report.
>
> For the second warning, this will indeed happen with sparse-0.5.0 but
> have been fixed later. It certainly shouldn't happen with sparse-0.5.1
> (where sparse have been teached about 'gnu_inline') and even more so
> with 0.5.2-rc1 where such warnings have been disabled by default.
>
> For the first warning, it looks as a problem which have also been
> fixed before sparse-0.5.1.
>
> @Steve,
>
> Can you check that you're really using a recent sparse like 0.5.1 or
> 0.5.2-rc1?

Looks like 0.5.2-rc1 helped with two but now am getting a new one.

I have two systems - both are now verified (sparse --version) as running
version 0.5.2-rc1 for the user doing the build (that was part of the
problem, it was finding sparse 0.5.0 when running as a different user than
the one who had installed version 0.5.2-rc1)  but I do get one error repeat=
edly
on one of the two Ubuntu systems (identical, current version, of Ubuntu)

  CHECK   fs/cifs/connect.c
fs/cifs/connect.c:825:25: warning: expression using sizeof(void)

See one of the dozen's of cases below:

cifs_dump_mem("Bad SMB: ", buf,
                        min_t(unsigned int, server->total_read, 48));

it looks like "min_t" is causing confusion for sparse (at least on the VM
running Ubuntu, the same version running on my laptop didn't generate
that when building with sparse).














--=20
Thanks,

Steve
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: Fwd: Noisy Sparse warnings
Date: Thu, 19 Apr 2018 20:01:16 +0000
Message-ID: <20180419182525.kw5zxkalgdyk2bmu () ltop ! local>
--------------------
On Thu, Apr 19, 2018 at 12:13:20PM -0500, Steve French wrote:
> Looks like 0.5.2-rc1 helped with two but now am getting a new one.
> 
> I have two systems - both are now verified (sparse --version) as running
> version 0.5.2-rc1 for the user doing the build (that was part of the
> problem, it was finding sparse 0.5.0 when running as a different user than
> the one who had installed version 0.5.2-rc1)

OK, make sense.

> but I do get one error repeatedly
> on one of the two Ubuntu systems (identical, current version, of Ubuntu)
> 
>   CHECK   fs/cifs/connect.c
> fs/cifs/connect.c:825:25: warning: expression using sizeof(void)

Yes, that's something new. The patch is not yet in any release or
pre-release but you can use one of:
	git://github.com/lucvoo/sparse-dev.git sparse-0.5
	git://github.com/lucvoo/sparse.git     master

> See one of the dozen's of cases below:
> 
> cifs_dump_mem("Bad SMB: ", buf,
>                         min_t(unsigned int, server->total_read, 48));
> 
> it looks like "min_t" is causing confusion for sparse.

Sparse used to warn when using 'sizeof(void)' since it's totally non-standard
and there was only a very few such uses in the kernel. But a recent change
in min/max changed that and now there is a lot of such warnings. Those
warnings are now disabled by the patch in the trees here above.

Cheers,
-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: HI ===

From: Lucy Boston <janiceharvey225 () gmail ! com>
To: linux-sparse
Subject: HI
Date: Tue, 27 Mar 2018 20:37:45 +0000
Message-ID: <CAOhfqoehKQtE0XAbc55tfEbhBhuqR-wDEjz=r6F6feTNyh_hdw () mail ! gmail ! com>
--------------------
-- 
Greeting, once again is me Lucy Boston this is twice am contacting you
please is very urgent respond to me for more details through my.
Email:

dr.lucyboston@gmail.com
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: LLVM fixes ===

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: LLVM fixes
Date: Sun, 25 Feb 2018 19:34:00 +0000
Message-ID: <CACXZuxeP8Nqh63uir=c_n-38CPPnhrXhz5tWonJEbB=zxhFqWw () mail ! gmail ! com>
--------------------
Hi,

I have a bunch of LLVM fixes in my project that is derived from
Sparse. These fixes are more comprehensive than the fixes posted in
this list - although in some cases they are derived from fixes posted
here.

I would like to submit the LLVM fixes from my repository - I am happy
to back port them to Sparse.

In some ways it is easier to post a single patch for sparse-llvm.c
that contains all the fixes - as then it can applied or removed easily
as a single change. Or I can post smaller patches if that is the
preferred approach.

In some cases there are patches that are based on ones posted here by
Luc - so I am not sure how to get these across with the correct
attribution.

Thanks and Regards
Dibyendu
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Christopher Li <sparse () chrisli ! org>
To: linux-sparse
Subject: Re: LLVM fixes
Date: Sun, 25 Feb 2018 19:56:04 +0000
Message-ID: <CANeU7Qn0z6gAWEji4Z1JsmmvudKeV1mtOjkAPMCNB6RuZ8Z84w () mail ! gmail ! com>
--------------------
On Sun, Feb 25, 2018 at 11:34 AM, Dibyendu Majumdar
<mobile@majumdar.org.uk> wrote:
> Hi,
>
> I have a bunch of LLVM fixes in my project that is derived from
> Sparse. These fixes are more comprehensive than the fixes posted in
> this list - although in some cases they are derived from fixes posted
> here.
>
> I would like to submit the LLVM fixes from my repository - I am happy
> to back port them to Sparse.

Sure, very welcome.

> In some ways it is easier to post a single patch for sparse-llvm.c
> that contains all the fixes - as then it can applied or removed easily
> as a single change. Or I can post smaller patches if that is the
> preferred approach.

It would be best to separate the patch you depend on vs your own
patches. How big is your change? If you have a link I can take a look.


> In some cases there are patches that are based on ones posted here by
> Luc - so I am not sure how to get these across with the correct
> attribution.

The best solution would be have Luc send small git merge request for
the LLVM patch you are depend on. Merge those first.

Failing that. I can apply sparse patch from patchworks as well.
It just more work for every one to go through that route.

Chris
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: Re: LLVM fixes
Date: Tue, 27 Feb 2018 22:04:42 +0000
Message-ID: <CACXZuxcYrn2tn1N8SzAf0pSpt2N+gjvbXLy1pd+=GdiMqUxozQ () mail ! gmail ! com>
--------------------
On 25 February 2018 at 20:18, Dibyendu Majumdar <mobile@majumdar.org.uk> wrote:
>>> In some cases there are patches that are based on ones posted here by
>>> Luc - so I am not sure how to get these across with the correct
>>> attribution.
>>
>> The best solution would be have Luc send small git merge request for
>> the LLVM patch you are depend on. Merge those first.
>>
>> Failing that. I can apply sparse patch from patchworks as well.
>> It just more work for every one to go through that route.
>>
>
> Okay I will identify the specific patches I need. I will try to play
> them in my copy / and try to then apply my changes on top. Hopefully
> this way I can send patches that can be merged.
>

Hi Chris, one of the patch sets I need are the ones where Luc added
type to several Sparse OP codes. These are in the series:

https://marc.info/?l=linux-sparse&m=149063626507168&w=2

I don't think Luc is able to resubmit these - are you able to apply
them somehow? These contain the OP_PUSH patch which you will need to
omit.

Thanks and Regards
Dibyendu
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: Re: LLVM fixes
Date: Tue, 27 Feb 2018 22:08:13 +0000
Message-ID: <CACXZuxcv-m0FMkb6PANx_GOcU9FNEd_wj2Yvat7k+u0rxXA0jQ () mail ! gmail ! com>
--------------------
On 27 February 2018 at 22:04, Dibyendu Majumdar <mobile@majumdar.org.uk> wrote:
> On 25 February 2018 at 20:18, Dibyendu Majumdar <mobile@majumdar.org.uk> wrote:
>>>> In some cases there are patches that are based on ones posted here by
>>>> Luc - so I am not sure how to get these across with the correct
>>>> attribution.
>>>
>>> The best solution would be have Luc send small git merge request for
>>> the LLVM patch you are depend on. Merge those first.
>>>
>>> Failing that. I can apply sparse patch from patchworks as well.
>>> It just more work for every one to go through that route.
>>>
>>
>> Okay I will identify the specific patches I need. I will try to play
>> them in my copy / and try to then apply my changes on top. Hopefully
>> this way I can send patches that can be merged.
>>
>
> Hi Chris, one of the patch sets I need are the ones where Luc added
> type to several Sparse OP codes. These are in the series:
>
> https://marc.info/?l=linux-sparse&m=149063626507168&w=2
>
> I don't think Luc is able to resubmit these - are you able to apply
> them somehow? These contain the OP_PUSH patch which you will need to
> omit.
>



BTW you could apply these to a branch (suggest llvm-fixes?) - at least
that way I can work against that branch without impacting anything
else.

Thanks and Regards
Dibyendu
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: Re: LLVM fixes
Date: Wed, 28 Feb 2018 10:41:11 +0000
Message-ID: <20180228104111.4358230.94951.18007 () majumdar ! org ! uk>
--------------------
On Tue, Feb 27, 2018 at 2:08 PM, Dibyendu Majumdar
<mobile@majumdar.org.uk> wrote:
>
>> BTW you could apply these to a branch (suggest llvm-fixes?) - at least
>> that way I can work against that branch without impacting anything
>> else.
=E2=80=8E
> Sure, I can give it a try. No sure that series will apply without the OP_=
PUSH
> patch. Is that llvm-fixes topic branch OK to rebase?

Sorry not familiar with what that means but do as is easiest for you. I can=
 adjust. If you can't omit the op_push I can always submit a patch to undo =
it
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: Re: LLVM fixes
Date: Wed, 28 Feb 2018 12:59:53 +0000
Message-ID: <20180228125953.4358230.87462.18011 () majumdar ! org ! uk>
--------------------


On Tue, Feb 27, 2018 at 2:08 PM, Dibyendu Majumdar
<mobile@majumdar.org.uk> wrote:
>
>> BTW you could apply these to a branch (suggest llvm-fixes?) - at least
>> that way I can work against that branch without impacting anything
>> else.
=E2=80=8E
> Sure, I can give it a try. No sure that series will apply without the OP_=
PUSH
> patch. Is that llvm-fixes topic branch OK to rebase?

> Sorry not familiar with what that means but do as is easiest for you. I c=
an > > adjust. If you can't omit the op_push I can always submit a patch to=
 undo it=E2=80=8E

Read up about rebase - yes that is fine with me.

Thanks and regards
Dibyendu=C2=A0
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: Noisy Sparse warnings ===

From: aaptel () suse ! com (=?utf-8?Q?Aur=C3=A9lien?= Aptel)
To: linux-sparse
Subject: Fwd: Noisy Sparse warnings
Date: Wed, 18 Apr 2018 09:26:48 +0000
Message-ID: <87k1t4dfvr.fsf () suse ! com>
--------------------
Just forwarding a question I saw on linux-fsdevel:

| From:   Steve French <smfrench@gmail.com>
| Date:   Wed, 4 Apr 2018 18:05:04 -0500
| Subject: Noisy Sparse warnings
| To:     LKML <linux-kernel@vger.kernel.org>,
|         linux-fsdevel <linux-fsdevel@vger.kernel.org>
| Sender: linux-fsdevel-owner@vger.kernel.org
| Precedence: bulk
| List-ID: <linux-fsdevel.vger.kernel.org>
| X-Mailing-List: linux-fsdevel@vger.kernel.org
| 
| I have been getting the following sparse warnings repeatedly on my
| current Ubutu (17.10) when compiling the current mainline kernel:
| 
| ./arch/x86/include/asm/nospec-branch.h:144:38: warning: Unknown escape '@'
| ./include/linux/string.h:239:1: error: attribute 'gnu_inline': unknown attribute
| 
| I get the same sparse error whether I use the default sparse that
| Ubuntu ships (0.5.0) or the current sparse (0.5.2-rc1)
| 
| Ideas?


-- 
AurÃ©lien Aptel / SUSE Labs Samba Team
GPG: 1839 CB5F 9F5B FB9B AA97  8C99 03C8 A49B 521B D5D3
SUSE Linux GmbH, MaxfeldstraÃe 5, 90409 NÃ¼rnberg, Germany
GF: Felix ImendÃ¶rffer, Jane Smithard, Graham Norton, HRB 21284 (AG NÃ¼rnberg)
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: PROPOSAL: Extend inline asm syntax with size spec ===

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Wed, 31 Oct 2018 16:31:16 +0000
Message-ID: <20181031163115.GG5994 () gate ! crashing ! org>
--------------------
On Wed, Oct 31, 2018 at 01:55:26PM +0100, Peter Zijlstra wrote:
> Anyway, with the below patch, I get:
> 
>    text    data     bss     dec     hex filename
> 17385183        5064780 1953892 24403855        1745f8f defconfig-build/vmlinux
> 17385678        5064780 1953892 24404350        174617e defconfig-build/vmlinux
> 
> Which shows we inline more (look for asm_volatile for the actual
> changes).
> 
> 
> So yes, this seems like something we could work with.

Great to hear.  Note that with my latest patches
(see https://gcc.gnu.org/ml/gcc-patches/2018-10/msg01931.html ) there
is no required order "asm volatile inline" anymore, so you can just say
"asm inline volatile".  (And similar for "goto" as well).


Segher
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 12:25:02 +0000
Message-ID: <20181129122500.GX23873 () gate ! crashing ! org>
--------------------
On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> On Wed, Oct 10, 2018 at 1:14 AM Segher Boessenkool
> <segher@kernel.crashing.org> wrote:
> >
> > On Mon, Oct 08, 2018 at 11:07:46AM +0200, Richard Biener wrote:
> > > On Mon, 8 Oct 2018, Segher Boessenkool wrote:
> > > > On Sun, Oct 07, 2018 at 03:53:26PM +0000, Michael Matz wrote:
> > > > > On Sun, 7 Oct 2018, Segher Boessenkool wrote:
> > > > > > On Sun, Oct 07, 2018 at 11:18:06AM +0200, Borislav Petkov wrote:
> > > > > > > Now, Richard suggested doing something like:
> > > > > > >
> > > > > > >  1) inline asm ("...")
> > > > > >
> > > > > > What would the semantics of this be?
> > > > >
> > > > > The size of the inline asm wouldn't be counted towards the inliner size
> > > > > limits (or be counted as "1").
> > > >
> > > > That sounds like a good option.
> > >
> > > Yes, I also like it for simplicity.  It also avoids the requirement
> > > of translating the number (in bytes?) given by the user to
> > > "number of GIMPLE instructions" as needed by the inliner.
> >
> > This patch implements this, for C only so far.  And the syntax is
> > "asm inline", which is more in line with other syntax.
> >
> > How does this look?
> 
> 
> Thank you very much for your work.
> 
> 
> https://gcc.gnu.org/ml/gcc-patches/2018-10/msg01932.html
> 
> How is the progress of this in GCC ML?

Latest patch was pinged a few times:
https://gcc.gnu.org/ml/gcc-patches/2018-11/msg01569.html .

I'll ping it again.  Will fix the subject as well if I remember to, sigh.

> I am really hoping the issue will be solved by compiler
> instead of the in-kernel workaround.

This will only be fixed from GCC 9 on, if the compiler adopts it.  The
kernel wants to support ancient GCC, so it will need to have a workaround
for older GCC versions anyway.


Segher
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 12:25:02 +0000
Message-ID: <20181129122500.GX23873 () gate ! crashing ! org>
--------------------
On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> On Wed, Oct 10, 2018 at 1:14 AM Segher Boessenkool
> <segher@kernel.crashing.org> wrote:
> >
> > On Mon, Oct 08, 2018 at 11:07:46AM +0200, Richard Biener wrote:
> > > On Mon, 8 Oct 2018, Segher Boessenkool wrote:
> > > > On Sun, Oct 07, 2018 at 03:53:26PM +0000, Michael Matz wrote:
> > > > > On Sun, 7 Oct 2018, Segher Boessenkool wrote:
> > > > > > On Sun, Oct 07, 2018 at 11:18:06AM +0200, Borislav Petkov wrote:
> > > > > > > Now, Richard suggested doing something like:
> > > > > > >
> > > > > > >  1) inline asm ("...")
> > > > > >
> > > > > > What would the semantics of this be?
> > > > >
> > > > > The size of the inline asm wouldn't be counted towards the inliner size
> > > > > limits (or be counted as "1").
> > > >
> > > > That sounds like a good option.
> > >
> > > Yes, I also like it for simplicity.  It also avoids the requirement
> > > of translating the number (in bytes?) given by the user to
> > > "number of GIMPLE instructions" as needed by the inliner.
> >
> > This patch implements this, for C only so far.  And the syntax is
> > "asm inline", which is more in line with other syntax.
> >
> > How does this look?
> 
> 
> Thank you very much for your work.
> 
> 
> https://gcc.gnu.org/ml/gcc-patches/2018-10/msg01932.html
> 
> How is the progress of this in GCC ML?

Latest patch was pinged a few times:
https://gcc.gnu.org/ml/gcc-patches/2018-11/msg01569.html .

I'll ping it again.  Will fix the subject as well if I remember to, sigh.

> I am really hoping the issue will be solved by compiler
> instead of the in-kernel workaround.

This will only be fixed from GCC 9 on, if the compiler adopts it.  The
kernel wants to support ancient GCC, so it will need to have a workaround
for older GCC versions anyway.


Segher
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 12:25:02 +0000
Message-ID: <20181129122500.GX23873 () gate ! crashing ! org>
--------------------
On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> On Wed, Oct 10, 2018 at 1:14 AM Segher Boessenkool
> <segher@kernel.crashing.org> wrote:
> >
> > On Mon, Oct 08, 2018 at 11:07:46AM +0200, Richard Biener wrote:
> > > On Mon, 8 Oct 2018, Segher Boessenkool wrote:
> > > > On Sun, Oct 07, 2018 at 03:53:26PM +0000, Michael Matz wrote:
> > > > > On Sun, 7 Oct 2018, Segher Boessenkool wrote:
> > > > > > On Sun, Oct 07, 2018 at 11:18:06AM +0200, Borislav Petkov wrote:
> > > > > > > Now, Richard suggested doing something like:
> > > > > > >
> > > > > > >  1) inline asm ("...")
> > > > > >
> > > > > > What would the semantics of this be?
> > > > >
> > > > > The size of the inline asm wouldn't be counted towards the inliner size
> > > > > limits (or be counted as "1").
> > > >
> > > > That sounds like a good option.
> > >
> > > Yes, I also like it for simplicity.  It also avoids the requirement
> > > of translating the number (in bytes?) given by the user to
> > > "number of GIMPLE instructions" as needed by the inliner.
> >
> > This patch implements this, for C only so far.  And the syntax is
> > "asm inline", which is more in line with other syntax.
> >
> > How does this look?
> 
> 
> Thank you very much for your work.
> 
> 
> https://gcc.gnu.org/ml/gcc-patches/2018-10/msg01932.html
> 
> How is the progress of this in GCC ML?

Latest patch was pinged a few times:
https://gcc.gnu.org/ml/gcc-patches/2018-11/msg01569.html .

I'll ping it again.  Will fix the subject as well if I remember to, sigh.

> I am really hoping the issue will be solved by compiler
> instead of the in-kernel workaround.

This will only be fixed from GCC 9 on, if the compiler adopts it.  The
kernel wants to support ancient GCC, so it will need to have a workaround
for older GCC versions anyway.


Segher
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 12:25:02 +0000
Message-ID: <20181129122500.GX23873 () gate ! crashing ! org>
--------------------
On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> On Wed, Oct 10, 2018 at 1:14 AM Segher Boessenkool
> <segher@kernel.crashing.org> wrote:
> >
> > On Mon, Oct 08, 2018 at 11:07:46AM +0200, Richard Biener wrote:
> > > On Mon, 8 Oct 2018, Segher Boessenkool wrote:
> > > > On Sun, Oct 07, 2018 at 03:53:26PM +0000, Michael Matz wrote:
> > > > > On Sun, 7 Oct 2018, Segher Boessenkool wrote:
> > > > > > On Sun, Oct 07, 2018 at 11:18:06AM +0200, Borislav Petkov wrote:
> > > > > > > Now, Richard suggested doing something like:
> > > > > > >
> > > > > > >  1) inline asm ("...")
> > > > > >
> > > > > > What would the semantics of this be?
> > > > >
> > > > > The size of the inline asm wouldn't be counted towards the inliner size
> > > > > limits (or be counted as "1").
> > > >
> > > > That sounds like a good option.
> > >
> > > Yes, I also like it for simplicity.  It also avoids the requirement
> > > of translating the number (in bytes?) given by the user to
> > > "number of GIMPLE instructions" as needed by the inliner.
> >
> > This patch implements this, for C only so far.  And the syntax is
> > "asm inline", which is more in line with other syntax.
> >
> > How does this look?
> 
> 
> Thank you very much for your work.
> 
> 
> https://gcc.gnu.org/ml/gcc-patches/2018-10/msg01932.html
> 
> How is the progress of this in GCC ML?

Latest patch was pinged a few times:
https://gcc.gnu.org/ml/gcc-patches/2018-11/msg01569.html .

I'll ping it again.  Will fix the subject as well if I remember to, sigh.

> I am really hoping the issue will be solved by compiler
> instead of the in-kernel workaround.

This will only be fixed from GCC 9 on, if the compiler adopts it.  The
kernel wants to support ancient GCC, so it will need to have a workaround
for older GCC versions anyway.


Segher
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 12:25:02 +0000
Message-ID: <20181129122500.GX23873 () gate ! crashing ! org>
--------------------
On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> On Wed, Oct 10, 2018 at 1:14 AM Segher Boessenkool
> <segher@kernel.crashing.org> wrote:
> >
> > On Mon, Oct 08, 2018 at 11:07:46AM +0200, Richard Biener wrote:
> > > On Mon, 8 Oct 2018, Segher Boessenkool wrote:
> > > > On Sun, Oct 07, 2018 at 03:53:26PM +0000, Michael Matz wrote:
> > > > > On Sun, 7 Oct 2018, Segher Boessenkool wrote:
> > > > > > On Sun, Oct 07, 2018 at 11:18:06AM +0200, Borislav Petkov wrote:
> > > > > > > Now, Richard suggested doing something like:
> > > > > > >
> > > > > > >  1) inline asm ("...")
> > > > > >
> > > > > > What would the semantics of this be?
> > > > >
> > > > > The size of the inline asm wouldn't be counted towards the inliner size
> > > > > limits (or be counted as "1").
> > > >
> > > > That sounds like a good option.
> > >
> > > Yes, I also like it for simplicity.  It also avoids the requirement
> > > of translating the number (in bytes?) given by the user to
> > > "number of GIMPLE instructions" as needed by the inliner.
> >
> > This patch implements this, for C only so far.  And the syntax is
> > "asm inline", which is more in line with other syntax.
> >
> > How does this look?
> 
> 
> Thank you very much for your work.
> 
> 
> https://gcc.gnu.org/ml/gcc-patches/2018-10/msg01932.html
> 
> How is the progress of this in GCC ML?

Latest patch was pinged a few times:
https://gcc.gnu.org/ml/gcc-patches/2018-11/msg01569.html .

I'll ping it again.  Will fix the subject as well if I remember to, sigh.

> I am really hoping the issue will be solved by compiler
> instead of the in-kernel workaround.

This will only be fixed from GCC 9 on, if the compiler adopts it.  The
kernel wants to support ancient GCC, so it will need to have a workaround
for older GCC versions anyway.


Segher
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 12:25:02 +0000
Message-ID: <20181129122500.GX23873 () gate ! crashing ! org>
--------------------
On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> On Wed, Oct 10, 2018 at 1:14 AM Segher Boessenkool
> <segher@kernel.crashing.org> wrote:
> >
> > On Mon, Oct 08, 2018 at 11:07:46AM +0200, Richard Biener wrote:
> > > On Mon, 8 Oct 2018, Segher Boessenkool wrote:
> > > > On Sun, Oct 07, 2018 at 03:53:26PM +0000, Michael Matz wrote:
> > > > > On Sun, 7 Oct 2018, Segher Boessenkool wrote:
> > > > > > On Sun, Oct 07, 2018 at 11:18:06AM +0200, Borislav Petkov wrote:
> > > > > > > Now, Richard suggested doing something like:
> > > > > > >
> > > > > > >  1) inline asm ("...")
> > > > > >
> > > > > > What would the semantics of this be?
> > > > >
> > > > > The size of the inline asm wouldn't be counted towards the inliner size
> > > > > limits (or be counted as "1").
> > > >
> > > > That sounds like a good option.
> > >
> > > Yes, I also like it for simplicity.  It also avoids the requirement
> > > of translating the number (in bytes?) given by the user to
> > > "number of GIMPLE instructions" as needed by the inliner.
> >
> > This patch implements this, for C only so far.  And the syntax is
> > "asm inline", which is more in line with other syntax.
> >
> > How does this look?
> 
> 
> Thank you very much for your work.
> 
> 
> https://gcc.gnu.org/ml/gcc-patches/2018-10/msg01932.html
> 
> How is the progress of this in GCC ML?

Latest patch was pinged a few times:
https://gcc.gnu.org/ml/gcc-patches/2018-11/msg01569.html .

I'll ping it again.  Will fix the subject as well if I remember to, sigh.

> I am really hoping the issue will be solved by compiler
> instead of the in-kernel workaround.

This will only be fixed from GCC 9 on, if the compiler adopts it.  The
kernel wants to support ancient GCC, so it will need to have a workaround
for older GCC versions anyway.


Segher
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 12:25:02 +0000
Message-ID: <20181129122500.GX23873 () gate ! crashing ! org>
--------------------
On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> On Wed, Oct 10, 2018 at 1:14 AM Segher Boessenkool
> <segher@kernel.crashing.org> wrote:
> >
> > On Mon, Oct 08, 2018 at 11:07:46AM +0200, Richard Biener wrote:
> > > On Mon, 8 Oct 2018, Segher Boessenkool wrote:
> > > > On Sun, Oct 07, 2018 at 03:53:26PM +0000, Michael Matz wrote:
> > > > > On Sun, 7 Oct 2018, Segher Boessenkool wrote:
> > > > > > On Sun, Oct 07, 2018 at 11:18:06AM +0200, Borislav Petkov wrote:
> > > > > > > Now, Richard suggested doing something like:
> > > > > > >
> > > > > > >  1) inline asm ("...")
> > > > > >
> > > > > > What would the semantics of this be?
> > > > >
> > > > > The size of the inline asm wouldn't be counted towards the inliner size
> > > > > limits (or be counted as "1").
> > > >
> > > > That sounds like a good option.
> > >
> > > Yes, I also like it for simplicity.  It also avoids the requirement
> > > of translating the number (in bytes?) given by the user to
> > > "number of GIMPLE instructions" as needed by the inliner.
> >
> > This patch implements this, for C only so far.  And the syntax is
> > "asm inline", which is more in line with other syntax.
> >
> > How does this look?
> 
> 
> Thank you very much for your work.
> 
> 
> https://gcc.gnu.org/ml/gcc-patches/2018-10/msg01932.html
> 
> How is the progress of this in GCC ML?

Latest patch was pinged a few times:
https://gcc.gnu.org/ml/gcc-patches/2018-11/msg01569.html .

I'll ping it again.  Will fix the subject as well if I remember to, sigh.

> I am really hoping the issue will be solved by compiler
> instead of the in-kernel workaround.

This will only be fixed from GCC 9 on, if the compiler adopts it.  The
kernel wants to support ancient GCC, so it will need to have a workaround
for older GCC versions anyway.


Segher
================================================================================

From: Borislav Petkov <bp () alien8 ! de>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:07:18 +0000
Message-ID: <20181129130718.GA3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> But, I'd like to ask if x86 people want to keep this macros.s approach.
> Revert 77b0bf55bc675 right now
> assuming the compiler will eventually solve the issue?

Yap, considering how elegant the compiler solution is and how much
problems this macros-in-asm thing causes, I think we should patch
out the latter and wait for gcc9. I mean, the savings are not so
mind-blowing to have to deal with the fallout.

But this is just my opinion.

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
================================================================================

From: Borislav Petkov via Virtualization <virtualization () lists ! linux-foundation ! org>
To: linux-virtualization
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:07:18 +0000
Message-ID: <20181129130718.GA3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> But, I'd like to ask if x86 people want to keep this macros.s approach.
> Revert 77b0bf55bc675 right now
> assuming the compiler will eventually solve the issue?

Yap, considering how elegant the compiler solution is and how much
problems this macros-in-asm thing causes, I think we should patch
out the latter and wait for gcc9. I mean, the savings are not so
mind-blowing to have to deal with the fallout.

But this is just my opinion.

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: "Borislav Petkov via gcc" <gcc () gcc ! gnu ! org>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:07:18 +0000
Message-ID: <20181129130718.GA3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> But, I'd like to ask if x86 people want to keep this macros.s approach.
> Revert 77b0bf55bc675 right now
> assuming the compiler will eventually solve the issue?

Yap, considering how elegant the compiler solution is and how much
problems this macros-in-asm thing causes, I think we should patch
out the latter and wait for gcc9. I mean, the savings are not so
mind-blowing to have to deal with the fallout.

But this is just my opinion.

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
================================================================================

From: Borislav Petkov <bp () alien8 ! de>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:07:18 +0000
Message-ID: <20181129130718.GA3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> But, I'd like to ask if x86 people want to keep this macros.s approach.
> Revert 77b0bf55bc675 right now
> assuming the compiler will eventually solve the issue?

Yap, considering how elegant the compiler solution is and how much
problems this macros-in-asm thing causes, I think we should patch
out the latter and wait for gcc9. I mean, the savings are not so
mind-blowing to have to deal with the fallout.

But this is just my opinion.

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
================================================================================

From: Borislav Petkov via Virtualization <virtualization () lists ! linux-foundation ! org>
To: linux-virtualization
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:07:18 +0000
Message-ID: <20181129130718.GA3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> But, I'd like to ask if x86 people want to keep this macros.s approach.
> Revert 77b0bf55bc675 right now
> assuming the compiler will eventually solve the issue?

Yap, considering how elegant the compiler solution is and how much
problems this macros-in-asm thing causes, I think we should patch
out the latter and wait for gcc9. I mean, the savings are not so
mind-blowing to have to deal with the fallout.

But this is just my opinion.

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: Borislav Petkov <bp () alien8 ! de>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:07:18 +0000
Message-ID: <20181129130718.GA3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> But, I'd like to ask if x86 people want to keep this macros.s approach.
> Revert 77b0bf55bc675 right now
> assuming the compiler will eventually solve the issue?

Yap, considering how elegant the compiler solution is and how much
problems this macros-in-asm thing causes, I think we should patch
out the latter and wait for gcc9. I mean, the savings are not so
mind-blowing to have to deal with the fallout.

But this is just my opinion.

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
================================================================================

From: Borislav Petkov via Virtualization <virtualization () lists ! linux-foundation ! org>
To: linux-virtualization
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:07:18 +0000
Message-ID: <20181129130718.GA3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> But, I'd like to ask if x86 people want to keep this macros.s approach.
> Revert 77b0bf55bc675 right now
> assuming the compiler will eventually solve the issue?

Yap, considering how elegant the compiler solution is and how much
problems this macros-in-asm thing causes, I think we should patch
out the latter and wait for gcc9. I mean, the savings are not so
mind-blowing to have to deal with the fallout.

But this is just my opinion.

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:09:25 +0000
Message-ID: <alpine.LSU.2.20.1811291408400.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> > But, I'd like to ask if x86 people want to keep this macros.s approach.
> > Revert 77b0bf55bc675 right now
> > assuming the compiler will eventually solve the issue?
> 
> Yap, considering how elegant the compiler solution is and how much
> problems this macros-in-asm thing causes, I think we should patch
> out the latter and wait for gcc9. I mean, the savings are not so
> mind-blowing to have to deal with the fallout.
> 
> But this is just my opinion.

I'd be not opposed to backporting the asm inline support.

Of course we still have to be happy with it and install the patch ;)

Are you (kernel folks) happy with asm inline ()?

Richard.

> Thx.
> 
> 

-- 
Richard Biener <rguenther@suse.de>
SUSE LINUX GmbH, GF: Felix Imendoerffer, Jane Smithard, Graham Norton, HRB 21284 (AG Nuernberg)
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:09:25 +0000
Message-ID: <alpine.LSU.2.20.1811291408400.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> > But, I'd like to ask if x86 people want to keep this macros.s approach.
> > Revert 77b0bf55bc675 right now
> > assuming the compiler will eventually solve the issue?
> 
> Yap, considering how elegant the compiler solution is and how much
> problems this macros-in-asm thing causes, I think we should patch
> out the latter and wait for gcc9. I mean, the savings are not so
> mind-blowing to have to deal with the fallout.
> 
> But this is just my opinion.

I'd be not opposed to backporting the asm inline support.

Of course we still have to be happy with it and install the patch ;)

Are you (kernel folks) happy with asm inline ()?

Richard.

> Thx.
> 
> 

-- 
Richard Biener <rguenther@suse.de>
SUSE LINUX GmbH, GF: Felix Imendoerffer, Jane Smithard, Graham Norton, HRB 21284 (AG Nuernberg)
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:09:25 +0000
Message-ID: <alpine.LSU.2.20.1811291408400.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> > But, I'd like to ask if x86 people want to keep this macros.s approach.
> > Revert 77b0bf55bc675 right now
> > assuming the compiler will eventually solve the issue?
> 
> Yap, considering how elegant the compiler solution is and how much
> problems this macros-in-asm thing causes, I think we should patch
> out the latter and wait for gcc9. I mean, the savings are not so
> mind-blowing to have to deal with the fallout.
> 
> But this is just my opinion.

I'd be not opposed to backporting the asm inline support.

Of course we still have to be happy with it and install the patch ;)

Are you (kernel folks) happy with asm inline ()?

Richard.

> Thx.
> 
> 

-- 
Richard Biener <rguenther@suse.de>
SUSE LINUX GmbH, GF: Felix Imendoerffer, Jane Smithard, Graham Norton, HRB 21284 (AG Nuernberg)
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:09:25 +0000
Message-ID: <alpine.LSU.2.20.1811291408400.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> > But, I'd like to ask if x86 people want to keep this macros.s approach.
> > Revert 77b0bf55bc675 right now
> > assuming the compiler will eventually solve the issue?
> 
> Yap, considering how elegant the compiler solution is and how much
> problems this macros-in-asm thing causes, I think we should patch
> out the latter and wait for gcc9. I mean, the savings are not so
> mind-blowing to have to deal with the fallout.
> 
> But this is just my opinion.

I'd be not opposed to backporting the asm inline support.

Of course we still have to be happy with it and install the patch ;)

Are you (kernel folks) happy with asm inline ()?

Richard.

> Thx.
> 
> 

-- 
Richard Biener <rguenther@suse.de>
SUSE LINUX GmbH, GF: Felix Imendoerffer, Jane Smithard, Graham Norton, HRB 21284 (AG Nuernberg)
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:09:25 +0000
Message-ID: <alpine.LSU.2.20.1811291408400.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> > But, I'd like to ask if x86 people want to keep this macros.s approach.
> > Revert 77b0bf55bc675 right now
> > assuming the compiler will eventually solve the issue?
> 
> Yap, considering how elegant the compiler solution is and how much
> problems this macros-in-asm thing causes, I think we should patch
> out the latter and wait for gcc9. I mean, the savings are not so
> mind-blowing to have to deal with the fallout.
> 
> But this is just my opinion.

I'd be not opposed to backporting the asm inline support.

Of course we still have to be happy with it and install the patch ;)

Are you (kernel folks) happy with asm inline ()?

Richard.

> Thx.
> 
> 

-- 
Richard Biener <rguenther@suse.de>
SUSE LINUX GmbH, GF: Felix Imendoerffer, Jane Smithard, Graham Norton, HRB 21284 (AG Nuernberg)
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:09:25 +0000
Message-ID: <alpine.LSU.2.20.1811291408400.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> > But, I'd like to ask if x86 people want to keep this macros.s approach.
> > Revert 77b0bf55bc675 right now
> > assuming the compiler will eventually solve the issue?
> 
> Yap, considering how elegant the compiler solution is and how much
> problems this macros-in-asm thing causes, I think we should patch
> out the latter and wait for gcc9. I mean, the savings are not so
> mind-blowing to have to deal with the fallout.
> 
> But this is just my opinion.

I'd be not opposed to backporting the asm inline support.

Of course we still have to be happy with it and install the patch ;)

Are you (kernel folks) happy with asm inline ()?

Richard.

> Thx.
> 
> 

-- 
Richard Biener <rguenther@suse.de>
SUSE LINUX GmbH, GF: Felix Imendoerffer, Jane Smithard, Graham Norton, HRB 21284 (AG Nuernberg)
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:09:25 +0000
Message-ID: <alpine.LSU.2.20.1811291408400.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> > But, I'd like to ask if x86 people want to keep this macros.s approach.
> > Revert 77b0bf55bc675 right now
> > assuming the compiler will eventually solve the issue?
> 
> Yap, considering how elegant the compiler solution is and how much
> problems this macros-in-asm thing causes, I think we should patch
> out the latter and wait for gcc9. I mean, the savings are not so
> mind-blowing to have to deal with the fallout.
> 
> But this is just my opinion.

I'd be not opposed to backporting the asm inline support.

Of course we still have to be happy with it and install the patch ;)

Are you (kernel folks) happy with asm inline ()?

Richard.

> Thx.
> 
> 

-- 
Richard Biener <rguenther@suse.de>
SUSE LINUX GmbH, GF: Felix Imendoerffer, Jane Smithard, Graham Norton, HRB 21284 (AG Nuernberg)
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:09:25 +0000
Message-ID: <alpine.LSU.2.20.1811291408400.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 08:46:34PM +0900, Masahiro Yamada wrote:
> > But, I'd like to ask if x86 people want to keep this macros.s approach.
> > Revert 77b0bf55bc675 right now
> > assuming the compiler will eventually solve the issue?
> 
> Yap, considering how elegant the compiler solution is and how much
> problems this macros-in-asm thing causes, I think we should patch
> out the latter and wait for gcc9. I mean, the savings are not so
> mind-blowing to have to deal with the fallout.
> 
> But this is just my opinion.

I'd be not opposed to backporting the asm inline support.

Of course we still have to be happy with it and install the patch ;)

Are you (kernel folks) happy with asm inline ()?

Richard.

> Thx.
> 
> 

-- 
Richard Biener <rguenther@suse.de>
SUSE LINUX GmbH, GF: Felix Imendoerffer, Jane Smithard, Graham Norton, HRB 21284 (AG Nuernberg)
================================================================================

From: "Borislav Petkov via gcc" <gcc () gcc ! gnu ! org>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:16:11 +0000
Message-ID: <20181129131611.GC3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> I'd be not opposed to backporting the asm inline support.

Even better! :-)

> Of course we still have to be happy with it and install the patch ;)
> 
> Are you (kernel folks) happy with asm inline ()?

Yes, I think we are:

https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
================================================================================

From: Borislav Petkov <bp () alien8 ! de>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:16:11 +0000
Message-ID: <20181129131611.GC3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> I'd be not opposed to backporting the asm inline support.

Even better! :-)

> Of course we still have to be happy with it and install the patch ;)
> 
> Are you (kernel folks) happy with asm inline ()?

Yes, I think we are:

https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
================================================================================

From: Borislav Petkov via Virtualization <virtualization () lists ! linux-foundation ! org>
To: linux-virtualization
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:16:11 +0000
Message-ID: <20181129131611.GC3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> I'd be not opposed to backporting the asm inline support.

Even better! :-)

> Of course we still have to be happy with it and install the patch ;)
> 
> Are you (kernel folks) happy with asm inline ()?

Yes, I think we are:

https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: Borislav Petkov <bp () alien8 ! de>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:16:11 +0000
Message-ID: <20181129131611.GC3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> I'd be not opposed to backporting the asm inline support.

Even better! :-)

> Of course we still have to be happy with it and install the patch ;)
> 
> Are you (kernel folks) happy with asm inline ()?

Yes, I think we are:

https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
================================================================================

From: "Borislav Petkov via gcc" <gcc () gcc ! gnu ! org>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:16:11 +0000
Message-ID: <20181129131611.GC3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> I'd be not opposed to backporting the asm inline support.

Even better! :-)

> Of course we still have to be happy with it and install the patch ;)
> 
> Are you (kernel folks) happy with asm inline ()?

Yes, I think we are:

https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
================================================================================

From: Borislav Petkov via Virtualization <virtualization () lists ! linux-foundation ! org>
To: linux-virtualization
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:16:11 +0000
Message-ID: <20181129131611.GC3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> I'd be not opposed to backporting the asm inline support.

Even better! :-)

> Of course we still have to be happy with it and install the patch ;)
> 
> Are you (kernel folks) happy with asm inline ()?

Yes, I think we are:

https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: "Borislav Petkov via gcc" <gcc () gcc ! gnu ! org>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:16:11 +0000
Message-ID: <20181129131611.GC3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> I'd be not opposed to backporting the asm inline support.

Even better! :-)

> Of course we still have to be happy with it and install the patch ;)
> 
> Are you (kernel folks) happy with asm inline ()?

Yes, I think we are:

https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
================================================================================

From: Borislav Petkov <bp () alien8 ! de>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:16:11 +0000
Message-ID: <20181129131611.GC3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> I'd be not opposed to backporting the asm inline support.

Even better! :-)

> Of course we still have to be happy with it and install the patch ;)
> 
> Are you (kernel folks) happy with asm inline ()?

Yes, I think we are:

https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
================================================================================

From: Borislav Petkov via Virtualization <virtualization () lists ! linux-foundation ! org>
To: linux-virtualization
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:16:11 +0000
Message-ID: <20181129131611.GC3070 () nazgul ! tnic>
--------------------
On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> I'd be not opposed to backporting the asm inline support.

Even better! :-)

> Of course we still have to be happy with it and install the patch ;)
> 
> Are you (kernel folks) happy with asm inline ()?

Yes, I think we are:

https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

Thx.

-- 
Regards/Gruss,
    Boris.

ECO tip #101: Trim your mails when you reply.
--
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:24:55 +0000
Message-ID: <alpine.LSU.2.20.1811291423490.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> > I'd be not opposed to backporting the asm inline support.
> 
> Even better! :-)
> 
> > Of course we still have to be happy with it and install the patch ;)
> > 
> > Are you (kernel folks) happy with asm inline ()?
> 
> Yes, I think we are:
> 
> https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

OK, Segher - can you ping the latest version of the patch please?

Thanks,
Richard.
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:24:55 +0000
Message-ID: <alpine.LSU.2.20.1811291423490.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> > I'd be not opposed to backporting the asm inline support.
> 
> Even better! :-)
> 
> > Of course we still have to be happy with it and install the patch ;)
> > 
> > Are you (kernel folks) happy with asm inline ()?
> 
> Yes, I think we are:
> 
> https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

OK, Segher - can you ping the latest version of the patch please?

Thanks,
Richard.
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:24:55 +0000
Message-ID: <alpine.LSU.2.20.1811291423490.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> > I'd be not opposed to backporting the asm inline support.
> 
> Even better! :-)
> 
> > Of course we still have to be happy with it and install the patch ;)
> > 
> > Are you (kernel folks) happy with asm inline ()?
> 
> Yes, I think we are:
> 
> https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

OK, Segher - can you ping the latest version of the patch please?

Thanks,
Richard.
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:24:55 +0000
Message-ID: <alpine.LSU.2.20.1811291423490.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> > I'd be not opposed to backporting the asm inline support.
> 
> Even better! :-)
> 
> > Of course we still have to be happy with it and install the patch ;)
> > 
> > Are you (kernel folks) happy with asm inline ()?
> 
> Yes, I think we are:
> 
> https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

OK, Segher - can you ping the latest version of the patch please?

Thanks,
Richard.
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:24:55 +0000
Message-ID: <alpine.LSU.2.20.1811291423490.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> > I'd be not opposed to backporting the asm inline support.
> 
> Even better! :-)
> 
> > Of course we still have to be happy with it and install the patch ;)
> > 
> > Are you (kernel folks) happy with asm inline ()?
> 
> Yes, I think we are:
> 
> https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

OK, Segher - can you ping the latest version of the patch please?

Thanks,
Richard.
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:24:55 +0000
Message-ID: <alpine.LSU.2.20.1811291423490.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> > I'd be not opposed to backporting the asm inline support.
> 
> Even better! :-)
> 
> > Of course we still have to be happy with it and install the patch ;)
> > 
> > Are you (kernel folks) happy with asm inline ()?
> 
> Yes, I think we are:
> 
> https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

OK, Segher - can you ping the latest version of the patch please?

Thanks,
Richard.
================================================================================

From: Richard Biener <rguenther () suse ! de>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 29 Nov 2018 13:24:55 +0000
Message-ID: <alpine.LSU.2.20.1811291423490.1827 () zhemvz ! fhfr ! qr>
--------------------
On Thu, 29 Nov 2018, Borislav Petkov wrote:

> On Thu, Nov 29, 2018 at 02:09:25PM +0100, Richard Biener wrote:
> > I'd be not opposed to backporting the asm inline support.
> 
> Even better! :-)
> 
> > Of course we still have to be happy with it and install the patch ;)
> > 
> > Are you (kernel folks) happy with asm inline ()?
> 
> Yes, I think we are:
> 
> https://lkml.kernel.org/r/20181031125526.GA13219@hirez.programming.kicks-ass.net

OK, Segher - can you ping the latest version of the patch please?

Thanks,
Richard.
================================================================================

From: Boris Petkov <bp () alien8 ! de>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 09:06:02 +0000
Message-ID: <CF26ACBE-500C-47E8-B79D-E1ABE938031D () alien8 ! de>
--------------------
On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kerne=
l=2Ecrashing=2Eorg> wrote:
>This will only be fixed from GCC 9 on, if the compiler adopts it=2E  The
>kernel wants to support ancient GCC, so it will need to have a
>workaround
>for older GCC versions anyway=2E

What about backporting it, like Richard says?


--=20
Sent from a small device: formatting sux and brevity is inevitable=2E 
================================================================================

From: "Boris Petkov via gcc" <gcc () gcc ! gnu ! org>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 09:06:02 +0000
Message-ID: <CF26ACBE-500C-47E8-B79D-E1ABE938031D () alien8 ! de>
--------------------
On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kerne=
l.crashing.org> wrote:
>This will only be fixed from GCC 9 on, if the compiler adopts it.  The
>kernel wants to support ancient GCC, so it will need to have a
>workaround
>for older GCC versions anyway.

What about backporting it, like Richard says?


--=20
Sent from a small device: formatting sux and brevity is inevitable.
================================================================================

From: Boris Petkov via Virtualization <virtualization () lists ! linux-foundation ! org>
To: linux-virtualization
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 09:06:02 +0000
Message-ID: <CF26ACBE-500C-47E8-B79D-E1ABE938031D () alien8 ! de>
--------------------
On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
>This will only be fixed from GCC 9 on, if the compiler adopts it.  The
>kernel wants to support ancient GCC, so it will need to have a
>workaround
>for older GCC versions anyway.

What about backporting it, like Richard says?


-- 
Sent from a small device: formatting sux and brevity is inevitable. 
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: Boris Petkov <bp () alien8 ! de>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 09:06:02 +0000
Message-ID: <CF26ACBE-500C-47E8-B79D-E1ABE938031D () alien8 ! de>
--------------------
On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kerne=
l=2Ecrashing=2Eorg> wrote:
>This will only be fixed from GCC 9 on, if the compiler adopts it=2E  The
>kernel wants to support ancient GCC, so it will need to have a
>workaround
>for older GCC versions anyway=2E

What about backporting it, like Richard says?


--=20
Sent from a small device: formatting sux and brevity is inevitable=2E 
================================================================================

From: Boris Petkov <bp () alien8 ! de>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 09:06:02 +0000
Message-ID: <CF26ACBE-500C-47E8-B79D-E1ABE938031D () alien8 ! de>
--------------------
On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kerne=
l=2Ecrashing=2Eorg> wrote:
>This will only be fixed from GCC 9 on, if the compiler adopts it=2E  The
>kernel wants to support ancient GCC, so it will need to have a
>workaround
>for older GCC versions anyway=2E

What about backporting it, like Richard says?


--=20
Sent from a small device: formatting sux and brevity is inevitable=2E 
================================================================================

From: "Boris Petkov via gcc" <gcc () gcc ! gnu ! org>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 09:06:02 +0000
Message-ID: <CF26ACBE-500C-47E8-B79D-E1ABE938031D () alien8 ! de>
--------------------
On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kerne=
l.crashing.org> wrote:
>This will only be fixed from GCC 9 on, if the compiler adopts it.  The
>kernel wants to support ancient GCC, so it will need to have a
>workaround
>for older GCC versions anyway.

What about backporting it, like Richard says?


--=20
Sent from a small device: formatting sux and brevity is inevitable.
================================================================================

From: Boris Petkov via Virtualization <virtualization () lists ! linux-foundation ! org>
To: linux-virtualization
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 09:06:02 +0000
Message-ID: <CF26ACBE-500C-47E8-B79D-E1ABE938031D () alien8 ! de>
--------------------
On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
>This will only be fixed from GCC 9 on, if the compiler adopts it.  The
>kernel wants to support ancient GCC, so it will need to have a
>workaround
>for older GCC versions anyway.

What about backporting it, like Richard says?


-- 
Sent from a small device: formatting sux and brevity is inevitable. 
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: Boris Petkov <bp () alien8 ! de>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 09:06:02 +0000
Message-ID: <CF26ACBE-500C-47E8-B79D-E1ABE938031D () alien8 ! de>
--------------------
On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kerne=
l=2Ecrashing=2Eorg> wrote:
>This will only be fixed from GCC 9 on, if the compiler adopts it=2E  The
>kernel wants to support ancient GCC, so it will need to have a
>workaround
>for older GCC versions anyway=2E

What about backporting it, like Richard says?


--=20
Sent from a small device: formatting sux and brevity is inevitable=2E 
================================================================================

From: Boris Petkov <bp () alien8 ! de>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 09:06:02 +0000
Message-ID: <CF26ACBE-500C-47E8-B79D-E1ABE938031D () alien8 ! de>
--------------------
On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kerne=
l=2Ecrashing=2Eorg> wrote:
>This will only be fixed from GCC 9 on, if the compiler adopts it=2E  The
>kernel wants to support ancient GCC, so it will need to have a
>workaround
>for older GCC versions anyway=2E

What about backporting it, like Richard says?


--=20
Sent from a small device: formatting sux and brevity is inevitable=2E 
================================================================================

From: Boris Petkov via Virtualization <virtualization () lists ! linux-foundation ! org>
To: linux-virtualization
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 09:06:02 +0000
Message-ID: <CF26ACBE-500C-47E8-B79D-E1ABE938031D () alien8 ! de>
--------------------
On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
>This will only be fixed from GCC 9 on, if the compiler adopts it.  The
>kernel wants to support ancient GCC, so it will need to have a
>workaround
>for older GCC versions anyway.

What about backporting it, like Richard says?


-- 
Sent from a small device: formatting sux and brevity is inevitable. 
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 13:16:34 +0000
Message-ID: <20181130131633.GU23873 () gate ! crashing ! org>
--------------------
On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> >kernel wants to support ancient GCC, so it will need to have a
> >workaround
> >for older GCC versions anyway.
> 
> What about backporting it, like Richard says?

Let me first get it into GCC trunk :-)

It should backport fine, sure; and I'll work on that.


Segher
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 13:16:34 +0000
Message-ID: <20181130131633.GU23873 () gate ! crashing ! org>
--------------------
On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> >kernel wants to support ancient GCC, so it will need to have a
> >workaround
> >for older GCC versions anyway.
> 
> What about backporting it, like Richard says?

Let me first get it into GCC trunk :-)

It should backport fine, sure; and I'll work on that.


Segher
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 13:16:34 +0000
Message-ID: <20181130131633.GU23873 () gate ! crashing ! org>
--------------------
On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> >kernel wants to support ancient GCC, so it will need to have a
> >workaround
> >for older GCC versions anyway.
> 
> What about backporting it, like Richard says?

Let me first get it into GCC trunk :-)

It should backport fine, sure; and I'll work on that.


Segher
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 13:16:34 +0000
Message-ID: <20181130131633.GU23873 () gate ! crashing ! org>
--------------------
On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> >kernel wants to support ancient GCC, so it will need to have a
> >workaround
> >for older GCC versions anyway.
> 
> What about backporting it, like Richard says?

Let me first get it into GCC trunk :-)

It should backport fine, sure; and I'll work on that.


Segher
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 13:16:34 +0000
Message-ID: <20181130131633.GU23873 () gate ! crashing ! org>
--------------------
On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> >kernel wants to support ancient GCC, so it will need to have a
> >workaround
> >for older GCC versions anyway.
> 
> What about backporting it, like Richard says?

Let me first get it into GCC trunk :-)

It should backport fine, sure; and I'll work on that.


Segher
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: gcc
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 13:16:34 +0000
Message-ID: <20181130131633.GU23873 () gate ! crashing ! org>
--------------------
On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> >kernel wants to support ancient GCC, so it will need to have a
> >workaround
> >for older GCC versions anyway.
> 
> What about backporting it, like Richard says?

Let me first get it into GCC trunk :-)

It should backport fine, sure; and I'll work on that.


Segher
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 13:16:34 +0000
Message-ID: <20181130131633.GU23873 () gate ! crashing ! org>
--------------------
On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> >kernel wants to support ancient GCC, so it will need to have a
> >workaround
> >for older GCC versions anyway.
> 
> What about backporting it, like Richard says?

Let me first get it into GCC trunk :-)

It should backport fine, sure; and I'll work on that.


Segher
================================================================================

From: Segher Boessenkool <segher () kernel ! crashing ! org>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Fri, 30 Nov 2018 13:16:34 +0000
Message-ID: <20181130131633.GU23873 () gate ! crashing ! org>
--------------------
On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> >kernel wants to support ancient GCC, so it will need to have a
> >workaround
> >for older GCC versions anyway.
> 
> What about backporting it, like Richard says?

Let me first get it into GCC trunk :-)

It should backport fine, sure; and I'll work on that.


Segher
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Mon, 10 Dec 2018 08:16:28 +0000
Message-ID: <CAK7LNARSRZEctnTGiB8GOC-D5Q9_d5GpNm8JQ1EJ--WioDe_DA () mail ! gmail ! com>
--------------------
Hi Segher,


On Sun, Dec 2, 2018 at 3:48 PM Segher Boessenkool
<segher@kernel.crashing.org> wrote:
>
> On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> > On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> > >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> > >kernel wants to support ancient GCC, so it will need to have a
> > >workaround
> > >for older GCC versions anyway.
> >
> > What about backporting it, like Richard says?
>
> Let me first get it into GCC trunk :-)
>
> It should backport fine, sure; and I'll work on that.
>



Now, I can see it in the GCC trunk.  Hooray!!!



commit 6de46ad5326fc5e6b730a2feb8c62d09c1561f92
Author: segher <segher@138bc75d-0d04-0410-961f-82ee72b054a4>
Date:   Thu Dec 6 17:56:58 2018 +0000

    asm inline

    The Linux kernel people want a feature that makes GCC pretend some
    inline assembler code is tiny (while it would think it is huge), so
    that such code will be inlined essentially always instead of
    essentially never.

    This patch lets you say "asm inline" instead of just "asm", with the
    result that that inline assembler is always counted as minimum cost
    for inlining.  It implements this for C and C++, making "inline"
    another asm-qualifier (supplementing "volatile" and "goto").




-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-virtualization
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Mon, 10 Dec 2018 08:16:28 +0000
Message-ID: <CAK7LNARSRZEctnTGiB8GOC-D5Q9_d5GpNm8JQ1EJ--WioDe_DA () mail ! gmail ! com>
--------------------
Hi Segher,


On Sun, Dec 2, 2018 at 3:48 PM Segher Boessenkool
<segher@kernel.crashing.org> wrote:
>
> On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> > On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> > >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> > >kernel wants to support ancient GCC, so it will need to have a
> > >workaround
> > >for older GCC versions anyway.
> >
> > What about backporting it, like Richard says?
>
> Let me first get it into GCC trunk :-)
>
> It should backport fine, sure; and I'll work on that.
>



Now, I can see it in the GCC trunk.  Hooray!!!



commit 6de46ad5326fc5e6b730a2feb8c62d09c1561f92
Author: segher <segher@138bc75d-0d04-0410-961f-82ee72b054a4>
Date:   Thu Dec 6 17:56:58 2018 +0000

    asm inline

    The Linux kernel people want a feature that makes GCC pretend some
    inline assembler code is tiny (while it would think it is huge), so
    that such code will be inlined essentially always instead of
    essentially never.

    This patch lets you say "asm inline" instead of just "asm", with the
    result that that inline assembler is always counted as minimum cost
    for inlining.  It implements this for C and C++, making "inline"
    another asm-qualifier (supplementing "volatile" and "goto").




-- 
Best Regards
Masahiro Yamada
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Mon, 10 Dec 2018 08:16:28 +0000
Message-ID: <CAK7LNARSRZEctnTGiB8GOC-D5Q9_d5GpNm8JQ1EJ--WioDe_DA () mail ! gmail ! com>
--------------------
Hi Segher,


On Sun, Dec 2, 2018 at 3:48 PM Segher Boessenkool
<segher@kernel.crashing.org> wrote:
>
> On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> > On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> > >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> > >kernel wants to support ancient GCC, so it will need to have a
> > >workaround
> > >for older GCC versions anyway.
> >
> > What about backporting it, like Richard says?
>
> Let me first get it into GCC trunk :-)
>
> It should backport fine, sure; and I'll work on that.
>



Now, I can see it in the GCC trunk.  Hooray!!!



commit 6de46ad5326fc5e6b730a2feb8c62d09c1561f92
Author: segher <segher@138bc75d-0d04-0410-961f-82ee72b054a4>
Date:   Thu Dec 6 17:56:58 2018 +0000

    asm inline

    The Linux kernel people want a feature that makes GCC pretend some
    inline assembler code is tiny (while it would think it is huge), so
    that such code will be inlined essentially always instead of
    essentially never.

    This patch lets you say "asm inline" instead of just "asm", with the
    result that that inline assembler is always counted as minimum cost
    for inlining.  It implements this for C and C++, making "inline"
    another asm-qualifier (supplementing "volatile" and "goto").




-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-virtualization
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Mon, 10 Dec 2018 08:16:28 +0000
Message-ID: <CAK7LNARSRZEctnTGiB8GOC-D5Q9_d5GpNm8JQ1EJ--WioDe_DA () mail ! gmail ! com>
--------------------
Hi Segher,


On Sun, Dec 2, 2018 at 3:48 PM Segher Boessenkool
<segher@kernel.crashing.org> wrote:
>
> On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> > On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> > >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> > >kernel wants to support ancient GCC, so it will need to have a
> > >workaround
> > >for older GCC versions anyway.
> >
> > What about backporting it, like Richard says?
>
> Let me first get it into GCC trunk :-)
>
> It should backport fine, sure; and I'll work on that.
>



Now, I can see it in the GCC trunk.  Hooray!!!



commit 6de46ad5326fc5e6b730a2feb8c62d09c1561f92
Author: segher <segher@138bc75d-0d04-0410-961f-82ee72b054a4>
Date:   Thu Dec 6 17:56:58 2018 +0000

    asm inline

    The Linux kernel people want a feature that makes GCC pretend some
    inline assembler code is tiny (while it would think it is huge), so
    that such code will be inlined essentially always instead of
    essentially never.

    This patch lets you say "asm inline" instead of just "asm", with the
    result that that inline assembler is always counted as minimum cost
    for inlining.  It implements this for C and C++, making "inline"
    another asm-qualifier (supplementing "volatile" and "goto").




-- 
Best Regards
Masahiro Yamada
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Mon, 10 Dec 2018 08:16:28 +0000
Message-ID: <CAK7LNARSRZEctnTGiB8GOC-D5Q9_d5GpNm8JQ1EJ--WioDe_DA () mail ! gmail ! com>
--------------------
Hi Segher,


On Sun, Dec 2, 2018 at 3:48 PM Segher Boessenkool
<segher@kernel.crashing.org> wrote:
>
> On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> > On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> > >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> > >kernel wants to support ancient GCC, so it will need to have a
> > >workaround
> > >for older GCC versions anyway.
> >
> > What about backporting it, like Richard says?
>
> Let me first get it into GCC trunk :-)
>
> It should backport fine, sure; and I'll work on that.
>



Now, I can see it in the GCC trunk.  Hooray!!!



commit 6de46ad5326fc5e6b730a2feb8c62d09c1561f92
Author: segher <segher@138bc75d-0d04-0410-961f-82ee72b054a4>
Date:   Thu Dec 6 17:56:58 2018 +0000

    asm inline

    The Linux kernel people want a feature that makes GCC pretend some
    inline assembler code is tiny (while it would think it is huge), so
    that such code will be inlined essentially always instead of
    essentially never.

    This patch lets you say "asm inline" instead of just "asm", with the
    result that that inline assembler is always counted as minimum cost
    for inlining.  It implements this for C and C++, making "inline"
    another asm-qualifier (supplementing "volatile" and "goto").




-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-virtualization
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Mon, 10 Dec 2018 08:16:28 +0000
Message-ID: <CAK7LNARSRZEctnTGiB8GOC-D5Q9_d5GpNm8JQ1EJ--WioDe_DA () mail ! gmail ! com>
--------------------
Hi Segher,


On Sun, Dec 2, 2018 at 3:48 PM Segher Boessenkool
<segher@kernel.crashing.org> wrote:
>
> On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> > On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> > >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> > >kernel wants to support ancient GCC, so it will need to have a
> > >workaround
> > >for older GCC versions anyway.
> >
> > What about backporting it, like Richard says?
>
> Let me first get it into GCC trunk :-)
>
> It should backport fine, sure; and I'll work on that.
>



Now, I can see it in the GCC trunk.  Hooray!!!



commit 6de46ad5326fc5e6b730a2feb8c62d09c1561f92
Author: segher <segher@138bc75d-0d04-0410-961f-82ee72b054a4>
Date:   Thu Dec 6 17:56:58 2018 +0000

    asm inline

    The Linux kernel people want a feature that makes GCC pretend some
    inline assembler code is tiny (while it would think it is huge), so
    that such code will be inlined essentially always instead of
    essentially never.

    This patch lets you say "asm inline" instead of just "asm", with the
    result that that inline assembler is always counted as minimum cost
    for inlining.  It implements this for C and C++, making "inline"
    another asm-qualifier (supplementing "volatile" and "goto").




-- 
Best Regards
Masahiro Yamada
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Mon, 10 Dec 2018 08:16:28 +0000
Message-ID: <CAK7LNARSRZEctnTGiB8GOC-D5Q9_d5GpNm8JQ1EJ--WioDe_DA () mail ! gmail ! com>
--------------------
Hi Segher,


On Sun, Dec 2, 2018 at 3:48 PM Segher Boessenkool
<segher@kernel.crashing.org> wrote:
>
> On Fri, Nov 30, 2018 at 10:06:02AM +0100, Boris Petkov wrote:
> > On November 29, 2018 1:25:02 PM GMT+01:00, Segher Boessenkool <segher@kernel.crashing.org> wrote:
> > >This will only be fixed from GCC 9 on, if the compiler adopts it.  The
> > >kernel wants to support ancient GCC, so it will need to have a
> > >workaround
> > >for older GCC versions anyway.
> >
> > What about backporting it, like Richard says?
>
> Let me first get it into GCC trunk :-)
>
> It should backport fine, sure; and I'll work on that.
>



Now, I can see it in the GCC trunk.  Hooray!!!



commit 6de46ad5326fc5e6b730a2feb8c62d09c1561f92
Author: segher <segher@138bc75d-0d04-0410-961f-82ee72b054a4>
Date:   Thu Dec 6 17:56:58 2018 +0000

    asm inline

    The Linux kernel people want a feature that makes GCC pretend some
    inline assembler code is tiny (while it would think it is huge), so
    that such code will be inlined essentially always instead of
    essentially never.

    This patch lets you say "asm inline" instead of just "asm", with the
    result that that inline assembler is always counted as minimum cost
    for inlining.  It implements this for C and C++, making "inline"
    another asm-qualifier (supplementing "volatile" and "goto").




-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 27 Dec 2018 04:47:34 +0000
Message-ID: <CAK7LNASo5jedRxjX5zevLSmZRce3fERcFc=qn_JtJXx1Ss3iJA () mail ! gmail ! com>
--------------------
Hi Peter,


On Wed, Oct 31, 2018 at 9:58 PM Peter Zijlstra <peterz@infradead.org> wrote:
>
> On Sat, Oct 13, 2018 at 09:33:35PM +0200, Borislav Petkov wrote:
> > Ok,
> >
> > with Segher's help I've been playing with his patch ontop of bleeding
> > edge gcc 9 and here are my observations. Please double-check me for
> > booboos so that they can be addressed while there's time.
> >
> > So here's what I see ontop of 4.19-rc7:
> >
> > First marked the alternative asm() as inline and undeffed the "inline"
> > keyword. I need to do that because of the funky games we do with
> > "inline" redefinitions in include/linux/compiler_types.h.
> >
> > And Segher hinted at either doing:
> >
> > asm volatile inline(...
> >
> > or
> >
> > asm volatile __inline__(...
> >
> > but both "inline" variants are defined as macros in that file.
> >
> > Which means we either need to #undef inline before using it in asm() or
> > come up with something cleverer.
>
> # git grep -e "\<__inline__\>" | wc -l
> 488
> # git grep -e "\<__inline\>" | wc -l
> 56
> # git grep -e "\<inline\>" | wc -l
> 69598
>
> And we already have scripts/checkpatch.pl:
>
>   # Check for __inline__ and __inline, prefer inline
>
> Which suggests we do:
>
> git grep -l "\<__inline\(\|__\)\>" | while read file
> do
>         sed -i -e 's/\<__inline\(\|__\)\>/inline/g' $file
> done
>
> and get it over with.


Do you have a plan to really do this?

This is a nice cleanup anyway.

I think the last minute of MW is
a good timing for the global replacement like this.




>
> Anyway, with the below patch, I get:
>
>    text    data     bss     dec     hex filename
> 17385183        5064780 1953892 24403855        1745f8f defconfig-build/vmlinux
> 17385678        5064780 1953892 24404350        174617e defconfig-build/vmlinux
>
> Which shows we inline more (look for asm_volatile for the actual
> changes).
>
>
> So yes, this seems like something we could work with.
>
> ---
>  Documentation/trace/tracepoint-analysis.rst        |  2 +-
>  Documentation/translations/ja_JP/SubmittingPatches |  4 +--
>  Documentation/translations/zh_CN/SubmittingPatches |  4 +--
>  arch/alpha/include/asm/atomic.h                    | 12 +++----
>  arch/alpha/include/asm/bitops.h                    |  6 ++--
>  arch/alpha/include/asm/compiler.h                  |  4 +--
>  arch/alpha/include/asm/dma.h                       | 22 ++++++------
>  arch/alpha/include/asm/floppy.h                    |  4 +--
>  arch/alpha/include/asm/irq.h                       |  2 +-
>  arch/alpha/include/asm/local.h                     |  4 +--
>  arch/alpha/include/asm/smp.h                       |  2 +-
>  arch/arm/mach-iop32x/include/mach/uncompress.h     |  2 +-
>  arch/arm/mach-iop33x/include/mach/uncompress.h     |  2 +-
>  arch/arm/mach-ixp4xx/include/mach/uncompress.h     |  2 +-
>  arch/ia64/hp/common/sba_iommu.c                    |  2 +-
>  arch/ia64/hp/sim/simeth.c                          |  2 +-
>  arch/ia64/include/asm/atomic.h                     |  8 ++---
>  arch/ia64/include/asm/bitops.h                     | 34 +++++++++---------
>  arch/ia64/include/asm/delay.h                      | 14 ++++----
>  arch/ia64/include/asm/irq.h                        |  2 +-
>  arch/ia64/include/asm/page.h                       |  2 +-
>  arch/ia64/include/asm/sn/leds.h                    |  2 +-
>  arch/ia64/include/asm/uaccess.h                    |  4 +--
>  arch/ia64/include/uapi/asm/rse.h                   | 12 +++----
>  arch/ia64/include/uapi/asm/swab.h                  |  6 ++--
>  arch/ia64/oprofile/backtrace.c                     |  4 +--
>  arch/m68k/include/asm/blinken.h                    |  2 +-
>  arch/m68k/include/asm/checksum.h                   |  2 +-
>  arch/m68k/include/asm/dma.h                        | 32 ++++++++---------
>  arch/m68k/include/asm/floppy.h                     |  8 ++---
>  arch/m68k/include/asm/nettel.h                     |  8 ++---
>  arch/m68k/mac/iop.c                                | 14 ++++----
>  arch/mips/include/asm/atomic.h                     | 16 ++++-----
>  arch/mips/include/asm/checksum.h                   |  2 +-
>  arch/mips/include/asm/dma.h                        | 20 +++++------
>  arch/mips/include/asm/jazz.h                       |  2 +-
>  arch/mips/include/asm/local.h                      |  4 +--
>  arch/mips/include/asm/string.h                     |  8 ++---
>  arch/mips/kernel/binfmt_elfn32.c                   |  2 +-
>  arch/nds32/include/asm/swab.h                      |  4 +--
>  arch/parisc/include/asm/atomic.h                   | 20 +++++------
>  arch/parisc/include/asm/bitops.h                   | 18 +++++-----
>  arch/parisc/include/asm/checksum.h                 |  4 +--
>  arch/parisc/include/asm/compat.h                   |  2 +-
>  arch/parisc/include/asm/delay.h                    |  2 +-
>  arch/parisc/include/asm/dma.h                      | 20 +++++------
>  arch/parisc/include/asm/ide.h                      |  8 ++---
>  arch/parisc/include/asm/irq.h                      |  2 +-
>  arch/parisc/include/asm/spinlock.h                 | 12 +++----
>  arch/powerpc/include/asm/atomic.h                  | 40 +++++++++++-----------
>  arch/powerpc/include/asm/bitops.h                  | 28 +++++++--------
>  arch/powerpc/include/asm/dma.h                     | 20 +++++------
>  arch/powerpc/include/asm/edac.h                    |  2 +-
>  arch/powerpc/include/asm/irq.h                     |  2 +-
>  arch/powerpc/include/asm/local.h                   | 14 ++++----
>  arch/sh/include/asm/pgtable_64.h                   |  2 +-
>  arch/sh/include/asm/processor_32.h                 |  4 +--
>  arch/sh/include/cpu-sh3/cpu/dac.h                  |  6 ++--
>  arch/x86/include/asm/alternative.h                 | 14 ++++----
>  arch/x86/um/asm/checksum.h                         |  4 +--
>  arch/x86/um/asm/checksum_32.h                      |  4 +--
>  arch/xtensa/include/asm/checksum.h                 | 14 ++++----
>  arch/xtensa/include/asm/cmpxchg.h                  |  4 +--
>  arch/xtensa/include/asm/irq.h                      |  2 +-
>  block/partitions/amiga.c                           |  2 +-
>  drivers/atm/he.c                                   |  6 ++--
>  drivers/atm/idt77252.c                             |  6 ++--
>  drivers/gpu/drm/mga/mga_drv.h                      |  2 +-
>  drivers/gpu/drm/mga/mga_state.c                    | 14 ++++----
>  drivers/gpu/drm/r128/r128_drv.h                    |  2 +-
>  drivers/gpu/drm/r128/r128_state.c                  | 14 ++++----
>  drivers/gpu/drm/via/via_irq.c                      |  2 +-
>  drivers/gpu/drm/via/via_verifier.c                 | 30 ++++++++--------
>  drivers/isdn/hardware/eicon/platform.h             | 14 ++++----
>  drivers/isdn/i4l/isdn_net.c                        | 14 ++++----
>  drivers/isdn/i4l/isdn_net.h                        |  8 ++---
>  drivers/media/pci/ivtv/ivtv-ioctl.c                |  2 +-
>  drivers/net/ethernet/sun/sungem.c                  |  8 ++---
>  drivers/net/ethernet/sun/sunhme.c                  |  6 ++--
>  drivers/net/hamradio/baycom_ser_fdx.c              |  2 +-
>  drivers/net/wan/lapbether.c                        |  2 +-
>  drivers/net/wan/n2.c                               |  4 +--
>  drivers/parisc/led.c                               |  4 +--
>  drivers/parisc/sba_iommu.c                         |  2 +-
>  drivers/parport/parport_gsc.c                      |  2 +-
>  drivers/parport/parport_gsc.h                      |  4 +--
>  drivers/parport/parport_pc.c                       |  2 +-
>  drivers/scsi/lpfc/lpfc_scsi.c                      |  2 +-
>  drivers/scsi/pcmcia/sym53c500_cs.c                 |  4 +--
>  drivers/scsi/qla2xxx/qla_inline.h                  |  2 +-
>  drivers/scsi/qla2xxx/qla_os.c                      |  4 +--
>  drivers/staging/rtl8723bs/core/rtw_pwrctrl.c       |  4 +--
>  drivers/staging/rtl8723bs/core/rtw_wlan_util.c     |  2 +-
>  drivers/staging/rtl8723bs/include/drv_types.h      |  6 ++--
>  drivers/staging/rtl8723bs/include/ieee80211.h      |  6 ++--
>  drivers/staging/rtl8723bs/include/osdep_service.h  | 10 +++---
>  .../rtl8723bs/include/osdep_service_linux.h        | 14 ++++----
>  drivers/staging/rtl8723bs/include/rtw_mlme.h       | 14 ++++----
>  drivers/staging/rtl8723bs/include/rtw_recv.h       | 16 ++++-----
>  drivers/staging/rtl8723bs/include/sta_info.h       |  2 +-
>  drivers/staging/rtl8723bs/include/wifi.h           | 14 ++++----
>  drivers/staging/rtl8723bs/include/wlan_bssdef.h    |  2 +-
>  drivers/tty/amiserial.c                            |  2 +-
>  drivers/tty/serial/ip22zilog.c                     |  2 +-
>  drivers/tty/serial/sunsab.c                        |  4 +--
>  drivers/tty/serial/sunzilog.c                      |  2 +-
>  drivers/video/fbdev/core/fbcon.c                   | 20 +++++------
>  drivers/video/fbdev/ffb.c                          |  2 +-
>  drivers/video/fbdev/intelfb/intelfbdrv.c           | 10 +++---
>  drivers/video/fbdev/intelfb/intelfbhw.c            |  2 +-
>  drivers/w1/masters/matrox_w1.c                     |  4 +--
>  fs/coda/coda_linux.h                               |  6 ++--
>  fs/freevxfs/vxfs_inode.c                           |  2 +-
>  fs/nfsd/nfsfh.h                                    |  4 +--
>  include/acpi/platform/acgcc.h                      |  2 +-
>  include/acpi/platform/acintel.h                    |  2 +-
>  include/asm-generic/ide_iops.h                     |  8 ++---
>  include/linux/atalk.h                              |  4 +--
>  include/linux/ceph/messenger.h                     |  2 +-
>  include/linux/compiler_types.h                     |  4 +--
>  include/linux/hdlc.h                               |  4 +--
>  include/linux/inetdevice.h                         |  8 ++---
>  include/linux/parport.h                            |  4 +--
>  include/linux/parport_pc.h                         | 22 ++++++------
>  include/net/ax25.h                                 |  2 +-
>  include/net/checksum.h                             |  2 +-
>  include/net/dn_nsp.h                               | 16 ++++-----
>  include/net/ip.h                                   |  2 +-
>  include/net/ip6_checksum.h                         |  2 +-
>  include/net/ipx.h                                  | 10 +++---
>  include/net/llc_c_ev.h                             |  4 +--
>  include/net/llc_conn.h                             |  4 +--
>  include/net/llc_s_ev.h                             |  2 +-
>  include/net/netrom.h                               |  8 ++---
>  include/net/scm.h                                  | 14 ++++----
>  include/net/udplite.h                              |  2 +-
>  include/net/x25.h                                  |  8 ++---
>  include/net/xfrm.h                                 | 18 +++++-----
>  include/uapi/linux/atm.h                           |  4 +--
>  include/uapi/linux/atmsap.h                        |  2 +-
>  include/uapi/linux/map_to_7segment.h               |  2 +-
>  include/uapi/linux/netfilter_arp/arp_tables.h      |  2 +-
>  include/uapi/linux/netfilter_bridge/ebtables.h     |  2 +-
>  include/uapi/linux/netfilter_ipv4/ip_tables.h      |  2 +-
>  include/uapi/linux/netfilter_ipv6/ip6_tables.h     |  2 +-
>  include/video/newport.h                            | 12 +++----
>  lib/zstd/mem.h                                     |  2 +-
>  net/appletalk/atalk_proc.c                         |  4 +--
>  net/appletalk/ddp.c                                |  2 +-
>  net/core/neighbour.c                               |  2 +-
>  net/core/scm.c                                     |  2 +-
>  net/decnet/dn_nsp_in.c                             |  2 +-
>  net/decnet/dn_nsp_out.c                            |  2 +-
>  net/decnet/dn_route.c                              |  2 +-
>  net/decnet/dn_table.c                              |  4 +--
>  net/ipv4/igmp.c                                    |  2 +-
>  net/ipv6/af_inet6.c                                |  2 +-
>  net/ipv6/icmp.c                                    |  4 +--
>  net/ipv6/udp.c                                     |  4 +--
>  net/lapb/lapb_iface.c                              |  4 +--
>  net/llc/llc_input.c                                |  2 +-
>  scripts/checkpatch.pl                              |  8 ++---
>  scripts/genksyms/keywords.c                        |  4 +--
>  scripts/kernel-doc                                 |  4 +--
>  sound/sparc/amd7930.c                              |  6 ++--
>  165 files changed, 547 insertions(+), 547 deletions(-)


-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 27 Dec 2018 04:47:34 +0000
Message-ID: <CAK7LNASo5jedRxjX5zevLSmZRce3fERcFc=qn_JtJXx1Ss3iJA () mail ! gmail ! com>
--------------------
Hi Peter,


On Wed, Oct 31, 2018 at 9:58 PM Peter Zijlstra <peterz@infradead.org> wrote:
>
> On Sat, Oct 13, 2018 at 09:33:35PM +0200, Borislav Petkov wrote:
> > Ok,
> >
> > with Segher's help I've been playing with his patch ontop of bleeding
> > edge gcc 9 and here are my observations. Please double-check me for
> > booboos so that they can be addressed while there's time.
> >
> > So here's what I see ontop of 4.19-rc7:
> >
> > First marked the alternative asm() as inline and undeffed the "inline"
> > keyword. I need to do that because of the funky games we do with
> > "inline" redefinitions in include/linux/compiler_types.h.
> >
> > And Segher hinted at either doing:
> >
> > asm volatile inline(...
> >
> > or
> >
> > asm volatile __inline__(...
> >
> > but both "inline" variants are defined as macros in that file.
> >
> > Which means we either need to #undef inline before using it in asm() or
> > come up with something cleverer.
>
> # git grep -e "\<__inline__\>" | wc -l
> 488
> # git grep -e "\<__inline\>" | wc -l
> 56
> # git grep -e "\<inline\>" | wc -l
> 69598
>
> And we already have scripts/checkpatch.pl:
>
>   # Check for __inline__ and __inline, prefer inline
>
> Which suggests we do:
>
> git grep -l "\<__inline\(\|__\)\>" | while read file
> do
>         sed -i -e 's/\<__inline\(\|__\)\>/inline/g' $file
> done
>
> and get it over with.


Do you have a plan to really do this?

This is a nice cleanup anyway.

I think the last minute of MW is
a good timing for the global replacement like this.




>
> Anyway, with the below patch, I get:
>
>    text    data     bss     dec     hex filename
> 17385183        5064780 1953892 24403855        1745f8f defconfig-build/vmlinux
> 17385678        5064780 1953892 24404350        174617e defconfig-build/vmlinux
>
> Which shows we inline more (look for asm_volatile for the actual
> changes).
>
>
> So yes, this seems like something we could work with.
>
> ---
>  Documentation/trace/tracepoint-analysis.rst        |  2 +-
>  Documentation/translations/ja_JP/SubmittingPatches |  4 +--
>  Documentation/translations/zh_CN/SubmittingPatches |  4 +--
>  arch/alpha/include/asm/atomic.h                    | 12 +++----
>  arch/alpha/include/asm/bitops.h                    |  6 ++--
>  arch/alpha/include/asm/compiler.h                  |  4 +--
>  arch/alpha/include/asm/dma.h                       | 22 ++++++------
>  arch/alpha/include/asm/floppy.h                    |  4 +--
>  arch/alpha/include/asm/irq.h                       |  2 +-
>  arch/alpha/include/asm/local.h                     |  4 +--
>  arch/alpha/include/asm/smp.h                       |  2 +-
>  arch/arm/mach-iop32x/include/mach/uncompress.h     |  2 +-
>  arch/arm/mach-iop33x/include/mach/uncompress.h     |  2 +-
>  arch/arm/mach-ixp4xx/include/mach/uncompress.h     |  2 +-
>  arch/ia64/hp/common/sba_iommu.c                    |  2 +-
>  arch/ia64/hp/sim/simeth.c                          |  2 +-
>  arch/ia64/include/asm/atomic.h                     |  8 ++---
>  arch/ia64/include/asm/bitops.h                     | 34 +++++++++---------
>  arch/ia64/include/asm/delay.h                      | 14 ++++----
>  arch/ia64/include/asm/irq.h                        |  2 +-
>  arch/ia64/include/asm/page.h                       |  2 +-
>  arch/ia64/include/asm/sn/leds.h                    |  2 +-
>  arch/ia64/include/asm/uaccess.h                    |  4 +--
>  arch/ia64/include/uapi/asm/rse.h                   | 12 +++----
>  arch/ia64/include/uapi/asm/swab.h                  |  6 ++--
>  arch/ia64/oprofile/backtrace.c                     |  4 +--
>  arch/m68k/include/asm/blinken.h                    |  2 +-
>  arch/m68k/include/asm/checksum.h                   |  2 +-
>  arch/m68k/include/asm/dma.h                        | 32 ++++++++---------
>  arch/m68k/include/asm/floppy.h                     |  8 ++---
>  arch/m68k/include/asm/nettel.h                     |  8 ++---
>  arch/m68k/mac/iop.c                                | 14 ++++----
>  arch/mips/include/asm/atomic.h                     | 16 ++++-----
>  arch/mips/include/asm/checksum.h                   |  2 +-
>  arch/mips/include/asm/dma.h                        | 20 +++++------
>  arch/mips/include/asm/jazz.h                       |  2 +-
>  arch/mips/include/asm/local.h                      |  4 +--
>  arch/mips/include/asm/string.h                     |  8 ++---
>  arch/mips/kernel/binfmt_elfn32.c                   |  2 +-
>  arch/nds32/include/asm/swab.h                      |  4 +--
>  arch/parisc/include/asm/atomic.h                   | 20 +++++------
>  arch/parisc/include/asm/bitops.h                   | 18 +++++-----
>  arch/parisc/include/asm/checksum.h                 |  4 +--
>  arch/parisc/include/asm/compat.h                   |  2 +-
>  arch/parisc/include/asm/delay.h                    |  2 +-
>  arch/parisc/include/asm/dma.h                      | 20 +++++------
>  arch/parisc/include/asm/ide.h                      |  8 ++---
>  arch/parisc/include/asm/irq.h                      |  2 +-
>  arch/parisc/include/asm/spinlock.h                 | 12 +++----
>  arch/powerpc/include/asm/atomic.h                  | 40 +++++++++++-----------
>  arch/powerpc/include/asm/bitops.h                  | 28 +++++++--------
>  arch/powerpc/include/asm/dma.h                     | 20 +++++------
>  arch/powerpc/include/asm/edac.h                    |  2 +-
>  arch/powerpc/include/asm/irq.h                     |  2 +-
>  arch/powerpc/include/asm/local.h                   | 14 ++++----
>  arch/sh/include/asm/pgtable_64.h                   |  2 +-
>  arch/sh/include/asm/processor_32.h                 |  4 +--
>  arch/sh/include/cpu-sh3/cpu/dac.h                  |  6 ++--
>  arch/x86/include/asm/alternative.h                 | 14 ++++----
>  arch/x86/um/asm/checksum.h                         |  4 +--
>  arch/x86/um/asm/checksum_32.h                      |  4 +--
>  arch/xtensa/include/asm/checksum.h                 | 14 ++++----
>  arch/xtensa/include/asm/cmpxchg.h                  |  4 +--
>  arch/xtensa/include/asm/irq.h                      |  2 +-
>  block/partitions/amiga.c                           |  2 +-
>  drivers/atm/he.c                                   |  6 ++--
>  drivers/atm/idt77252.c                             |  6 ++--
>  drivers/gpu/drm/mga/mga_drv.h                      |  2 +-
>  drivers/gpu/drm/mga/mga_state.c                    | 14 ++++----
>  drivers/gpu/drm/r128/r128_drv.h                    |  2 +-
>  drivers/gpu/drm/r128/r128_state.c                  | 14 ++++----
>  drivers/gpu/drm/via/via_irq.c                      |  2 +-
>  drivers/gpu/drm/via/via_verifier.c                 | 30 ++++++++--------
>  drivers/isdn/hardware/eicon/platform.h             | 14 ++++----
>  drivers/isdn/i4l/isdn_net.c                        | 14 ++++----
>  drivers/isdn/i4l/isdn_net.h                        |  8 ++---
>  drivers/media/pci/ivtv/ivtv-ioctl.c                |  2 +-
>  drivers/net/ethernet/sun/sungem.c                  |  8 ++---
>  drivers/net/ethernet/sun/sunhme.c                  |  6 ++--
>  drivers/net/hamradio/baycom_ser_fdx.c              |  2 +-
>  drivers/net/wan/lapbether.c                        |  2 +-
>  drivers/net/wan/n2.c                               |  4 +--
>  drivers/parisc/led.c                               |  4 +--
>  drivers/parisc/sba_iommu.c                         |  2 +-
>  drivers/parport/parport_gsc.c                      |  2 +-
>  drivers/parport/parport_gsc.h                      |  4 +--
>  drivers/parport/parport_pc.c                       |  2 +-
>  drivers/scsi/lpfc/lpfc_scsi.c                      |  2 +-
>  drivers/scsi/pcmcia/sym53c500_cs.c                 |  4 +--
>  drivers/scsi/qla2xxx/qla_inline.h                  |  2 +-
>  drivers/scsi/qla2xxx/qla_os.c                      |  4 +--
>  drivers/staging/rtl8723bs/core/rtw_pwrctrl.c       |  4 +--
>  drivers/staging/rtl8723bs/core/rtw_wlan_util.c     |  2 +-
>  drivers/staging/rtl8723bs/include/drv_types.h      |  6 ++--
>  drivers/staging/rtl8723bs/include/ieee80211.h      |  6 ++--
>  drivers/staging/rtl8723bs/include/osdep_service.h  | 10 +++---
>  .../rtl8723bs/include/osdep_service_linux.h        | 14 ++++----
>  drivers/staging/rtl8723bs/include/rtw_mlme.h       | 14 ++++----
>  drivers/staging/rtl8723bs/include/rtw_recv.h       | 16 ++++-----
>  drivers/staging/rtl8723bs/include/sta_info.h       |  2 +-
>  drivers/staging/rtl8723bs/include/wifi.h           | 14 ++++----
>  drivers/staging/rtl8723bs/include/wlan_bssdef.h    |  2 +-
>  drivers/tty/amiserial.c                            |  2 +-
>  drivers/tty/serial/ip22zilog.c                     |  2 +-
>  drivers/tty/serial/sunsab.c                        |  4 +--
>  drivers/tty/serial/sunzilog.c                      |  2 +-
>  drivers/video/fbdev/core/fbcon.c                   | 20 +++++------
>  drivers/video/fbdev/ffb.c                          |  2 +-
>  drivers/video/fbdev/intelfb/intelfbdrv.c           | 10 +++---
>  drivers/video/fbdev/intelfb/intelfbhw.c            |  2 +-
>  drivers/w1/masters/matrox_w1.c                     |  4 +--
>  fs/coda/coda_linux.h                               |  6 ++--
>  fs/freevxfs/vxfs_inode.c                           |  2 +-
>  fs/nfsd/nfsfh.h                                    |  4 +--
>  include/acpi/platform/acgcc.h                      |  2 +-
>  include/acpi/platform/acintel.h                    |  2 +-
>  include/asm-generic/ide_iops.h                     |  8 ++---
>  include/linux/atalk.h                              |  4 +--
>  include/linux/ceph/messenger.h                     |  2 +-
>  include/linux/compiler_types.h                     |  4 +--
>  include/linux/hdlc.h                               |  4 +--
>  include/linux/inetdevice.h                         |  8 ++---
>  include/linux/parport.h                            |  4 +--
>  include/linux/parport_pc.h                         | 22 ++++++------
>  include/net/ax25.h                                 |  2 +-
>  include/net/checksum.h                             |  2 +-
>  include/net/dn_nsp.h                               | 16 ++++-----
>  include/net/ip.h                                   |  2 +-
>  include/net/ip6_checksum.h                         |  2 +-
>  include/net/ipx.h                                  | 10 +++---
>  include/net/llc_c_ev.h                             |  4 +--
>  include/net/llc_conn.h                             |  4 +--
>  include/net/llc_s_ev.h                             |  2 +-
>  include/net/netrom.h                               |  8 ++---
>  include/net/scm.h                                  | 14 ++++----
>  include/net/udplite.h                              |  2 +-
>  include/net/x25.h                                  |  8 ++---
>  include/net/xfrm.h                                 | 18 +++++-----
>  include/uapi/linux/atm.h                           |  4 +--
>  include/uapi/linux/atmsap.h                        |  2 +-
>  include/uapi/linux/map_to_7segment.h               |  2 +-
>  include/uapi/linux/netfilter_arp/arp_tables.h      |  2 +-
>  include/uapi/linux/netfilter_bridge/ebtables.h     |  2 +-
>  include/uapi/linux/netfilter_ipv4/ip_tables.h      |  2 +-
>  include/uapi/linux/netfilter_ipv6/ip6_tables.h     |  2 +-
>  include/video/newport.h                            | 12 +++----
>  lib/zstd/mem.h                                     |  2 +-
>  net/appletalk/atalk_proc.c                         |  4 +--
>  net/appletalk/ddp.c                                |  2 +-
>  net/core/neighbour.c                               |  2 +-
>  net/core/scm.c                                     |  2 +-
>  net/decnet/dn_nsp_in.c                             |  2 +-
>  net/decnet/dn_nsp_out.c                            |  2 +-
>  net/decnet/dn_route.c                              |  2 +-
>  net/decnet/dn_table.c                              |  4 +--
>  net/ipv4/igmp.c                                    |  2 +-
>  net/ipv6/af_inet6.c                                |  2 +-
>  net/ipv6/icmp.c                                    |  4 +--
>  net/ipv6/udp.c                                     |  4 +--
>  net/lapb/lapb_iface.c                              |  4 +--
>  net/llc/llc_input.c                                |  2 +-
>  scripts/checkpatch.pl                              |  8 ++---
>  scripts/genksyms/keywords.c                        |  4 +--
>  scripts/kernel-doc                                 |  4 +--
>  sound/sparc/amd7930.c                              |  6 ++--
>  165 files changed, 547 insertions(+), 547 deletions(-)


-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-virtualization
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 27 Dec 2018 04:47:34 +0000
Message-ID: <CAK7LNASo5jedRxjX5zevLSmZRce3fERcFc=qn_JtJXx1Ss3iJA () mail ! gmail ! com>
--------------------
Hi Peter,


On Wed, Oct 31, 2018 at 9:58 PM Peter Zijlstra <peterz@infradead.org> wrote:
>
> On Sat, Oct 13, 2018 at 09:33:35PM +0200, Borislav Petkov wrote:
> > Ok,
> >
> > with Segher's help I've been playing with his patch ontop of bleeding
> > edge gcc 9 and here are my observations. Please double-check me for
> > booboos so that they can be addressed while there's time.
> >
> > So here's what I see ontop of 4.19-rc7:
> >
> > First marked the alternative asm() as inline and undeffed the "inline"
> > keyword. I need to do that because of the funky games we do with
> > "inline" redefinitions in include/linux/compiler_types.h.
> >
> > And Segher hinted at either doing:
> >
> > asm volatile inline(...
> >
> > or
> >
> > asm volatile __inline__(...
> >
> > but both "inline" variants are defined as macros in that file.
> >
> > Which means we either need to #undef inline before using it in asm() or
> > come up with something cleverer.
>
> # git grep -e "\<__inline__\>" | wc -l
> 488
> # git grep -e "\<__inline\>" | wc -l
> 56
> # git grep -e "\<inline\>" | wc -l
> 69598
>
> And we already have scripts/checkpatch.pl:
>
>   # Check for __inline__ and __inline, prefer inline
>
> Which suggests we do:
>
> git grep -l "\<__inline\(\|__\)\>" | while read file
> do
>         sed -i -e 's/\<__inline\(\|__\)\>/inline/g' $file
> done
>
> and get it over with.


Do you have a plan to really do this?

This is a nice cleanup anyway.

I think the last minute of MW is
a good timing for the global replacement like this.




>
> Anyway, with the below patch, I get:
>
>    text    data     bss     dec     hex filename
> 17385183        5064780 1953892 24403855        1745f8f defconfig-build/vmlinux
> 17385678        5064780 1953892 24404350        174617e defconfig-build/vmlinux
>
> Which shows we inline more (look for asm_volatile for the actual
> changes).
>
>
> So yes, this seems like something we could work with.
>
> ---
>  Documentation/trace/tracepoint-analysis.rst        |  2 +-
>  Documentation/translations/ja_JP/SubmittingPatches |  4 +--
>  Documentation/translations/zh_CN/SubmittingPatches |  4 +--
>  arch/alpha/include/asm/atomic.h                    | 12 +++----
>  arch/alpha/include/asm/bitops.h                    |  6 ++--
>  arch/alpha/include/asm/compiler.h                  |  4 +--
>  arch/alpha/include/asm/dma.h                       | 22 ++++++------
>  arch/alpha/include/asm/floppy.h                    |  4 +--
>  arch/alpha/include/asm/irq.h                       |  2 +-
>  arch/alpha/include/asm/local.h                     |  4 +--
>  arch/alpha/include/asm/smp.h                       |  2 +-
>  arch/arm/mach-iop32x/include/mach/uncompress.h     |  2 +-
>  arch/arm/mach-iop33x/include/mach/uncompress.h     |  2 +-
>  arch/arm/mach-ixp4xx/include/mach/uncompress.h     |  2 +-
>  arch/ia64/hp/common/sba_iommu.c                    |  2 +-
>  arch/ia64/hp/sim/simeth.c                          |  2 +-
>  arch/ia64/include/asm/atomic.h                     |  8 ++---
>  arch/ia64/include/asm/bitops.h                     | 34 +++++++++---------
>  arch/ia64/include/asm/delay.h                      | 14 ++++----
>  arch/ia64/include/asm/irq.h                        |  2 +-
>  arch/ia64/include/asm/page.h                       |  2 +-
>  arch/ia64/include/asm/sn/leds.h                    |  2 +-
>  arch/ia64/include/asm/uaccess.h                    |  4 +--
>  arch/ia64/include/uapi/asm/rse.h                   | 12 +++----
>  arch/ia64/include/uapi/asm/swab.h                  |  6 ++--
>  arch/ia64/oprofile/backtrace.c                     |  4 +--
>  arch/m68k/include/asm/blinken.h                    |  2 +-
>  arch/m68k/include/asm/checksum.h                   |  2 +-
>  arch/m68k/include/asm/dma.h                        | 32 ++++++++---------
>  arch/m68k/include/asm/floppy.h                     |  8 ++---
>  arch/m68k/include/asm/nettel.h                     |  8 ++---
>  arch/m68k/mac/iop.c                                | 14 ++++----
>  arch/mips/include/asm/atomic.h                     | 16 ++++-----
>  arch/mips/include/asm/checksum.h                   |  2 +-
>  arch/mips/include/asm/dma.h                        | 20 +++++------
>  arch/mips/include/asm/jazz.h                       |  2 +-
>  arch/mips/include/asm/local.h                      |  4 +--
>  arch/mips/include/asm/string.h                     |  8 ++---
>  arch/mips/kernel/binfmt_elfn32.c                   |  2 +-
>  arch/nds32/include/asm/swab.h                      |  4 +--
>  arch/parisc/include/asm/atomic.h                   | 20 +++++------
>  arch/parisc/include/asm/bitops.h                   | 18 +++++-----
>  arch/parisc/include/asm/checksum.h                 |  4 +--
>  arch/parisc/include/asm/compat.h                   |  2 +-
>  arch/parisc/include/asm/delay.h                    |  2 +-
>  arch/parisc/include/asm/dma.h                      | 20 +++++------
>  arch/parisc/include/asm/ide.h                      |  8 ++---
>  arch/parisc/include/asm/irq.h                      |  2 +-
>  arch/parisc/include/asm/spinlock.h                 | 12 +++----
>  arch/powerpc/include/asm/atomic.h                  | 40 +++++++++++-----------
>  arch/powerpc/include/asm/bitops.h                  | 28 +++++++--------
>  arch/powerpc/include/asm/dma.h                     | 20 +++++------
>  arch/powerpc/include/asm/edac.h                    |  2 +-
>  arch/powerpc/include/asm/irq.h                     |  2 +-
>  arch/powerpc/include/asm/local.h                   | 14 ++++----
>  arch/sh/include/asm/pgtable_64.h                   |  2 +-
>  arch/sh/include/asm/processor_32.h                 |  4 +--
>  arch/sh/include/cpu-sh3/cpu/dac.h                  |  6 ++--
>  arch/x86/include/asm/alternative.h                 | 14 ++++----
>  arch/x86/um/asm/checksum.h                         |  4 +--
>  arch/x86/um/asm/checksum_32.h                      |  4 +--
>  arch/xtensa/include/asm/checksum.h                 | 14 ++++----
>  arch/xtensa/include/asm/cmpxchg.h                  |  4 +--
>  arch/xtensa/include/asm/irq.h                      |  2 +-
>  block/partitions/amiga.c                           |  2 +-
>  drivers/atm/he.c                                   |  6 ++--
>  drivers/atm/idt77252.c                             |  6 ++--
>  drivers/gpu/drm/mga/mga_drv.h                      |  2 +-
>  drivers/gpu/drm/mga/mga_state.c                    | 14 ++++----
>  drivers/gpu/drm/r128/r128_drv.h                    |  2 +-
>  drivers/gpu/drm/r128/r128_state.c                  | 14 ++++----
>  drivers/gpu/drm/via/via_irq.c                      |  2 +-
>  drivers/gpu/drm/via/via_verifier.c                 | 30 ++++++++--------
>  drivers/isdn/hardware/eicon/platform.h             | 14 ++++----
>  drivers/isdn/i4l/isdn_net.c                        | 14 ++++----
>  drivers/isdn/i4l/isdn_net.h                        |  8 ++---
>  drivers/media/pci/ivtv/ivtv-ioctl.c                |  2 +-
>  drivers/net/ethernet/sun/sungem.c                  |  8 ++---
>  drivers/net/ethernet/sun/sunhme.c                  |  6 ++--
>  drivers/net/hamradio/baycom_ser_fdx.c              |  2 +-
>  drivers/net/wan/lapbether.c                        |  2 +-
>  drivers/net/wan/n2.c                               |  4 +--
>  drivers/parisc/led.c                               |  4 +--
>  drivers/parisc/sba_iommu.c                         |  2 +-
>  drivers/parport/parport_gsc.c                      |  2 +-
>  drivers/parport/parport_gsc.h                      |  4 +--
>  drivers/parport/parport_pc.c                       |  2 +-
>  drivers/scsi/lpfc/lpfc_scsi.c                      |  2 +-
>  drivers/scsi/pcmcia/sym53c500_cs.c                 |  4 +--
>  drivers/scsi/qla2xxx/qla_inline.h                  |  2 +-
>  drivers/scsi/qla2xxx/qla_os.c                      |  4 +--
>  drivers/staging/rtl8723bs/core/rtw_pwrctrl.c       |  4 +--
>  drivers/staging/rtl8723bs/core/rtw_wlan_util.c     |  2 +-
>  drivers/staging/rtl8723bs/include/drv_types.h      |  6 ++--
>  drivers/staging/rtl8723bs/include/ieee80211.h      |  6 ++--
>  drivers/staging/rtl8723bs/include/osdep_service.h  | 10 +++---
>  .../rtl8723bs/include/osdep_service_linux.h        | 14 ++++----
>  drivers/staging/rtl8723bs/include/rtw_mlme.h       | 14 ++++----
>  drivers/staging/rtl8723bs/include/rtw_recv.h       | 16 ++++-----
>  drivers/staging/rtl8723bs/include/sta_info.h       |  2 +-
>  drivers/staging/rtl8723bs/include/wifi.h           | 14 ++++----
>  drivers/staging/rtl8723bs/include/wlan_bssdef.h    |  2 +-
>  drivers/tty/amiserial.c                            |  2 +-
>  drivers/tty/serial/ip22zilog.c                     |  2 +-
>  drivers/tty/serial/sunsab.c                        |  4 +--
>  drivers/tty/serial/sunzilog.c                      |  2 +-
>  drivers/video/fbdev/core/fbcon.c                   | 20 +++++------
>  drivers/video/fbdev/ffb.c                          |  2 +-
>  drivers/video/fbdev/intelfb/intelfbdrv.c           | 10 +++---
>  drivers/video/fbdev/intelfb/intelfbhw.c            |  2 +-
>  drivers/w1/masters/matrox_w1.c                     |  4 +--
>  fs/coda/coda_linux.h                               |  6 ++--
>  fs/freevxfs/vxfs_inode.c                           |  2 +-
>  fs/nfsd/nfsfh.h                                    |  4 +--
>  include/acpi/platform/acgcc.h                      |  2 +-
>  include/acpi/platform/acintel.h                    |  2 +-
>  include/asm-generic/ide_iops.h                     |  8 ++---
>  include/linux/atalk.h                              |  4 +--
>  include/linux/ceph/messenger.h                     |  2 +-
>  include/linux/compiler_types.h                     |  4 +--
>  include/linux/hdlc.h                               |  4 +--
>  include/linux/inetdevice.h                         |  8 ++---
>  include/linux/parport.h                            |  4 +--
>  include/linux/parport_pc.h                         | 22 ++++++------
>  include/net/ax25.h                                 |  2 +-
>  include/net/checksum.h                             |  2 +-
>  include/net/dn_nsp.h                               | 16 ++++-----
>  include/net/ip.h                                   |  2 +-
>  include/net/ip6_checksum.h                         |  2 +-
>  include/net/ipx.h                                  | 10 +++---
>  include/net/llc_c_ev.h                             |  4 +--
>  include/net/llc_conn.h                             |  4 +--
>  include/net/llc_s_ev.h                             |  2 +-
>  include/net/netrom.h                               |  8 ++---
>  include/net/scm.h                                  | 14 ++++----
>  include/net/udplite.h                              |  2 +-
>  include/net/x25.h                                  |  8 ++---
>  include/net/xfrm.h                                 | 18 +++++-----
>  include/uapi/linux/atm.h                           |  4 +--
>  include/uapi/linux/atmsap.h                        |  2 +-
>  include/uapi/linux/map_to_7segment.h               |  2 +-
>  include/uapi/linux/netfilter_arp/arp_tables.h      |  2 +-
>  include/uapi/linux/netfilter_bridge/ebtables.h     |  2 +-
>  include/uapi/linux/netfilter_ipv4/ip_tables.h      |  2 +-
>  include/uapi/linux/netfilter_ipv6/ip6_tables.h     |  2 +-
>  include/video/newport.h                            | 12 +++----
>  lib/zstd/mem.h                                     |  2 +-
>  net/appletalk/atalk_proc.c                         |  4 +--
>  net/appletalk/ddp.c                                |  2 +-
>  net/core/neighbour.c                               |  2 +-
>  net/core/scm.c                                     |  2 +-
>  net/decnet/dn_nsp_in.c                             |  2 +-
>  net/decnet/dn_nsp_out.c                            |  2 +-
>  net/decnet/dn_route.c                              |  2 +-
>  net/decnet/dn_table.c                              |  4 +--
>  net/ipv4/igmp.c                                    |  2 +-
>  net/ipv6/af_inet6.c                                |  2 +-
>  net/ipv6/icmp.c                                    |  4 +--
>  net/ipv6/udp.c                                     |  4 +--
>  net/lapb/lapb_iface.c                              |  4 +--
>  net/llc/llc_input.c                                |  2 +-
>  scripts/checkpatch.pl                              |  8 ++---
>  scripts/genksyms/keywords.c                        |  4 +--
>  scripts/kernel-doc                                 |  4 +--
>  sound/sparc/amd7930.c                              |  6 ++--
>  165 files changed, 547 insertions(+), 547 deletions(-)


-- 
Best Regards
Masahiro Yamada
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 27 Dec 2018 04:47:34 +0000
Message-ID: <CAK7LNASo5jedRxjX5zevLSmZRce3fERcFc=qn_JtJXx1Ss3iJA () mail ! gmail ! com>
--------------------
Hi Peter,


On Wed, Oct 31, 2018 at 9:58 PM Peter Zijlstra <peterz@infradead.org> wrote:
>
> On Sat, Oct 13, 2018 at 09:33:35PM +0200, Borislav Petkov wrote:
> > Ok,
> >
> > with Segher's help I've been playing with his patch ontop of bleeding
> > edge gcc 9 and here are my observations. Please double-check me for
> > booboos so that they can be addressed while there's time.
> >
> > So here's what I see ontop of 4.19-rc7:
> >
> > First marked the alternative asm() as inline and undeffed the "inline"
> > keyword. I need to do that because of the funky games we do with
> > "inline" redefinitions in include/linux/compiler_types.h.
> >
> > And Segher hinted at either doing:
> >
> > asm volatile inline(...
> >
> > or
> >
> > asm volatile __inline__(...
> >
> > but both "inline" variants are defined as macros in that file.
> >
> > Which means we either need to #undef inline before using it in asm() or
> > come up with something cleverer.
>
> # git grep -e "\<__inline__\>" | wc -l
> 488
> # git grep -e "\<__inline\>" | wc -l
> 56
> # git grep -e "\<inline\>" | wc -l
> 69598
>
> And we already have scripts/checkpatch.pl:
>
>   # Check for __inline__ and __inline, prefer inline
>
> Which suggests we do:
>
> git grep -l "\<__inline\(\|__\)\>" | while read file
> do
>         sed -i -e 's/\<__inline\(\|__\)\>/inline/g' $file
> done
>
> and get it over with.


Do you have a plan to really do this?

This is a nice cleanup anyway.

I think the last minute of MW is
a good timing for the global replacement like this.




>
> Anyway, with the below patch, I get:
>
>    text    data     bss     dec     hex filename
> 17385183        5064780 1953892 24403855        1745f8f defconfig-build/vmlinux
> 17385678        5064780 1953892 24404350        174617e defconfig-build/vmlinux
>
> Which shows we inline more (look for asm_volatile for the actual
> changes).
>
>
> So yes, this seems like something we could work with.
>
> ---
>  Documentation/trace/tracepoint-analysis.rst        |  2 +-
>  Documentation/translations/ja_JP/SubmittingPatches |  4 +--
>  Documentation/translations/zh_CN/SubmittingPatches |  4 +--
>  arch/alpha/include/asm/atomic.h                    | 12 +++----
>  arch/alpha/include/asm/bitops.h                    |  6 ++--
>  arch/alpha/include/asm/compiler.h                  |  4 +--
>  arch/alpha/include/asm/dma.h                       | 22 ++++++------
>  arch/alpha/include/asm/floppy.h                    |  4 +--
>  arch/alpha/include/asm/irq.h                       |  2 +-
>  arch/alpha/include/asm/local.h                     |  4 +--
>  arch/alpha/include/asm/smp.h                       |  2 +-
>  arch/arm/mach-iop32x/include/mach/uncompress.h     |  2 +-
>  arch/arm/mach-iop33x/include/mach/uncompress.h     |  2 +-
>  arch/arm/mach-ixp4xx/include/mach/uncompress.h     |  2 +-
>  arch/ia64/hp/common/sba_iommu.c                    |  2 +-
>  arch/ia64/hp/sim/simeth.c                          |  2 +-
>  arch/ia64/include/asm/atomic.h                     |  8 ++---
>  arch/ia64/include/asm/bitops.h                     | 34 +++++++++---------
>  arch/ia64/include/asm/delay.h                      | 14 ++++----
>  arch/ia64/include/asm/irq.h                        |  2 +-
>  arch/ia64/include/asm/page.h                       |  2 +-
>  arch/ia64/include/asm/sn/leds.h                    |  2 +-
>  arch/ia64/include/asm/uaccess.h                    |  4 +--
>  arch/ia64/include/uapi/asm/rse.h                   | 12 +++----
>  arch/ia64/include/uapi/asm/swab.h                  |  6 ++--
>  arch/ia64/oprofile/backtrace.c                     |  4 +--
>  arch/m68k/include/asm/blinken.h                    |  2 +-
>  arch/m68k/include/asm/checksum.h                   |  2 +-
>  arch/m68k/include/asm/dma.h                        | 32 ++++++++---------
>  arch/m68k/include/asm/floppy.h                     |  8 ++---
>  arch/m68k/include/asm/nettel.h                     |  8 ++---
>  arch/m68k/mac/iop.c                                | 14 ++++----
>  arch/mips/include/asm/atomic.h                     | 16 ++++-----
>  arch/mips/include/asm/checksum.h                   |  2 +-
>  arch/mips/include/asm/dma.h                        | 20 +++++------
>  arch/mips/include/asm/jazz.h                       |  2 +-
>  arch/mips/include/asm/local.h                      |  4 +--
>  arch/mips/include/asm/string.h                     |  8 ++---
>  arch/mips/kernel/binfmt_elfn32.c                   |  2 +-
>  arch/nds32/include/asm/swab.h                      |  4 +--
>  arch/parisc/include/asm/atomic.h                   | 20 +++++------
>  arch/parisc/include/asm/bitops.h                   | 18 +++++-----
>  arch/parisc/include/asm/checksum.h                 |  4 +--
>  arch/parisc/include/asm/compat.h                   |  2 +-
>  arch/parisc/include/asm/delay.h                    |  2 +-
>  arch/parisc/include/asm/dma.h                      | 20 +++++------
>  arch/parisc/include/asm/ide.h                      |  8 ++---
>  arch/parisc/include/asm/irq.h                      |  2 +-
>  arch/parisc/include/asm/spinlock.h                 | 12 +++----
>  arch/powerpc/include/asm/atomic.h                  | 40 +++++++++++-----------
>  arch/powerpc/include/asm/bitops.h                  | 28 +++++++--------
>  arch/powerpc/include/asm/dma.h                     | 20 +++++------
>  arch/powerpc/include/asm/edac.h                    |  2 +-
>  arch/powerpc/include/asm/irq.h                     |  2 +-
>  arch/powerpc/include/asm/local.h                   | 14 ++++----
>  arch/sh/include/asm/pgtable_64.h                   |  2 +-
>  arch/sh/include/asm/processor_32.h                 |  4 +--
>  arch/sh/include/cpu-sh3/cpu/dac.h                  |  6 ++--
>  arch/x86/include/asm/alternative.h                 | 14 ++++----
>  arch/x86/um/asm/checksum.h                         |  4 +--
>  arch/x86/um/asm/checksum_32.h                      |  4 +--
>  arch/xtensa/include/asm/checksum.h                 | 14 ++++----
>  arch/xtensa/include/asm/cmpxchg.h                  |  4 +--
>  arch/xtensa/include/asm/irq.h                      |  2 +-
>  block/partitions/amiga.c                           |  2 +-
>  drivers/atm/he.c                                   |  6 ++--
>  drivers/atm/idt77252.c                             |  6 ++--
>  drivers/gpu/drm/mga/mga_drv.h                      |  2 +-
>  drivers/gpu/drm/mga/mga_state.c                    | 14 ++++----
>  drivers/gpu/drm/r128/r128_drv.h                    |  2 +-
>  drivers/gpu/drm/r128/r128_state.c                  | 14 ++++----
>  drivers/gpu/drm/via/via_irq.c                      |  2 +-
>  drivers/gpu/drm/via/via_verifier.c                 | 30 ++++++++--------
>  drivers/isdn/hardware/eicon/platform.h             | 14 ++++----
>  drivers/isdn/i4l/isdn_net.c                        | 14 ++++----
>  drivers/isdn/i4l/isdn_net.h                        |  8 ++---
>  drivers/media/pci/ivtv/ivtv-ioctl.c                |  2 +-
>  drivers/net/ethernet/sun/sungem.c                  |  8 ++---
>  drivers/net/ethernet/sun/sunhme.c                  |  6 ++--
>  drivers/net/hamradio/baycom_ser_fdx.c              |  2 +-
>  drivers/net/wan/lapbether.c                        |  2 +-
>  drivers/net/wan/n2.c                               |  4 +--
>  drivers/parisc/led.c                               |  4 +--
>  drivers/parisc/sba_iommu.c                         |  2 +-
>  drivers/parport/parport_gsc.c                      |  2 +-
>  drivers/parport/parport_gsc.h                      |  4 +--
>  drivers/parport/parport_pc.c                       |  2 +-
>  drivers/scsi/lpfc/lpfc_scsi.c                      |  2 +-
>  drivers/scsi/pcmcia/sym53c500_cs.c                 |  4 +--
>  drivers/scsi/qla2xxx/qla_inline.h                  |  2 +-
>  drivers/scsi/qla2xxx/qla_os.c                      |  4 +--
>  drivers/staging/rtl8723bs/core/rtw_pwrctrl.c       |  4 +--
>  drivers/staging/rtl8723bs/core/rtw_wlan_util.c     |  2 +-
>  drivers/staging/rtl8723bs/include/drv_types.h      |  6 ++--
>  drivers/staging/rtl8723bs/include/ieee80211.h      |  6 ++--
>  drivers/staging/rtl8723bs/include/osdep_service.h  | 10 +++---
>  .../rtl8723bs/include/osdep_service_linux.h        | 14 ++++----
>  drivers/staging/rtl8723bs/include/rtw_mlme.h       | 14 ++++----
>  drivers/staging/rtl8723bs/include/rtw_recv.h       | 16 ++++-----
>  drivers/staging/rtl8723bs/include/sta_info.h       |  2 +-
>  drivers/staging/rtl8723bs/include/wifi.h           | 14 ++++----
>  drivers/staging/rtl8723bs/include/wlan_bssdef.h    |  2 +-
>  drivers/tty/amiserial.c                            |  2 +-
>  drivers/tty/serial/ip22zilog.c                     |  2 +-
>  drivers/tty/serial/sunsab.c                        |  4 +--
>  drivers/tty/serial/sunzilog.c                      |  2 +-
>  drivers/video/fbdev/core/fbcon.c                   | 20 +++++------
>  drivers/video/fbdev/ffb.c                          |  2 +-
>  drivers/video/fbdev/intelfb/intelfbdrv.c           | 10 +++---
>  drivers/video/fbdev/intelfb/intelfbhw.c            |  2 +-
>  drivers/w1/masters/matrox_w1.c                     |  4 +--
>  fs/coda/coda_linux.h                               |  6 ++--
>  fs/freevxfs/vxfs_inode.c                           |  2 +-
>  fs/nfsd/nfsfh.h                                    |  4 +--
>  include/acpi/platform/acgcc.h                      |  2 +-
>  include/acpi/platform/acintel.h                    |  2 +-
>  include/asm-generic/ide_iops.h                     |  8 ++---
>  include/linux/atalk.h                              |  4 +--
>  include/linux/ceph/messenger.h                     |  2 +-
>  include/linux/compiler_types.h                     |  4 +--
>  include/linux/hdlc.h                               |  4 +--
>  include/linux/inetdevice.h                         |  8 ++---
>  include/linux/parport.h                            |  4 +--
>  include/linux/parport_pc.h                         | 22 ++++++------
>  include/net/ax25.h                                 |  2 +-
>  include/net/checksum.h                             |  2 +-
>  include/net/dn_nsp.h                               | 16 ++++-----
>  include/net/ip.h                                   |  2 +-
>  include/net/ip6_checksum.h                         |  2 +-
>  include/net/ipx.h                                  | 10 +++---
>  include/net/llc_c_ev.h                             |  4 +--
>  include/net/llc_conn.h                             |  4 +--
>  include/net/llc_s_ev.h                             |  2 +-
>  include/net/netrom.h                               |  8 ++---
>  include/net/scm.h                                  | 14 ++++----
>  include/net/udplite.h                              |  2 +-
>  include/net/x25.h                                  |  8 ++---
>  include/net/xfrm.h                                 | 18 +++++-----
>  include/uapi/linux/atm.h                           |  4 +--
>  include/uapi/linux/atmsap.h                        |  2 +-
>  include/uapi/linux/map_to_7segment.h               |  2 +-
>  include/uapi/linux/netfilter_arp/arp_tables.h      |  2 +-
>  include/uapi/linux/netfilter_bridge/ebtables.h     |  2 +-
>  include/uapi/linux/netfilter_ipv4/ip_tables.h      |  2 +-
>  include/uapi/linux/netfilter_ipv6/ip6_tables.h     |  2 +-
>  include/video/newport.h                            | 12 +++----
>  lib/zstd/mem.h                                     |  2 +-
>  net/appletalk/atalk_proc.c                         |  4 +--
>  net/appletalk/ddp.c                                |  2 +-
>  net/core/neighbour.c                               |  2 +-
>  net/core/scm.c                                     |  2 +-
>  net/decnet/dn_nsp_in.c                             |  2 +-
>  net/decnet/dn_nsp_out.c                            |  2 +-
>  net/decnet/dn_route.c                              |  2 +-
>  net/decnet/dn_table.c                              |  4 +--
>  net/ipv4/igmp.c                                    |  2 +-
>  net/ipv6/af_inet6.c                                |  2 +-
>  net/ipv6/icmp.c                                    |  4 +--
>  net/ipv6/udp.c                                     |  4 +--
>  net/lapb/lapb_iface.c                              |  4 +--
>  net/llc/llc_input.c                                |  2 +-
>  scripts/checkpatch.pl                              |  8 ++---
>  scripts/genksyms/keywords.c                        |  4 +--
>  scripts/kernel-doc                                 |  4 +--
>  sound/sparc/amd7930.c                              |  6 ++--
>  165 files changed, 547 insertions(+), 547 deletions(-)


-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kernel
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 27 Dec 2018 04:47:34 +0000
Message-ID: <CAK7LNASo5jedRxjX5zevLSmZRce3fERcFc=qn_JtJXx1Ss3iJA () mail ! gmail ! com>
--------------------
Hi Peter,


On Wed, Oct 31, 2018 at 9:58 PM Peter Zijlstra <peterz@infradead.org> wrote:
>
> On Sat, Oct 13, 2018 at 09:33:35PM +0200, Borislav Petkov wrote:
> > Ok,
> >
> > with Segher's help I've been playing with his patch ontop of bleeding
> > edge gcc 9 and here are my observations. Please double-check me for
> > booboos so that they can be addressed while there's time.
> >
> > So here's what I see ontop of 4.19-rc7:
> >
> > First marked the alternative asm() as inline and undeffed the "inline"
> > keyword. I need to do that because of the funky games we do with
> > "inline" redefinitions in include/linux/compiler_types.h.
> >
> > And Segher hinted at either doing:
> >
> > asm volatile inline(...
> >
> > or
> >
> > asm volatile __inline__(...
> >
> > but both "inline" variants are defined as macros in that file.
> >
> > Which means we either need to #undef inline before using it in asm() or
> > come up with something cleverer.
>
> # git grep -e "\<__inline__\>" | wc -l
> 488
> # git grep -e "\<__inline\>" | wc -l
> 56
> # git grep -e "\<inline\>" | wc -l
> 69598
>
> And we already have scripts/checkpatch.pl:
>
>   # Check for __inline__ and __inline, prefer inline
>
> Which suggests we do:
>
> git grep -l "\<__inline\(\|__\)\>" | while read file
> do
>         sed -i -e 's/\<__inline\(\|__\)\>/inline/g' $file
> done
>
> and get it over with.


Do you have a plan to really do this?

This is a nice cleanup anyway.

I think the last minute of MW is
a good timing for the global replacement like this.




>
> Anyway, with the below patch, I get:
>
>    text    data     bss     dec     hex filename
> 17385183        5064780 1953892 24403855        1745f8f defconfig-build/vmlinux
> 17385678        5064780 1953892 24404350        174617e defconfig-build/vmlinux
>
> Which shows we inline more (look for asm_volatile for the actual
> changes).
>
>
> So yes, this seems like something we could work with.
>
> ---
>  Documentation/trace/tracepoint-analysis.rst        |  2 +-
>  Documentation/translations/ja_JP/SubmittingPatches |  4 +--
>  Documentation/translations/zh_CN/SubmittingPatches |  4 +--
>  arch/alpha/include/asm/atomic.h                    | 12 +++----
>  arch/alpha/include/asm/bitops.h                    |  6 ++--
>  arch/alpha/include/asm/compiler.h                  |  4 +--
>  arch/alpha/include/asm/dma.h                       | 22 ++++++------
>  arch/alpha/include/asm/floppy.h                    |  4 +--
>  arch/alpha/include/asm/irq.h                       |  2 +-
>  arch/alpha/include/asm/local.h                     |  4 +--
>  arch/alpha/include/asm/smp.h                       |  2 +-
>  arch/arm/mach-iop32x/include/mach/uncompress.h     |  2 +-
>  arch/arm/mach-iop33x/include/mach/uncompress.h     |  2 +-
>  arch/arm/mach-ixp4xx/include/mach/uncompress.h     |  2 +-
>  arch/ia64/hp/common/sba_iommu.c                    |  2 +-
>  arch/ia64/hp/sim/simeth.c                          |  2 +-
>  arch/ia64/include/asm/atomic.h                     |  8 ++---
>  arch/ia64/include/asm/bitops.h                     | 34 +++++++++---------
>  arch/ia64/include/asm/delay.h                      | 14 ++++----
>  arch/ia64/include/asm/irq.h                        |  2 +-
>  arch/ia64/include/asm/page.h                       |  2 +-
>  arch/ia64/include/asm/sn/leds.h                    |  2 +-
>  arch/ia64/include/asm/uaccess.h                    |  4 +--
>  arch/ia64/include/uapi/asm/rse.h                   | 12 +++----
>  arch/ia64/include/uapi/asm/swab.h                  |  6 ++--
>  arch/ia64/oprofile/backtrace.c                     |  4 +--
>  arch/m68k/include/asm/blinken.h                    |  2 +-
>  arch/m68k/include/asm/checksum.h                   |  2 +-
>  arch/m68k/include/asm/dma.h                        | 32 ++++++++---------
>  arch/m68k/include/asm/floppy.h                     |  8 ++---
>  arch/m68k/include/asm/nettel.h                     |  8 ++---
>  arch/m68k/mac/iop.c                                | 14 ++++----
>  arch/mips/include/asm/atomic.h                     | 16 ++++-----
>  arch/mips/include/asm/checksum.h                   |  2 +-
>  arch/mips/include/asm/dma.h                        | 20 +++++------
>  arch/mips/include/asm/jazz.h                       |  2 +-
>  arch/mips/include/asm/local.h                      |  4 +--
>  arch/mips/include/asm/string.h                     |  8 ++---
>  arch/mips/kernel/binfmt_elfn32.c                   |  2 +-
>  arch/nds32/include/asm/swab.h                      |  4 +--
>  arch/parisc/include/asm/atomic.h                   | 20 +++++------
>  arch/parisc/include/asm/bitops.h                   | 18 +++++-----
>  arch/parisc/include/asm/checksum.h                 |  4 +--
>  arch/parisc/include/asm/compat.h                   |  2 +-
>  arch/parisc/include/asm/delay.h                    |  2 +-
>  arch/parisc/include/asm/dma.h                      | 20 +++++------
>  arch/parisc/include/asm/ide.h                      |  8 ++---
>  arch/parisc/include/asm/irq.h                      |  2 +-
>  arch/parisc/include/asm/spinlock.h                 | 12 +++----
>  arch/powerpc/include/asm/atomic.h                  | 40 +++++++++++-----------
>  arch/powerpc/include/asm/bitops.h                  | 28 +++++++--------
>  arch/powerpc/include/asm/dma.h                     | 20 +++++------
>  arch/powerpc/include/asm/edac.h                    |  2 +-
>  arch/powerpc/include/asm/irq.h                     |  2 +-
>  arch/powerpc/include/asm/local.h                   | 14 ++++----
>  arch/sh/include/asm/pgtable_64.h                   |  2 +-
>  arch/sh/include/asm/processor_32.h                 |  4 +--
>  arch/sh/include/cpu-sh3/cpu/dac.h                  |  6 ++--
>  arch/x86/include/asm/alternative.h                 | 14 ++++----
>  arch/x86/um/asm/checksum.h                         |  4 +--
>  arch/x86/um/asm/checksum_32.h                      |  4 +--
>  arch/xtensa/include/asm/checksum.h                 | 14 ++++----
>  arch/xtensa/include/asm/cmpxchg.h                  |  4 +--
>  arch/xtensa/include/asm/irq.h                      |  2 +-
>  block/partitions/amiga.c                           |  2 +-
>  drivers/atm/he.c                                   |  6 ++--
>  drivers/atm/idt77252.c                             |  6 ++--
>  drivers/gpu/drm/mga/mga_drv.h                      |  2 +-
>  drivers/gpu/drm/mga/mga_state.c                    | 14 ++++----
>  drivers/gpu/drm/r128/r128_drv.h                    |  2 +-
>  drivers/gpu/drm/r128/r128_state.c                  | 14 ++++----
>  drivers/gpu/drm/via/via_irq.c                      |  2 +-
>  drivers/gpu/drm/via/via_verifier.c                 | 30 ++++++++--------
>  drivers/isdn/hardware/eicon/platform.h             | 14 ++++----
>  drivers/isdn/i4l/isdn_net.c                        | 14 ++++----
>  drivers/isdn/i4l/isdn_net.h                        |  8 ++---
>  drivers/media/pci/ivtv/ivtv-ioctl.c                |  2 +-
>  drivers/net/ethernet/sun/sungem.c                  |  8 ++---
>  drivers/net/ethernet/sun/sunhme.c                  |  6 ++--
>  drivers/net/hamradio/baycom_ser_fdx.c              |  2 +-
>  drivers/net/wan/lapbether.c                        |  2 +-
>  drivers/net/wan/n2.c                               |  4 +--
>  drivers/parisc/led.c                               |  4 +--
>  drivers/parisc/sba_iommu.c                         |  2 +-
>  drivers/parport/parport_gsc.c                      |  2 +-
>  drivers/parport/parport_gsc.h                      |  4 +--
>  drivers/parport/parport_pc.c                       |  2 +-
>  drivers/scsi/lpfc/lpfc_scsi.c                      |  2 +-
>  drivers/scsi/pcmcia/sym53c500_cs.c                 |  4 +--
>  drivers/scsi/qla2xxx/qla_inline.h                  |  2 +-
>  drivers/scsi/qla2xxx/qla_os.c                      |  4 +--
>  drivers/staging/rtl8723bs/core/rtw_pwrctrl.c       |  4 +--
>  drivers/staging/rtl8723bs/core/rtw_wlan_util.c     |  2 +-
>  drivers/staging/rtl8723bs/include/drv_types.h      |  6 ++--
>  drivers/staging/rtl8723bs/include/ieee80211.h      |  6 ++--
>  drivers/staging/rtl8723bs/include/osdep_service.h  | 10 +++---
>  .../rtl8723bs/include/osdep_service_linux.h        | 14 ++++----
>  drivers/staging/rtl8723bs/include/rtw_mlme.h       | 14 ++++----
>  drivers/staging/rtl8723bs/include/rtw_recv.h       | 16 ++++-----
>  drivers/staging/rtl8723bs/include/sta_info.h       |  2 +-
>  drivers/staging/rtl8723bs/include/wifi.h           | 14 ++++----
>  drivers/staging/rtl8723bs/include/wlan_bssdef.h    |  2 +-
>  drivers/tty/amiserial.c                            |  2 +-
>  drivers/tty/serial/ip22zilog.c                     |  2 +-
>  drivers/tty/serial/sunsab.c                        |  4 +--
>  drivers/tty/serial/sunzilog.c                      |  2 +-
>  drivers/video/fbdev/core/fbcon.c                   | 20 +++++------
>  drivers/video/fbdev/ffb.c                          |  2 +-
>  drivers/video/fbdev/intelfb/intelfbdrv.c           | 10 +++---
>  drivers/video/fbdev/intelfb/intelfbhw.c            |  2 +-
>  drivers/w1/masters/matrox_w1.c                     |  4 +--
>  fs/coda/coda_linux.h                               |  6 ++--
>  fs/freevxfs/vxfs_inode.c                           |  2 +-
>  fs/nfsd/nfsfh.h                                    |  4 +--
>  include/acpi/platform/acgcc.h                      |  2 +-
>  include/acpi/platform/acintel.h                    |  2 +-
>  include/asm-generic/ide_iops.h                     |  8 ++---
>  include/linux/atalk.h                              |  4 +--
>  include/linux/ceph/messenger.h                     |  2 +-
>  include/linux/compiler_types.h                     |  4 +--
>  include/linux/hdlc.h                               |  4 +--
>  include/linux/inetdevice.h                         |  8 ++---
>  include/linux/parport.h                            |  4 +--
>  include/linux/parport_pc.h                         | 22 ++++++------
>  include/net/ax25.h                                 |  2 +-
>  include/net/checksum.h                             |  2 +-
>  include/net/dn_nsp.h                               | 16 ++++-----
>  include/net/ip.h                                   |  2 +-
>  include/net/ip6_checksum.h                         |  2 +-
>  include/net/ipx.h                                  | 10 +++---
>  include/net/llc_c_ev.h                             |  4 +--
>  include/net/llc_conn.h                             |  4 +--
>  include/net/llc_s_ev.h                             |  2 +-
>  include/net/netrom.h                               |  8 ++---
>  include/net/scm.h                                  | 14 ++++----
>  include/net/udplite.h                              |  2 +-
>  include/net/x25.h                                  |  8 ++---
>  include/net/xfrm.h                                 | 18 +++++-----
>  include/uapi/linux/atm.h                           |  4 +--
>  include/uapi/linux/atmsap.h                        |  2 +-
>  include/uapi/linux/map_to_7segment.h               |  2 +-
>  include/uapi/linux/netfilter_arp/arp_tables.h      |  2 +-
>  include/uapi/linux/netfilter_bridge/ebtables.h     |  2 +-
>  include/uapi/linux/netfilter_ipv4/ip_tables.h      |  2 +-
>  include/uapi/linux/netfilter_ipv6/ip6_tables.h     |  2 +-
>  include/video/newport.h                            | 12 +++----
>  lib/zstd/mem.h                                     |  2 +-
>  net/appletalk/atalk_proc.c                         |  4 +--
>  net/appletalk/ddp.c                                |  2 +-
>  net/core/neighbour.c                               |  2 +-
>  net/core/scm.c                                     |  2 +-
>  net/decnet/dn_nsp_in.c                             |  2 +-
>  net/decnet/dn_nsp_out.c                            |  2 +-
>  net/decnet/dn_route.c                              |  2 +-
>  net/decnet/dn_table.c                              |  4 +--
>  net/ipv4/igmp.c                                    |  2 +-
>  net/ipv6/af_inet6.c                                |  2 +-
>  net/ipv6/icmp.c                                    |  4 +--
>  net/ipv6/udp.c                                     |  4 +--
>  net/lapb/lapb_iface.c                              |  4 +--
>  net/llc/llc_input.c                                |  2 +-
>  scripts/checkpatch.pl                              |  8 ++---
>  scripts/genksyms/keywords.c                        |  4 +--
>  scripts/kernel-doc                                 |  4 +--
>  sound/sparc/amd7930.c                              |  6 ++--
>  165 files changed, 547 insertions(+), 547 deletions(-)


-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-virtualization
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 27 Dec 2018 04:47:34 +0000
Message-ID: <CAK7LNASo5jedRxjX5zevLSmZRce3fERcFc=qn_JtJXx1Ss3iJA () mail ! gmail ! com>
--------------------
Hi Peter,


On Wed, Oct 31, 2018 at 9:58 PM Peter Zijlstra <peterz@infradead.org> wrote:
>
> On Sat, Oct 13, 2018 at 09:33:35PM +0200, Borislav Petkov wrote:
> > Ok,
> >
> > with Segher's help I've been playing with his patch ontop of bleeding
> > edge gcc 9 and here are my observations. Please double-check me for
> > booboos so that they can be addressed while there's time.
> >
> > So here's what I see ontop of 4.19-rc7:
> >
> > First marked the alternative asm() as inline and undeffed the "inline"
> > keyword. I need to do that because of the funky games we do with
> > "inline" redefinitions in include/linux/compiler_types.h.
> >
> > And Segher hinted at either doing:
> >
> > asm volatile inline(...
> >
> > or
> >
> > asm volatile __inline__(...
> >
> > but both "inline" variants are defined as macros in that file.
> >
> > Which means we either need to #undef inline before using it in asm() or
> > come up with something cleverer.
>
> # git grep -e "\<__inline__\>" | wc -l
> 488
> # git grep -e "\<__inline\>" | wc -l
> 56
> # git grep -e "\<inline\>" | wc -l
> 69598
>
> And we already have scripts/checkpatch.pl:
>
>   # Check for __inline__ and __inline, prefer inline
>
> Which suggests we do:
>
> git grep -l "\<__inline\(\|__\)\>" | while read file
> do
>         sed -i -e 's/\<__inline\(\|__\)\>/inline/g' $file
> done
>
> and get it over with.


Do you have a plan to really do this?

This is a nice cleanup anyway.

I think the last minute of MW is
a good timing for the global replacement like this.




>
> Anyway, with the below patch, I get:
>
>    text    data     bss     dec     hex filename
> 17385183        5064780 1953892 24403855        1745f8f defconfig-build/vmlinux
> 17385678        5064780 1953892 24404350        174617e defconfig-build/vmlinux
>
> Which shows we inline more (look for asm_volatile for the actual
> changes).
>
>
> So yes, this seems like something we could work with.
>
> ---
>  Documentation/trace/tracepoint-analysis.rst        |  2 +-
>  Documentation/translations/ja_JP/SubmittingPatches |  4 +--
>  Documentation/translations/zh_CN/SubmittingPatches |  4 +--
>  arch/alpha/include/asm/atomic.h                    | 12 +++----
>  arch/alpha/include/asm/bitops.h                    |  6 ++--
>  arch/alpha/include/asm/compiler.h                  |  4 +--
>  arch/alpha/include/asm/dma.h                       | 22 ++++++------
>  arch/alpha/include/asm/floppy.h                    |  4 +--
>  arch/alpha/include/asm/irq.h                       |  2 +-
>  arch/alpha/include/asm/local.h                     |  4 +--
>  arch/alpha/include/asm/smp.h                       |  2 +-
>  arch/arm/mach-iop32x/include/mach/uncompress.h     |  2 +-
>  arch/arm/mach-iop33x/include/mach/uncompress.h     |  2 +-
>  arch/arm/mach-ixp4xx/include/mach/uncompress.h     |  2 +-
>  arch/ia64/hp/common/sba_iommu.c                    |  2 +-
>  arch/ia64/hp/sim/simeth.c                          |  2 +-
>  arch/ia64/include/asm/atomic.h                     |  8 ++---
>  arch/ia64/include/asm/bitops.h                     | 34 +++++++++---------
>  arch/ia64/include/asm/delay.h                      | 14 ++++----
>  arch/ia64/include/asm/irq.h                        |  2 +-
>  arch/ia64/include/asm/page.h                       |  2 +-
>  arch/ia64/include/asm/sn/leds.h                    |  2 +-
>  arch/ia64/include/asm/uaccess.h                    |  4 +--
>  arch/ia64/include/uapi/asm/rse.h                   | 12 +++----
>  arch/ia64/include/uapi/asm/swab.h                  |  6 ++--
>  arch/ia64/oprofile/backtrace.c                     |  4 +--
>  arch/m68k/include/asm/blinken.h                    |  2 +-
>  arch/m68k/include/asm/checksum.h                   |  2 +-
>  arch/m68k/include/asm/dma.h                        | 32 ++++++++---------
>  arch/m68k/include/asm/floppy.h                     |  8 ++---
>  arch/m68k/include/asm/nettel.h                     |  8 ++---
>  arch/m68k/mac/iop.c                                | 14 ++++----
>  arch/mips/include/asm/atomic.h                     | 16 ++++-----
>  arch/mips/include/asm/checksum.h                   |  2 +-
>  arch/mips/include/asm/dma.h                        | 20 +++++------
>  arch/mips/include/asm/jazz.h                       |  2 +-
>  arch/mips/include/asm/local.h                      |  4 +--
>  arch/mips/include/asm/string.h                     |  8 ++---
>  arch/mips/kernel/binfmt_elfn32.c                   |  2 +-
>  arch/nds32/include/asm/swab.h                      |  4 +--
>  arch/parisc/include/asm/atomic.h                   | 20 +++++------
>  arch/parisc/include/asm/bitops.h                   | 18 +++++-----
>  arch/parisc/include/asm/checksum.h                 |  4 +--
>  arch/parisc/include/asm/compat.h                   |  2 +-
>  arch/parisc/include/asm/delay.h                    |  2 +-
>  arch/parisc/include/asm/dma.h                      | 20 +++++------
>  arch/parisc/include/asm/ide.h                      |  8 ++---
>  arch/parisc/include/asm/irq.h                      |  2 +-
>  arch/parisc/include/asm/spinlock.h                 | 12 +++----
>  arch/powerpc/include/asm/atomic.h                  | 40 +++++++++++-----------
>  arch/powerpc/include/asm/bitops.h                  | 28 +++++++--------
>  arch/powerpc/include/asm/dma.h                     | 20 +++++------
>  arch/powerpc/include/asm/edac.h                    |  2 +-
>  arch/powerpc/include/asm/irq.h                     |  2 +-
>  arch/powerpc/include/asm/local.h                   | 14 ++++----
>  arch/sh/include/asm/pgtable_64.h                   |  2 +-
>  arch/sh/include/asm/processor_32.h                 |  4 +--
>  arch/sh/include/cpu-sh3/cpu/dac.h                  |  6 ++--
>  arch/x86/include/asm/alternative.h                 | 14 ++++----
>  arch/x86/um/asm/checksum.h                         |  4 +--
>  arch/x86/um/asm/checksum_32.h                      |  4 +--
>  arch/xtensa/include/asm/checksum.h                 | 14 ++++----
>  arch/xtensa/include/asm/cmpxchg.h                  |  4 +--
>  arch/xtensa/include/asm/irq.h                      |  2 +-
>  block/partitions/amiga.c                           |  2 +-
>  drivers/atm/he.c                                   |  6 ++--
>  drivers/atm/idt77252.c                             |  6 ++--
>  drivers/gpu/drm/mga/mga_drv.h                      |  2 +-
>  drivers/gpu/drm/mga/mga_state.c                    | 14 ++++----
>  drivers/gpu/drm/r128/r128_drv.h                    |  2 +-
>  drivers/gpu/drm/r128/r128_state.c                  | 14 ++++----
>  drivers/gpu/drm/via/via_irq.c                      |  2 +-
>  drivers/gpu/drm/via/via_verifier.c                 | 30 ++++++++--------
>  drivers/isdn/hardware/eicon/platform.h             | 14 ++++----
>  drivers/isdn/i4l/isdn_net.c                        | 14 ++++----
>  drivers/isdn/i4l/isdn_net.h                        |  8 ++---
>  drivers/media/pci/ivtv/ivtv-ioctl.c                |  2 +-
>  drivers/net/ethernet/sun/sungem.c                  |  8 ++---
>  drivers/net/ethernet/sun/sunhme.c                  |  6 ++--
>  drivers/net/hamradio/baycom_ser_fdx.c              |  2 +-
>  drivers/net/wan/lapbether.c                        |  2 +-
>  drivers/net/wan/n2.c                               |  4 +--
>  drivers/parisc/led.c                               |  4 +--
>  drivers/parisc/sba_iommu.c                         |  2 +-
>  drivers/parport/parport_gsc.c                      |  2 +-
>  drivers/parport/parport_gsc.h                      |  4 +--
>  drivers/parport/parport_pc.c                       |  2 +-
>  drivers/scsi/lpfc/lpfc_scsi.c                      |  2 +-
>  drivers/scsi/pcmcia/sym53c500_cs.c                 |  4 +--
>  drivers/scsi/qla2xxx/qla_inline.h                  |  2 +-
>  drivers/scsi/qla2xxx/qla_os.c                      |  4 +--
>  drivers/staging/rtl8723bs/core/rtw_pwrctrl.c       |  4 +--
>  drivers/staging/rtl8723bs/core/rtw_wlan_util.c     |  2 +-
>  drivers/staging/rtl8723bs/include/drv_types.h      |  6 ++--
>  drivers/staging/rtl8723bs/include/ieee80211.h      |  6 ++--
>  drivers/staging/rtl8723bs/include/osdep_service.h  | 10 +++---
>  .../rtl8723bs/include/osdep_service_linux.h        | 14 ++++----
>  drivers/staging/rtl8723bs/include/rtw_mlme.h       | 14 ++++----
>  drivers/staging/rtl8723bs/include/rtw_recv.h       | 16 ++++-----
>  drivers/staging/rtl8723bs/include/sta_info.h       |  2 +-
>  drivers/staging/rtl8723bs/include/wifi.h           | 14 ++++----
>  drivers/staging/rtl8723bs/include/wlan_bssdef.h    |  2 +-
>  drivers/tty/amiserial.c                            |  2 +-
>  drivers/tty/serial/ip22zilog.c                     |  2 +-
>  drivers/tty/serial/sunsab.c                        |  4 +--
>  drivers/tty/serial/sunzilog.c                      |  2 +-
>  drivers/video/fbdev/core/fbcon.c                   | 20 +++++------
>  drivers/video/fbdev/ffb.c                          |  2 +-
>  drivers/video/fbdev/intelfb/intelfbdrv.c           | 10 +++---
>  drivers/video/fbdev/intelfb/intelfbhw.c            |  2 +-
>  drivers/w1/masters/matrox_w1.c                     |  4 +--
>  fs/coda/coda_linux.h                               |  6 ++--
>  fs/freevxfs/vxfs_inode.c                           |  2 +-
>  fs/nfsd/nfsfh.h                                    |  4 +--
>  include/acpi/platform/acgcc.h                      |  2 +-
>  include/acpi/platform/acintel.h                    |  2 +-
>  include/asm-generic/ide_iops.h                     |  8 ++---
>  include/linux/atalk.h                              |  4 +--
>  include/linux/ceph/messenger.h                     |  2 +-
>  include/linux/compiler_types.h                     |  4 +--
>  include/linux/hdlc.h                               |  4 +--
>  include/linux/inetdevice.h                         |  8 ++---
>  include/linux/parport.h                            |  4 +--
>  include/linux/parport_pc.h                         | 22 ++++++------
>  include/net/ax25.h                                 |  2 +-
>  include/net/checksum.h                             |  2 +-
>  include/net/dn_nsp.h                               | 16 ++++-----
>  include/net/ip.h                                   |  2 +-
>  include/net/ip6_checksum.h                         |  2 +-
>  include/net/ipx.h                                  | 10 +++---
>  include/net/llc_c_ev.h                             |  4 +--
>  include/net/llc_conn.h                             |  4 +--
>  include/net/llc_s_ev.h                             |  2 +-
>  include/net/netrom.h                               |  8 ++---
>  include/net/scm.h                                  | 14 ++++----
>  include/net/udplite.h                              |  2 +-
>  include/net/x25.h                                  |  8 ++---
>  include/net/xfrm.h                                 | 18 +++++-----
>  include/uapi/linux/atm.h                           |  4 +--
>  include/uapi/linux/atmsap.h                        |  2 +-
>  include/uapi/linux/map_to_7segment.h               |  2 +-
>  include/uapi/linux/netfilter_arp/arp_tables.h      |  2 +-
>  include/uapi/linux/netfilter_bridge/ebtables.h     |  2 +-
>  include/uapi/linux/netfilter_ipv4/ip_tables.h      |  2 +-
>  include/uapi/linux/netfilter_ipv6/ip6_tables.h     |  2 +-
>  include/video/newport.h                            | 12 +++----
>  lib/zstd/mem.h                                     |  2 +-
>  net/appletalk/atalk_proc.c                         |  4 +--
>  net/appletalk/ddp.c                                |  2 +-
>  net/core/neighbour.c                               |  2 +-
>  net/core/scm.c                                     |  2 +-
>  net/decnet/dn_nsp_in.c                             |  2 +-
>  net/decnet/dn_nsp_out.c                            |  2 +-
>  net/decnet/dn_route.c                              |  2 +-
>  net/decnet/dn_table.c                              |  4 +--
>  net/ipv4/igmp.c                                    |  2 +-
>  net/ipv6/af_inet6.c                                |  2 +-
>  net/ipv6/icmp.c                                    |  4 +--
>  net/ipv6/udp.c                                     |  4 +--
>  net/lapb/lapb_iface.c                              |  4 +--
>  net/llc/llc_input.c                                |  2 +-
>  scripts/checkpatch.pl                              |  8 ++---
>  scripts/genksyms/keywords.c                        |  4 +--
>  scripts/kernel-doc                                 |  4 +--
>  sound/sparc/amd7930.c                              |  6 ++--
>  165 files changed, 547 insertions(+), 547 deletions(-)


-- 
Best Regards
Masahiro Yamada
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: PROPOSAL: Extend inline asm syntax with size spec
Date: Thu, 27 Dec 2018 04:47:34 +0000
Message-ID: <CAK7LNASo5jedRxjX5zevLSmZRce3fERcFc=qn_JtJXx1Ss3iJA () mail ! gmail ! com>
--------------------
Hi Peter,


On Wed, Oct 31, 2018 at 9:58 PM Peter Zijlstra <peterz@infradead.org> wrote:
>
> On Sat, Oct 13, 2018 at 09:33:35PM +0200, Borislav Petkov wrote:
> > Ok,
> >
> > with Segher's help I've been playing with his patch ontop of bleeding
> > edge gcc 9 and here are my observations. Please double-check me for
> > booboos so that they can be addressed while there's time.
> >
> > So here's what I see ontop of 4.19-rc7:
> >
> > First marked the alternative asm() as inline and undeffed the "inline"
> > keyword. I need to do that because of the funky games we do with
> > "inline" redefinitions in include/linux/compiler_types.h.
> >
> > And Segher hinted at either doing:
> >
> > asm volatile inline(...
> >
> > or
> >
> > asm volatile __inline__(...
> >
> > but both "inline" variants are defined as macros in that file.
> >
> > Which means we either need to #undef inline before using it in asm() or
> > come up with something cleverer.
>
> # git grep -e "\<__inline__\>" | wc -l
> 488
> # git grep -e "\<__inline\>" | wc -l
> 56
> # git grep -e "\<inline\>" | wc -l
> 69598
>
> And we already have scripts/checkpatch.pl:
>
>   # Check for __inline__ and __inline, prefer inline
>
> Which suggests we do:
>
> git grep -l "\<__inline\(\|__\)\>" | while read file
> do
>         sed -i -e 's/\<__inline\(\|__\)\>/inline/g' $file
> done
>
> and get it over with.


Do you have a plan to really do this?

This is a nice cleanup anyway.

I think the last minute of MW is
a good timing for the global replacement like this.




>
> Anyway, with the below patch, I get:
>
>    text    data     bss     dec     hex filename
> 17385183        5064780 1953892 24403855        1745f8f defconfig-build/vmlinux
> 17385678        5064780 1953892 24404350        174617e defconfig-build/vmlinux
>
> Which shows we inline more (look for asm_volatile for the actual
> changes).
>
>
> So yes, this seems like something we could work with.
>
> ---
>  Documentation/trace/tracepoint-analysis.rst        |  2 +-
>  Documentation/translations/ja_JP/SubmittingPatches |  4 +--
>  Documentation/translations/zh_CN/SubmittingPatches |  4 +--
>  arch/alpha/include/asm/atomic.h                    | 12 +++----
>  arch/alpha/include/asm/bitops.h                    |  6 ++--
>  arch/alpha/include/asm/compiler.h                  |  4 +--
>  arch/alpha/include/asm/dma.h                       | 22 ++++++------
>  arch/alpha/include/asm/floppy.h                    |  4 +--
>  arch/alpha/include/asm/irq.h                       |  2 +-
>  arch/alpha/include/asm/local.h                     |  4 +--
>  arch/alpha/include/asm/smp.h                       |  2 +-
>  arch/arm/mach-iop32x/include/mach/uncompress.h     |  2 +-
>  arch/arm/mach-iop33x/include/mach/uncompress.h     |  2 +-
>  arch/arm/mach-ixp4xx/include/mach/uncompress.h     |  2 +-
>  arch/ia64/hp/common/sba_iommu.c                    |  2 +-
>  arch/ia64/hp/sim/simeth.c                          |  2 +-
>  arch/ia64/include/asm/atomic.h                     |  8 ++---
>  arch/ia64/include/asm/bitops.h                     | 34 +++++++++---------
>  arch/ia64/include/asm/delay.h                      | 14 ++++----
>  arch/ia64/include/asm/irq.h                        |  2 +-
>  arch/ia64/include/asm/page.h                       |  2 +-
>  arch/ia64/include/asm/sn/leds.h                    |  2 +-
>  arch/ia64/include/asm/uaccess.h                    |  4 +--
>  arch/ia64/include/uapi/asm/rse.h                   | 12 +++----
>  arch/ia64/include/uapi/asm/swab.h                  |  6 ++--
>  arch/ia64/oprofile/backtrace.c                     |  4 +--
>  arch/m68k/include/asm/blinken.h                    |  2 +-
>  arch/m68k/include/asm/checksum.h                   |  2 +-
>  arch/m68k/include/asm/dma.h                        | 32 ++++++++---------
>  arch/m68k/include/asm/floppy.h                     |  8 ++---
>  arch/m68k/include/asm/nettel.h                     |  8 ++---
>  arch/m68k/mac/iop.c                                | 14 ++++----
>  arch/mips/include/asm/atomic.h                     | 16 ++++-----
>  arch/mips/include/asm/checksum.h                   |  2 +-
>  arch/mips/include/asm/dma.h                        | 20 +++++------
>  arch/mips/include/asm/jazz.h                       |  2 +-
>  arch/mips/include/asm/local.h                      |  4 +--
>  arch/mips/include/asm/string.h                     |  8 ++---
>  arch/mips/kernel/binfmt_elfn32.c                   |  2 +-
>  arch/nds32/include/asm/swab.h                      |  4 +--
>  arch/parisc/include/asm/atomic.h                   | 20 +++++------
>  arch/parisc/include/asm/bitops.h                   | 18 +++++-----
>  arch/parisc/include/asm/checksum.h                 |  4 +--
>  arch/parisc/include/asm/compat.h                   |  2 +-
>  arch/parisc/include/asm/delay.h                    |  2 +-
>  arch/parisc/include/asm/dma.h                      | 20 +++++------
>  arch/parisc/include/asm/ide.h                      |  8 ++---
>  arch/parisc/include/asm/irq.h                      |  2 +-
>  arch/parisc/include/asm/spinlock.h                 | 12 +++----
>  arch/powerpc/include/asm/atomic.h                  | 40 +++++++++++-----------
>  arch/powerpc/include/asm/bitops.h                  | 28 +++++++--------
>  arch/powerpc/include/asm/dma.h                     | 20 +++++------
>  arch/powerpc/include/asm/edac.h                    |  2 +-
>  arch/powerpc/include/asm/irq.h                     |  2 +-
>  arch/powerpc/include/asm/local.h                   | 14 ++++----
>  arch/sh/include/asm/pgtable_64.h                   |  2 +-
>  arch/sh/include/asm/processor_32.h                 |  4 +--
>  arch/sh/include/cpu-sh3/cpu/dac.h                  |  6 ++--
>  arch/x86/include/asm/alternative.h                 | 14 ++++----
>  arch/x86/um/asm/checksum.h                         |  4 +--
>  arch/x86/um/asm/checksum_32.h                      |  4 +--
>  arch/xtensa/include/asm/checksum.h                 | 14 ++++----
>  arch/xtensa/include/asm/cmpxchg.h                  |  4 +--
>  arch/xtensa/include/asm/irq.h                      |  2 +-
>  block/partitions/amiga.c                           |  2 +-
>  drivers/atm/he.c                                   |  6 ++--
>  drivers/atm/idt77252.c                             |  6 ++--
>  drivers/gpu/drm/mga/mga_drv.h                      |  2 +-
>  drivers/gpu/drm/mga/mga_state.c                    | 14 ++++----
>  drivers/gpu/drm/r128/r128_drv.h                    |  2 +-
>  drivers/gpu/drm/r128/r128_state.c                  | 14 ++++----
>  drivers/gpu/drm/via/via_irq.c                      |  2 +-
>  drivers/gpu/drm/via/via_verifier.c                 | 30 ++++++++--------
>  drivers/isdn/hardware/eicon/platform.h             | 14 ++++----
>  drivers/isdn/i4l/isdn_net.c                        | 14 ++++----
>  drivers/isdn/i4l/isdn_net.h                        |  8 ++---
>  drivers/media/pci/ivtv/ivtv-ioctl.c                |  2 +-
>  drivers/net/ethernet/sun/sungem.c                  |  8 ++---
>  drivers/net/ethernet/sun/sunhme.c                  |  6 ++--
>  drivers/net/hamradio/baycom_ser_fdx.c              |  2 +-
>  drivers/net/wan/lapbether.c                        |  2 +-
>  drivers/net/wan/n2.c                               |  4 +--
>  drivers/parisc/led.c                               |  4 +--
>  drivers/parisc/sba_iommu.c                         |  2 +-
>  drivers/parport/parport_gsc.c                      |  2 +-
>  drivers/parport/parport_gsc.h                      |  4 +--
>  drivers/parport/parport_pc.c                       |  2 +-
>  drivers/scsi/lpfc/lpfc_scsi.c                      |  2 +-
>  drivers/scsi/pcmcia/sym53c500_cs.c                 |  4 +--
>  drivers/scsi/qla2xxx/qla_inline.h                  |  2 +-
>  drivers/scsi/qla2xxx/qla_os.c                      |  4 +--
>  drivers/staging/rtl8723bs/core/rtw_pwrctrl.c       |  4 +--
>  drivers/staging/rtl8723bs/core/rtw_wlan_util.c     |  2 +-
>  drivers/staging/rtl8723bs/include/drv_types.h      |  6 ++--
>  drivers/staging/rtl8723bs/include/ieee80211.h      |  6 ++--
>  drivers/staging/rtl8723bs/include/osdep_service.h  | 10 +++---
>  .../rtl8723bs/include/osdep_service_linux.h        | 14 ++++----
>  drivers/staging/rtl8723bs/include/rtw_mlme.h       | 14 ++++----
>  drivers/staging/rtl8723bs/include/rtw_recv.h       | 16 ++++-----
>  drivers/staging/rtl8723bs/include/sta_info.h       |  2 +-
>  drivers/staging/rtl8723bs/include/wifi.h           | 14 ++++----
>  drivers/staging/rtl8723bs/include/wlan_bssdef.h    |  2 +-
>  drivers/tty/amiserial.c                            |  2 +-
>  drivers/tty/serial/ip22zilog.c                     |  2 +-
>  drivers/tty/serial/sunsab.c                        |  4 +--
>  drivers/tty/serial/sunzilog.c                      |  2 +-
>  drivers/video/fbdev/core/fbcon.c                   | 20 +++++------
>  drivers/video/fbdev/ffb.c                          |  2 +-
>  drivers/video/fbdev/intelfb/intelfbdrv.c           | 10 +++---
>  drivers/video/fbdev/intelfb/intelfbhw.c            |  2 +-
>  drivers/w1/masters/matrox_w1.c                     |  4 +--
>  fs/coda/coda_linux.h                               |  6 ++--
>  fs/freevxfs/vxfs_inode.c                           |  2 +-
>  fs/nfsd/nfsfh.h                                    |  4 +--
>  include/acpi/platform/acgcc.h                      |  2 +-
>  include/acpi/platform/acintel.h                    |  2 +-
>  include/asm-generic/ide_iops.h                     |  8 ++---
>  include/linux/atalk.h                              |  4 +--
>  include/linux/ceph/messenger.h                     |  2 +-
>  include/linux/compiler_types.h                     |  4 +--
>  include/linux/hdlc.h                               |  4 +--
>  include/linux/inetdevice.h                         |  8 ++---
>  include/linux/parport.h                            |  4 +--
>  include/linux/parport_pc.h                         | 22 ++++++------
>  include/net/ax25.h                                 |  2 +-
>  include/net/checksum.h                             |  2 +-
>  include/net/dn_nsp.h                               | 16 ++++-----
>  include/net/ip.h                                   |  2 +-
>  include/net/ip6_checksum.h                         |  2 +-
>  include/net/ipx.h                                  | 10 +++---
>  include/net/llc_c_ev.h                             |  4 +--
>  include/net/llc_conn.h                             |  4 +--
>  include/net/llc_s_ev.h                             |  2 +-
>  include/net/netrom.h                               |  8 ++---
>  include/net/scm.h                                  | 14 ++++----
>  include/net/udplite.h                              |  2 +-
>  include/net/x25.h                                  |  8 ++---
>  include/net/xfrm.h                                 | 18 +++++-----
>  include/uapi/linux/atm.h                           |  4 +--
>  include/uapi/linux/atmsap.h                        |  2 +-
>  include/uapi/linux/map_to_7segment.h               |  2 +-
>  include/uapi/linux/netfilter_arp/arp_tables.h      |  2 +-
>  include/uapi/linux/netfilter_bridge/ebtables.h     |  2 +-
>  include/uapi/linux/netfilter_ipv4/ip_tables.h      |  2 +-
>  include/uapi/linux/netfilter_ipv6/ip6_tables.h     |  2 +-
>  include/video/newport.h                            | 12 +++----
>  lib/zstd/mem.h                                     |  2 +-
>  net/appletalk/atalk_proc.c                         |  4 +--
>  net/appletalk/ddp.c                                |  2 +-
>  net/core/neighbour.c                               |  2 +-
>  net/core/scm.c                                     |  2 +-
>  net/decnet/dn_nsp_in.c                             |  2 +-
>  net/decnet/dn_nsp_out.c                            |  2 +-
>  net/decnet/dn_route.c                              |  2 +-
>  net/decnet/dn_table.c                              |  4 +--
>  net/ipv4/igmp.c                                    |  2 +-
>  net/ipv6/af_inet6.c                                |  2 +-
>  net/ipv6/icmp.c                                    |  4 +--
>  net/ipv6/udp.c                                     |  4 +--
>  net/lapb/lapb_iface.c                              |  4 +--
>  net/llc/llc_input.c                                |  2 +-
>  scripts/checkpatch.pl                              |  8 ++---
>  scripts/genksyms/keywords.c                        |  4 +--
>  scripts/kernel-doc                                 |  4 +--
>  sound/sparc/amd7930.c                              |  6 ++--
>  165 files changed, 547 insertions(+), 547 deletions(-)


-- 
Best Regards
Masahiro Yamada
================================================================================


################################################################################

=== Thread: RFC: adding attribute(format) to check printf formatting ===

From: Ben Dooks <ben.dooks () codethink ! co ! uk>
To: linux-sparse
Subject: RFC: adding attribute(format) to check printf formatting
Date: Fri, 26 Oct 2018 15:26:29 +0000
Message-ID: <20181026152632.30318-1-ben.dooks () codethink ! co ! uk>
--------------------
This is a RFC on the series for dealing with format checking in
variadic functions. It isn't complete, but mostly works for the
test cases I've got locally.

I'll add some proper validation code later if people think this
is a useful addition.


================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: RFC: adding attribute(format) to check printf formatting
Date: Fri, 26 Oct 2018 21:06:28 +0000
Message-ID: <20181026210626.cucavsyee2xfhyyn () ltop ! local>
--------------------
On Fri, Oct 26, 2018 at 04:26:29PM +0100, Ben Dooks wrote:
> This is a RFC on the series for dealing with format checking in
> variadic functions. It isn't complete, but mostly works for the
> test cases I've got locally.

Thanks, It looks already quite good.
I've added the few remarks I have in the patches.

> I'll add some proper validation code later if people think this
> is a useful addition.

Oh yes, it's really useful.
There is some doc for the testsuite in Documentation/test-suite.rst
also available online at:
	https://sparse-doc.readthedocs.io/en/master/test-suite.html

-- Luc
================================================================================


################################################################################

=== Thread: RFCv2 - support for printf format parsing ===

From: Ben Dooks <ben.dooks () codethink ! co ! uk>
To: linux-sparse
Subject: RFCv2 - support for printf format parsing
Date: Mon, 29 Oct 2018 15:39:47 +0000
Message-ID: <20181029153952.13927-1-ben.dooks () codethink ! co ! uk>
--------------------
This is an updated set with most of the comments now fixed, but
a few issues that could do with looking at before finalising the
code.

1- the ctype fix doesn't work for prefixed __attrbitue__
2- do we need an 'ASN-any' type for pointers?

(the asn-any is for the case printf %p, where the value of the
 pointer is printed, and it is not de-refrenced)... maybe using
 asn:-1 for "any address space" is a good idea?

The only things to discuss:

- is the printf format parsing good enough?
- do we need to add a a __attribute__((linux-printk)) for printk fmts?
- should we add a -Wvariadic-format to force address-space safety?

I think this is getting close to merging.


================================================================================

From: Ben Dooks <ben.dooks () codethink ! co ! uk>
To: linux-sparse
Subject: Re: RFCv2 - support for printf format parsing
Date: Mon, 29 Oct 2018 15:44:52 +0000
Message-ID: <7a3e057d-f654-56fd-7cd0-4f3259576dd0 () codethink ! co ! uk>
--------------------
On 29/10/18 15:39, Ben Dooks wrote:
> This is an updated set with most of the comments now fixed, but
> a few issues that could do with looking at before finalising the
> code.
> 
> 1- the ctype fix doesn't work for prefixed __attrbitue__
> 2- do we need an 'ASN-any' type for pointers?
> 
> (the asn-any is for the case printf %p, where the value of the
>   pointer is printed, and it is not de-refrenced)... maybe using
>   asn:-1 for "any address space" is a good idea?
> 
> The only things to discuss:
> 
> - is the printf format parsing good enough?
> - do we need to add a a __attribute__((linux-printk)) for printk fmts?
> - should we add a -Wvariadic-format to force address-space safety?
> 
> I think this is getting close to merging.

Grr, just noticed a couple of missed formatting changes and an
error in patch merging. I'll sort those out now and start on the
tests.

-- 
Ben Dooks				http://www.codethink.co.uk/
Senior Engineer				Codethink - Providing Genius

https://www.codethink.co.uk/privacy.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: RFCv2 - support for printf format parsing
Date: Mon, 29 Oct 2018 22:08:36 +0000
Message-ID: <20181029220835.lo43yvmibaeksz7a () ltop ! local>
--------------------
On Mon, Oct 29, 2018 at 03:39:47PM +0000, Ben Dooks wrote:
> This is an updated set with most of the comments now fixed, but
> a few issues that could do with looking at before finalising the
> code.
> 
> 1- the ctype fix doesn't work for prefixed __attrbitue__
See my reply in patch 4/5.

> 2- do we need an 'ASN-any' type for pointers?
I don't think so (see also patch 4/5).
 
> (the asn-any is for the case printf %p, where the value of the
>  pointer is printed, and it is not de-refrenced)... maybe using
>  asn:-1 for "any address space" is a good idea?
> 
> The only things to discuss:
> 
> - is the printf format parsing good enough?
I think so, certainly as a first step. Maybe just also handle "%i".
See also my suggestion about using a table.

> - do we need to add a a __attribute__((linux-printk)) for printk fmts?
Good question. I don't think so because in the kernel printk() itself
is declared as using __attribute__((format(printf, 1, 2))).
But a mecanism will be needed in the kernel headers to specifiy
the extended conversion format and their corresponding types.
I was thinking something like:
#pragma extended_printf_formar "%pI4[bl]" const struct sockaddr_in *
but:
* it's just a suggestion
* I'm sure some people will hate to use #pragma
* sparse doesn't have real support of #pragma (but I have some
  code for it).
 
> - should we add a -Wvariadic-format to force address-space safety?
Probably (but with another name).
In shorter term, the warning givi-en by this series should be
conditionalized on -Wformat.

> I think this is getting close to merging.
Yes, it progress well and I'm all in favor to merge early the
basic functionalities. At this stage, what's missing the most
is some testcases. I also really think that the type checking
can't be done by simply using a type (as returned by
evaluate_printf_symbol()), a checking function is needed or
at least use your own generic function instead of trying
do let compatible_argument_type() do this check.

Kind regards,
-- Luc
================================================================================

From: Ben Dooks <ben.dooks () codethink ! co ! uk>
To: linux-sparse
Subject: Re: RFCv2 - support for printf format parsing
Date: Tue, 30 Oct 2018 11:06:03 +0000
Message-ID: <86a8ceeb-cbe2-8207-ca47-a332da47e7c6 () codethink ! co ! uk>
--------------------
On 29/10/18 22:08, Luc Van Oostenryck wrote:
> On Mon, Oct 29, 2018 at 03:39:47PM +0000, Ben Dooks wrote:
>> This is an updated set with most of the comments now fixed, but
>> a few issues that could do with looking at before finalising the
>> code.
>>
>> 1- the ctype fix doesn't work for prefixed __attrbitue__
> See my reply in patch 4/5.
> 
>> 2- do we need an 'ASN-any' type for pointers?
> I don't think so (see also patch 4/5).
>   
>> (the asn-any is for the case printf %p, where the value of the
>>   pointer is printed, and it is not de-refrenced)... maybe using
>>   asn:-1 for "any address space" is a good idea?
>>
>> The only things to discuss:
>>
>> - is the printf format parsing good enough?
> I think so, certainly as a first step. Maybe just also handle "%i".
> See also my suggestion about using a table.

ok, i'm thinking something like:

static void fmt_validate_str(void *data, struct expression **expr, 
struct symbol *sym);		// todo //

tatic void fmt_validate_ptr(void *data, struct expression **expr, struct 
symbol *sym);		// todo //

static void fmt_validate_type(void *data, struct expression **expr, 
struct symbol *sym)
{
         struct symbol *source = degenerate(*expr);
	struct symbol *target = data;

	/* check source vs. target */
	/* note, we need to be fairly strict, long != long long, etc */

}

struct format_string {
	const char *fmt,
	void (validate)(void *data, struct expression *expr, struct symbol *sym);
	void *data
} fmts[] = {
	{ "s", .validate = fmt_validate_str, },
	{ "p", .validate = fmt_validate_ptr, },
	{ "d", .validate = fmt_validate_type, .data = &int_ctype },
	{ "ld", .validate = fmt_validate_type, .data = &long_type },
	{ "lld", .validate = fmt_validate_type, .data = &llong_type },
	{ }
}

where the best match is searched for, and then that function is executed.

> 
>> - do we need to add a a __attribute__((linux-printk)) for printk fmts?
> Good question. I don't think so because in the kernel printk() itself
> is declared as using __attribute__((format(printf, 1, 2))).
> But a mecanism will be needed in the kernel headers to specifiy
> the extended conversion format and their corresponding types.
> I was thinking something like:
> #pragma extended_printf_formar "%pI4[bl]" const struct sockaddr_in *
> but:
> * it's just a suggestion
> * I'm sure some people will hate to use #pragma
> * sparse doesn't have real support of #pragma (but I have some
>    code for it).

I sort of like that, we should probably start a discussion about
this on the kernel lists..

>> - should we add a -Wvariadic-format to force address-space safety?
> Probably (but with another name).
> In shorter term, the warning givi-en by this series should be
> conditionalized on -Wformat.

Ok, added.

>> I think this is getting close to merging.
> Yes, it progress well and I'm all in favor to merge early the
> basic functionalities. At this stage, what's missing the most
> is some testcases. I also really think that the type checking
> can't be done by simply using a type (as returned by
> evaluate_printf_symbol()), a checking function is needed or
> at least use your own generic function instead of trying
> do let compatible_argument_type() do this check.

I'll look into how to do this better.

Is there any chance of getting a ptrlist patch to allow fetching
pointers from arbitrary positions in the list, it would mean we
can move the code that's doing the variadic bits out of the main
checking loop altogether.

For when you want to merge, is it ok to send patches again or
do you prefer a mergable branch somewhere?

-- 
Ben Dooks				http://www.codethink.co.uk/
Senior Engineer				Codethink - Providing Genius

https://www.codethink.co.uk/privacy.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: RFCv2 - support for printf format parsing
Date: Tue, 30 Oct 2018 11:55:57 +0000
Message-ID: <20181030115556.uh3jgpqnh7ztrx6g () ltop ! local>
--------------------
On Tue, Oct 30, 2018 at 11:06:03AM +0000, Ben Dooks wrote:
> On 29/10/18 22:08, Luc Van Oostenryck wrote:
> > On Mon, Oct 29, 2018 at 03:39:47PM +0000, Ben Dooks wrote:
> > > This is an updated set with most of the comments now fixed, but
> > > a few issues that could do with looking at before finalising the
> > > code.
> > > 
> > > 1- the ctype fix doesn't work for prefixed __attrbitue__
> > See my reply in patch 4/5.
> > 
> > > 2- do we need an 'ASN-any' type for pointers?
> > I don't think so (see also patch 4/5).
> > > (the asn-any is for the case printf %p, where the value of the
> > >   pointer is printed, and it is not de-refrenced)... maybe using
> > >   asn:-1 for "any address space" is a good idea?
> > > 
> > > The only things to discuss:
> > > 
> > > - is the printf format parsing good enough?
> > I think so, certainly as a first step. Maybe just also handle "%i".
> > See also my suggestion about using a table.
> 
> ok, i'm thinking something like:
> 
> static void fmt_validate_str(void *data, struct expression **expr,
> struct symbol *sym);		// todo //
> 
> tatic void fmt_validate_ptr(void *data, struct expression **expr,
> struct symbol *sym);		// todo //
> 
> static void fmt_validate_type(void *data, struct expression **expr,
> struct symbol *sym)
> {
>         struct symbol *source = degenerate(*expr);
> 	struct symbol *target = data;
> 
> 	/* check source vs. target */
> 	/* note, we need to be fairly strict, long != long long, etc */
> 
> }
> 
> struct format_string {
> 	const char *fmt,
> 	void (validate)(void *data, struct expression *expr, struct symbol *sym);
> 	void *data
> } fmts[] = {
> 	{ "s", .validate = fmt_validate_str, },
> 	{ "p", .validate = fmt_validate_ptr, },
> 	{ "d", .validate = fmt_validate_type, .data = &int_ctype },
> 	{ "ld", .validate = fmt_validate_type, .data = &long_type },
> 	{ "lld", .validate = fmt_validate_type, .data = &llong_type },
> 	{ }
> }
> 
> where the best match is searched for, and then that function is executed.

It's essentially what I had in mind. 
I was thinking about a perfect match, not a best match, so that later we
can use a hash table or simply reuse lookup_symbol() with a new namespace.
But it's a detail.
 
> > > - do we need to add a a __attribute__((linux-printk)) for printk fmts?
> > Good question. I don't think so because in the kernel printk() itself
> > is declared as using __attribute__((format(printf, 1, 2))).
> > But a mecanism will be needed in the kernel headers to specifiy
> > the extended conversion format and their corresponding types.
> > I was thinking something like:
> > #pragma extended_printf_formar "%pI4[bl]" const struct sockaddr_in *
> > but:
> > * it's just a suggestion
> > * I'm sure some people will hate to use #pragma
> > * sparse doesn't have real support of #pragma (but I have some
> >    code for it).
> 
> I sort of like that, we should probably start a discussion about
> this on the kernel lists..

IMO, better to first come with something already working.

I'm myself not really excited of using #pragma but it has the
advantage to directly address the problem. All other solutions
I can see are indirect and try to reuse another mechanism to
add this format-to-type information as a side-effect.

> > > - should we add a -Wvariadic-format to force address-space safety?
> > Probably (but with another name).
> > In shorter term, the warning givi-en by this series should be
> > conditionalized on -Wformat.
> 
> Ok, added.
> 
> > > I think this is getting close to merging.
> > Yes, it progress well and I'm all in favor to merge early the
> > basic functionalities. At this stage, what's missing the most
> > is some testcases. I also really think that the type checking
> > can't be done by simply using a type (as returned by
> > evaluate_printf_symbol()), a checking function is needed or
> > at least use your own generic function instead of trying
> > do let compatible_argument_type() do this check.
> 
> I'll look into how to do this better.
> 
> Is there any chance of getting a ptrlist patch to allow fetching
> pointers from arbitrary positions in the list, it would mean we
> can move the code that's doing the variadic bits out of the main
> checking loop altogether.

I just sent one but, most probably, it will only apply to the tree
I maintain @ git://github.com/lucvoo/sparse.git

> For when you want to merge, is it ok to send patches again or
> do you prefer a mergable branch somewhere?

I'm fine with both with a slight preference for a git branch.

Kind regards,
-- Luc
================================================================================


################################################################################

=== Thread: Reintroduce sparse-next ===

From: Randy Dunlap <rdunlap () infradead ! org>
To: linux-sparse
Subject: Re: Reintroduce sparse-next
Date: Tue, 27 Feb 2018 20:36:26 +0000
Message-ID: <8cb43f1f-f732-6f66-67f0-342b58d6dadd () infradead ! org>
--------------------
On 02/26/2018 04:46 PM, Luc Van Oostenryck wrote:
> On Mon, Feb 26, 2018 at 04:37:41PM -0800, Randy Dunlap wrote:
>>
>> OK, I made the request.  We'll see how it goes.
> 
> Many thanks, Randy.
>  
>> Sadly I don't know much about best practices. Some portions of the Linux kernel
>> community use bugzilla.kernel.org (e.g., ACPI) and other just say something like,
>> "Please stick to the [project] mailing list."  And some projects have an
>> automated gateway for bugzilla<->email reply/update, which I also asked about.
> 
> I like email reporting for its flexibility and for first triaging
> but bug trkaker are much better to ... track, search and remember.
> So the automatic gateway is really great.
> 
> -- Luc

Here is some usage info on bugzilla.kernel.org:

https://korg.wiki.kernel.org/userdoc/bugzilla


-- 
~Randy
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: Reminder, ===

From: Juliet Muhammad <ics () cnrs ! fr>
To: linux-sparse
Subject: Reminder,
Date: Thu, 27 Sep 2018 00:50:32 +0000
Message-ID: <8e3ff1aa-589e-48ef-b18e-eccbfe70af42 () CNCH02WVP ! core-res ! rootcore ! local>
--------------------
Hello 

   Please i still await your response regarding my previous email.
================================================================================


################################################################################

=== Thread: Sparse v0.5.2 ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: Sparse v0.5.2
Date: Mon, 30 Apr 2018 18:19:26 +0000
Message-ID: <20180430181924.xojpvcprw72636rh () ltop ! local>
--------------------
On Mon, Apr 30, 2018 at 11:00:17AM -0700, Randy Dunlap wrote:
> On 04/30/2018 04:22 AM, Luc Van Oostenryck wrote:
> > The latest release of sparse have been pushed to the official
> > repository. It's a smaller release than the previous one but
> > it contains some important to not be flooded by unimportant
> > warnings while compiling the kernel.
> 
> Great, thanks.
> 
> As I did with v0.5.1, I request that the release info be made available
> on the Sparse wiki at https://sparse.wiki.kernel.org/

Sure, it's normal.
Alas, like I tried for 0.5.1, I can't login on the wiki and the
'request an account' failed, so it's Chris (or anyone else who
can login there that will need to do it).

-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: Sparse v0.6.0 released ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Sparse v0.6.0 released
Date: Wed, 26 Dec 2018 07:54:58 +0000
Message-ID: <20181226075457.vanje7epsu3n27yt () ltop ! local>
--------------------
Sparse v0.6.0 is now out.

The source code can be found at its usual repository:
  git://git.kernel.org/pub/scm/devel/sparse/sparse.git v0.6.0

The tarballs are found at:
    https://www.kernel.org/pub/software/devel/sparse/dist/

Many thanks to people who have contributed to the 888 non-merge
patches of this release:
  Ramsay Jones, Randy Dunlap, Uwe Kleine-König, Joey Pabalinas,
  John Levon, Ben Dooks, Jann Horn, Logan Gunthorpe, Vincenzo
  Frascino and Tycho Andersen.

Special thanks to Ramsay Jones who has reviewed numerous of my
patches, tested many of my series and found many of my typos.


Best wishes for 2019
-- Luc Van Oostenryck


The changes since the pre-release (v0.6.0-rc1) are:
* add -Wbitwise-pointer to warn on casts to/from bitwise pointers
* beautify how types are displayed:
  * no more <noident> 
  * no more redundant type specifiers
  * remove double space
* some cleanup of the build system:
  * only need includedir from llvm-config
  * check if sparse-llvm needs libc++
  * remove -finline-functions from CFLAGS
* some random cleanup:
  * remove unneeded declarations in "compat.h"
  * remove unused arg in add_branch()
  * allocate BBs after the guards
  * remove redundant check of _Bool bitsize
  * remove unused regno()
  * remove self-assignment of base_type
* some documentation:
  * update comment for struct-expression::cast_expression
  * fix list formatting in inline doc
  * document that identifiers are now OK for address spaces
* add TODO list.

The main changes since the previous release (v0.5.2) are:
  * by default, disable warnings about unknown attributes
  * no more warnings about sizeof(void) unless -Wpointer-arith is given
  * add support for __has_attribute() & __has_builtin()
  * many fixes for type evaluation/checking
  * the build should be more friendly for distros
  * a huge number of testcases have been added to the testsuite
  * the handling of cast instructions has been completely revamped
  * the SSA conversion has been is now corrected and has been
    rewritten with a variant of the classical algorithm
  * documentation is now handled with Sphinx and inline doc
    is extracted from the code.
  * online documentation can be found at
      https://sparse-doc.readthedocs.io/en/master/

A more complete list of changes is:
  * add predefined macros for __INTMAX_TYPE__, __INT_MAX__, ...
  * prepare to identify & display the address spaces by name
  * fix linearization of non-constant switch-cases
  * manpage: update maintainer info
  * manpage: add AUTHORS section
  * fixes for -dD
  * add support for -dM
  * remove more complex phi-nodes
  * fix linearization/SSA when missing a return
  * fix linearization/SSA of (nested) logical expressions
  * fix linearization of unreachable switch + label
  * add support for __has_attribute()
  * consolidate instruction's properties into an opcode table
  * fix: do not optimize away accesses to volatile bitfields
  * support mode(__pointer__) and mode(__byte__)
  * do 'classical' SSA conversion (via the iterated dominance frontier).
  * fix buggy recursion in kill_dead_stores()
  * kill dead stores again after memops simplification is done.
  * simplify TRUNC((x & M') | y, N)
  * simplify AND(SHIFT(a | b, S), M)
  * simplify TRUNC(SHIFT(a | b, S), N)
  * add simplification of TRUNC(TRUNC((x))
  * add simplification of SHL(LSR(x), S), S)
  * generate integer-wide OP_ADD & OP_SUB in linearize_inc_dec()
  * simplify mask instructions & bitfield accesses
  * fix a few bugs related to the linearization of logical expressions
  * simplify those logical expressions.
  * add optimized version of some list operations
  * some simplifications of OP_SETNE & OP_SETEQ with 0 and 1
  * several simplifications involving casts and/or bitfields
  * give a correct & sensible warning on negative or over-sized shifts.
  * add conservative simplification of such shifts.
  * do not optimize the meaningless shift:
    * any shift with a negative count
    * OP_ASRs with an over-sized shift count.
  * try to give a correct negative/too-big error message during simplification.
  * simplify chains of shifts.
  * simplify ZEXT + ASR into ZEXT + LSR
  * cse: try the next pair instead of keeping the first instruction
  * cse: compare casts only by kind a size, not by C type.
  * optimize away OP_UTPTR & OP_PTRTU which are nops.
  * cleanup of list walking macros:
    * make untagged pointers the normal case
    * use structurally equivalent struct for all pointer lists
      to avoid needless casting to and fro struct ptrlist
    * simplify PREPARE/NEXT/RESET logic by using common PTR_NEXT()
  * add validation of the IR.
  * improve expansion of builtin dynamic macros (__FILE__, ...)
  * add support for __INCLUDE_LEVEL__ & __BASE_FILE__
  * mprove generation of predefined macros
  * add support for builtins doing overflow checking.
  * add support for the __has_builtin() macro.
  * improve Sphinx doc for IR instructions
    * have those instructions in the index
    * have a nicer presentation of the generated doc thanks
      to not having to use level-4 headings anymore
  * fixes & improvements to the testsuite; mainly:
    - allow to run the testsuite on all the tests of a subdir
    - teach 'format' to directly append to the testcase
    - validate the 'check-...' tags

-- Luc Van Oostenryck
================================================================================


################################################################################

=== Thread: [ANNOUNCE] Sparse v0.6.0-rc1 ===

From: Ramsay Jones <ramsay () ramsayjones ! plus ! com>
To: linux-sparse
Subject: Re: [ANNOUNCE] Sparse v0.6.0-rc1
Date: Wed, 19 Dec 2018 00:37:41 +0000
Message-ID: <edd57691-a71d-07cd-5f5d-a75e619e9d10 () ramsayjones ! plus ! com>
--------------------


On 17/12/2018 23:57, Luc Van Oostenryck wrote:
> Sparse pre-release v0.6.0-rc1 is now pushed out and contains
> 850 patches made by 10 people.
> 
> The source code can be found at its usual repository:
>   git://git.kernel.org/pub/scm/devel/sparse/sparse.git v0.6.0-rc1

I have tested v0.6.0-rc1, as usual, on LM-19 64-bit, LM-18.3 32-bit,
cygwin 64-bit and fedora-28 64-bit. (Both sparse and its use with git).
No errors or regressions found!

Also, I have looked at the 'dump macros' output:

    $ gcc -m64 -dM -E - </dev/null | sort >ggg-64
    $ gcc -m32 -dM -E - </dev/null | sort >ggg-32
    $ gcc -mx32 -dM -E - </dev/null | sort >ggg-x32
    $ gcc -m16 -dM -E - </dev/null | sort >ggg-16
    $ gcc -dM -E - </dev/null | sort >ggg
    $ ./sparse -m64 -dM -E - </dev/null | sort >sss-64
    $ ./sparse -m32 -dM -E - </dev/null | sort >sss-32
    $ ./sparse -mx32 -dM -E - </dev/null | sort >sss-x32
    $ ./sparse -m16 -dM -E - </dev/null | sort >sss-16
    $ ./sparse -dM -E - </dev/null | sort >sss
    $

on each 64-bit platform:

    $ diff ggg ggg-64
    $ diff sss sss-64
    $ meld ggg sss
    $ 

... has shown no issues.

on the only 32-bit platform:

    $ diff ggg ggg-32
    $ diff sss sss-32
    $ meld ggg sss
    $ 

... has also shown no issues.

Yes, I have seen some issues in some pairs not compared above, but
I don't think that should hold up the release. (I haven't studied
_all_ of the above pairs yet - I wanted to report what I had found
so far tonight ...).

[Note: I tested specifically commit 105e081 (ie the tag v0.6.0-rc1)
because I had already started testing, before seeing commit 1cf02bb]

> The tarballs are found at:
>     https://www.kernel.org/pub/software/devel/sparse/dist/

Ah, yes, I never think about testing the tarballs! (I guess that is
what prompted commit 1cf02bb?).

This looks good to me! :-D

Thanks!

ATB,
Ramsay Jones
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [ANNOUNCE] Sparse v0.6.0-rc1
Date: Wed, 19 Dec 2018 07:27:26 +0000
Message-ID: <20181219072725.as6edtswiryb2a4d () ltop ! local>
--------------------
On Wed, Dec 19, 2018 at 12:37:41AM +0000, Ramsay Jones wrote:
> 
> 
> On 17/12/2018 23:57, Luc Van Oostenryck wrote:
> > Sparse pre-release v0.6.0-rc1 is now pushed out and contains
> > 850 patches made by 10 people.
> > 
> > The source code can be found at its usual repository:
> >   git://git.kernel.org/pub/scm/devel/sparse/sparse.git v0.6.0-rc1
> 
> I have tested v0.6.0-rc1, as usual, on LM-19 64-bit, LM-18.3 32-bit,
> cygwin 64-bit and fedora-28 64-bit. (Both sparse and its use with git).
> No errors or regressions found!

Great!
 
> Also, I have looked at the 'dump macros' output:
> 
>     $ gcc -m64 -dM -E - </dev/null | sort >ggg-64
>     $ gcc -m32 -dM -E - </dev/null | sort >ggg-32
>     $ gcc -mx32 -dM -E - </dev/null | sort >ggg-x32
>     $ gcc -m16 -dM -E - </dev/null | sort >ggg-16
>     $ gcc -dM -E - </dev/null | sort >ggg
>     $ ./sparse -m64 -dM -E - </dev/null | sort >sss-64
>     $ ./sparse -m32 -dM -E - </dev/null | sort >sss-32
>     $ ./sparse -mx32 -dM -E - </dev/null | sort >sss-x32
>     $ ./sparse -m16 -dM -E - </dev/null | sort >sss-16
>     $ ./sparse -dM -E - </dev/null | sort >sss
>     $
> 
> on each 64-bit platform:
> 
>     $ diff ggg ggg-64
>     $ diff sss sss-64
>     $ meld ggg sss
>     $ 
> 
> ... has shown no issues.
> 
> on the only 32-bit platform:
> 
>     $ diff ggg ggg-32
>     $ diff sss sss-32
>     $ meld ggg sss
>     $ 
> 
> ... has also shown no issues.

Excellent, thanks you for that.
I've also done essentially the same on some recent Ubuntu 32 & 64-bit,
an older Debian 64-bit, some Ubuntu on ARM64, CentOS on ppc64 & ppc64el,
and some tests on Debian with sparc64 and Solaris 10 & 11 on sparc32.
I've also checked it with just a cross-compiler for the different ABI
on mips32 & mips64, RISC-V 32 & 64-bit and S390X but these may depends
on how the compiler was configured when built).
I'll give a try later on some {Net,Free,Open}BSD and ARM32.
 
> Yes, I have seen some issues in some pairs not compared above, but
> I don't think that should hold up the release. (I haven't studied
> _all_ of the above pairs yet - I wanted to report what I had found
> so far tonight ...).

Thank you very much.
I'm happy to see that cygwin 64-bit is OK (as it's one I don't test).
 
> [Note: I tested specifically commit 105e081 (ie the tag v0.6.0-rc1)
> because I had already started testing, before seeing commit 1cf02bb]

Yes, sure, 1cf02bb is just a small fix with no functional effect.
 
> > The tarballs are found at:
> >     https://www.kernel.org/pub/software/devel/sparse/dist/
> 
> Ah, yes, I never think about testing the tarballs! (I guess that is
> what prompted commit 1cf02bb?).

I just add them because not everyone is using git but well ...
It's more *because* of them that I forgot to update the version in
the Makefile.

Best regards thanks again
-- Luc
================================================================================


################################################################################

=== Thread: [ANN] dmr_C alpha release 0.4 containing OMR JIT backend ===

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: [ANN] dmr_C alpha release 0.4 containing OMR JIT backend
Date: Tue, 26 Jun 2018 22:39:23 +0000
Message-ID: <CACXZuxcMBw7N+qSDVVD=wf6HL97DQspMWF1HSQ5FzSv_AmHz8g () mail ! gmail ! com>
--------------------
dmr_C is a parser and JIT compiler for C, with LLVM, OMRJIT and
NanoJIT backends. dmr_C is derived from the Linux Sparse project.

Main highlights of this release are:

+ New JIT backend using OMRJIT is available as a preview. Note that
this backend is very new and still under initial development and
testing

For instructions on how to build and use dmr_C, please refer to the
documentation on the project site
(https://github.com/dibyendumajumdar/dmr_c).

Note that this is an alpha quality release - there are several known
issues and bugs. Please check the documentation as well as the issues
list for further details.

Thanks and Regards
Dibyendu
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Derek M Jones <derek () knosof ! co ! uk>
To: linux-sparse
Subject: Re: [ANN] dmr_C alpha release 0.4 containing OMR JIT backend
Date: Wed, 27 Jun 2018 11:47:42 +0000
Message-ID: <5ee32466-edeb-725e-d25d-bc9c31494286 () knosof ! co ! uk>
--------------------
Dibyendu,

> dmr_C is a parser and JIT compiler for C, with LLVM, OMRJIT and
> NanoJIT backends. dmr_C is derived from the Linux Sparse project.
> 
> Main highlights of this release are:

A JIT is needed for certain ways of generating Fibonacci numbers:
http://shape-of-code.coding-guidelines.com/2011/06/18/fibonacci-and-jit-compilers/


-- 
Derek M. Jones           Software analysis
tel: +44 (0)1252 520667  blog:shape-of-code.coding-guidelines.com
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [Bug 198977] [sparse-llvm] Add option to save output to a named file ===

From: bugzilla-daemon () bugzilla ! kernel ! org
To: linux-sparse
Subject: [Bug 198977] [sparse-llvm] Add option to save output to a named file
Date: Tue, 25 Dec 2018 12:14:03 +0000
Message-ID: <bug-198977-200559-5gvX3Xh6AI () https ! bugzilla ! kernel ! org/>
--------------------
https://bugzilla.kernel.org/show_bug.cgi?id=198977

Luc Van Oostenryck (luc.vanoostenryck@gmail.com) changed:

           What    |Removed                     |Added
----------------------------------------------------------------------------
             Status|RESOLVED                    |CLOSED
         Resolution|PATCH_ALREADY_AVAILABLE     |CODE_FIX

-- 
You are receiving this mail because:
You are watching the assignee of the bug.
================================================================================


################################################################################

=== Thread: [Bug 198979] [sparse-llvm] Tests for Sparse-LLVM backend to validate Sparse code generation ===

From: bugzilla-daemon () bugzilla ! kernel ! org
To: linux-sparse
Subject: [Bug 198979] [sparse-llvm] Tests for Sparse-LLVM backend to validate Sparse code generation
Date: Tue, 25 Dec 2018 12:14:45 +0000
Message-ID: <bug-198979-200559-zvY4KjRgfP () https ! bugzilla ! kernel ! org/>
--------------------
https://bugzilla.kernel.org/show_bug.cgi?id=198979

Luc Van Oostenryck (luc.vanoostenryck@gmail.com) changed:

           What    |Removed                     |Added
----------------------------------------------------------------------------
             Status|RESOLVED                    |CLOSED

-- 
You are receiving this mail because:
You are watching the assignee of the bug.
================================================================================


################################################################################

=== Thread: [DRAFT PATCH 0/3] add support for restricted enums ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [DRAFT PATCH 0/3] add support for restricted enums
Date: Wed, 21 Feb 2018 22:39:05 +0000
Message-ID: <20180221223908.38904-1-luc.vanoostenryck () gmail ! com>
--------------------
This is really ugly for now but it show what need to be done
to support restricted enums (not how it must be done).

It need to incorporate changes to support correctly type attribues.
There is two problems with this for now:
1) scope of type attributes, for example in:
 1a)
	struct foo { ... } __aligned(8);
   the attribute is purely ignored. And in
	struct foo { ... } __aligned(8) var;
   the attribute is attached to var, not to the struct.
 1b)
	struct __attribute foo { ... } ...
   is parsed exactly like
	__attribute struct foo { ... } ...
   while their scope are in fact different (type vs. declaration).

2) enumerator values can have plenty of unexpected types.
   For example in:
	enum foo {
		FOO = (char) 1,
		BAR = (void*) 2,
		BAZ = (int*) 3,
	};
   then typeof(FOO) is 'char', typeof(BAR) is 'void*' and
   typeof(BAZ) is 'int*'. This was quite surprising to me and
   not at all what GCC do. 
   To me:
	- BAR & BAZ here above should be an error or
	  at least issues a warning.
	- typeof(FOO) should be 'enum foo' (and same for
	  BAR & BAZ if not errors.
	- of course, if the enum is declared as 'enum __bitwise foo'
	  then typeof(FOO) will be of the same type too
	  and thus will be of a restricted type only compatible
	  with other values of this enumeration type.


When using this series on the kernel (v4.15, defconfig) we got
a few new sparse warning:
1) a problem with (fs/aio.c)
  struct kioctx {
	...
          struct __percpu kioctx_cpu *cpu;
	...
  };

  should be:
	__percpu struct kioctx_cpu *cpu;


2) a bunch of errors with pci_ers_result_t:
  which is declared like:
	typedef unsigned int __bitwise pci_ers_result_t;
	enum pci_ers_result {
		/* no result/none/not supported in device driver */
		PCI_ERS_RESULT_NONE = (__force pci_ers_result_t) 1,
		...
	};

3) same with pci_channel...
	typedef unsigned int __bitwise pci_channel_state_t;
	enum pci_channel_state {
		/* I/O channel is in normal state */
		pci_channel_io_normal = (__force pci_channel_state_t) 1,
		...
	};

Of course 2) & 3) are, IMO, an horrible abuse of sparse type system
(and its current limitation and are exactly what Matthew want to avoid.

I can restore current behaviour of 1) but I think it should simply
use the correct syntax. In fact I think that declaring:
	struct name { ... };
and then using 'struct __some_attribute__ name' should be an error.


----------------------------------------------------------------
This series is also available for evaluation in the Git repository at:
  git://github.com/lucvoo/sparse-dev.git bitwise-enum-v0

----------------------------------------------------------------
Luc Van Oostenryck (3):
      add testcase for restricted enum
      extract apply_bitwise() from declaration_specifiers()
      teach sparse about restricted enums

 parse.c                      | 61 +++++++++++++++++++++++++++++++-------------
 validation/enum-restricted.c | 21 +++++++++++++++
 2 files changed, 64 insertions(+), 18 deletions(-)
 create mode 100644 validation/enum-restricted.c
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Matthew Wilcox <willy () infradead ! org>
To: linux-sparse
Subject: Re: [DRAFT PATCH 0/3] add support for restricted enums
Date: Thu, 22 Feb 2018 00:43:53 +0000
Message-ID: <20180222004353.GA4946 () bombadil ! infradead ! org>
--------------------
On Wed, Feb 21, 2018 at 11:39:05PM +0100, Luc Van Oostenryck wrote:
> When using this series on the kernel (v4.15, defconfig) we got
> a few new sparse warning:
> 1) a problem with (fs/aio.c)
>   struct kioctx {
> 	...
>           struct __percpu kioctx_cpu *cpu;
> 	...
>   };
> 
>   should be:
> 	__percpu struct kioctx_cpu *cpu;

There's another example of this:

drivers/net/wireless/intel/iwlwifi/pcie/internal.h:     struct __percpu iwl_tso_hdr_page *tso_hdr_page;

I assume that "struct kioctx_cpu __percpu *cpu" is still acceptable?
That seems to be the majority of uses of __percpu in the kernel today.

> I can restore current behaviour of 1) but I think it should simply
> use the correct syntax. In fact I think that declaring:
> 	struct name { ... };
> and then using 'struct __some_attribute__ name' should be an error.

I think it depends on the attribute.  __bitwise is different from __percpu
here; __bitwise tells you something about the type whereas __percpu tells
you something about this particular pointer.  It's quite legitimate for
me to have a non-percpu struct kioctx_cpu, and percpu struct kioctx_cpu.
I'd say __rcu is in the same category as __percpu here.

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [DRAFT PATCH 0/3] add support for restricted enums
Date: Thu, 22 Feb 2018 02:35:55 +0000
Message-ID: <20180222023554.2w46qfeujs3hw2kf () ltop ! local>
--------------------
On Wed, Feb 21, 2018 at 04:43:53PM -0800, Matthew Wilcox wrote:
> On Wed, Feb 21, 2018 at 11:39:05PM +0100, Luc Van Oostenryck wrote:
> > When using this series on the kernel (v4.15, defconfig) we got
> > a few new sparse warning:
> > 1) a problem with (fs/aio.c)
> >   struct kioctx {
> > 	...
> >           struct __percpu kioctx_cpu *cpu;
> > 	...
> >   };
> > 
> >   should be:
> > 	__percpu struct kioctx_cpu *cpu;
> 
> There's another example of this:
> 
> drivers/net/wireless/intel/iwlwifi/pcie/internal.h:     struct __percpu iwl_tso_hdr_page *tso_hdr_page;
> 
> I assume that "struct kioctx_cpu __percpu *cpu" is still acceptable?
> That seems to be the majority of uses of __percpu in the kernel today.

I think we should accept what GCC accept for non-sparse specific
attributes. I'll need to check a bit.
Since I write:
	const unsigned int *ptr;
and it seems really weird to me to write:
	unsigned int const *ptr;
I tend to see
	__percpu struct tag *ptr;
as much 'more natural' than
	struct tag __percpy *ptr;

> > I can restore current behaviour of 1) but I think it should simply
> > use the correct syntax. In fact I think that declaring:
> > 	struct name { ... };
> > and then using 'struct __some_attribute__ name' should be an error.
> 
> I think it depends on the attribute.  __bitwise is different from __percpu
> here; __bitwise tells you something about the type whereas __percpu tells
> you something about this particular pointer.  It's quite legitimate for
> me to have a non-percpu struct kioctx_cpu, and percpu struct kioctx_cpu.
> I'd say __rcu is in the same category as __percpu here.

Here I disagree strongly. It should *not* depends on the attribute.
It's just a question of syntax for attaching the attribute to:
- the type itself. Correct example would be:
	struct __attr__ name { };
	struct name { ... } __attr__;
- or to the variable.

If the type 'struct kioctx_cpu' exist in itself, ok. And if you need
to use an attribute for a variable of this type, fine, and this
variable will have as type the original type + the attribute.
What I tried to say here above is that if you declare a type as:
	struct name ...;
then the name 'struct name' will be attached to the type 'struct name'
so if later you use somewhere 'struct __attr__ name' (which is the
syntax for attaching an attribute to a type) you necessarily create
a problem because you're trying to redefine the type 'struct name'.
This situation can only arise for struct, union and enums.

But yes, __bitwise only make sense as an attribute for a type so
it must be used as such. So, it makes no sense to write:
	__bitwise unsigned int var;
because we won't be able to ue any value for this var.
On the contrary, I suppose that for __rcu or __percpu it (almost)
always make sense to be able to access the non-attributed
version of the type, so it make no or little sense to write;
	struct __percpu name { ... };
because then all variables declared as:
	struct name var;
will be 'percpu'.

-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [GIT PULL] Sparse v0.5.2 ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [GIT PULL] Sparse v0.5.2
Date: Sat, 24 Mar 2018 19:53:21 +0000
Message-ID: <20180324195321.98234-1-luc.vanoostenryck () gmail ! com>
--------------------
Chris,

Please pull the following for v0.5.2.
There is no changes since -rc1 but the version number.

Cheers,
Luc

---

The following changes since commit d1c2f8d3d4205ca1ae7cf0ec2cbd89a7fce73e5c:

  bump up version to 0.5.2-RC1 (2018-03-03 16:42:32 -0800)

are available in the Git repository at:

  git://github.com/lucvoo/sparse.git tags/v0.5.2

for you to fetch changes up to 979043ca8aae37304d2e3d43c9f281d9b8d92ba6:

  Sparse v0.5.2 (2018-03-24 20:16:03 +0100)


Note: since I've used a signed tag, you will need to use
      git pull --ff-only ...

----------------------------------------------------------------
Luc Van Oostenryck (1):
      Sparse v0.5.2

 Makefile | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 0/2] add mechanisms to ignore some testcases ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 0/2] add mechanisms to ignore some testcases
Date: Mon, 30 Apr 2018 13:21:30 +0000
Message-ID: <20180430132132.62055-1-luc.vanoostenryck () gmail ! com>
--------------------
These series provides two means to ignore some testcases,
either based on a preprocessor test or a static assertion.

This allow to ignore some testcases in meaningless or wrong
situations.

Luc Van Oostenryck (2):
  testsuite: add check-cp-if
  testsuite: add check-assert

 Documentation/test-suite |  8 ++++++++
 validation/test-suite    | 26 ++++++++++++++++++++++++++
 2 files changed, 34 insertions(+)

-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 0/2] avoid reporting 'builtin:0:0: ...' ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 0/2] avoid reporting 'builtin:0:0: ...'
Date: Sun, 27 May 2018 00:08:53 +0000
Message-ID: <20180527000855.19459-1-luc.vanoostenryck () gmail ! com>
--------------------
This 2 patches series avoid to display error messages
where the given position is 'builtin:0:0:'.
A first case is caused when reaching the EOF and can be given
the correct end-of-stream position.
For the second case, these error messages are only issued
after a previous error have been detected and properly
reported, they deosn't point to the real problem and are
now no more displayed.

Luc Van Oostenryck (2):
  give a position to end-of-input
  avoid multiple error message after parsing error

 lib.c                             |  9 ++++++++-
 token.h                           |  1 +
 tokenize.c                        |  1 +
 validation/check_byte_count-ice.c |  4 ++--
 validation/error-at-eof.c         | 10 ++++++++++
 5 files changed, 22 insertions(+), 3 deletions(-)
 create mode 100644 validation/error-at-eof.c

-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 0/2] doc: add the IR instructions in to index ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 0/2] doc: add the IR instructions in to index
Date: Mon, 21 May 2018 01:52:57 +0000
Message-ID: <20180521015259.57316-1-luc.vanoostenryck () gmail ! com>
--------------------
This series add a small directive for the IR instructions.
This allows to:
* have those instructions in the index
* have a nicer presentation of the generated doc thanks
  to not having to use level-4 headings anymore

Luc Van Oostenryck (2):
  doc: convert IR.md to reST
  doc: add sphinx domain for IR instruction indexation

 Documentation/IR.md         | 356 -------------------------------
 Documentation/IR.rst        | 411 ++++++++++++++++++++++++++++++++++++
 Documentation/conf.py       |   1 +
 Documentation/doc-guide.rst |   4 +
 Documentation/sphinx/ir.py  |  75 +++++++
 5 files changed, 491 insertions(+), 356 deletions(-)
 delete mode 100644 Documentation/IR.md
 create mode 100644 Documentation/IR.rst
 create mode 100755 Documentation/sphinx/ir.py

-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 0/2] fixup for the -vcompound testcase on 32 bit archs ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 0/2] fixup for the -vcompound testcase on 32 bit archs
Date: Mon, 30 Apr 2018 16:10:03 +0000
Message-ID: <20180430161005.92480-1-luc.vanoostenryck () gmail ! com>
--------------------
These two patches avoid the testcase to fail on 32 bit archs
and on x86-64-x32.

They need to be applied on top of the original testcase and
top of the 'check-assert-cpp-if' series (see [0]-[3]).

@Randy,
I post those as separate patches but most probably I'll squash
them with your original version before reaching the main tree.

Cheers,
-- Luc

[0] https://github.com/lucvoo/sparse-dev/commits/check-assert-cpp-if
[1] https://marc.info/?l=linux-sparse&m=152509450015140
[2] https://marc.info/?l=linux-sparse&m=152509450015141
[3] https://marc.info/?l=linux-sparse&m=152509450215143

---

Luc Van Oostenryck (2):
  testcase: make the -vcompound testcase standalone
  testcase: ignore -vcompound test on 32 bit archs

 validation/compound-sizes.c | 7 ++++---
 1 file changed, 4 insertions(+), 3 deletions(-)

-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Randy Dunlap <rdunlap () infradead ! org>
To: linux-sparse
Subject: Re: [PATCH 0/2] fixup for the -vcompound testcase on 32 bit archs
Date: Mon, 30 Apr 2018 22:03:08 +0000
Message-ID: <443ed706-266a-78f3-b2c4-00907ff91c3e () infradead ! org>
--------------------
On 04/30/2018 09:10 AM, Luc Van Oostenryck wrote:
> These two patches avoid the testcase to fail on 32 bit archs
> and on x86-64-x32.
> 
> They need to be applied on top of the original testcase and
> top of the 'check-assert-cpp-if' series (see [0]-[3]).
> 
> @Randy,
> I post those as separate patches but most probably I'll squash
> them with your original version before reaching the main tree.

Thanks for taking care of this.

FWIW:
Acked-by: Randy Dunlap <rdunlap@infreadead.org>


> Cheers,
> -- Luc
> 
> [0] https://github.com/lucvoo/sparse-dev/commits/check-assert-cpp-if
> [1] https://marc.info/?l=linux-sparse&m=152509450015140
> [2] https://marc.info/?l=linux-sparse&m=152509450015141
> [3] https://marc.info/?l=linux-sparse&m=152509450215143
> 
> ---
> 
> Luc Van Oostenryck (2):
>   testcase: make the -vcompound testcase standalone
>   testcase: ignore -vcompound test on 32 bit archs
> 
>  validation/compound-sizes.c | 7 ++++---
>  1 file changed, 4 insertions(+), 3 deletions(-)


-- 
~Randy
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH 0/2] fixup for the -vcompound testcase on 32 bit archs
Date: Sun, 06 May 2018 08:41:32 +0000
Message-ID: <20180506084131.ee775nepc4rvrkxs () ltop ! local>
--------------------
On Mon, Apr 30, 2018 at 03:03:08PM -0700, Randy Dunlap wrote:
> On 04/30/2018 09:10 AM, Luc Van Oostenryck wrote:
> > These two patches avoid the testcase to fail on 32 bit archs
> > and on x86-64-x32.
> > 
> > They need to be applied on top of the original testcase and
> > top of the 'check-assert-cpp-if' series (see [0]-[3]).
> > 
> > @Randy,
> > I post those as separate patches but most probably I'll squash
> > them with your original version before reaching the main tree.

This squashed patch has now been pushed to:
	git://github.com/lucvoo/sparse.git

-- Luc 
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 0/2] label: avoid multiple definitions ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 0/2] label: avoid multiple definitions
Date: Sat, 26 May 2018 18:42:53 +0000
Message-ID: <20180526184255.53856-1-luc.vanoostenryck () gmail ! com>
--------------------
This series avoid assertion failures and the corresponding
SIGABORTs which may occur when processing code containing
a label defined multiple times.

Luc Van Oostenryck (2):
  label: add testcase for label redefinition
  label: avoid multiple definitions

 parse.c                      |  9 ++++++---
 validation/label-redefined.c | 17 +++++++++++++++++
 2 files changed, 23 insertions(+), 3 deletions(-)
 create mode 100644 validation/label-redefined.c

-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 0/3] add support for mode __pointer__ & __byte__ ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 0/3] add support for mode __pointer__ & __byte__
Date: Mon, 30 Jul 2018 16:34:37 +0000
Message-ID: <20180730163440.2303-1-luc.vanoostenryck () gmail ! com>
--------------------
This series contains missing support for mode __pointer__ &
__byte__ as well as a small preliminary cleanup.

This series is available for testing & review in the repository at:
  git://github.com/lucvoo/sparse-dev.git mode-pointer

----------------------------------------------------------------
Luc Van Oostenryck (3):
  mode keywords don't need MOD_{CHAR,LONG,...}
  add support for mode __pointer__
  add support for mode __byte__

 parse.c | 44 +++++++++++++++++++++++++++++++-------------
 1 file changed, 31 insertions(+), 13 deletions(-)

-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 0/3] fix linearization of unreachable switch + label ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 0/3] fix linearization of unreachable switch + label
Date: Thu, 30 Aug 2018 23:10:12 +0000
Message-ID: <20180830231015.48281-1-luc.vanoostenryck () gmail ! com>
--------------------
Currently an unreachable switch statement is simply discarded. Thus,
if the statement contains some (reachable) labels, the branches to
these labels correspond to branches to non-existing BBs.
The object of this series is fix the linearization of such switches.


This series is available for review & testing in the Git repository at:
  git://github.com/lucvoo/sparse-dev.git dead-switch

----------------------------------------------------------------
Luc Van Oostenryck (3):
      add tescase for unreachable label in switch
      ir-validate: add validation branch to dead BB
      fix linearization of unreachable switch (with reachable label).

 ir.c                                   | 37 +++++++++++++++++++++++++++++++---
 linearize.c                            | 11 +++++-----
 linearize.h                            |  6 ++++++
 validation/linear/unreachable-label0.c | 19 +++++++++++++++++
 4 files changed, 65 insertions(+), 8 deletions(-)
 create mode 100644 validation/linear/unreachable-label0.c
================================================================================


################################################################################

=== Thread: [PATCH 0/3] fix typing of enums ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 0/3] fix typing of enums
Date: Wed, 18 Apr 2018 15:39:56 +0000
Message-ID: <20180418153959.33271-1-luc.vanoostenryck () gmail ! com>
--------------------
This series teaches sparse to be as strict with enums as with
structs & unions instead of considering them as equivalent
to integers.

The 1st patch is purely aesthetical, the meat is in the 3rd one.

Surprisingly, this fix reveals ~200 enum/int errors/abuses in the
kernel, some (or most) clearly trying to workaround the problem
with bitwise & enum at the origin of this series.

Luc Van Oostenryck (3):
  fix show typename of enums
  add testcase for enum / int type difference
  fix enum typing

 evaluate.c                 |  5 ++-
 show-parse.c               |  2 +-
 validation/enum-mismatch.c |  4 +-
 validation/typediff-enum.c | 82 ++++++++++++++++++++++++++++++++++++++
 4 files changed, 88 insertions(+), 5 deletions(-)
 create mode 100644 validation/typediff-enum.c

-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 0/4] add support for __has_attribute() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 0/4] add support for __has_attribute()
Date: Fri, 31 Aug 2018 23:16:31 +0000
Message-ID: <20180831231635.90772-1-luc.vanoostenryck () gmail ! com>
--------------------
__has_attribute() is supported since GCC5 and begin to
be used by the kernel. It's more than time to add it
to sparse too, which is done with this series.


This series is available for review and testing in the Git repository at:
  git://github.com/lucvoo/sparse-dev.git has-attribute

Once accepted, it will be upstreamed to:
  git://github.com/lucvoo/sparse.git master

----------------------------------------------------------------
Luc Van Oostenryck (4):
      has-attr: add testcase for __has_attribute()
      has-attr: move 'mode' next to '__mode__'
      has-attr: add __designated_init__ & transparent_union
      has-attr: add support for __has_attribute()

 ident-list.h                            |  1 +
 lib.c                                   |  1 +
 parse.c                                 |  4 ++-
 pre-process.c                           | 33 +++++++++++++------
 validation/preprocessor/has-attribute.c | 56 +++++++++++++++++++++++++++++++++
 5 files changed, 85 insertions(+), 10 deletions(-)
 create mode 100644 validation/preprocessor/has-attribute.c
================================================================================


################################################################################

=== Thread: [PATCH 0/4] fix testsuite on 32-bit ===

From: Ramsay Jones <ramsay () ramsayjones ! plus ! com>
To: linux-sparse
Subject: Re: [PATCH 0/4] fix testsuite on 32-bit
Date: Sat, 15 Sep 2018 23:19:33 +0000
Message-ID: <b5a109d0-2e64-391f-db21-36d14437dfe9 () ramsayjones ! plus ! com>
--------------------


On 10/09/18 00:53, Luc Van Oostenryck wrote:
> This series contains fixes for the three tests that failed
> on 32-bit. It also contains a small improvement in the SSA
> conversion which is related to one of the failed test.

Sorry for the late reply, but I finally found time to test
this series on 32bit (and 64bit) Linux. You will not be
surprised to hear that it fixes all reported issues.

Thanks!

ATB,
Ramsay Jones
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH 0/4] fix testsuite on 32-bit
Date: Mon, 24 Sep 2018 14:37:11 +0000
Message-ID: <20180924143709.7aazwfqkh6rkzr2i () ltop ! local>
--------------------
On Sun, Sep 16, 2018 at 12:19:33AM +0100, Ramsay Jones wrote:
> 
> 
> On 10/09/18 00:53, Luc Van Oostenryck wrote:
> > This series contains fixes for the three tests that failed
> > on 32-bit. It also contains a small improvement in the SSA
> > conversion which is related to one of the failed test.
> 
> Sorry for the late reply, but I finally found time to test
> this series on 32bit (and 64bit) Linux. You will not be
> surprised to hear that it fixes all reported issues.

Many thanks for confirming this.

-- Luc
================================================================================


################################################################################

=== Thread: [PATCH 0/4] improve the tests about the generated IR ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 0/4] improve the tests about the generated IR
Date: Mon, 19 Mar 2018 00:54:14 +0000
Message-ID: <20180319005418.18548-1-luc.vanoostenryck () gmail ! com>
--------------------
This small series contains some improvements for testing
the generated IR.

Luc Van Oostenryck (4):
  testsuite: reorganize tests for compound literals
  testsuite: add a few more tests catching quadratic behaviour
  testsuite: improve mem2reg testcases
  testsuite: remove useless test for loop-linearization

 validation/crash-select.c                          |  18 +++
 validation/{ => linear}/compound-literal00.c       |   0
 validation/linear/compound-literal01.c             |  18 +++
 .../compound-literal02.c}                          |  10 +-
 validation/loop-linearization.c                    | 136 ---------------------
 validation/{ => mem2reg}/alias-distinct.c          |   0
 validation/{ => mem2reg}/alias-mixed.c             |   0
 validation/{ => mem2reg}/alias-same.c              |   0
 validation/mem2reg/cond-expr.c                     |   1 +
 validation/mem2reg/cond-expr5.c                    |   6 +-
 validation/mem2reg/init-global-array.c             |  12 +-
 validation/mem2reg/init-local-array.c              |  13 +-
 validation/{ => mem2reg}/kill-casts.c              |   0
 validation/mem2reg/loop02-global.c                 |   2 +-
 validation/mem2reg/missing-return.c                |  34 ++++++
 validation/mem2reg/quadra01.c                      |  27 ++++
 validation/mem2reg/quadra02.c                      |  18 +++
 validation/{ => mem2reg}/reload-aliasing.c         |   0
 validation/mem2reg/store-deadborn.c                |   9 ++
 validation/{linear => mem2reg}/stray-phisrc.c      |   0
 validation/mem2reg/struct.c                        |  32 +++++
 validation/mem2reg/unused-var.c                    |  23 ++++
 validation/{mem2reg => optim}/killed-insn.c        |   7 +-
 validation/optim/null-phi.c                        |   9 ++
 validation/repeat.h                                |  24 ++++
 25 files changed, 239 insertions(+), 160 deletions(-)
 create mode 100644 validation/crash-select.c
 rename validation/{ => linear}/compound-literal00.c (100%)
 create mode 100644 validation/linear/compound-literal01.c
 rename validation/{compound-literal01.c => linear/compound-literal02.c} (61%)
 delete mode 100644 validation/loop-linearization.c
 rename validation/{ => mem2reg}/alias-distinct.c (100%)
 rename validation/{ => mem2reg}/alias-mixed.c (100%)
 rename validation/{ => mem2reg}/alias-same.c (100%)
 rename validation/{ => mem2reg}/kill-casts.c (100%)
 create mode 100644 validation/mem2reg/missing-return.c
 create mode 100644 validation/mem2reg/quadra01.c
 create mode 100644 validation/mem2reg/quadra02.c
 rename validation/{ => mem2reg}/reload-aliasing.c (100%)
 create mode 100644 validation/mem2reg/store-deadborn.c
 rename validation/{linear => mem2reg}/stray-phisrc.c (100%)
 create mode 100644 validation/mem2reg/struct.c
 create mode 100644 validation/mem2reg/unused-var.c
 rename validation/{mem2reg => optim}/killed-insn.c (53%)
 create mode 100644 validation/optim/null-phi.c
 create mode 100644 validation/repeat.h

-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 0/5] some list optimizations ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 0/5] some list optimizations
Date: Tue, 24 Jul 2018 20:34:53 +0000
Message-ID: <20180724203458.46429-1-luc.vanoostenryck () gmail ! com>
--------------------
This series contains optimizations of some list operations.

In most common situations, most list used are quite small
(mean of 5 elements) and these optimizations don't give
any kind of measureable speedups (but no slowdowns either).
However, in some (maybe pathological) situations, the lists
can be quite long, especially the ones for pseudo usage, and
then these optimizations can be quite effective: up to a
combined speedup of 45%.

Note: I'm not yet 100% convinced that patch 5 is totally safe.
Note: the 'speedup' of patches 3 & 4 is lost in the noise.


This series is available for review & testing in the Git repository at:
  git://github.com/lucvoo/sparse-dev.git list-optims

----------------------------------------------------------------
Luc Van Oostenryck (5):
  add copy_ptr_list()
  add ptr_list_empty()
  add ptr_list_multiple()
  add lookup_ptr_list_entry()
  no VOID test in convert_instruction_target()

 flow.c      |   7 +---
 linearize.h |  17 ++++++++-
 liveness.c  |   9 -----
 ptrlist.c   | 107 ++++++++++++++++++++++++++++++++++++++++++++++++++++
 ptrlist.h   |   5 +++
 simplify.c  |   4 +-
 6 files changed, 132 insertions(+), 17 deletions(-)

-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 0/6] fix & improve parsing of contexts ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 0/6] fix & improve parsing of contexts
Date: Sat, 26 May 2018 15:14:21 +0000
Message-ID: <20180526151427.30789-1-luc.vanoostenryck () gmail ! com>
--------------------
This series somehow improve the parsing of context
attributes and statements, like:
* fix some crashes
* stricter syntax, like allowing only the correct number of args
* improve error reporting

One possibly controversial change is the enforcing of the
functional syntax for the __context__ statement. IOW, accept:
	__context__(...);
and reject the never really used
	__context__ ...;

Luc Van Oostenryck (6):
  add helper for new parsing errors: unexpected()
  context: fix parsing of attribute 'context'
  context: __context__(...) expect a constant expression
  context: fix crashes while parsing '__context__;' or '__context__(;'
  context: stricter syntax for __context__ statement
  context: extra warning for __context__() & friends

 lib.c                     | 17 +++++++++-
 lib.h                     |  2 ++
 linearize.c               |  6 +---
 parse.c                   | 65 +++++++++++++++------------------------
 validation/attr-context.c | 40 ++++++++++++++++++++++++
 validation/context-stmt.c | 62 +++++++++++++++++++++++++++++++++++++
 6 files changed, 146 insertions(+), 46 deletions(-)
 create mode 100644 validation/attr-context.c
 create mode 100644 validation/context-stmt.c

-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 0/8] remove trivial phi-nodes ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 0/8] remove trivial phi-nodes
Date: Thu, 30 Aug 2018 22:26:08 +0000
Message-ID: <20180830222616.47360-1-luc.vanoostenryck () gmail ! com>
--------------------
Some phi-nodes are not needed. For example, a phi-node having a
unique value (either a single operand or several but all identical)
can be safely replaced with this unique value. Such trivial ph
nodes are already removed. The object of this series is to detect
and remove more complex trivial phi-nodes: ones where the operands
are themselves defined by a phi-node and when doing the transitive
closure there is only a single independent value. Such phi-nodes
can also be safely be replaced by this unique value.

This series is available for review & testing  in the Git repository at:
  git://github.com/lucvoo/sparse-dev.git rem-trivial-phi

----------------------------------------------------------------
Luc Van Oostenryck (8):
      move DEF_OPCODE() to header file
      trivial-phi: add testcase for unneeded trivial phi-nodes
      trivial-phi: make clean_up_phi() more sequential
      trivial-phi: extract trivial_phi() from clean_up_phi()
      trivial-phi: early return
      trivial-phi: use a temp var for the real source
      trivial-phi: directly return the unique value
      trivial-phi: remove more complex trivial phi-nodes

 linearize.h                     |  8 ++++++
 simplify.c                      | 63 ++++++++++++++++++++++++++++-------------
 validation/optim/trivial-phis.c | 14 +++++++++
 3 files changed, 66 insertions(+), 19 deletions(-)
 create mode 100644 validation/optim/trivial-phis.c
================================================================================


################################################################################

=== Thread: [PATCH 00/10] add support for sphinx-doc ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 00/10] add support for sphinx-doc
Date: Sat, 24 Feb 2018 01:47:07 +0000
Message-ID: <20180224014717.56552-1-luc.vanoostenryck () gmail ! com>
--------------------
This series add support for sphinx. With this we can have
nice HTML doc generated from either .rst or .md files.
One of the goal is, of course, to have this doc; another one
is to have a better input format for the manpage.

For the moment the generated doc can be accessed at:
	http://sparse-doc.rtfd.io

Luc Van Oostenryck (10):
  doc: fix typo in options.md
  doc: fix markdown syntax
  doc: fix headings
  doc: add minimal support for sphinx-doc
  doc: add logo
  doc: automatically set the copyright date
  doc: allow .md with py3-sphinx
  doc: move sparse.txt to markdown and rename it
  doc: the man page in reST
  doc: fix weirdness with option lists

 Documentation/.gitignore           |   3 +
 Documentation/Makefile             |  26 +++
 Documentation/conf.py              | 169 +++++++++++++++
 Documentation/index.rst            |  36 ++++
 Documentation/logo.svg             |  94 ++++++++
 Documentation/nocast-vs-bitwise.md |  41 ++++
 Documentation/options.md           |  18 +-
 Documentation/project-ideas.md     |   5 +-
 Documentation/sparse.rst           | 427 +++++++++++++++++++++++++++++++++++++
 Documentation/sparse.txt           |  45 ----
 10 files changed, 807 insertions(+), 57 deletions(-)
 create mode 100644 Documentation/.gitignore
 create mode 100644 Documentation/Makefile
 create mode 100644 Documentation/conf.py
 create mode 100644 Documentation/index.rst
 create mode 100644 Documentation/logo.svg
 create mode 100644 Documentation/nocast-vs-bitwise.md
 create mode 100644 Documentation/sparse.rst
 delete mode 100644 Documentation/sparse.txt

-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Randy Dunlap <rdunlap () infradead ! org>
To: linux-sparse
Subject: Re: [PATCH 00/10] add support for sphinx-doc
Date: Wed, 28 Feb 2018 19:36:50 +0000
Message-ID: <b5736c9e-e8fc-ac8b-2e41-72e6ab92f437 () infradead ! org>
--------------------
On 02/23/2018 05:47 PM, Luc Van Oostenryck wrote:
> This series add support for sphinx. With this we can have
> nice HTML doc generated from either .rst or .md files.
> One of the goal is, of course, to have this doc; another one
> is to have a better input format for the manpage.
> 
> For the moment the generated doc can be accessed at:
> 	http://sparse-doc.rtfd.io

That's a good start.  Thanks.

> Luc Van Oostenryck (10):
>   doc: fix typo in options.md
>   doc: fix markdown syntax
>   doc: fix headings
>   doc: add minimal support for sphinx-doc
>   doc: add logo
>   doc: automatically set the copyright date
>   doc: allow .md with py3-sphinx
>   doc: move sparse.txt to markdown and rename it
>   doc: the man page in reST
>   doc: fix weirdness with option lists

+	@sed '/^\.BI \\-[a-zA-Z]\\fB /s/\\fB //' < build/man/$@ > $@

I would prefer to see a little explanation of that sed magic, if you don't mind.


>  Documentation/.gitignore           |   3 +
>  Documentation/Makefile             |  26 +++
>  Documentation/conf.py              | 169 +++++++++++++++
>  Documentation/index.rst            |  36 ++++
>  Documentation/logo.svg             |  94 ++++++++
>  Documentation/nocast-vs-bitwise.md |  41 ++++
>  Documentation/options.md           |  18 +-
>  Documentation/project-ideas.md     |   5 +-
>  Documentation/sparse.rst           | 427 +++++++++++++++++++++++++++++++++++++
>  Documentation/sparse.txt           |  45 ----
>  10 files changed, 807 insertions(+), 57 deletions(-)
>  create mode 100644 Documentation/.gitignore
>  create mode 100644 Documentation/Makefile
>  create mode 100644 Documentation/conf.py
>  create mode 100644 Documentation/index.rst
>  create mode 100644 Documentation/logo.svg
>  create mode 100644 Documentation/nocast-vs-bitwise.md
>  create mode 100644 Documentation/sparse.rst
>  delete mode 100644 Documentation/sparse.txt


-- 
~Randy
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 01/10] doc: fix typo in options.md ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 01/10] doc: fix typo in options.md
Date: Sat, 24 Feb 2018 01:47:08 +0000
Message-ID: <20180224014717.56552-2-luc.vanoostenryck () gmail ! com>
--------------------
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/options.md | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/Documentation/options.md b/Documentation/options.md
index 14698a981..1e35b8227 100644
--- a/Documentation/options.md
+++ b/Documentation/options.md
@@ -1,6 +1,6 @@
 # Options
 
-This file is a complement of man page for sparse but meant
+This file is a complement of the man page for sparse but meant
 for options not to be used by sparse itself but by the other
 tools.
 
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 03/10] doc: fix headings ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 03/10] doc: fix headings
Date: Sat, 24 Feb 2018 01:47:10 +0000
Message-ID: <20180224014717.56552-4-luc.vanoostenryck () gmail ! com>
--------------------
To have proper entris in the index pages, we need to use headings
consistently.

Fix the few headings that used some headings non-consistent with
the others.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/project-ideas.md | 5 ++---
 1 file changed, 2 insertions(+), 3 deletions(-)

diff --git a/Documentation/project-ideas.md b/Documentation/project-ideas.md
index 380f850b7..bff5f3d66 100644
--- a/Documentation/project-ideas.md
+++ b/Documentation/project-ideas.md
@@ -9,8 +9,7 @@ Why hacking on sparse
    With sparse-llvm, sparse uses llvm as back end to emit real machine code.
 
 New developer hacking on sparse
-==============================
-
+------------------------------
 
 * All sparse warning messages should include the option how
    to disable it.  
@@ -29,7 +28,7 @@ used. (-Wno-vla)"
 * checker error output database
 
 For experienced developers
-==========================
+--------------------------
 
 * merge C type on incremental declare of C type and function prototype.
 * move attribute out of ctype to allow easier to add new attribute.
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 04/10] doc: add minimal support for sphinx-doc ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 04/10] doc: add minimal support for sphinx-doc
Date: Sat, 24 Feb 2018 01:47:11 +0000
Message-ID: <20180224014717.56552-5-luc.vanoostenryck () gmail ! com>
--------------------
With this we can generate HTML for the docs in this directory.
For the moment, more as an experiment as anything else, this
doc is available at http://sparse-doc.rtfd.io

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/.gitignore |   1 +
 Documentation/Makefile   |  22 +++++++
 Documentation/conf.py    | 161 +++++++++++++++++++++++++++++++++++++++++++++++
 Documentation/index.rst  |  28 +++++++++
 4 files changed, 212 insertions(+)
 create mode 100644 Documentation/.gitignore
 create mode 100644 Documentation/Makefile
 create mode 100644 Documentation/conf.py
 create mode 100644 Documentation/index.rst

diff --git a/Documentation/.gitignore b/Documentation/.gitignore
new file mode 100644
index 000000000..378eac25d
--- /dev/null
+++ b/Documentation/.gitignore
@@ -0,0 +1 @@
+build
diff --git a/Documentation/Makefile b/Documentation/Makefile
new file mode 100644
index 000000000..8bb5b9e49
--- /dev/null
+++ b/Documentation/Makefile
@@ -0,0 +1,22 @@
+# Minimal makefile for Sphinx documentation
+#
+
+# You can set these variables from the command line.
+SPHINXOPTS    = -a
+SPHINXBUILD   = sphinx-build
+SPHINXPROJ    = sparse
+SOURCEDIR     = .
+BUILDDIR      = build
+
+targets := help
+targets += html
+
+
+# Put it first so that "make" without argument is like "make help".
+help:
+
+# route all targets to Sphinx using the new "make mode" option.
+$(targets): conf.py Makefile
+	@$(SPHINXBUILD) -M  $@  "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS)
+
+.PHONY: Makefile	# avoid circular deps with the catch-all rule
diff --git a/Documentation/conf.py b/Documentation/conf.py
new file mode 100644
index 000000000..99cd39997
--- /dev/null
+++ b/Documentation/conf.py
@@ -0,0 +1,161 @@
+# -*- coding: utf-8 -*-
+#
+# This file is execfile()d with the current directory set to its
+# containing dir.
+#
+# Note that not all possible configuration values are present in this
+# autogenerated file.
+#
+# All configuration values have a default; values that are commented out
+# serve to show the default.
+
+# If extensions (or modules to document with autodoc) are in another
+# directory, add these directories to sys.path here. If the directory
+# is relative to the documentation root, use os.path.abspath to make
+# it absolute, like shown here.
+# sys.path.insert(0, os.path.abspath('.'))
+
+#
+# import os
+# import sys
+
+# -- General configuration ------------------------------------------------
+
+# If your documentation needs a minimal Sphinx version, state it here.
+#
+# needs_sphinx = '1.0'
+
+# Add any Sphinx extension module names here, as strings. They can be
+# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
+# ones.
+extensions = [
+]
+
+source_parsers = {
+   '.md': 'recommonmark.parser.CommonMarkParser',
+}
+
+# Add any paths that contain templates here, relative to this directory.
+templates_path = ['templates']
+
+# The suffix(es) of source filenames.
+# You can specify multiple suffix as a list of string:
+#
+source_suffix = ['.rst', '.md']
+
+# The master toctree document.
+master_doc = 'index'
+
+# General information about the project.
+project = 'sparse'
+copyright = '2017'
+author = "sparse's development community"
+
+# The version info for the project you're documenting, acts as replacement for
+# |version| and |release|, also used in various other places throughout the
+# built documents.
+#
+# The short X.Y version.
+version = u'0.5.9'
+# The full version, including alpha/beta/rc tags.
+release = u'0.5.9'
+
+# The language for content autogenerated by Sphinx. Refer to documentation
+# for a list of supported languages.
+#
+# This is also used if you do content translation via gettext catalogs.
+# Usually you set "language" from the command line for these cases.
+language = None
+
+# List of patterns, relative to source directory, that match files and
+# directories to ignore when looking for source files.
+# This patterns also effect to html_static_path and html_extra_path
+exclude_patterns = ['build']
+
+# The name of the Pygments (syntax highlighting) style to use.
+pygments_style = 'sphinx'
+
+# If true, `todo` and `todoList` produce output, else they produce nothing.
+todo_include_todos = True
+
+# -- Options for HTML output ----------------------------------------------
+
+# The theme to use for HTML and HTML Help pages.  See the documentation for
+# a list of builtin themes.
+#
+html_theme = 'classic'
+# html_theme_options = {}
+
+# Add any paths that contain custom static files (such as style sheets) here,
+# relative to this directory. They are copied after the builtin static files,
+# so a file named "default.css" will overwrite the builtin "default.css".
+# html_static_path = ['sphinx/static']
+
+# Custom sidebar templates, must be a dictionary that maps document names
+# to template names.
+#
+# This is required for the alabaster theme
+# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
+html_sidebars = {
+	'**': [
+	    'relations.html',  # needs 'show_related': True theme option to display
+	    'searchbox.html',
+	]
+}
+
+html_logo = None
+
+# -- Options for HTMLHelp output ------------------------------------------
+
+# Output file base name for HTML help builder.
+htmlhelp_basename = 'sparsedoc'
+
+
+# -- Options for LaTeX output ---------------------------------------------
+
+latex_elements = {
+	# The paper size ('letterpaper' or 'a4paper').
+	#
+	'papersize': 'a4paper',
+
+	# The font size ('10pt', '11pt' or '12pt').
+	#
+	# 'pointsize': '10pt',
+
+	# Additional stuff for the LaTeX preamble.
+	#
+	# 'preamble': '',
+
+	# Latex figure (float) alignment
+	#
+	# 'figure_align': 'htbp',
+}
+
+# Grouping the document tree into LaTeX files. List of tuples
+# (source start file, target name, title,
+#  author, documentclass [howto, manual, or own class]).
+latex_documents = [
+	('index', 'sparse.tex', u'sparse Documentation', author, 'manual'),
+]
+
+
+# -- Options for manual page output ---------------------------------------
+
+# One entry per manual page. List of tuples
+# (source start file, name, description, authors, manual section).
+man_pages = [
+	('sparse', 'sparse', u'Semantic Parser for C', [u'Linus Torvalds', author], 1)
+]
+
+
+# -- Options for Texinfo output -------------------------------------------
+
+# Grouping the document tree into Texinfo files. List of tuples
+# (source start file, target name, title, author,
+#  dir menu entry, description, category)
+texinfo_documents = [
+	('index', 'sparse', u'sparse Documentation', author, 'sparse', 'C semantic parser & checker', 'Software development'),
+]
+
+
+# vim: tabstop=4
diff --git a/Documentation/index.rst b/Documentation/index.rst
new file mode 100644
index 000000000..f68bb42f2
--- /dev/null
+++ b/Documentation/index.rst
@@ -0,0 +1,28 @@
+.. sparse documentation master file.
+
+Welcome to sparse's documentation
+=================================
+
+.. toctree::
+   :maxdepth: 1
+
+Developer documentation
+-----------------------
+.. toctree::
+   :maxdepth: 1
+
+   options
+   IR
+
+How to contribute
+-----------------
+.. toctree::
+   :maxdepth: 1
+
+   submitting-patches
+   project-ideas
+
+Indices and tables
+==================
+
+* :ref:`genindex`
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 05/10] bool: add testcase for bool simplification ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 05/10] bool: add testcase for bool simplification
Date: Tue, 26 Jun 2018 05:59:57 +0000
Message-ID: <20180626060002.35753-6-luc.vanoostenryck () gmail ! com>
--------------------
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/optim/bool-simplify2.c | 29 +++++++++++++++++++++++++++++
 1 file changed, 29 insertions(+)
 create mode 100644 validation/optim/bool-simplify2.c

diff --git a/validation/optim/bool-simplify2.c b/validation/optim/bool-simplify2.c
new file mode 100644
index 000000000..7e7548fc5
--- /dev/null
+++ b/validation/optim/bool-simplify2.c
@@ -0,0 +1,29 @@
+static int foo(int a, int b, int c)
+{
+	return a || b || c;
+}
+
+/*
+ * check-name: bool-simplify2
+ * check-command: test-linearize $file
+ *
+ * check-output-pattern(4): setne\\.
+ * check-output-pattern(2): zext\\.
+ *
+ * check-output-start
+foo:
+.L0:
+	<entry-point>
+	setne.1     %r2 <- %arg1, $0
+	setne.1     %r4 <- %arg2, $0
+	or-bool.1   %r5 <- %r2, %r4
+	zext.32     %r6 <- (1) %r5
+	setne.1     %r7 <- %r6, $0
+	setne.1     %r9 <- %arg3, $0
+	or-bool.1   %r10 <- %r7, %r9
+	zext.32     %r11 <- (1) %r10
+	ret.32      %r11
+
+
+ * check-output-end
+ */
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 05/10] doc: add logo ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 05/10] doc: add logo
Date: Sat, 24 Feb 2018 01:47:12 +0000
Message-ID: <20180224014717.56552-6-luc.vanoostenryck () gmail ! com>
--------------------
A logo is always nice fot a project, so add one.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/conf.py  |  2 +-
 Documentation/logo.svg | 94 ++++++++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 95 insertions(+), 1 deletion(-)
 create mode 100644 Documentation/logo.svg

diff --git a/Documentation/conf.py b/Documentation/conf.py
index 99cd39997..8d7c62abc 100644
--- a/Documentation/conf.py
+++ b/Documentation/conf.py
@@ -103,7 +103,7 @@ html_sidebars = {
 	]
 }
 
-html_logo = None
+html_logo = 'logo.svg'
 
 # -- Options for HTMLHelp output ------------------------------------------
 
diff --git a/Documentation/logo.svg b/Documentation/logo.svg
new file mode 100644
index 000000000..d91da9db1
--- /dev/null
+++ b/Documentation/logo.svg
@@ -0,0 +1,94 @@
+<?xml version="1.0" encoding="UTF-8" standalone="no"?>
+<svg
+   xmlns:dc="http://purl.org/dc/elements/1.1/"
+   xmlns:cc="http://creativecommons.org/ns#"
+   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
+   xmlns:svg="http://www.w3.org/2000/svg"
+   xmlns="http://www.w3.org/2000/svg"
+   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
+   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
+   width="425.19684"
+   height="170.07874"
+   id="svg2"
+   version="1.1"
+   inkscape:version="0.48.5 r10040"
+   sodipodi:docname="logo.svg">
+  <metadata
+     id="metadata24">
+    <rdf:RDF>
+      <cc:Work
+         rdf:about="">
+        <dc:format>image/svg+xml</dc:format>
+        <dc:type
+           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
+      </cc:Work>
+    </rdf:RDF>
+  </metadata>
+  <defs
+     id="defs22" />
+  <sodipodi:namedview
+     pagecolor="#ffffff"
+     bordercolor="#666666"
+     borderopacity="1"
+     objecttolerance="10"
+     gridtolerance="10"
+     guidetolerance="10"
+     inkscape:pageopacity="0"
+     inkscape:pageshadow="2"
+     inkscape:window-width="1122"
+     inkscape:window-height="764"
+     id="namedview20"
+     showgrid="false"
+     units="mm"
+     inkscape:zoom="0.41627715"
+     inkscape:cx="115.87347"
+     inkscape:cy="283.465"
+     inkscape:window-x="0"
+     inkscape:window-y="0"
+     inkscape:window-maximized="0"
+     inkscape:current-layer="svg2" />
+  <g
+     id="g4"
+     transform="matrix(0.8,0,0,0.8,-15.072782,-220.54503)">
+    <title
+       id="title6">Layer 1</title>
+    <path
+       id="svg_2"
+       d="m 118.652,337.78799 0,8.36701 c -8.532,-5.01502 -15.868,-7.52301 -22.008003,-7.52301 -5.905998,0 -10.839996,1.793 -14.800995,5.379 -3.962006,3.586 -5.941002,8.02801 -5.941002,13.324 0,3.797 1.170998,7.22 3.515999,10.26599 2.343002,3.04801 7.242004,5.97702 14.695,8.789 7.453001,2.81198 12.890001,5.15702 16.312001,7.03101 3.421,1.87601 6.328,4.595 8.719,8.156 2.391,3.56299 3.586,8.181 3.586,13.85199 0,7.54801 -2.767,13.922 -8.297,19.125 -5.532,5.20301 -12.282,7.805 -20.250002,7.805 -8.155998,0 -16.547997,-2.858 -25.171997,-8.57801 l 0,-8.92999 C 78.526,421.836 86.776,425.328 93.761001,425.328 c 5.811996,0 10.722999,-1.88602 14.729999,-5.66 4.00699,-3.77399 6.012,-8.40302 6.012,-13.88901 0,-4.173 -1.243,-7.935 -3.727,-11.28601 -2.484,-3.353 -7.521,-6.48499 -15.111999,-9.4 -7.590004,-2.914 -13.014999,-5.26199 -16.275002,-7.04098 -3.261002,-1.77902 -6.028,-4.32202 -8.303001,-7.625 -2.275002,-3.302 -3.411995,-7.55402 -3.411995,-12.755 0,-7.21402 2.764999,-13.362 8.296997,-18.44599 5.529999,-5.082 12.186996,-7.62402 19.969002,-7.62402 7.640998,-9.8e-4 15.210998,2.06201 22.711998,6.186 z"
+       inkscape:connector-curvature="0" />
+    <path
+       id="svg_3"
+       d="m 139.886,332.866 27.06999,0 c 10.35901,0 18.29201,2.32 23.80101,6.961 5.50701,4.64099 8.262,10.96899 8.262,18.98401 0,8.10998 -2.777,14.53198 -8.332,19.26599 -5.555,4.73501 -13.7,7.10199 -24.43399,7.10199 l -18.77301,0 0,45.914 -7.594,0 0,-98.22699 z m 7.594,6.328 0,39.65601 18.281,0 c 8.202,0 14.437,-1.73301 18.70299,-5.20301 4.265,-3.46802 6.39799,-8.367 6.39799,-14.695 0,-6.047 -2.08599,-10.85099 -6.25799,-14.414 -4.173,-3.56199 -10.149,-5.344 -17.92999,-5.344 l -19.194,0 z"
+       inkscape:connector-curvature="0" />
+    <path
+       id="svg_4"
+       d="m 239.248,332.16299 45.41301,98.93 -7.96802,0 -39.59999,-86.555 -39.45801,86.555 -7.96699,0 45.408,-98.93 4.172,0 z"
+       inkscape:connector-curvature="0"
+       style="fill:#ff0000" />
+    <path
+       id="svg_5"
+       d="m 296.543,332.866 25.172,0 c 10.54699,0 18.60699,2.285 24.18801,6.85501 5.577,4.56997 8.36698,10.74698 8.36698,18.52698 0,11.345 -6.23499,19.31301 -18.703,23.906 3.234,1.547 7.59402,6.539 13.078,14.97702 l 22.21902,33.961 -8.99002,0 -17.10898,-26.92102 c -5.71701,-9.004 -10.169,-14.61798 -13.35501,-16.84699 -3.18701,-2.22799 -7.85,-3.34201 -13.987,-3.34201 l -13.28399,0 0,47.10901 -7.594,0 0,-98.225 -0.002,0 z m 7.59399,6.328 0,38.461 16.31201,0 c 8.297,0 14.63498,-1.67499 19.01999,-5.02701 4.38199,-3.35098 6.57401,-8.09698 6.57401,-14.23798 0,-6.04702 -2.228,-10.75802 -6.68002,-14.13302 -4.45499,-3.375 -10.85398,-5.06198 -19.19498,-5.06198 l -16.03101,0 0,-0.001 z"
+       inkscape:connector-curvature="0" />
+    <path
+       id="svg_6"
+       d="m 425.285,337.78799 0,8.36701 c -8.53299,-5.01502 -15.86801,-7.52301 -22.00799,-7.52301 -5.90601,0 -10.841,1.793 -14.801,5.379 -3.96301,3.586 -5.94101,8.02801 -5.94101,13.324 0,3.797 1.16999,7.22 3.51599,10.26599 2.34302,3.04801 7.24201,5.97702 14.69501,8.789 7.453,2.81198 12.88998,5.15702 16.31201,7.03101 3.41999,1.87601 6.32797,4.595 8.719,8.156 2.39099,3.56299 3.586,8.181 3.586,13.85199 0,7.54801 -2.76801,13.922 -8.297,19.125 -5.53201,5.20301 -12.28201,7.805 -20.25,7.805 -8.15601,0 -16.54901,-2.858 -25.172,-8.57801 l 0,-8.92999 c 9.51499,6.98502 17.76398,10.47702 24.75,10.47702 5.81098,0 10.72299,-1.88602 14.72998,-5.66 4.00699,-3.77399 6.012,-8.40302 6.012,-13.88901 0,-4.173 -1.24299,-7.935 -3.72699,-11.28601 -2.48401,-3.353 -7.521,-6.48499 -15.112,-9.4 -7.59101,-2.914 -13.01499,-5.26199 -16.27399,-7.04098 -3.26203,-1.77902 -6.02902,-4.32202 -8.30301,-7.625 -2.27499,-3.302 -3.41199,-7.55402 -3.41199,-12.755 0,-7.21402 2.76499,-13.362 8.297,-18.44599 5.52899,-5.082 12.18698,-7.62402 19.96899,-7.62402 7.63901,-9.8e-4 15.20999,2.06201 22.711,6.186 z"
+       inkscape:connector-curvature="0" />
+    <path
+       id="svg_7"
+       d="m 500.379,332.866 0,6.328 -45.98401,0 0,39.30499 20.60501,0 0,6.39801 -20.60501,0 0,39.86701 47.10901,0 0,6.328 -54.70301,0 0,-98.22702 53.57801,0 0,0.001 z"
+       inkscape:connector-curvature="0"
+       style="fill:#00f200" />
+    <g
+       id="svg_8" />
+    <g
+       id="svg_9" />
+    <g
+       id="svg_10" />
+    <g
+       id="svg_11" />
+    <g
+       id="svg_12" />
+    <g
+       id="svg_13" />
+  </g>
+</svg>
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 06/10] bool: simplify ZEXT in bool -> int -> bool ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 06/10] bool: simplify ZEXT in bool -> int -> bool
Date: Tue, 26 Jun 2018 05:59:58 +0000
Message-ID: <20180626060002.35753-7-luc.vanoostenryck () gmail ! com>
--------------------
Because of C's integer promotion, in code like 'a == 0',
the operand 'a' must be promoted to int. So, if 'a' is
of type 'bool', it results in following linearization:
	zext.32  %t <- (1) %a
	setne.32 %r <- %t, $0

While this promotion is required by the standard at C level,
here, from an operational PoV, the zero-extension is unneeded
since the result will be the same without it.

Change this by simplifying away such zero-extensions.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c                        | 15 ++++++++++++++-
 validation/optim/bool-int-bool.c  | 12 ++++++++++++
 validation/optim/bool-simplify2.c |  5 ++---
 3 files changed, 28 insertions(+), 4 deletions(-)
 create mode 100644 validation/optim/bool-int-bool.c

diff --git a/simplify.c b/simplify.c
index c88ea5c33..9f0d13b55 100644
--- a/simplify.c
+++ b/simplify.c
@@ -612,9 +612,22 @@ static int simplify_seteq_setne(struct instruction *insn, long long value)
 		remove_usage(old, &insn->src1);
 		return REPEAT_CSE;
 
+	case OP_ZEXT:
+		if (def->orig_type->bit_size == 1) {
+			// Convert:
+			//	zext.n	%s <- (1) %a
+			//	setne.n %r <- %s, $0
+			// into:
+			//	setne.n %s <- %a, $0
+			// and same for setne/eq ... 0/1
+			return replace_pseudo(insn, &insn->src1, def->src1);
+		}
+		break;
+
 	default:
-		return 0;
+		break;
 	}
+	return 0;
 }
 
 static int simplify_constant_rightside(struct instruction *insn)
diff --git a/validation/optim/bool-int-bool.c b/validation/optim/bool-int-bool.c
new file mode 100644
index 000000000..de34a68bb
--- /dev/null
+++ b/validation/optim/bool-int-bool.c
@@ -0,0 +1,12 @@
+_Bool beq0(_Bool a) { return (a == 0); }
+_Bool beq1(_Bool a) { return (a == 1); }
+_Bool bne0(_Bool a) { return (a != 0); }
+_Bool bne1(_Bool a) { return (a != 1); }
+
+/*
+ * check-name: bool - int - bool constants
+ * check-command: test-linearize -Wno-decl $file
+ *
+ * check-output-ignore
+ * check-output-excludes: cast\\.
+ */
diff --git a/validation/optim/bool-simplify2.c b/validation/optim/bool-simplify2.c
index 7e7548fc5..72fc2f2ea 100644
--- a/validation/optim/bool-simplify2.c
+++ b/validation/optim/bool-simplify2.c
@@ -8,7 +8,7 @@ static int foo(int a, int b, int c)
  * check-command: test-linearize $file
  *
  * check-output-pattern(4): setne\\.
- * check-output-pattern(2): zext\\.
+ * check-output-pattern(1): zext\\.
  *
  * check-output-start
 foo:
@@ -17,8 +17,7 @@ foo:
 	setne.1     %r2 <- %arg1, $0
 	setne.1     %r4 <- %arg2, $0
 	or-bool.1   %r5 <- %r2, %r4
-	zext.32     %r6 <- (1) %r5
-	setne.1     %r7 <- %r6, $0
+	setne.1     %r7 <- %r5, $0
 	setne.1     %r9 <- %arg3, $0
 	or-bool.1   %r10 <- %r7, %r9
 	zext.32     %r11 <- (1) %r10
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 06/10] doc: automatically set the copyright date ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 06/10] doc: automatically set the copyright date
Date: Sat, 24 Feb 2018 01:47:13 +0000
Message-ID: <20180224014717.56552-7-luc.vanoostenryck () gmail ! com>
--------------------
Sphinx allow to set the copyright date; I would hate to have
to update this every year, so let generate it automatically.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/conf.py | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/Documentation/conf.py b/Documentation/conf.py
index 8d7c62abc..0e475c07e 100644
--- a/Documentation/conf.py
+++ b/Documentation/conf.py
@@ -18,6 +18,7 @@
 #
 # import os
 # import sys
+import datetime
 
 # -- General configuration ------------------------------------------------
 
@@ -48,7 +49,7 @@ master_doc = 'index'
 
 # General information about the project.
 project = 'sparse'
-copyright = '2017'
+copyright = '2003 - ' + str(datetime.datetime.now().year)
 author = "sparse's development community"
 
 # The version info for the project you're documenting, acts as replacement for
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 08/10] bool: simplify 'x != 0' or 'x == 1' to 'x' ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 08/10] bool: simplify 'x != 0' or 'x == 1' to 'x'
Date: Tue, 26 Jun 2018 06:00:00 +0000
Message-ID: <20180626060002.35753-9-luc.vanoostenryck () gmail ! com>
--------------------
These two comparisons are no-ops when the operand is a bool.

Simplify away these comparisons.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c                         | 7 +++++++
 validation/optim/bool-context-fp.c | 6 ++----
 validation/optim/bool-simplify2.c  | 5 ++---
 3 files changed, 11 insertions(+), 7 deletions(-)

diff --git a/simplify.c b/simplify.c
index 9f0d13b55..89532646a 100644
--- a/simplify.c
+++ b/simplify.c
@@ -625,6 +625,13 @@ static int simplify_seteq_setne(struct instruction *insn, long long value)
 		break;
 
 	default:
+		if (def->size == 1 && !inverse)
+			// Replace:
+			//	setne.1	%r <- %s, $0
+			// or:
+			//	seteq.1	%r <- %s, $1
+			// by %s
+			return replace_with_pseudo(insn, old);
 		break;
 	}
 	return 0;
diff --git a/validation/optim/bool-context-fp.c b/validation/optim/bool-context-fp.c
index c3c2e546c..6c80eeb17 100644
--- a/validation/optim/bool-context-fp.c
+++ b/validation/optim/bool-context-fp.c
@@ -54,8 +54,7 @@ bfior:
 	fcmpune.1   %r20 <- %arg1, %r19
 	fcmpune.1   %r23 <- %arg2, %r19
 	or-bool.1   %r24 <- %r20, %r23
-	setne.1     %r26 <- %r24, $0
-	ret.1       %r26
+	ret.1       %r24
 
 
 ifior:
@@ -76,8 +75,7 @@ bfand:
 	fcmpune.1   %r39 <- %arg1, %r38
 	fcmpune.1   %r42 <- %arg2, %r38
 	and-bool.1  %r43 <- %r39, %r42
-	setne.1     %r45 <- %r43, $0
-	ret.1       %r45
+	ret.1       %r43
 
 
 ifand:
diff --git a/validation/optim/bool-simplify2.c b/validation/optim/bool-simplify2.c
index 72fc2f2ea..e015fe8ec 100644
--- a/validation/optim/bool-simplify2.c
+++ b/validation/optim/bool-simplify2.c
@@ -7,7 +7,7 @@ static int foo(int a, int b, int c)
  * check-name: bool-simplify2
  * check-command: test-linearize $file
  *
- * check-output-pattern(4): setne\\.
+ * check-output-pattern(3): setne\\.
  * check-output-pattern(1): zext\\.
  *
  * check-output-start
@@ -17,9 +17,8 @@ foo:
 	setne.1     %r2 <- %arg1, $0
 	setne.1     %r4 <- %arg2, $0
 	or-bool.1   %r5 <- %r2, %r4
-	setne.1     %r7 <- %r5, $0
 	setne.1     %r9 <- %arg3, $0
-	or-bool.1   %r10 <- %r7, %r9
+	or-bool.1   %r10 <- %r5, %r9
 	zext.32     %r11 <- (1) %r10
 	ret.32      %r11
 
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 08/10] doc: move sparse.txt to markdown and rename it ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 08/10] doc: move sparse.txt to markdown and rename it
Date: Sat, 24 Feb 2018 01:47:15 +0000
Message-ID: <20180224014717.56552-9-luc.vanoostenryck () gmail ! com>
--------------------
The file 'sparse.txt' contains some explanation about the
differences between the attributes 'nocast' and 'bitwise'.
It's valuable information.

So convert it to markdown syntax and rename it to a more
self-explanatory name.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/index.rst            |  7 ++++++
 Documentation/nocast-vs-bitwise.md | 41 ++++++++++++++++++++++++++++++++++
 Documentation/sparse.txt           | 45 --------------------------------------
 3 files changed, 48 insertions(+), 45 deletions(-)
 create mode 100644 Documentation/nocast-vs-bitwise.md
 delete mode 100644 Documentation/sparse.txt

diff --git a/Documentation/index.rst b/Documentation/index.rst
index f68bb42f2..d5aa0c5bf 100644
--- a/Documentation/index.rst
+++ b/Documentation/index.rst
@@ -6,6 +6,13 @@ Welcome to sparse's documentation
 .. toctree::
    :maxdepth: 1
 
+User documentation
+------------------
+.. toctree::
+   :maxdepth: 1
+
+   nocast-vs-bitwise
+
 Developer documentation
 -----------------------
 .. toctree::
diff --git a/Documentation/nocast-vs-bitwise.md b/Documentation/nocast-vs-bitwise.md
new file mode 100644
index 000000000..b649abcd5
--- /dev/null
+++ b/Documentation/nocast-vs-bitwise.md
@@ -0,0 +1,41 @@
+# __nocast vs __bitwise
+
+`__nocast` warns about explicit or implicit casting to different types.
+HOWEVER, it doesn't consider two 32-bit integers to be different
+types, so a `__nocast int` type may be returned as a regular `int`
+type and then the `__nocast` is lost.
+
+So `__nocast` on integer types is usually not that powerful. It just
+gets lost too easily. It's more useful for things like pointers. It
+also doesn't warn about the mixing: you can add integers to `__nocast`
+integer types, and it's not really considered anything wrong.
+
+`__bitwise` ends up being a *stronger integer separation*. That one
+doesn't allow you to mix with non-bitwise integers, so now it's much
+harder to lose the type by mistake.
+
+So the basic rule is:
+
+ - `__nocast` on its own tends to be more useful for *big* integers
+that still need to act like integers, but you want to make it much
+less likely that they get truncated by mistake. So a 64-bit integer
+that you don't want to mistakenly/silently be returned as `int`, for
+example. But they mix well with random integer types, so you can add
+to them etc without using anything special. However, that mixing also
+means that the `__nocast` really gets lost fairly easily.
+
+ - `__bitwise` is for *unique types* that cannot be mixed with other
+types, and that you'd never want to just use as a random integer (the
+integer `0` is special, though, and gets silently accepted - it's
+kind of like `NULL` for pointers). So `gfp_t` or the `safe endianness`
+types would be `__bitwise`: you can only operate on them by doing
+specific operations that know about *that* particular type.
+
+Generally, you want `__bitwise` if you are looking for type safety.
+`__nocast` really is pretty weak.
+
+## Reference:
+
+* Linus' e-mail about `__nocast` vs `__bitwise`:
+
+  <https://marc.info/?l=linux-mm&m=133245421127324&w=2>
diff --git a/Documentation/sparse.txt b/Documentation/sparse.txt
deleted file mode 100644
index 383376c04..000000000
--- a/Documentation/sparse.txt
+++ /dev/null
@@ -1,45 +0,0 @@
-Sparse
-~~~~~~
-
-__nocast vs __bitwise:
-
-__nocast warns about explicit or implicit casting to different types.
-
-HOWEVER, it doesn't consider two 32-bit integers to be different
-types, so a __nocast 'int' type may be returned as a regular 'int'
-type and then the __nocast is lost.
-
-So "__nocast" on integer types is usually not that powerful. It just
-gets lost too easily. It's more useful for things like pointers. It
-also doesn't warn about the mixing: you can add integers to __nocast
-integer types, and it's not really considered anything wrong.
-
-__bitwise ends up being a "stronger integer separation". That one
-doesn't allow you to mix with non-bitwise integers, so now it's much
-harder to lose the type by mistake.
-
-So the basic rule is:
-
- - "__nocast" on its own tends to be more useful for *big* integers
-that still need to act like integers, but you want to make it much
-less likely that they get truncated by mistake. So a 64-bit integer
-that you don't want to mistakenly/silently be returned as "int", for
-example. But they mix well with random integer types, so you can add
-to them etc without using anything special. However, that mixing also
-means that the __nocast really gets lost fairly easily.
-
- - "__bitwise" is for *unique types* that cannot be mixed with other
-types, and that you'd never want to just use as a random integer (the
-integer 0 is special, though, and gets silently accepted iirc - it's
-kind of like "NULL" for pointers). So "gfp_t" or the "safe endianness"
-types would be __bitwise: you can only operate on them by doing
-specific operations that know about *that* particular type.
-
-Generally, you want __bitwise if you are looking for type safety.
-"__nocast" really is pretty weak.
-
-Reference:
-
-* Linus' e-mail about __nocast vs __bitwise:
-
-  http://marc.info/?l=linux-mm&m=133245421127324&w=2
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 09/10] bool: generate plain OP_{AND,OR} instead of OP_{AND,OR}_BOOL ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 09/10] bool: generate plain OP_{AND,OR} instead of OP_{AND,OR}_BOOL
Date: Tue, 26 Jun 2018 06:00:01 +0000
Message-ID: <20180626060002.35753-10-luc.vanoostenryck () gmail ! com>
--------------------
Now that OP_AND_BOOL and OP_OR_BOOL are always given boolean
operands, they are just a special case of 1 bit OP_AND & OP_OR.

To avoid to have to repeat CSE, simplification patterns, ...
better to generate plain OP_AND & OP_OR instead.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 linearize.c                        |  2 +-
 validation/optim/bool-context-fp.c |  8 ++++----
 validation/optim/bool-simplify.c   | 25 +++++++++++++++++++++++++
 validation/optim/bool-simplify2.c  |  4 ++--
 4 files changed, 32 insertions(+), 7 deletions(-)

diff --git a/linearize.c b/linearize.c
index 726e911bc..bd9914507 100644
--- a/linearize.c
+++ b/linearize.c
@@ -1487,7 +1487,7 @@ static pseudo_t linearize_call_expression(struct entrypoint *ep, struct expressi
 static pseudo_t linearize_binop_bool(struct entrypoint *ep, struct expression *expr)
 {
 	pseudo_t src1, src2, dst;
-	int op = (expr->op == SPECIAL_LOGICAL_OR) ? OP_OR_BOOL : OP_AND_BOOL;
+	int op = (expr->op == SPECIAL_LOGICAL_OR) ? OP_OR : OP_AND;
 
 	src1 = linearize_expression_to_bool(ep, expr->left);
 	src2 = linearize_expression_to_bool(ep, expr->right);
diff --git a/validation/optim/bool-context-fp.c b/validation/optim/bool-context-fp.c
index 6c80eeb17..6325ee365 100644
--- a/validation/optim/bool-context-fp.c
+++ b/validation/optim/bool-context-fp.c
@@ -53,7 +53,7 @@ bfior:
 	setfval.32  %r19 <- 0.000000
 	fcmpune.1   %r20 <- %arg1, %r19
 	fcmpune.1   %r23 <- %arg2, %r19
-	or-bool.1   %r24 <- %r20, %r23
+	or.1        %r24 <- %r20, %r23
 	ret.1       %r24
 
 
@@ -63,7 +63,7 @@ ifior:
 	setfval.32  %r29 <- 0.000000
 	fcmpune.1   %r30 <- %arg1, %r29
 	fcmpune.1   %r33 <- %arg2, %r29
-	or-bool.1   %r34 <- %r30, %r33
+	or.1        %r34 <- %r30, %r33
 	zext.32     %r35 <- (1) %r34
 	ret.32      %r35
 
@@ -74,7 +74,7 @@ bfand:
 	setfval.32  %r38 <- 0.000000
 	fcmpune.1   %r39 <- %arg1, %r38
 	fcmpune.1   %r42 <- %arg2, %r38
-	and-bool.1  %r43 <- %r39, %r42
+	and.1       %r43 <- %r39, %r42
 	ret.1       %r43
 
 
@@ -84,7 +84,7 @@ ifand:
 	setfval.32  %r48 <- 0.000000
 	fcmpune.1   %r49 <- %arg1, %r48
 	fcmpune.1   %r52 <- %arg2, %r48
-	and-bool.1  %r53 <- %r49, %r52
+	and.1       %r53 <- %r49, %r52
 	zext.32     %r54 <- (1) %r53
 	ret.32      %r54
 
diff --git a/validation/optim/bool-simplify.c b/validation/optim/bool-simplify.c
index 5b3cf449e..68aabb782 100644
--- a/validation/optim/bool-simplify.c
+++ b/validation/optim/bool-simplify.c
@@ -18,6 +18,17 @@ int or_1(int a)
 	return a || 1;
 }
 
+// try again but with something true but != 1
+int and_2(int a)
+{
+	return a && 2;
+}
+
+int or_2(int a)
+{
+	return a || 2;
+}
+
 /*
  * check-name: bool-simplify
  * check-command: test-linearize -Wno-decl $file
@@ -51,5 +62,19 @@ or_1:
 	ret.32      $1
 
 
+and_2:
+.L8:
+	<entry-point>
+	setne.1     %r26 <- %arg1, $0
+	zext.32     %r29 <- (1) %r26
+	ret.32      %r29
+
+
+or_2:
+.L10:
+	<entry-point>
+	ret.32      $1
+
+
  * check-output-end
  */
diff --git a/validation/optim/bool-simplify2.c b/validation/optim/bool-simplify2.c
index e015fe8ec..70b0587ea 100644
--- a/validation/optim/bool-simplify2.c
+++ b/validation/optim/bool-simplify2.c
@@ -16,9 +16,9 @@ foo:
 	<entry-point>
 	setne.1     %r2 <- %arg1, $0
 	setne.1     %r4 <- %arg2, $0
-	or-bool.1   %r5 <- %r2, %r4
+	or.1        %r5 <- %r2, %r4
 	setne.1     %r9 <- %arg3, $0
-	or-bool.1   %r10 <- %r5, %r9
+	or.1        %r10 <- %r5, %r9
 	zext.32     %r11 <- (1) %r10
 	ret.32      %r11
 
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 09/10] doc: the man page in reST ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 09/10] doc: the man page in reST
Date: Sat, 24 Feb 2018 01:47:16 +0000
Message-ID: <20180224014717.56552-10-luc.vanoostenryck () gmail ! com>
--------------------
Manpages are a good thing but as an input format it's a
nightmare. It would be nice to have a more adapted format
for it.

So convert the manpage to reST and add to the Makefile what
is needed to generate the man page from it.

NB. This is still an experiment, but it's my intention to
    make this the source file for the manpage.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/.gitignore |   2 +
 Documentation/Makefile   |   4 +
 Documentation/index.rst  |   1 +
 Documentation/sparse.rst | 427 +++++++++++++++++++++++++++++++++++++++++++++++
 4 files changed, 434 insertions(+)
 create mode 100644 Documentation/sparse.rst

diff --git a/Documentation/.gitignore b/Documentation/.gitignore
index 378eac25d..b67ed65f5 100644
--- a/Documentation/.gitignore
+++ b/Documentation/.gitignore
@@ -1 +1,3 @@
 build
+sparse.1
+cgcc.1
diff --git a/Documentation/Makefile b/Documentation/Makefile
index 8bb5b9e49..2e8cbdfd7 100644
--- a/Documentation/Makefile
+++ b/Documentation/Makefile
@@ -10,6 +10,7 @@ BUILDDIR      = build
 
 targets := help
 targets += html
+targets += man
 
 
 # Put it first so that "make" without argument is like "make help".
@@ -19,4 +20,7 @@ help:
 $(targets): conf.py Makefile
 	@$(SPHINXBUILD) -M  $@  "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS)
 
+%.1: %.rst man
+	@mv build/man/$@ $@
+
 .PHONY: Makefile	# avoid circular deps with the catch-all rule
diff --git a/Documentation/index.rst b/Documentation/index.rst
index d5aa0c5bf..afa1ccc92 100644
--- a/Documentation/index.rst
+++ b/Documentation/index.rst
@@ -11,6 +11,7 @@ User documentation
 .. toctree::
    :maxdepth: 1
 
+   sparse
    nocast-vs-bitwise
 
 Developer documentation
diff --git a/Documentation/sparse.rst b/Documentation/sparse.rst
new file mode 100644
index 000000000..6d7dbe033
--- /dev/null
+++ b/Documentation/sparse.rst
@@ -0,0 +1,427 @@
+.. Sparse manpage by Josh Triplett
+.. highlight:: c
+
+sparse - Semantic Parser for C
+##############################
+
+SYNOPSIS
+========
+``sparse`` [*options*]... *file.c*
+
+DESCRIPTION
+===========
+Sparse parses C source and looks for errors, producing warnings on standard
+error.
+
+Sparse accepts options controlling the set of warnings to generate. To turn
+on warnings Sparse does not issue by default, use the corresponding warning
+option ``-Wsomething``. Sparse issues some warnings by default; to turn
+off those warnings, pass the negation of the associated warning option,
+``-Wno-something``.
+
+OPTIONS
+=======
+
+WARNING OPTIONS
+---------------
+
+-Wsparse-all
+	Turn on all sparse warnings, except for those explicitly disabled via
+	``-Wno-something``.
+
+
+-Wsparse-error
+	Turn all sparse warnings into errors.
+
+
+-Waddress-space
+	Warn about code which mixing pointers to different address spaces.
+
+	Sparse allows on pointers an extended attribute
+	``__attribute((address_space(n)))`` which designates a pointer target
+	in address space *n* (a constant integer). With ``-Waddress-space``,
+	Sparse treats pointers with identical target types but different
+	address spaces as distinct types. To override this warning, such as
+	for functions which convert pointers between address spaces, use a
+	type that includes ``__attribute__((force))``.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-address-space``.
+
+
+-Wbitwise
+	Warn about unsupported operations or type mismatches with restricted
+	integer types.
+
+	Sparse supports an extended attribute, ``__attribute__((bitwise))``,
+	which creates a new restricted integer type from a base integer type,
+	distinct from the base integer type and from any other restricted
+	integer type not declared in the same declaration or ``typedef``.
+	For example, this allows programs to create typedefs for integer
+	types with specific endianness. With ``-Wbitwise``, Sparse will warn
+	on any use of a restricted type in arithmetic operations other than
+	bitwise operations, and on any conversion of one restricted type into
+	another, except via a cast that includes ``__attribute__((force))``.
+
+	``__bitwise`` ends up being a "stronger integer separation", one that
+	doesn't allow you to mix with non-bitwise integers, so now it's much
+	harder to lose the type by mistake.
+
+	``__bitwise`` is for *unique types* that cannot be mixed with other
+	types, and that you'd never want to just use as a random integer (the
+	integer 0 is special, though, and gets silently accepted iirc - it's
+	kind of like "NULL" for pointers). So ``gfp_t`` or the safe endianness
+	types like ``le16`` would be ``__bitwise``. You can only operate on
+	them by doing specific operations that know about *that* particular
+	type.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-bitwise``.
+
+
+-Wcast-to-as
+	Warn about casts which add an address space to a pointer type.
+
+	A cast that includes ``__attribute__((force))`` will suppress this
+	warning.
+
+	Sparse does not issue these warnings by default.
+
+
+-Wcast-truncate
+	Warn about casts that truncate constant values.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-cast-truncate``.
+
+
+-Wconstexpr-not-const
+	Warn if a non-constant expression is encountered when really
+	expecting a constant expression instead.
+	Currently, this warns when initializing an object of static storage
+	duration with an initializer which is not a constant expression.
+
+	Sparse does not issue these warnings by default.
+
+
+-Wcontext
+	Warn about potential errors in synchronization or other delimited
+	contexts.
+
+	Sparse supports several means of designating functions or statements
+	that delimit contexts, such as synchronization. Functions with the
+	extended attribute ``__attribute__((context(expression, in, out))``
+	require the context *expression* (for instance, a lock) to have the
+	value *in* (a constant nonnegative integer) when called, and return
+	with the value *out* (a constant nonnegative integer). For APIs
+	defined via macros, use the statement form ``__context__(expression,
+	in, out)`` in the body of the macro.
+
+	With ``-Wcontext`` Sparse will warn when it sees a function change
+	the context without indicating this with a ``context`` attribute,
+	either by decreasing a context below zero (such as by releasing a
+	lock without acquiring it), or returning with a changed context
+	(such as by acquiring a lock without releasing it). Sparse will
+	also warn about blocks of code which may potentially execute with
+	different contexts.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-context``.
+
+
+-Wdecl
+	Warn about any non-``static`` variable or function definition that
+	has no previous declaration.
+
+	Private symbols (functions and variables) internal to a given source
+	file should use ``static``, to allow additional compiler
+	optimizations, allow detection of unused symbols, and prevent other
+	code from relying on these internal symbols. Public symbols used by
+	other source files will need declarations visible to those other
+	source files, such as in a header file. All declarations should fall
+	into one of these two categories. Thus, with ``-Wdecl``, Sparse warns
+	about any symbol definition with neither ``static`` nor a declaration.
+	To fix this warning, declare private symbols ``static``, and ensure
+	that the files defining public symbols have the symbol declarations
+	available first (such as by including the appropriate header file).
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-decl``.
+
+
+-Wdeclaration-after-statement
+	Warn about declarations that are not at the start of a block.
+
+	These declarations are permitted in C99 but not in C89.
+
+	Sparse issues these warnings by default only when the C dialect is
+	C89 (i.e. with ``-ansi`` or ``-std=c89``). To turn them off, use
+	``-Wno-declaration-after-statement``.
+
+
+-Wdefault-bitfield-sign
+	Warn about any bitfield with no explicit signedness.
+
+	Bitfields have no standard-specified default signedness (C99 6.7.2).
+	A bitfield without an explicit ``signed`` or ``unsigned`` creates a
+	portability problem for software that relies on the available range
+	of values. To fix this, specify the bitfield type as ``signed`` or
+	``unsigned`` explicitly.
+
+	Sparse does not issue these warnings by default.
+
+
+-Wdesignated-init
+	Warn about positional initialization of structs marked as requiring
+	designated initializers.
+
+	Sparse allows an attribute ``__attribute__((designated_init))``
+	which marks a struct as requiring designated initializers. Sparse
+	will warn about positional initialization of a struct variable or
+	struct literal of a type that has this attribute.
+
+	Requiring designated initializers for a particular struct type will
+	insulate code using that struct type from changes to the layout of
+	the type, avoiding the need to change initializers for that type
+	unless they initialize a removed or incompatibly changed field.
+
+	Common examples of this type of struct include collections of
+	function pointers for the implementations of a class of related
+	operations, for which the default ``NULL`` for an unmentioned field
+	in a designated initializer will correctly indicate the absence of
+	that operation.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-designated-init``.
+
+
+-Wdo-while
+	Warn about do-while loops that do not delimit the loop body with
+	braces.
+
+	Sparse does not issue these warnings by default.
+
+
+-Wenum-mismatch
+	Warn about the use of an expression of an incorrect ``enum`` type
+	when initializing another ``enum`` type, assigning to another
+	``enum`` type, or passing an argument to a function which expects
+	another ``enum`` type.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-enum-mismatch``.
+
+
+-Winit-cstring
+	Warn about initialization of a char array with a too long constant
+	C string.
+
+	If the size of the char array and the length of the string are the
+	same, there is no space for the last nul char of the string in the
+	array::
+
+		char s[3] = "abc";
+
+	If the array is used as a byte array, not as C string, this
+	warning is just noise. However, if the array is passed to functions
+	dealing with C string like printf(%s) and strcmp, it may cause a
+	trouble.
+
+	Sparse does not issue these warnings by default.
+
+
+-Wmemcpy-max-count
+	Warn about call of ``memcpy()``, ``memset()``, ``copy_from_user()``,
+	or ``copy_to_user()`` with a large compile-time byte count.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-memcpy-max-count``.
+
+	The limit can be changed with ``-fmemcpy-max-count=COUNT``,
+	the default being ``100000``.
+
+
+-Wnon-pointer-null
+	Warn about the use of 0 as a NULL pointer.
+
+	0 has integer type. NULL has pointer type.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-non-pointer-null``.
+
+
+-Wold-initializer
+	Warn about the use of the pre-C99 GCC syntax for designated
+	initializers.
+
+	C99 provides a standard syntax for designated fields in ``struct``
+	or ``union`` initializers::
+
+		struct structname var = { .field = value };
+
+	GCC also has an old, non-standard syntax for designated initializers
+	which predates C99::
+
+		struct structname var = { field: value };
+
+	Sparse will warn about the use of GCC's non-standard syntax for
+	designated initializers. To fix this warning, convert designated
+	initializers to use the standard C99 syntax.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-old-initializer``.
+
+
+-Wone-bit-signed-bitfield
+	Warn about any one-bit ``signed`` bitfields.
+
+	A one-bit ``signed`` bitfield can only have the values 0 and -1, or
+	with some compilers only 0; this results in unexpected behavior for
+	programs which expected the ability to store 0 and 1.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-one-bit-signed-bitfield``.
+
+
+-Wparen-string
+	Warn about the use of a parenthesized string to initialize an array.
+
+	Standard C syntax does not permit a parenthesized string as an array
+	initializer. GCC allows this syntax as an extension. With
+	``-Wparen-string``, Sparse will warn about this syntax.
+
+	Sparse does not issue these warnings by default.
+
+
+-Wptr-subtraction-blows
+	Warn when subtracting two pointers to a type with a non-power-of-two
+	size.
+
+	Subtracting two pointers to a given type gives a difference in terms
+	of the number of items of that type. To generate this value, compilers
+	will usually need to divide the difference by the size of the type,
+	an potentially expensive operation for sizes other than powers of two.
+
+	Code written using pointer subtraction can often use another approach
+	instead, such as array indexing with an explicit array index variable,
+	which may allow compilers to generate more efficient code.
+
+	Sparse does not issue these warnings by default.
+
+
+-Wreturn-void
+	Warn if a function with return type void returns a void expression.
+
+	C99 permits this, and in some cases this allows for more generic code
+	in macros that use typeof or take a type as a macro argument.
+	However, some programs consider this poor style, and those programs
+	can use ``-Wreturn-void`` to get warnings about it.
+
+	Sparse does not issue these warnings by default.
+
+
+-Wshadow
+	Warn when declaring a symbol which shadows a declaration with the
+	same name in an outer scope.
+
+	Such declarations can lead to error-prone code.
+
+	Sparse does not issue these warnings by default.
+
+
+-Wsizeof-bool
+	Warn when checking the sizeof a _Bool.
+
+	C99 does not specify the sizeof a _Bool. gcc uses 1.
+
+	Sparse does not issue these warnings by default.
+
+
+-Wtransparent-union
+	Warn about any declaration using the GCC extension
+	``__attribute__((transparent_union))``.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-transparent-union``.
+
+
+-Wtypesign
+	Warn when converting a pointer to an integer type into a pointer to
+	an integer type with different signedness.
+
+	Sparse does not issue these warnings by default.
+
+
+-Wundef
+	Warn about preprocessor conditionals that use the value of an
+	undefined preprocessor symbol.
+
+	Standard C (C99 6.10.1) permits using the value of an undefined
+	preprocessor symbol in preprocessor conditionals, and specifies it
+	has a value of 0. However, this behavior can lead to subtle errors.
+
+	Sparse does not issue these warnings by default.
+
+
+MISC OPTIONS
+------------
+
+-gcc-base-dir dir
+	Look for compiler-provided system headers in *dir*/include/ and
+	*dir*/include-fixed/.
+
+
+-multiarch-dir dir
+	Look for system headers in the multiarch subdirectory *dir*.
+	The *dir* name would normally take the form of the target's
+	normalized GNU triplet. (e.g. i386-linux-gnu).
+
+
+DEBUG OPTIONS
+-------------
+
+-fmem-report
+	Report some statistics about memory allocation used by the tool.
+
+
+OTHER OPTIONS
+-------------
+
+-fmax-warnings=COUNT
+	Set the maximum number of displayed warnings
+	to *COUNT*, which should be a numerical value or ``unlimited``.
+	The default limit is 100.
+
+
+-fmemcpy-max-count=COUNT
+	Set the limit for the warnings given by ``-Wmemcpy-max-count``.
+	A *COUNT* of ``unlimited`` or ``0`` will effectively disable the
+	warning. The default limit is 100000.
+
+
+-ftabstop=WIDTH
+	Set the distance between tab stops. This helps sparse report correct
+	column numbers in warnings or errors. If the value is less than 1 or
+	greater than 100, the option is ignored. The default is 8.
+
+
+-f[no-]unsigned-char, -f[no-]signed-char
+	Let plain ``char`` be unsigned or signed.
+	By default chars are signed.
+
+
+SEE ALSO
+========
+:manpage:`cgcc(1)`
+
+HOMEPAGE
+========
+`http://www.kernel.org/pub/software/devel/sparse/`
+
+MAILING LIST
+============
+linux-sparse@vger.kernel.org
+
+MAINTAINER
+==========
+Christopher Li <sparse@chrisli.org>
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1) ===

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-kbuild
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 17:54:02 +0000
Message-ID: <CAKwvOdnWkPi_2UJ7VKYZ6cuWi+BDgpEUf+tcrfZLUYawbm+grw () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 10:50 AM Bernd Petrovitsch
<bernd@petrovitsch.priv.at> wrote:
>
> Hi all!
>
> On 22/10/18 13:07, Miguel Ojeda wrote:
> > On Mon, Oct 22, 2018 at 12:54 PM Dan Carpenter <dan.carpenter@oracle.com> wrote:
> >>
> >> Doing both is super ugly.  Let's just do comments until Eclipse gets
> >> updated.
>
> Yes, "Eclipse" as the IDE.
>
> And yes but IMHO better super ugly than loosing the warning - YMMV.
>
> For the archives: I have Eclipse Photon/June 2016 here. And "no break"
> is the (default) string in a comment used by Eclipse (it can be
> customized and is actually a regexp but it must be in a comment).
>
> >> I had wanted to move to the attribute because that would simplify things
> >> in Smatch but it's not a huge deal to delay for another year.
> >
> > I can re-send them later on, no problem. On the other hand, doing the
> > changes will push tools to get updated sooner ;-)
> >
> > If tools were doing something as fancy as comment parsing for
> > diagnostics, they should have been updated with the attribute support
> > (either gcc's or C++17's) -- it has been more than a year now since
> > gcc 7.1 and the C++17 final draft. (Note that this does not apply for
> > things like clang, since they weren't doing comment parsing to begin
> > with.)
>
> That would be nice. And if they agree on the same texts (or accept per
> default all somewhat widely used and/or old ones).
>
> After stumbling over
> https://stackoverflow.com/questions/16935935/how-do-i-turn-off-a-static-code-analysis-warning-on-a-line-by-line-warning-in-cd,
> looking into Eclipses Window -> Preferences -> C/C++ -> Code Analysis ->
> "No break at the end of case" screen (that's the screenshot there) and I
> tried various things:
>
> Preface:
> I have
> ----  snip  ----
> #define __fallthrough __attribute__((fallthrough))
> ----  snip  ----
> for gcc >= 7 (because clang doesn't know it and I had also older
> gcc's in use before).
>
> So:
> - Adding a comment to the #define doesn't change anything for Eclipse.
> - Eclipse looks *only* in comments for the string/regexp given
>   the warnings configuration (and that comment must be on the line
>   directly before the "case").
> - Eclipse understands [[fallthrough]] out-of-the-box though (which
>   is C++11 AFAIK) as does g++-7 (I use -std=gnu++17 - most of the
>   sources are C++, but not all) and clang++-6 (all the current standard
>   Ubuntu-18.06/Bionic packages).
>   Eclipse "accepts" [[fallthrough]] only in C++ sources (and not in C
>   sources).
> - Neither gcc nor clang understand [[fallthrough]] (so it's probably a
>   no-go for the Kernel with C89 anyways).
>
> MfG,
>         Bernd
>
> PS: clang++ errors with "fallthrough annotation in unreachable code" if
>     [[fallthrough]] is after an assert(). clang-devs there, please, the
>     fallthrough doesn't really generated code (I hope;-).
>     I have lots of switch()es which catch undefined values (for enums
>     et. al.) with "default"+assert() and fall through to the most safe
>     case (for the deployed version).

Can you send me a link to a simple reproducer in godbolt (godbolt.org)
and we'll take a look?

> --
> "I dislike type abstraction if it has no real reason. And saving
> on typing is not a good reason - if your typing speed is the main
> issue when you're coding, you're doing something seriously wrong."
>     - Linus Torvalds



-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-doc
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 17:54:02 +0000
Message-ID: <CAKwvOdnWkPi_2UJ7VKYZ6cuWi+BDgpEUf+tcrfZLUYawbm+grw () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 10:50 AM Bernd Petrovitsch
<bernd@petrovitsch.priv.at> wrote:
>
> Hi all!
>
> On 22/10/18 13:07, Miguel Ojeda wrote:
> > On Mon, Oct 22, 2018 at 12:54 PM Dan Carpenter <dan.carpenter@oracle.com> wrote:
> >>
> >> Doing both is super ugly.  Let's just do comments until Eclipse gets
> >> updated.
>
> Yes, "Eclipse" as the IDE.
>
> And yes but IMHO better super ugly than loosing the warning - YMMV.
>
> For the archives: I have Eclipse Photon/June 2016 here. And "no break"
> is the (default) string in a comment used by Eclipse (it can be
> customized and is actually a regexp but it must be in a comment).
>
> >> I had wanted to move to the attribute because that would simplify things
> >> in Smatch but it's not a huge deal to delay for another year.
> >
> > I can re-send them later on, no problem. On the other hand, doing the
> > changes will push tools to get updated sooner ;-)
> >
> > If tools were doing something as fancy as comment parsing for
> > diagnostics, they should have been updated with the attribute support
> > (either gcc's or C++17's) -- it has been more than a year now since
> > gcc 7.1 and the C++17 final draft. (Note that this does not apply for
> > things like clang, since they weren't doing comment parsing to begin
> > with.)
>
> That would be nice. And if they agree on the same texts (or accept per
> default all somewhat widely used and/or old ones).
>
> After stumbling over
> https://stackoverflow.com/questions/16935935/how-do-i-turn-off-a-static-code-analysis-warning-on-a-line-by-line-warning-in-cd,
> looking into Eclipses Window -> Preferences -> C/C++ -> Code Analysis ->
> "No break at the end of case" screen (that's the screenshot there) and I
> tried various things:
>
> Preface:
> I have
> ----  snip  ----
> #define __fallthrough __attribute__((fallthrough))
> ----  snip  ----
> for gcc >= 7 (because clang doesn't know it and I had also older
> gcc's in use before).
>
> So:
> - Adding a comment to the #define doesn't change anything for Eclipse.
> - Eclipse looks *only* in comments for the string/regexp given
>   the warnings configuration (and that comment must be on the line
>   directly before the "case").
> - Eclipse understands [[fallthrough]] out-of-the-box though (which
>   is C++11 AFAIK) as does g++-7 (I use -std=gnu++17 - most of the
>   sources are C++, but not all) and clang++-6 (all the current standard
>   Ubuntu-18.06/Bionic packages).
>   Eclipse "accepts" [[fallthrough]] only in C++ sources (and not in C
>   sources).
> - Neither gcc nor clang understand [[fallthrough]] (so it's probably a
>   no-go for the Kernel with C89 anyways).
>
> MfG,
>         Bernd
>
> PS: clang++ errors with "fallthrough annotation in unreachable code" if
>     [[fallthrough]] is after an assert(). clang-devs there, please, the
>     fallthrough doesn't really generated code (I hope;-).
>     I have lots of switch()es which catch undefined values (for enums
>     et. al.) with "default"+assert() and fall through to the most safe
>     case (for the deployed version).

Can you send me a link to a simple reproducer in godbolt (godbolt.org)
and we'll take a look?

> --
> "I dislike type abstraction if it has no real reason. And saving
> on typing is not a good reason - if your typing speed is the main
> issue when you're coding, you're doing something seriously wrong."
>     - Linus Torvalds



-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-kernel
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 17:54:02 +0000
Message-ID: <CAKwvOdnWkPi_2UJ7VKYZ6cuWi+BDgpEUf+tcrfZLUYawbm+grw () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 10:50 AM Bernd Petrovitsch
<bernd@petrovitsch.priv.at> wrote:
>
> Hi all!
>
> On 22/10/18 13:07, Miguel Ojeda wrote:
> > On Mon, Oct 22, 2018 at 12:54 PM Dan Carpenter <dan.carpenter@oracle.com> wrote:
> >>
> >> Doing both is super ugly.  Let's just do comments until Eclipse gets
> >> updated.
>
> Yes, "Eclipse" as the IDE.
>
> And yes but IMHO better super ugly than loosing the warning - YMMV.
>
> For the archives: I have Eclipse Photon/June 2016 here. And "no break"
> is the (default) string in a comment used by Eclipse (it can be
> customized and is actually a regexp but it must be in a comment).
>
> >> I had wanted to move to the attribute because that would simplify things
> >> in Smatch but it's not a huge deal to delay for another year.
> >
> > I can re-send them later on, no problem. On the other hand, doing the
> > changes will push tools to get updated sooner ;-)
> >
> > If tools were doing something as fancy as comment parsing for
> > diagnostics, they should have been updated with the attribute support
> > (either gcc's or C++17's) -- it has been more than a year now since
> > gcc 7.1 and the C++17 final draft. (Note that this does not apply for
> > things like clang, since they weren't doing comment parsing to begin
> > with.)
>
> That would be nice. And if they agree on the same texts (or accept per
> default all somewhat widely used and/or old ones).
>
> After stumbling over
> https://stackoverflow.com/questions/16935935/how-do-i-turn-off-a-static-code-analysis-warning-on-a-line-by-line-warning-in-cd,
> looking into Eclipses Window -> Preferences -> C/C++ -> Code Analysis ->
> "No break at the end of case" screen (that's the screenshot there) and I
> tried various things:
>
> Preface:
> I have
> ----  snip  ----
> #define __fallthrough __attribute__((fallthrough))
> ----  snip  ----
> for gcc >= 7 (because clang doesn't know it and I had also older
> gcc's in use before).
>
> So:
> - Adding a comment to the #define doesn't change anything for Eclipse.
> - Eclipse looks *only* in comments for the string/regexp given
>   the warnings configuration (and that comment must be on the line
>   directly before the "case").
> - Eclipse understands [[fallthrough]] out-of-the-box though (which
>   is C++11 AFAIK) as does g++-7 (I use -std=gnu++17 - most of the
>   sources are C++, but not all) and clang++-6 (all the current standard
>   Ubuntu-18.06/Bionic packages).
>   Eclipse "accepts" [[fallthrough]] only in C++ sources (and not in C
>   sources).
> - Neither gcc nor clang understand [[fallthrough]] (so it's probably a
>   no-go for the Kernel with C89 anyways).
>
> MfG,
>         Bernd
>
> PS: clang++ errors with "fallthrough annotation in unreachable code" if
>     [[fallthrough]] is after an assert(). clang-devs there, please, the
>     fallthrough doesn't really generated code (I hope;-).
>     I have lots of switch()es which catch undefined values (for enums
>     et. al.) with "default"+assert() and fall through to the most safe
>     case (for the deployed version).

Can you send me a link to a simple reproducer in godbolt (godbolt.org)
and we'll take a look?

> --
> "I dislike type abstraction if it has no real reason. And saving
> on typing is not a good reason - if your typing speed is the main
> issue when you're coding, you're doing something seriously wrong."
>     - Linus Torvalds



-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-ext4
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 17:54:02 +0000
Message-ID: <CAKwvOdnWkPi_2UJ7VKYZ6cuWi+BDgpEUf+tcrfZLUYawbm+grw () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 10:50 AM Bernd Petrovitsch
<bernd@petrovitsch.priv.at> wrote:
>
> Hi all!
>
> On 22/10/18 13:07, Miguel Ojeda wrote:
> > On Mon, Oct 22, 2018 at 12:54 PM Dan Carpenter <dan.carpenter@oracle.com> wrote:
> >>
> >> Doing both is super ugly.  Let's just do comments until Eclipse gets
> >> updated.
>
> Yes, "Eclipse" as the IDE.
>
> And yes but IMHO better super ugly than loosing the warning - YMMV.
>
> For the archives: I have Eclipse Photon/June 2016 here. And "no break"
> is the (default) string in a comment used by Eclipse (it can be
> customized and is actually a regexp but it must be in a comment).
>
> >> I had wanted to move to the attribute because that would simplify things
> >> in Smatch but it's not a huge deal to delay for another year.
> >
> > I can re-send them later on, no problem. On the other hand, doing the
> > changes will push tools to get updated sooner ;-)
> >
> > If tools were doing something as fancy as comment parsing for
> > diagnostics, they should have been updated with the attribute support
> > (either gcc's or C++17's) -- it has been more than a year now since
> > gcc 7.1 and the C++17 final draft. (Note that this does not apply for
> > things like clang, since they weren't doing comment parsing to begin
> > with.)
>
> That would be nice. And if they agree on the same texts (or accept per
> default all somewhat widely used and/or old ones).
>
> After stumbling over
> https://stackoverflow.com/questions/16935935/how-do-i-turn-off-a-static-code-analysis-warning-on-a-line-by-line-warning-in-cd,
> looking into Eclipses Window -> Preferences -> C/C++ -> Code Analysis ->
> "No break at the end of case" screen (that's the screenshot there) and I
> tried various things:
>
> Preface:
> I have
> ----  snip  ----
> #define __fallthrough __attribute__((fallthrough))
> ----  snip  ----
> for gcc >= 7 (because clang doesn't know it and I had also older
> gcc's in use before).
>
> So:
> - Adding a comment to the #define doesn't change anything for Eclipse.
> - Eclipse looks *only* in comments for the string/regexp given
>   the warnings configuration (and that comment must be on the line
>   directly before the "case").
> - Eclipse understands [[fallthrough]] out-of-the-box though (which
>   is C++11 AFAIK) as does g++-7 (I use -std=gnu++17 - most of the
>   sources are C++, but not all) and clang++-6 (all the current standard
>   Ubuntu-18.06/Bionic packages).
>   Eclipse "accepts" [[fallthrough]] only in C++ sources (and not in C
>   sources).
> - Neither gcc nor clang understand [[fallthrough]] (so it's probably a
>   no-go for the Kernel with C89 anyways).
>
> MfG,
>         Bernd
>
> PS: clang++ errors with "fallthrough annotation in unreachable code" if
>     [[fallthrough]] is after an assert(). clang-devs there, please, the
>     fallthrough doesn't really generated code (I hope;-).
>     I have lots of switch()es which catch undefined values (for enums
>     et. al.) with "default"+assert() and fall through to the most safe
>     case (for the deployed version).

Can you send me a link to a simple reproducer in godbolt (godbolt.org)
and we'll take a look?

> --
> "I dislike type abstraction if it has no real reason. And saving
> on typing is not a good reason - if your typing speed is the main
> issue when you're coding, you're doing something seriously wrong."
>     - Linus Torvalds



-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Bernd Petrovitsch <bernd () petrovitsch ! priv ! at>
To: linux-ext4
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 18:13:46 +0000
Message-ID: <9c52827b-4783-8f39-9030-a166d5436f05 () petrovitsch ! priv ! at>
--------------------
This is a multi-part message in MIME format.
--------------AFED424BB543102162459081
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi all!

On 22/10/18 19:54, Nick Desaulniers wrote:
> On Mon, Oct 22, 2018 at 10:50 AM Bernd Petrovitsch
> <bernd@petrovitsch.priv.at> wrote:
[...]
>> PS: clang++ errors with "fallthrough annotation in unreachable code" i=
f
>>     [[fallthrough]] is after an assert(). clang-devs there, please, th=
e
>>     fallthrough doesn't really generated code (I hope;-).
[...]
> Can you send me a link to a simple reproducer in godbolt (godbolt.org)
> and we'll take a look?

Does https://godbolt.org/z/2Y4zIo do it - I'm a godbolt-newbie?

For
----  snip  ----
#include <cassert>

int main(void)
{
  switch (1) {
  default:
    assert(0);
    [[fallthrough]];
  case 1:
    ;
  }
  return 0;
}
----  snip  ----
Just "clang++ -Wimplicit-fallthrough -Werror" it .....

MfG,
	Bernd
--=20
"I dislike type abstraction if it has no real reason. And saving
on typing is not a good reason - if your typing speed is the main
issue when you're coding, you're doing something seriously wrong."
    - Linus Torvalds

--------------AFED424BB543102162459081
Content-Type: application/pgp-keys;
 name="pEpkey.asc"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: attachment;
 filename="pEpkey.asc"

-----BEGIN PGP PUBLIC KEY BLOCK-----
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3D=3D
=3Dv2Yn
-----END PGP PUBLIC KEY BLOCK-----

--------------AFED424BB543102162459081--
================================================================================

From: Bernd Petrovitsch <bernd () petrovitsch ! priv ! at>
To: linux-doc
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 18:13:46 +0000
Message-ID: <9c52827b-4783-8f39-9030-a166d5436f05 () petrovitsch ! priv ! at>
--------------------
This is a multi-part message in MIME format.
--------------AFED424BB543102162459081
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi all!

On 22/10/18 19:54, Nick Desaulniers wrote:
> On Mon, Oct 22, 2018 at 10:50 AM Bernd Petrovitsch
> <bernd@petrovitsch.priv.at> wrote:
[...]
>> PS: clang++ errors with "fallthrough annotation in unreachable code" i=
f
>>     [[fallthrough]] is after an assert(). clang-devs there, please, th=
e
>>     fallthrough doesn't really generated code (I hope;-).
[...]
> Can you send me a link to a simple reproducer in godbolt (godbolt.org)
> and we'll take a look?

Does https://godbolt.org/z/2Y4zIo do it - I'm a godbolt-newbie?

For
----  snip  ----
#include <cassert>

int main(void)
{
  switch (1) {
  default:
    assert(0);
    [[fallthrough]];
  case 1:
    ;
  }
  return 0;
}
----  snip  ----
Just "clang++ -Wimplicit-fallthrough -Werror" it .....

MfG,
	Bernd
--=20
"I dislike type abstraction if it has no real reason. And saving
on typing is not a good reason - if your typing speed is the main
issue when you're coding, you're doing something seriously wrong."
    - Linus Torvalds

--------------AFED424BB543102162459081
Content-Type: application/pgp-keys;
 name="pEpkey.asc"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: attachment;
 filename="pEpkey.asc"

-----BEGIN PGP PUBLIC KEY BLOCK-----
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3D=3D
=3Dv2Yn
-----END PGP PUBLIC KEY BLOCK-----

--------------AFED424BB543102162459081--
================================================================================

From: Bernd Petrovitsch <bernd () petrovitsch ! priv ! at>
To: linux-kbuild
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 18:13:46 +0000
Message-ID: <9c52827b-4783-8f39-9030-a166d5436f05 () petrovitsch ! priv ! at>
--------------------
This is a multi-part message in MIME format.
--------------AFED424BB543102162459081
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi all!

On 22/10/18 19:54, Nick Desaulniers wrote:
> On Mon, Oct 22, 2018 at 10:50 AM Bernd Petrovitsch
> <bernd@petrovitsch.priv.at> wrote:
[...]
>> PS: clang++ errors with "fallthrough annotation in unreachable code" i=
f
>>     [[fallthrough]] is after an assert(). clang-devs there, please, th=
e
>>     fallthrough doesn't really generated code (I hope;-).
[...]
> Can you send me a link to a simple reproducer in godbolt (godbolt.org)
> and we'll take a look?

Does https://godbolt.org/z/2Y4zIo do it - I'm a godbolt-newbie?

For
----  snip  ----
#include <cassert>

int main(void)
{
  switch (1) {
  default:
    assert(0);
    [[fallthrough]];
  case 1:
    ;
  }
  return 0;
}
----  snip  ----
Just "clang++ -Wimplicit-fallthrough -Werror" it .....

MfG,
	Bernd
--=20
"I dislike type abstraction if it has no real reason. And saving
on typing is not a good reason - if your typing speed is the main
issue when you're coding, you're doing something seriously wrong."
    - Linus Torvalds

--------------AFED424BB543102162459081
Content-Type: application/pgp-keys;
 name="pEpkey.asc"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: attachment;
 filename="pEpkey.asc"

-----BEGIN PGP PUBLIC KEY BLOCK-----
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3D=3D
=3Dv2Yn
-----END PGP PUBLIC KEY BLOCK-----

--------------AFED424BB543102162459081--
================================================================================

From: Bernd Petrovitsch <bernd () petrovitsch ! priv ! at>
To: linux-kernel
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 18:13:46 +0000
Message-ID: <9c52827b-4783-8f39-9030-a166d5436f05 () petrovitsch ! priv ! at>
--------------------
This is a multi-part message in MIME format.
--------------AFED424BB543102162459081
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi all!

On 22/10/18 19:54, Nick Desaulniers wrote:
> On Mon, Oct 22, 2018 at 10:50 AM Bernd Petrovitsch
> <bernd@petrovitsch.priv.at> wrote:
[...]
>> PS: clang++ errors with "fallthrough annotation in unreachable code" i=
f
>>     [[fallthrough]] is after an assert(). clang-devs there, please, th=
e
>>     fallthrough doesn't really generated code (I hope;-).
[...]
> Can you send me a link to a simple reproducer in godbolt (godbolt.org)
> and we'll take a look?

Does https://godbolt.org/z/2Y4zIo do it - I'm a godbolt-newbie?

For
----  snip  ----
#include <cassert>

int main(void)
{
  switch (1) {
  default:
    assert(0);
    [[fallthrough]];
  case 1:
    ;
  }
  return 0;
}
----  snip  ----
Just "clang++ -Wimplicit-fallthrough -Werror" it .....

MfG,
	Bernd
--=20
"I dislike type abstraction if it has no real reason. And saving
on typing is not a good reason - if your typing speed is the main
issue when you're coding, you're doing something seriously wrong."
    - Linus Torvalds

--------------AFED424BB543102162459081
Content-Type: application/pgp-keys;
 name="pEpkey.asc"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: attachment;
 filename="pEpkey.asc"

-----BEGIN PGP PUBLIC KEY BLOCK-----
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3D=3D
=3Dv2Yn
-----END PGP PUBLIC KEY BLOCK-----

--------------AFED424BB543102162459081--
================================================================================

From: Bernd Petrovitsch <bernd () petrovitsch ! priv ! at>
To: linux-sparse
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 18:13:46 +0000
Message-ID: <9c52827b-4783-8f39-9030-a166d5436f05 () petrovitsch ! priv ! at>
--------------------
This is a multi-part message in MIME format.
--------------AFED424BB543102162459081
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi all!

On 22/10/18 19:54, Nick Desaulniers wrote:
> On Mon, Oct 22, 2018 at 10:50 AM Bernd Petrovitsch
> <bernd@petrovitsch.priv.at> wrote:
[...]
>> PS: clang++ errors with "fallthrough annotation in unreachable code" i=
f
>>     [[fallthrough]] is after an assert(). clang-devs there, please, th=
e
>>     fallthrough doesn't really generated code (I hope;-).
[...]
> Can you send me a link to a simple reproducer in godbolt (godbolt.org)
> and we'll take a look?

Does https://godbolt.org/z/2Y4zIo do it - I'm a godbolt-newbie?

For
----  snip  ----
#include <cassert>

int main(void)
{
  switch (1) {
  default:
    assert(0);
    [[fallthrough]];
  case 1:
    ;
  }
  return 0;
}
----  snip  ----
Just "clang++ -Wimplicit-fallthrough -Werror" it .....

MfG,
	Bernd
--=20
"I dislike type abstraction if it has no real reason. And saving
on typing is not a good reason - if your typing speed is the main
issue when you're coding, you're doing something seriously wrong."
    - Linus Torvalds

--------------AFED424BB543102162459081
Content-Type: application/pgp-keys;
 name="pEpkey.asc"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: attachment;
 filename="pEpkey.asc"

-----BEGIN PGP PUBLIC KEY BLOCK-----
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3D=3D
=3Dv2Yn
-----END PGP PUBLIC KEY BLOCK-----

--------------AFED424BB543102162459081--
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-ext4
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 19:56:37 +0000
Message-ID: <CAKwvOd=_mnxnG3cFb6qBhO5KLFxjrgsavoQ37asY3O1B4LLkwA () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 11:15 AM Bernd Petrovitsch
<bernd@petrovitsch.priv.at> wrote:
>
> Hi all!
>
> On 22/10/18 19:54, Nick Desaulniers wrote:
> > On Mon, Oct 22, 2018 at 10:50 AM Bernd Petrovitsch
> > <bernd@petrovitsch.priv.at> wrote:
> [...]
> >> PS: clang++ errors with "fallthrough annotation in unreachable code" if
> >>     [[fallthrough]] is after an assert(). clang-devs there, please, the
> >>     fallthrough doesn't really generated code (I hope;-).
> [...]
> > Can you send me a link to a simple reproducer in godbolt (godbolt.org)
> > and we'll take a look?
>
> Does https://godbolt.org/z/2Y4zIo do it - I'm a godbolt-newbie?

Moving the kernel folks to bcc, since we don't need to be discussing
C++ on LKML.
https://godbolt.org/z/B1fo9Z shows that this works as intended, for
cases that cannot be statically proven.  I guess I'm looking for a
more realistic code sample to show why putting a `break;` statement
there is untenable?

>
> For
> ----  snip  ----
> #include <cassert>
>
> int main(void)
> {
>   switch (1) {
>   default:
>     assert(0);
>     [[fallthrough]];
>   case 1:
>     ;
>   }
>   return 0;
> }
> ----  snip  ----
> Just "clang++ -Wimplicit-fallthrough -Werror" it .....
>
> MfG,
>         Bernd
> --
> "I dislike type abstraction if it has no real reason. And saving
> on typing is not a good reason - if your typing speed is the main
> issue when you're coding, you're doing something seriously wrong."
>     - Linus Torvalds



-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-doc
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 19:56:37 +0000
Message-ID: <CAKwvOd=_mnxnG3cFb6qBhO5KLFxjrgsavoQ37asY3O1B4LLkwA () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 11:15 AM Bernd Petrovitsch
<bernd@petrovitsch.priv.at> wrote:
>
> Hi all!
>
> On 22/10/18 19:54, Nick Desaulniers wrote:
> > On Mon, Oct 22, 2018 at 10:50 AM Bernd Petrovitsch
> > <bernd@petrovitsch.priv.at> wrote:
> [...]
> >> PS: clang++ errors with "fallthrough annotation in unreachable code" if
> >>     [[fallthrough]] is after an assert(). clang-devs there, please, the
> >>     fallthrough doesn't really generated code (I hope;-).
> [...]
> > Can you send me a link to a simple reproducer in godbolt (godbolt.org)
> > and we'll take a look?
>
> Does https://godbolt.org/z/2Y4zIo do it - I'm a godbolt-newbie?

Moving the kernel folks to bcc, since we don't need to be discussing
C++ on LKML.
https://godbolt.org/z/B1fo9Z shows that this works as intended, for
cases that cannot be statically proven.  I guess I'm looking for a
more realistic code sample to show why putting a `break;` statement
there is untenable?

>
> For
> ----  snip  ----
> #include <cassert>
>
> int main(void)
> {
>   switch (1) {
>   default:
>     assert(0);
>     [[fallthrough]];
>   case 1:
>     ;
>   }
>   return 0;
> }
> ----  snip  ----
> Just "clang++ -Wimplicit-fallthrough -Werror" it .....
>
> MfG,
>         Bernd
> --
> "I dislike type abstraction if it has no real reason. And saving
> on typing is not a good reason - if your typing speed is the main
> issue when you're coding, you're doing something seriously wrong."
>     - Linus Torvalds



-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-kbuild
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 19:56:37 +0000
Message-ID: <CAKwvOd=_mnxnG3cFb6qBhO5KLFxjrgsavoQ37asY3O1B4LLkwA () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 11:15 AM Bernd Petrovitsch
<bernd@petrovitsch.priv.at> wrote:
>
> Hi all!
>
> On 22/10/18 19:54, Nick Desaulniers wrote:
> > On Mon, Oct 22, 2018 at 10:50 AM Bernd Petrovitsch
> > <bernd@petrovitsch.priv.at> wrote:
> [...]
> >> PS: clang++ errors with "fallthrough annotation in unreachable code" if
> >>     [[fallthrough]] is after an assert(). clang-devs there, please, the
> >>     fallthrough doesn't really generated code (I hope;-).
> [...]
> > Can you send me a link to a simple reproducer in godbolt (godbolt.org)
> > and we'll take a look?
>
> Does https://godbolt.org/z/2Y4zIo do it - I'm a godbolt-newbie?

Moving the kernel folks to bcc, since we don't need to be discussing
C++ on LKML.
https://godbolt.org/z/B1fo9Z shows that this works as intended, for
cases that cannot be statically proven.  I guess I'm looking for a
more realistic code sample to show why putting a `break;` statement
there is untenable?

>
> For
> ----  snip  ----
> #include <cassert>
>
> int main(void)
> {
>   switch (1) {
>   default:
>     assert(0);
>     [[fallthrough]];
>   case 1:
>     ;
>   }
>   return 0;
> }
> ----  snip  ----
> Just "clang++ -Wimplicit-fallthrough -Werror" it .....
>
> MfG,
>         Bernd
> --
> "I dislike type abstraction if it has no real reason. And saving
> on typing is not a good reason - if your typing speed is the main
> issue when you're coding, you're doing something seriously wrong."
>     - Linus Torvalds



-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-sparse
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 19:56:37 +0000
Message-ID: <CAKwvOd=_mnxnG3cFb6qBhO5KLFxjrgsavoQ37asY3O1B4LLkwA () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 11:15 AM Bernd Petrovitsch
<bernd@petrovitsch.priv.at> wrote:
>
> Hi all!
>
> On 22/10/18 19:54, Nick Desaulniers wrote:
> > On Mon, Oct 22, 2018 at 10:50 AM Bernd Petrovitsch
> > <bernd@petrovitsch.priv.at> wrote:
> [...]
> >> PS: clang++ errors with "fallthrough annotation in unreachable code" if
> >>     [[fallthrough]] is after an assert(). clang-devs there, please, the
> >>     fallthrough doesn't really generated code (I hope;-).
> [...]
> > Can you send me a link to a simple reproducer in godbolt (godbolt.org)
> > and we'll take a look?
>
> Does https://godbolt.org/z/2Y4zIo do it - I'm a godbolt-newbie?

Moving the kernel folks to bcc, since we don't need to be discussing
C++ on LKML.
https://godbolt.org/z/B1fo9Z shows that this works as intended, for
cases that cannot be statically proven.  I guess I'm looking for a
more realistic code sample to show why putting a `break;` statement
there is untenable?

>
> For
> ----  snip  ----
> #include <cassert>
>
> int main(void)
> {
>   switch (1) {
>   default:
>     assert(0);
>     [[fallthrough]];
>   case 1:
>     ;
>   }
>   return 0;
> }
> ----  snip  ----
> Just "clang++ -Wimplicit-fallthrough -Werror" it .....
>
> MfG,
>         Bernd
> --
> "I dislike type abstraction if it has no real reason. And saving
> on typing is not a good reason - if your typing speed is the main
> issue when you're coding, you're doing something seriously wrong."
>     - Linus Torvalds



-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kernel
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 20:59:16 +0000
Message-ID: <CANiq72kUJOSrR=h2=2jXfkBPLrR=DrN8X4OoKOqAte4sMLxkdQ () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 7:17 PM Nick Desaulniers
<ndesaulniers@google.com> wrote:
>
> On Mon, Oct 22, 2018 at 2:43 AM Miguel Ojeda
> <miguel.ojeda.sandonis@gmail.com> wrote:
> >
> > On Mon, Oct 22, 2018 at 11:41 AM Miguel Ojeda
> > <miguel.ojeda.sandonis@gmail.com> wrote:
> > >
> > >   * clang does *not* support the attribute in C.
> >
> > ...nor the warning at all, by the way (which is why this is OK).
> >
> > Cheers,
> > Miguel
>
> Oh? We're going through and annotating all of Android's C++ source
> right now with it.
> https://clang.llvm.org/docs/DiagnosticsReference.html#wimplicit-fallthrough
>

Sure, but I am talking about the C mode only. Please read the previous
messages in the thread :-)

> Though it looks like the attribute syntax I like from GCC is not yet
> supported in Clang.

Indeed, that is what I explained in the last message.

> https://bugs.llvm.org/show_bug.cgi?id=37135
> https://github.com/ClangBuiltLinux/linux/issues/235
> I'll take a look at adding support.
>
> So Kees sent me this embarrassing example:
> https://godbolt.org/z/gV_c9_
> (try changing the language from C++ to C; it works)!  That's embarrassing!

Yup, that is what I have been testing yesterday -- see the linked
thread in the commit message for the summary of the results.

> https://bugs.llvm.org/show_bug.cgi?id=39382
> https://github.com/ClangBuiltLinux/linux/issues/236
> Will get these both fixed up.

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 20:59:16 +0000
Message-ID: <CANiq72kUJOSrR=h2=2jXfkBPLrR=DrN8X4OoKOqAte4sMLxkdQ () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 7:17 PM Nick Desaulniers
<ndesaulniers@google.com> wrote:
>
> On Mon, Oct 22, 2018 at 2:43 AM Miguel Ojeda
> <miguel.ojeda.sandonis@gmail.com> wrote:
> >
> > On Mon, Oct 22, 2018 at 11:41 AM Miguel Ojeda
> > <miguel.ojeda.sandonis@gmail.com> wrote:
> > >
> > >   * clang does *not* support the attribute in C.
> >
> > ...nor the warning at all, by the way (which is why this is OK).
> >
> > Cheers,
> > Miguel
>
> Oh? We're going through and annotating all of Android's C++ source
> right now with it.
> https://clang.llvm.org/docs/DiagnosticsReference.html#wimplicit-fallthrough
>

Sure, but I am talking about the C mode only. Please read the previous
messages in the thread :-)

> Though it looks like the attribute syntax I like from GCC is not yet
> supported in Clang.

Indeed, that is what I explained in the last message.

> https://bugs.llvm.org/show_bug.cgi?id=37135
> https://github.com/ClangBuiltLinux/linux/issues/235
> I'll take a look at adding support.
>
> So Kees sent me this embarrassing example:
> https://godbolt.org/z/gV_c9_
> (try changing the language from C++ to C; it works)!  That's embarrassing!

Yup, that is what I have been testing yesterday -- see the linked
thread in the commit message for the summary of the results.

> https://bugs.llvm.org/show_bug.cgi?id=39382
> https://github.com/ClangBuiltLinux/linux/issues/236
> Will get these both fixed up.

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-doc
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 20:59:16 +0000
Message-ID: <CANiq72kUJOSrR=h2=2jXfkBPLrR=DrN8X4OoKOqAte4sMLxkdQ () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 7:17 PM Nick Desaulniers
<ndesaulniers@google.com> wrote:
>
> On Mon, Oct 22, 2018 at 2:43 AM Miguel Ojeda
> <miguel.ojeda.sandonis@gmail.com> wrote:
> >
> > On Mon, Oct 22, 2018 at 11:41 AM Miguel Ojeda
> > <miguel.ojeda.sandonis@gmail.com> wrote:
> > >
> > >   * clang does *not* support the attribute in C.
> >
> > ...nor the warning at all, by the way (which is why this is OK).
> >
> > Cheers,
> > Miguel
>
> Oh? We're going through and annotating all of Android's C++ source
> right now with it.
> https://clang.llvm.org/docs/DiagnosticsReference.html#wimplicit-fallthrough
>

Sure, but I am talking about the C mode only. Please read the previous
messages in the thread :-)

> Though it looks like the attribute syntax I like from GCC is not yet
> supported in Clang.

Indeed, that is what I explained in the last message.

> https://bugs.llvm.org/show_bug.cgi?id=37135
> https://github.com/ClangBuiltLinux/linux/issues/235
> I'll take a look at adding support.
>
> So Kees sent me this embarrassing example:
> https://godbolt.org/z/gV_c9_
> (try changing the language from C++ to C; it works)!  That's embarrassing!

Yup, that is what I have been testing yesterday -- see the linked
thread in the commit message for the summary of the results.

> https://bugs.llvm.org/show_bug.cgi?id=39382
> https://github.com/ClangBuiltLinux/linux/issues/236
> Will get these both fixed up.

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kbuild
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 20:59:16 +0000
Message-ID: <CANiq72kUJOSrR=h2=2jXfkBPLrR=DrN8X4OoKOqAte4sMLxkdQ () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 7:17 PM Nick Desaulniers
<ndesaulniers@google.com> wrote:
>
> On Mon, Oct 22, 2018 at 2:43 AM Miguel Ojeda
> <miguel.ojeda.sandonis@gmail.com> wrote:
> >
> > On Mon, Oct 22, 2018 at 11:41 AM Miguel Ojeda
> > <miguel.ojeda.sandonis@gmail.com> wrote:
> > >
> > >   * clang does *not* support the attribute in C.
> >
> > ...nor the warning at all, by the way (which is why this is OK).
> >
> > Cheers,
> > Miguel
>
> Oh? We're going through and annotating all of Android's C++ source
> right now with it.
> https://clang.llvm.org/docs/DiagnosticsReference.html#wimplicit-fallthrough
>

Sure, but I am talking about the C mode only. Please read the previous
messages in the thread :-)

> Though it looks like the attribute syntax I like from GCC is not yet
> supported in Clang.

Indeed, that is what I explained in the last message.

> https://bugs.llvm.org/show_bug.cgi?id=37135
> https://github.com/ClangBuiltLinux/linux/issues/235
> I'll take a look at adding support.
>
> So Kees sent me this embarrassing example:
> https://godbolt.org/z/gV_c9_
> (try changing the language from C++ to C; it works)!  That's embarrassing!

Yup, that is what I have been testing yesterday -- see the linked
thread in the commit message for the summary of the results.

> https://bugs.llvm.org/show_bug.cgi?id=39382
> https://github.com/ClangBuiltLinux/linux/issues/236
> Will get these both fixed up.

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-ext4
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 20:59:16 +0000
Message-ID: <CANiq72kUJOSrR=h2=2jXfkBPLrR=DrN8X4OoKOqAte4sMLxkdQ () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 7:17 PM Nick Desaulniers
<ndesaulniers@google.com> wrote:
>
> On Mon, Oct 22, 2018 at 2:43 AM Miguel Ojeda
> <miguel.ojeda.sandonis@gmail.com> wrote:
> >
> > On Mon, Oct 22, 2018 at 11:41 AM Miguel Ojeda
> > <miguel.ojeda.sandonis@gmail.com> wrote:
> > >
> > >   * clang does *not* support the attribute in C.
> >
> > ...nor the warning at all, by the way (which is why this is OK).
> >
> > Cheers,
> > Miguel
>
> Oh? We're going through and annotating all of Android's C++ source
> right now with it.
> https://clang.llvm.org/docs/DiagnosticsReference.html#wimplicit-fallthrough
>

Sure, but I am talking about the C mode only. Please read the previous
messages in the thread :-)

> Though it looks like the attribute syntax I like from GCC is not yet
> supported in Clang.

Indeed, that is what I explained in the last message.

> https://bugs.llvm.org/show_bug.cgi?id=37135
> https://github.com/ClangBuiltLinux/linux/issues/235
> I'll take a look at adding support.
>
> So Kees sent me this embarrassing example:
> https://godbolt.org/z/gV_c9_
> (try changing the language from C++ to C; it works)!  That's embarrassing!

Yup, that is what I have been testing yesterday -- see the linked
thread in the commit message for the summary of the results.

> https://bugs.llvm.org/show_bug.cgi?id=39382
> https://github.com/ClangBuiltLinux/linux/issues/236
> Will get these both fixed up.

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kbuild
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 21:17:15 +0000
Message-ID: <CANiq72k2z78uae1GWjECo+OB7GC90b8irt0SHYtnSXT8F2Grrw () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 7:36 PM Nick Desaulniers
<ndesaulniers@google.com> wrote:
>
> On Sun, Oct 21, 2018 at 10:14 AM Miguel Ojeda
> <miguel.ojeda.sandonis@gmail.com> wrote:
> >
> > of its kind) to deal with this: [[fallthrough]] is meant to be
> > a new control keyword in the form of an extension.
>
> I think we can leave the details about the [[]] notation out. IIUC,
> it's only applicable to C++.

No, because C++ is the driving force for the standard attributes
syntax; i.e. C2x is adding them because of the syntax published by
C++17; and possibly gcc 7.1 added support for fallthrough (and comment
parsing) due to C++17 too.

Basically, it is a small paragraph explaining how this came to be.

> > +#if __has_attribute(__fallthrough__)
> > +# define __fallthrough                  __attribute__((__fallthrough__))
> > +#else
> > +# define __fallthrough
>
> While this is in the correct format as the other attributes in this
> file, I think this particular attribute is a special case, because of
> the variety of fallbacks and differing support for them.  I'd like to

No, is it the correct format because we cannot add support for the
other syntax in gcc; so the best way to proceed is to simply wait
until clang supports the GNU attribute in C mode.

The tooling, of course, is another matter and independent of this.

> see in the commit message maybe a list of tools we'd like to support

Yes, I already said I would write it in one of the other threads.

> and links to the feature requests/bug reports for them.  I acknowledge
> it's more work to file bugs, but it's being a good open source
> citizen, IMO.

Who said we aren't going to do it? :-)

I was actually in the process of checking which OSS tools supported
what and how easy it was to fix in each of them (gcc's [[...]] syntax,
clang's GNU and C++ attrs in C mode, cppcheck's fallthrough
support...), but it takes time; I prefer to do the research
beforehand; so that the submitted bug reports are better/more
precise/more helpful, etc.

However, you already sent the LLVM report (without mentioning this
thread or me, nor the -fdouble-square-bracket-attributes clang flag
that I mentioned). That is a bit rude :-) Please take things a little
easier, there is no need to rush stuff. If I didn't have submitted the
LLVM bug report is because I hadn't finish looking at the issue. In
general, I think it is polite (and also more productive to avoid
duplicating efforts) to first ask whoever is working on something
before you rush to do it...

>
> I'm also curious which is the first version of GCC to support the
> comment format?

gcc 7.1 started everything. It is stated in the commit message and
several messages/threads already. Again, please pause, relax and read
a bit before sending stuff around :-)

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-ext4
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 21:17:15 +0000
Message-ID: <CANiq72k2z78uae1GWjECo+OB7GC90b8irt0SHYtnSXT8F2Grrw () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 7:36 PM Nick Desaulniers
<ndesaulniers@google.com> wrote:
>
> On Sun, Oct 21, 2018 at 10:14 AM Miguel Ojeda
> <miguel.ojeda.sandonis@gmail.com> wrote:
> >
> > of its kind) to deal with this: [[fallthrough]] is meant to be
> > a new control keyword in the form of an extension.
>
> I think we can leave the details about the [[]] notation out. IIUC,
> it's only applicable to C++.

No, because C++ is the driving force for the standard attributes
syntax; i.e. C2x is adding them because of the syntax published by
C++17; and possibly gcc 7.1 added support for fallthrough (and comment
parsing) due to C++17 too.

Basically, it is a small paragraph explaining how this came to be.

> > +#if __has_attribute(__fallthrough__)
> > +# define __fallthrough                  __attribute__((__fallthrough__))
> > +#else
> > +# define __fallthrough
>
> While this is in the correct format as the other attributes in this
> file, I think this particular attribute is a special case, because of
> the variety of fallbacks and differing support for them.  I'd like to

No, is it the correct format because we cannot add support for the
other syntax in gcc; so the best way to proceed is to simply wait
until clang supports the GNU attribute in C mode.

The tooling, of course, is another matter and independent of this.

> see in the commit message maybe a list of tools we'd like to support

Yes, I already said I would write it in one of the other threads.

> and links to the feature requests/bug reports for them.  I acknowledge
> it's more work to file bugs, but it's being a good open source
> citizen, IMO.

Who said we aren't going to do it? :-)

I was actually in the process of checking which OSS tools supported
what and how easy it was to fix in each of them (gcc's [[...]] syntax,
clang's GNU and C++ attrs in C mode, cppcheck's fallthrough
support...), but it takes time; I prefer to do the research
beforehand; so that the submitted bug reports are better/more
precise/more helpful, etc.

However, you already sent the LLVM report (without mentioning this
thread or me, nor the -fdouble-square-bracket-attributes clang flag
that I mentioned). That is a bit rude :-) Please take things a little
easier, there is no need to rush stuff. If I didn't have submitted the
LLVM bug report is because I hadn't finish looking at the issue. In
general, I think it is polite (and also more productive to avoid
duplicating efforts) to first ask whoever is working on something
before you rush to do it...

>
> I'm also curious which is the first version of GCC to support the
> comment format?

gcc 7.1 started everything. It is stated in the commit message and
several messages/threads already. Again, please pause, relax and read
a bit before sending stuff around :-)

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 21:17:15 +0000
Message-ID: <CANiq72k2z78uae1GWjECo+OB7GC90b8irt0SHYtnSXT8F2Grrw () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 7:36 PM Nick Desaulniers
<ndesaulniers@google.com> wrote:
>
> On Sun, Oct 21, 2018 at 10:14 AM Miguel Ojeda
> <miguel.ojeda.sandonis@gmail.com> wrote:
> >
> > of its kind) to deal with this: [[fallthrough]] is meant to be
> > a new control keyword in the form of an extension.
>
> I think we can leave the details about the [[]] notation out. IIUC,
> it's only applicable to C++.

No, because C++ is the driving force for the standard attributes
syntax; i.e. C2x is adding them because of the syntax published by
C++17; and possibly gcc 7.1 added support for fallthrough (and comment
parsing) due to C++17 too.

Basically, it is a small paragraph explaining how this came to be.

> > +#if __has_attribute(__fallthrough__)
> > +# define __fallthrough                  __attribute__((__fallthrough__))
> > +#else
> > +# define __fallthrough
>
> While this is in the correct format as the other attributes in this
> file, I think this particular attribute is a special case, because of
> the variety of fallbacks and differing support for them.  I'd like to

No, is it the correct format because we cannot add support for the
other syntax in gcc; so the best way to proceed is to simply wait
until clang supports the GNU attribute in C mode.

The tooling, of course, is another matter and independent of this.

> see in the commit message maybe a list of tools we'd like to support

Yes, I already said I would write it in one of the other threads.

> and links to the feature requests/bug reports for them.  I acknowledge
> it's more work to file bugs, but it's being a good open source
> citizen, IMO.

Who said we aren't going to do it? :-)

I was actually in the process of checking which OSS tools supported
what and how easy it was to fix in each of them (gcc's [[...]] syntax,
clang's GNU and C++ attrs in C mode, cppcheck's fallthrough
support...), but it takes time; I prefer to do the research
beforehand; so that the submitted bug reports are better/more
precise/more helpful, etc.

However, you already sent the LLVM report (without mentioning this
thread or me, nor the -fdouble-square-bracket-attributes clang flag
that I mentioned). That is a bit rude :-) Please take things a little
easier, there is no need to rush stuff. If I didn't have submitted the
LLVM bug report is because I hadn't finish looking at the issue. In
general, I think it is polite (and also more productive to avoid
duplicating efforts) to first ask whoever is working on something
before you rush to do it...

>
> I'm also curious which is the first version of GCC to support the
> comment format?

gcc 7.1 started everything. It is stated in the commit message and
several messages/threads already. Again, please pause, relax and read
a bit before sending stuff around :-)

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kernel
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 21:34:16 +0000
Message-ID: <CANiq72kLOV63XZx2zCnnSuvEyXucqpnjb5dnF=gRZ_q-hEGO2Q () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 7:50 PM Bernd Petrovitsch
<bernd@petrovitsch.priv.at> wrote:
>
> Hi all!
>
> On 22/10/18 13:07, Miguel Ojeda wrote:
> > On Mon, Oct 22, 2018 at 12:54 PM Dan Carpenter <dan.carpenter@oracle.com> wrote:
> >>
> >> Doing both is super ugly.  Let's just do comments until Eclipse gets
> >> updated.
>
> Yes, "Eclipse" as the IDE.
>
> And yes but IMHO better super ugly than loosing the warning - YMMV.

I think Dan meant too simply not touch anything (i.e. not losing the
warning anywhere).

>
> For the archives: I have Eclipse Photon/June 2016 here. And "no break"
> is the (default) string in a comment used by Eclipse (it can be
> customized and is actually a regexp but it must be in a comment).
>

Hm... that means they don't support (by default) the same regexps as
GCC; which is bad: it means that it is only equivalent to the most
relaxed level in gcc, 1:

    """
    -Wimplicit-fallthrough=1 matches .* regular expression, any
comment is used as fallthrough comment.
    """

    See https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html

i.e. any other level above will either not match Eclipse or not match gcc.

> >> I had wanted to move to the attribute because that would simplify things
> >> in Smatch but it's not a huge deal to delay for another year.
> >
> > I can re-send them later on, no problem. On the other hand, doing the
> > changes will push tools to get updated sooner ;-)
> >
> > If tools were doing something as fancy as comment parsing for
> > diagnostics, they should have been updated with the attribute support
> > (either gcc's or C++17's) -- it has been more than a year now since
> > gcc 7.1 and the C++17 final draft. (Note that this does not apply for
> > things like clang, since they weren't doing comment parsing to begin
> > with.)
>
> That would be nice. And if they agree on the same texts (or accept per
> default all somewhat widely used and/or old ones).
>
> After stumbling over
> https://stackoverflow.com/questions/16935935/how-do-i-turn-off-a-static-code-analysis-warning-on-a-line-by-line-warning-in-cd,
> looking into Eclipses Window -> Preferences -> C/C++ -> Code Analysis ->
> "No break at the end of case" screen (that's the screenshot there) and I
> tried various things:
>
> Preface:
> I have
> ----  snip  ----
> #define __fallthrough __attribute__((fallthrough))
> ----  snip  ----
> for gcc >= 7 (because clang doesn't know it and I had also older
> gcc's in use before).
>
> So:
> - Adding a comment to the #define doesn't change anything for Eclipse.

It shouldn't according to the standard -- but who knows... :-)

> - Eclipse looks *only* in comments for the string/regexp given
>   the warnings configuration (and that comment must be on the line
>   directly before the "case").
> - Eclipse understands [[fallthrough]] out-of-the-box though (which
>   is C++11 AFAIK) as does g++-7 (I use -std=gnu++17 - most of the
>   sources are C++, but not all) and clang++-6 (all the current standard
>   Ubuntu-18.06/Bionic packages).

Eclipse understanding [[fallthrough]] is very good, actually.

>   Eclipse "accepts" [[fallthrough]] only in C++ sources (and not in C
>   sources).

Bad, but I guess they will add it to C eventually, since it is
probably coming for C2x.

> - Neither gcc nor clang understand [[fallthrough]] (so it's probably a
>   no-go for the Kernel with C89 anyways).

clang does it if you enable -fdouble-square-bracket-attributes (please
see my other messages). gcc will at some point (if C2x gets the
attributes), but at the moment the C parser is different than the C++
parser and there is no support for it on trunk that I could see, so
they will have to copy the support; i.e. it will take a bit more time
than clang, likely.

Thanks a *lot* for taking a look at Eclipse!

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-ext4
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 21:34:16 +0000
Message-ID: <CANiq72kLOV63XZx2zCnnSuvEyXucqpnjb5dnF=gRZ_q-hEGO2Q () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 7:50 PM Bernd Petrovitsch
<bernd@petrovitsch.priv.at> wrote:
>
> Hi all!
>
> On 22/10/18 13:07, Miguel Ojeda wrote:
> > On Mon, Oct 22, 2018 at 12:54 PM Dan Carpenter <dan.carpenter@oracle.com> wrote:
> >>
> >> Doing both is super ugly.  Let's just do comments until Eclipse gets
> >> updated.
>
> Yes, "Eclipse" as the IDE.
>
> And yes but IMHO better super ugly than loosing the warning - YMMV.

I think Dan meant too simply not touch anything (i.e. not losing the
warning anywhere).

>
> For the archives: I have Eclipse Photon/June 2016 here. And "no break"
> is the (default) string in a comment used by Eclipse (it can be
> customized and is actually a regexp but it must be in a comment).
>

Hm... that means they don't support (by default) the same regexps as
GCC; which is bad: it means that it is only equivalent to the most
relaxed level in gcc, 1:

    """
    -Wimplicit-fallthrough=1 matches .* regular expression, any
comment is used as fallthrough comment.
    """

    See https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html

i.e. any other level above will either not match Eclipse or not match gcc.

> >> I had wanted to move to the attribute because that would simplify things
> >> in Smatch but it's not a huge deal to delay for another year.
> >
> > I can re-send them later on, no problem. On the other hand, doing the
> > changes will push tools to get updated sooner ;-)
> >
> > If tools were doing something as fancy as comment parsing for
> > diagnostics, they should have been updated with the attribute support
> > (either gcc's or C++17's) -- it has been more than a year now since
> > gcc 7.1 and the C++17 final draft. (Note that this does not apply for
> > things like clang, since they weren't doing comment parsing to begin
> > with.)
>
> That would be nice. And if they agree on the same texts (or accept per
> default all somewhat widely used and/or old ones).
>
> After stumbling over
> https://stackoverflow.com/questions/16935935/how-do-i-turn-off-a-static-code-analysis-warning-on-a-line-by-line-warning-in-cd,
> looking into Eclipses Window -> Preferences -> C/C++ -> Code Analysis ->
> "No break at the end of case" screen (that's the screenshot there) and I
> tried various things:
>
> Preface:
> I have
> ----  snip  ----
> #define __fallthrough __attribute__((fallthrough))
> ----  snip  ----
> for gcc >= 7 (because clang doesn't know it and I had also older
> gcc's in use before).
>
> So:
> - Adding a comment to the #define doesn't change anything for Eclipse.

It shouldn't according to the standard -- but who knows... :-)

> - Eclipse looks *only* in comments for the string/regexp given
>   the warnings configuration (and that comment must be on the line
>   directly before the "case").
> - Eclipse understands [[fallthrough]] out-of-the-box though (which
>   is C++11 AFAIK) as does g++-7 (I use -std=gnu++17 - most of the
>   sources are C++, but not all) and clang++-6 (all the current standard
>   Ubuntu-18.06/Bionic packages).

Eclipse understanding [[fallthrough]] is very good, actually.

>   Eclipse "accepts" [[fallthrough]] only in C++ sources (and not in C
>   sources).

Bad, but I guess they will add it to C eventually, since it is
probably coming for C2x.

> - Neither gcc nor clang understand [[fallthrough]] (so it's probably a
>   no-go for the Kernel with C89 anyways).

clang does it if you enable -fdouble-square-bracket-attributes (please
see my other messages). gcc will at some point (if C2x gets the
attributes), but at the moment the C parser is different than the C++
parser and there is no support for it on trunk that I could see, so
they will have to copy the support; i.e. it will take a bit more time
than clang, likely.

Thanks a *lot* for taking a look at Eclipse!

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-doc
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 21:34:16 +0000
Message-ID: <CANiq72kLOV63XZx2zCnnSuvEyXucqpnjb5dnF=gRZ_q-hEGO2Q () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 7:50 PM Bernd Petrovitsch
<bernd@petrovitsch.priv.at> wrote:
>
> Hi all!
>
> On 22/10/18 13:07, Miguel Ojeda wrote:
> > On Mon, Oct 22, 2018 at 12:54 PM Dan Carpenter <dan.carpenter@oracle.com> wrote:
> >>
> >> Doing both is super ugly.  Let's just do comments until Eclipse gets
> >> updated.
>
> Yes, "Eclipse" as the IDE.
>
> And yes but IMHO better super ugly than loosing the warning - YMMV.

I think Dan meant too simply not touch anything (i.e. not losing the
warning anywhere).

>
> For the archives: I have Eclipse Photon/June 2016 here. And "no break"
> is the (default) string in a comment used by Eclipse (it can be
> customized and is actually a regexp but it must be in a comment).
>

Hm... that means they don't support (by default) the same regexps as
GCC; which is bad: it means that it is only equivalent to the most
relaxed level in gcc, 1:

    """
    -Wimplicit-fallthrough=1 matches .* regular expression, any
comment is used as fallthrough comment.
    """

    See https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html

i.e. any other level above will either not match Eclipse or not match gcc.

> >> I had wanted to move to the attribute because that would simplify things
> >> in Smatch but it's not a huge deal to delay for another year.
> >
> > I can re-send them later on, no problem. On the other hand, doing the
> > changes will push tools to get updated sooner ;-)
> >
> > If tools were doing something as fancy as comment parsing for
> > diagnostics, they should have been updated with the attribute support
> > (either gcc's or C++17's) -- it has been more than a year now since
> > gcc 7.1 and the C++17 final draft. (Note that this does not apply for
> > things like clang, since they weren't doing comment parsing to begin
> > with.)
>
> That would be nice. And if they agree on the same texts (or accept per
> default all somewhat widely used and/or old ones).
>
> After stumbling over
> https://stackoverflow.com/questions/16935935/how-do-i-turn-off-a-static-code-analysis-warning-on-a-line-by-line-warning-in-cd,
> looking into Eclipses Window -> Preferences -> C/C++ -> Code Analysis ->
> "No break at the end of case" screen (that's the screenshot there) and I
> tried various things:
>
> Preface:
> I have
> ----  snip  ----
> #define __fallthrough __attribute__((fallthrough))
> ----  snip  ----
> for gcc >= 7 (because clang doesn't know it and I had also older
> gcc's in use before).
>
> So:
> - Adding a comment to the #define doesn't change anything for Eclipse.

It shouldn't according to the standard -- but who knows... :-)

> - Eclipse looks *only* in comments for the string/regexp given
>   the warnings configuration (and that comment must be on the line
>   directly before the "case").
> - Eclipse understands [[fallthrough]] out-of-the-box though (which
>   is C++11 AFAIK) as does g++-7 (I use -std=gnu++17 - most of the
>   sources are C++, but not all) and clang++-6 (all the current standard
>   Ubuntu-18.06/Bionic packages).

Eclipse understanding [[fallthrough]] is very good, actually.

>   Eclipse "accepts" [[fallthrough]] only in C++ sources (and not in C
>   sources).

Bad, but I guess they will add it to C eventually, since it is
probably coming for C2x.

> - Neither gcc nor clang understand [[fallthrough]] (so it's probably a
>   no-go for the Kernel with C89 anyways).

clang does it if you enable -fdouble-square-bracket-attributes (please
see my other messages). gcc will at some point (if C2x gets the
attributes), but at the moment the C parser is different than the C++
parser and there is no support for it on trunk that I could see, so
they will have to copy the support; i.e. it will take a bit more time
than clang, likely.

Thanks a *lot* for taking a look at Eclipse!

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 21:34:16 +0000
Message-ID: <CANiq72kLOV63XZx2zCnnSuvEyXucqpnjb5dnF=gRZ_q-hEGO2Q () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 7:50 PM Bernd Petrovitsch
<bernd@petrovitsch.priv.at> wrote:
>
> Hi all!
>
> On 22/10/18 13:07, Miguel Ojeda wrote:
> > On Mon, Oct 22, 2018 at 12:54 PM Dan Carpenter <dan.carpenter@oracle.com> wrote:
> >>
> >> Doing both is super ugly.  Let's just do comments until Eclipse gets
> >> updated.
>
> Yes, "Eclipse" as the IDE.
>
> And yes but IMHO better super ugly than loosing the warning - YMMV.

I think Dan meant too simply not touch anything (i.e. not losing the
warning anywhere).

>
> For the archives: I have Eclipse Photon/June 2016 here. And "no break"
> is the (default) string in a comment used by Eclipse (it can be
> customized and is actually a regexp but it must be in a comment).
>

Hm... that means they don't support (by default) the same regexps as
GCC; which is bad: it means that it is only equivalent to the most
relaxed level in gcc, 1:

    """
    -Wimplicit-fallthrough=1 matches .* regular expression, any
comment is used as fallthrough comment.
    """

    See https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html

i.e. any other level above will either not match Eclipse or not match gcc.

> >> I had wanted to move to the attribute because that would simplify things
> >> in Smatch but it's not a huge deal to delay for another year.
> >
> > I can re-send them later on, no problem. On the other hand, doing the
> > changes will push tools to get updated sooner ;-)
> >
> > If tools were doing something as fancy as comment parsing for
> > diagnostics, they should have been updated with the attribute support
> > (either gcc's or C++17's) -- it has been more than a year now since
> > gcc 7.1 and the C++17 final draft. (Note that this does not apply for
> > things like clang, since they weren't doing comment parsing to begin
> > with.)
>
> That would be nice. And if they agree on the same texts (or accept per
> default all somewhat widely used and/or old ones).
>
> After stumbling over
> https://stackoverflow.com/questions/16935935/how-do-i-turn-off-a-static-code-analysis-warning-on-a-line-by-line-warning-in-cd,
> looking into Eclipses Window -> Preferences -> C/C++ -> Code Analysis ->
> "No break at the end of case" screen (that's the screenshot there) and I
> tried various things:
>
> Preface:
> I have
> ----  snip  ----
> #define __fallthrough __attribute__((fallthrough))
> ----  snip  ----
> for gcc >= 7 (because clang doesn't know it and I had also older
> gcc's in use before).
>
> So:
> - Adding a comment to the #define doesn't change anything for Eclipse.

It shouldn't according to the standard -- but who knows... :-)

> - Eclipse looks *only* in comments for the string/regexp given
>   the warnings configuration (and that comment must be on the line
>   directly before the "case").
> - Eclipse understands [[fallthrough]] out-of-the-box though (which
>   is C++11 AFAIK) as does g++-7 (I use -std=gnu++17 - most of the
>   sources are C++, but not all) and clang++-6 (all the current standard
>   Ubuntu-18.06/Bionic packages).

Eclipse understanding [[fallthrough]] is very good, actually.

>   Eclipse "accepts" [[fallthrough]] only in C++ sources (and not in C
>   sources).

Bad, but I guess they will add it to C eventually, since it is
probably coming for C2x.

> - Neither gcc nor clang understand [[fallthrough]] (so it's probably a
>   no-go for the Kernel with C89 anyways).

clang does it if you enable -fdouble-square-bracket-attributes (please
see my other messages). gcc will at some point (if C2x gets the
attributes), but at the moment the C parser is different than the C++
parser and there is no support for it on trunk that I could see, so
they will have to copy the support; i.e. it will take a bit more time
than clang, likely.

Thanks a *lot* for taking a look at Eclipse!

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kbuild
Subject: Re: [PATCH 1/2] Compiler Attributes: add support for __fallthrough (gcc >= 7.1)
Date: Mon, 22 Oct 2018 21:34:16 +0000
Message-ID: <CANiq72kLOV63XZx2zCnnSuvEyXucqpnjb5dnF=gRZ_q-hEGO2Q () mail ! gmail ! com>
--------------------
On Mon, Oct 22, 2018 at 7:50 PM Bernd Petrovitsch
<bernd@petrovitsch.priv.at> wrote:
>
> Hi all!
>
> On 22/10/18 13:07, Miguel Ojeda wrote:
> > On Mon, Oct 22, 2018 at 12:54 PM Dan Carpenter <dan.carpenter@oracle.com> wrote:
> >>
> >> Doing both is super ugly.  Let's just do comments until Eclipse gets
> >> updated.
>
> Yes, "Eclipse" as the IDE.
>
> And yes but IMHO better super ugly than loosing the warning - YMMV.

I think Dan meant too simply not touch anything (i.e. not losing the
warning anywhere).

>
> For the archives: I have Eclipse Photon/June 2016 here. And "no break"
> is the (default) string in a comment used by Eclipse (it can be
> customized and is actually a regexp but it must be in a comment).
>

Hm... that means they don't support (by default) the same regexps as
GCC; which is bad: it means that it is only equivalent to the most
relaxed level in gcc, 1:

    """
    -Wimplicit-fallthrough=1 matches .* regular expression, any
comment is used as fallthrough comment.
    """

    See https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html

i.e. any other level above will either not match Eclipse or not match gcc.

> >> I had wanted to move to the attribute because that would simplify things
> >> in Smatch but it's not a huge deal to delay for another year.
> >
> > I can re-send them later on, no problem. On the other hand, doing the
> > changes will push tools to get updated sooner ;-)
> >
> > If tools were doing something as fancy as comment parsing for
> > diagnostics, they should have been updated with the attribute support
> > (either gcc's or C++17's) -- it has been more than a year now since
> > gcc 7.1 and the C++17 final draft. (Note that this does not apply for
> > things like clang, since they weren't doing comment parsing to begin
> > with.)
>
> That would be nice. And if they agree on the same texts (or accept per
> default all somewhat widely used and/or old ones).
>
> After stumbling over
> https://stackoverflow.com/questions/16935935/how-do-i-turn-off-a-static-code-analysis-warning-on-a-line-by-line-warning-in-cd,
> looking into Eclipses Window -> Preferences -> C/C++ -> Code Analysis ->
> "No break at the end of case" screen (that's the screenshot there) and I
> tried various things:
>
> Preface:
> I have
> ----  snip  ----
> #define __fallthrough __attribute__((fallthrough))
> ----  snip  ----
> for gcc >= 7 (because clang doesn't know it and I had also older
> gcc's in use before).
>
> So:
> - Adding a comment to the #define doesn't change anything for Eclipse.

It shouldn't according to the standard -- but who knows... :-)

> - Eclipse looks *only* in comments for the string/regexp given
>   the warnings configuration (and that comment must be on the line
>   directly before the "case").
> - Eclipse understands [[fallthrough]] out-of-the-box though (which
>   is C++11 AFAIK) as does g++-7 (I use -std=gnu++17 - most of the
>   sources are C++, but not all) and clang++-6 (all the current standard
>   Ubuntu-18.06/Bionic packages).

Eclipse understanding [[fallthrough]] is very good, actually.

>   Eclipse "accepts" [[fallthrough]] only in C++ sources (and not in C
>   sources).

Bad, but I guess they will add it to C eventually, since it is
probably coming for C2x.

> - Neither gcc nor clang understand [[fallthrough]] (so it's probably a
>   no-go for the Kernel with C89 anyways).

clang does it if you enable -fdouble-square-bracket-attributes (please
see my other messages). gcc will at some point (if C2x gets the
attributes), but at the moment the C parser is different than the C++
parser and there is no support for it on trunk that I could see, so
they will have to copy the support; i.e. it will take a bit more time
than clang, likely.

Thanks a *lot* for taking a look at Eclipse!

Cheers,
Miguel
================================================================================


################################################################################

=== Thread: [PATCH 1/2] doc: convert IR.md to reST ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 1/2] doc: convert IR.md to reST
Date: Mon, 21 May 2018 01:52:58 +0000
Message-ID: <20180521015259.57316-2-luc.vanoostenryck () gmail ! com>
--------------------
The doc for IR instructions was written in MarkDown format which
is quite easy and quite close to normal text.
However Markdown has a few limitations:
1) structuration via level-x headings is not nice when rendered
2) it would be nice to have the instructions in the index

So convert this in reStructuredText format as it will
directly solve problme 1) and prepare to solve problem 2)

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/IR.md  | 356 -------------------------------------
 Documentation/IR.rst | 405 +++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 405 insertions(+), 356 deletions(-)
 delete mode 100644 Documentation/IR.md
 create mode 100644 Documentation/IR.rst

diff --git a/Documentation/IR.md b/Documentation/IR.md
deleted file mode 100644
index 03fa3f807..000000000
--- a/Documentation/IR.md
+++ /dev/null
@@ -1,356 +0,0 @@
-# Sparse's Intermediate Representation
-
-
-## Instruction
-This document briefly describes which field of struct instruction is
-used by which operation.
-
-Some of those fields are used by almost all instructions,
-some others are specific to only one or a few instructions.
-The common ones are:
-- .src1, .src2, .src3: (pseudo_t) operands of binops or ternary ops.
-- .src: (pseudo_t) operand of unary ops (alias for .src1).
-- .target: (pseudo_t) result of unary, binary & ternary ops, is sometimes used
-	otherwise by some others instructions.
-- .cond: (pseudo_t) input operands for condition (alias .src/.src1)
-- .type: (symbol*) usually the type of .result, sometimes of the operands
-
-### Terminators
-#### OP_RET
-Return from subroutine.
-- .src : returned value (NULL if void)
-- .type: type of .src
-
-#### OP_BR
-Unconditional branch
-- .bb_true: destination basic block
-
-#### OP_CBR
-Conditional branch
-- .cond: condition
-- .type: type of .cond, must be an integral type
-- .bb_true, .bb_false: destination basic blocks
-
-#### OP_SWITCH
-Switch / multi-branch
-- .cond: condition
-- .type: type of .cond, must be an integral type
-- .multijmp_list: pairs of case-value - destination basic block
-
-#### OP_COMPUTEDGOTO
-Computed goto / branch to register
-- .src: address to branch to (void*)
-- .multijmp_list: list of possible destination basic blocks
-
-### Arithmetic binops
-They all follow the same signature:
-- .src1, .src1: operands (types must be compatible with .target)
-- .target: result of the operation (must be an integral type)
-- .type: type of .target
-
-#### OP_ADD
-Integer addition.
-
-#### OP_SUB
-Integer subtraction.
-
-#### OP_MUL
-Integer multiplication.
-
-#### OP_DIVU
-Integer unsigned division.
-
-#### OP_DIVS
-Integer signed division.
-
-#### OP_MODU
-Integer unsigned remainder.
-
-#### OP_MODS
-Integer signed remainder.
-
-#### OP_SHL
-Shift left (integer only)
-
-#### OP_LSR
-Logical Shift right (integer only)
-
-#### OP_ASR
-Arithmetic Shift right (integer only)
-
-### Floating-point binops
-They all follow the same signature:
-- .src1, .src1: operands (types must be compatible with .target)
-- .target: result of the operation (must be a floating-point type)
-- .type: type of .target
-
-#### OP_FADD
-Floating-point addition.
-
-#### OP_FSUB
-Floating-point subtraction.
-
-#### OP_FMUL
-Floating-point multiplication.
-
-#### OP_FDIV
-Floating-point division.
-
-### Logical ops
-They all follow the same signature:
-- .src1, .src2: operands (types must be compatible with .target)
-- .target: result of the operation
-- .type: type of .target, must be an integral type
-
-#### OP_AND
-#### OP_OR
-#### OP_XOR
-
-### Boolean ops
-#### OP_AND_BOOL
-#### OP_OR_BOOL
-
-### Integer compares
-They all have the following signature:
-- .src1, .src2: operands (types must be compatible)
-- .target: result of the operation (0/1 valued integer)
-- .type: type of .target, must be an integral type
-
-#### OP_SET_EQ
-Compare equal.
-
-#### OP_SET_NE
-Compare not-equal.
-
-#### OP_SET_LE
-Compare less-than-or-equal (signed).
-
-#### OP_SET_GE
-Compare greater-than-or-equal (signed).
-
-#### OP_SET_LT
-Compare less-than (signed).
-
-#### OP_SET_GT
-Compare greater-than (signed).
-
-#### OP_SET_B
-Compare less-than (unsigned).
-
-#### OP_SET_A
-Compare greater-than (unsigned).
-
-#### OP_SET_BE
-Compare less-than-or-equal (unsigned).
-
-#### OP_SET_AE
-Compare greater-than-or-equal (unsigned).
-
-### Floating-point compares
-They all have the same signature as the integer compares.
-The usual 6 operations exist in two versions: 'ordered' and
-'unordered'. Theses operations first check if any operand is a
-NaN and if it is the case the ordered compares return false
-and then unordered return true, otherwise the result of the
-comparison, now garanted to be done on non-NaNs, is returned.
-
-#### OP_FCMP_OEQ
-Floating-point compare ordered equal
-
-#### OP_FCMP_ONE
-Floating-point compare ordered not-equal
-
-#### OP_FCMP_OLE
-Floating-point compare ordered less-than-or-equal
-
-#### OP_FCMP_OGE
-Floating-point compare ordered greater-or-equal
-
-#### OP_FCMP_OLT
-Floating-point compare ordered less-than
-
-#### OP_FCMP_OGT
-Floating-point compare ordered greater-than
-
-
-#### OP_FCMP_UEQ
-Floating-point compare unordered equal
-
-#### OP_FCMP_UNE
-Floating-point compare unordered not-equal
-
-#### OP_FCMP_ULE
-Floating-point compare unordered less-than-or-equal
-
-#### OP_FCMP_UGE
-Floating-point compare unordered greater-or-equal
-
-#### OP_FCMP_ULT
-Floating-point compare unordered less-than
-
-#### OP_FCMP_UGT
-Floating-point compare unordered greater-than
-
-
-#### OP_FCMP_ORD
-Floating-point compare ordered: return true if both operands are ordered
-(none of the operands are a NaN) and false otherwise.
-
-#### OP_FCMP_UNO
-Floating-point compare unordered: return false if no operands is ordered
-and true otherwise.
-
-### Unary ops
-#### OP_NOT
-Logical not.
-- .src: operand (type must be compatible with .target)
-- .target: result of the operation
-- .type: type of .target, must be an integral type
-
-#### OP_NEG
-Integer negation.
-- .src: operand (type must be compatible with .target)
-- .target: result of the operation (must be an integral type)
-- .type: type of .target
-
-#### OP_FNEG
-Floating-point negation.
-- .src: operand (type must be compatible with .target)
-- .target: result of the operation (must be a floating-point type)
-- .type: type of .target
-
-#### OP_COPY
-Copy (only needed after out-of-SSA).
-- .src: operand (type must be compatible with .target)
-- .target: result of the operation
-- .type: type of .target
-
-### Type conversions
-They all have the following signature:
-- .src: source value
-- .orig_type: type of .src
-- .target: result value
-- .type: type of .target
-
-#### OP_CAST
-Cast to unsigned integer (and to void pointer).
-
-#### OP_SCAST
-Cast to signed integer.
-
-#### OP_FPCAST
-Cast to floating-point.
-
-#### OP_PTRCAST
-Cast to pointer.
-
-### Ternary ops
-#### OP_SEL
-- .src1: condition, must be of integral type
-- .src2, .src3: operands (types must be compatible with .target)
-- .target: result of the operation
-- .type: type of .target
-
-#### OP_RANGE
-Range/bounds checking (only used for an unused sparse extension).
-- .src1: value to be checked
-- .src2, src3: bound of the value (must be constants?)
-- .type: type of .src[123]?
-
-### Memory ops
-#### OP_LOAD
-Load.
-- .src: base address to load from
-- .offset: address offset
-- .target: loaded value
-- .type: type of .target
-
-#### OP_STORE
-Store.
-- .src: base address to store to
-- .offset: address offset
-- .target: value to be stored
-- .type: type of .target
-
-### Others
-#### OP_SYMADDR
-Create a pseudo corresponding to the address of a symbol.
-- .symbol: (pseudo_t) input symbol (alias .src)
-- .target: symbol's address
-
-#### OP_SETFVAL
-Create a pseudo corresponding to a floating-point literal.
-- .fvalue: the literal's value (long double)
-- .target: the corresponding pseudo
-- .type: type of the literal & .target
-
-#### OP_SETVAL
-Create a pseudo corresponding to a string literal or a label-as-value.
-The value is given as an expression EXPR_STRING or EXPR_LABEL.
-- .val: (expression) input expression
-- .target: the resulting value
-- .type: type of .target, the value
-
-#### OP_PHI
-Phi-node (for SSA form).
-- .phi_list: phi-operands (type must be compatible with .target)
-- .target: "result"
-- .type: type of .target
-
-#### OP_PHISOURCE
-Phi-node source.
-Like OP_COPY but exclusively used to give a defining instructions
-(and thus also a type) to *all* OP_PHI operands.
-- .phi_src: operand (type must be compatible with .target, alias .src)
-- .target: the "result" PSEUDO_PHI
-- .type: type of .target
-- .phi_users: list of phi instructions using the target pseudo
-
-#### OP_CALL
-Function call.
-- .func: (pseudo_t) the function (can be a symbol or a "register", alias .src))
-- .arguments: (pseudo_list) list of the associated arguments
-- .target: function return value (if any)
-- .type: type of .target
-- .fntypes: (symbol_list) list of the function's types: the first enrty is the full function type, the next ones are the type of each arguments
-
-#### OP_INLINED_CALL
-Only used as an annotation to show that the instructions just above
-correspond to a function that have been inlined.
-- .func: (pseudo_t) the function (must be a symbol, alias .src))
-- .arguments: list of pseudos that where the function's arguments
-- .target: function return value (if any)
-- .type: type of .target
-
-#### OP_SLICE
-Extract a "slice" from an aggregate.
-- .base: (pseudo_t) aggregate (alias .src)
-- .from, .len: offet & size of the "slice" within the aggregate
-- .target: result
-- .type: type of .target
-
-#### OP_ASM
-Inlined assembly code.
-- .string: asm template
-- .asm_rules: asm constraints, rules
-
-### Sparse tagging (line numbers, context, whatever)
-#### OP_CONTEXT
-Currently only used for lock/unlock tracking.
-- .context_expr: unused
-- .increment: (1 for locking, -1 for unlocking)
-- .check: (ignore the instruction if 0)
-
-### Misc ops
-#### OP_ENTRY
-Function entry point (no associated semantic).
-
-#### OP_BADOP
-Invalid operation (should never be generated).
-
-#### OP_NOP
-No-op (should never be generated).
-
-#### OP_DEATHNOTE
-Annotation telling the pseudo will be death after the next
-instruction (other than some other annotation, that is).
diff --git a/Documentation/IR.rst b/Documentation/IR.rst
new file mode 100644
index 000000000..ec297eed0
--- /dev/null
+++ b/Documentation/IR.rst
@@ -0,0 +1,405 @@
+Sparse's Intermediate Representation
+====================================
+
+Instructions
+~~~~~~~~~~~~
+
+This document briefly describes which field of struct instruction is
+used by which operation.
+
+Some of those fields are used by almost all instructions,
+some others are specific to only one or a few instructions.
+The common ones are:
+
+* .src1, .src2, .src3: (pseudo_t) operands of binops or ternary ops.
+* .src: (pseudo_t) operand of unary ops (alias for .src1).
+* .target: (pseudo_t) result of unary, binary & ternary ops, is
+  sometimes used otherwise by some others instructions.
+* .cond: (pseudo_t) input operands for condition (alias .src/.src1)
+* .type: (symbol*) usually the type of .result, sometimes of the operands
+
+Terminators
+-----------
+OP_RET
+	Return from subroutine.
+
+	* .src : returned value (NULL if void)
+	* .type: type of .src
+
+OP_BR
+	Unconditional branch
+
+	* .bb_true: destination basic block
+
+OP_CBR
+	Conditional branch
+
+	* .cond: condition
+	* .type: type of .cond, must be an integral type
+	* .bb_true, .bb_false: destination basic blocks
+
+OP_SWITCH
+	Switch / multi-branch
+
+	* .cond: condition
+	* .type: type of .cond, must be an integral type
+	* .multijmp_list: pairs of case-value - destination basic block
+
+OP_COMPUTEDGOTO
+	Computed goto / branch to register
+
+	* .src: address to branch to (void*)
+	* .multijmp_list: list of possible destination basic blocks
+
+Arithmetic binops
+-----------------
+They all follow the same signature:
+	* .src1, .src1: operands (types must be compatible with .target)
+	* .target: result of the operation (must be an integral type)
+	* .type: type of .target
+
+OP_ADD
+	Integer addition.
+
+OP_SUB
+	Integer subtraction.
+
+OP_MUL
+	Integer multiplication.
+
+OP_DIVU
+	Integer unsigned division.
+
+OP_DIVS
+	Integer signed division.
+
+OP_MODU
+	Integer unsigned remainder.
+
+OP_MODS
+	Integer signed remainder.
+
+OP_SHL
+	Shift left (integer only)
+
+OP_LSR
+	Logical Shift right (integer only)
+
+OP_ASR
+	Arithmetic Shift right (integer only)
+
+Floating-point binops
+---------------------
+They all follow the same signature:
+	* .src1, .src1: operands (types must be compatible with .target)
+	* .target: result of the operation (must be a floating-point type)
+	* .type: type of .target
+
+OP_FADD
+	Floating-point addition.
+
+OP_FSUB
+	Floating-point subtraction.
+
+OP_FMUL
+	Floating-point multiplication.
+
+OP_FDIV
+	Floating-point division.
+
+Logical ops
+-----------
+They all follow the same signature:
+	* .src1, .src2: operands (types must be compatible with .target)
+	* .target: result of the operation
+	* .type: type of .target, must be an integral type
+
+OP_AND
+	Logical AND
+
+OP_OR
+	Logical OR
+
+OP_XOR
+	Logical XOR
+
+Boolean ops
+-----------
+OP_AND_BOOL
+	Boolean AND
+
+OP_OR_BOOL
+	Boolean OR
+
+Integer compares
+----------------
+They all have the following signature:
+	* .src1, .src2: operands (types must be compatible)
+	* .target: result of the operation (0/1 valued integer)
+	* .type: type of .target, must be an integral type
+
+OP_SET_EQ
+	Compare equal.
+
+OP_SET_NE
+	Compare not-equal.
+
+OP_SET_LE
+	Compare less-than-or-equal (signed).
+
+OP_SET_GE
+	Compare greater-than-or-equal (signed).
+
+OP_SET_LT
+	Compare less-than (signed).
+
+OP_SET_GT
+	Compare greater-than (signed).
+
+OP_SET_B
+	Compare less-than (unsigned).
+
+OP_SET_A
+	Compare greater-than (unsigned).
+
+OP_SET_BE
+	Compare less-than-or-equal (unsigned).
+
+OP_SET_AE
+	Compare greater-than-or-equal (unsigned).
+
+Floating-point compares
+-----------------------
+They all have the same signature as the integer compares.
+
+The usual 6 operations exist in two versions: 'ordered' and
+'unordered'. These operations first check if any operand is a
+NaN and if it is the case the ordered compares return false
+and then unordered return true, otherwise the result of the
+comparison, now guaranteed to be done on non-NaNs, is returned.
+
+OP_FCMP_OEQ
+	Floating-point compare ordered equal
+
+OP_FCMP_ONE
+	Floating-point compare ordered not-equal
+
+OP_FCMP_OLE
+	Floating-point compare ordered less-than-or-equal
+
+OP_FCMP_OGE
+	Floating-point compare ordered greater-or-equal
+
+OP_FCMP_OLT
+	Floating-point compare ordered less-than
+
+OP_FCMP_OGT
+	Floating-point compare ordered greater-than
+
+
+OP_FCMP_UEQ
+	Floating-point compare unordered equal
+
+OP_FCMP_UNE
+	Floating-point compare unordered not-equal
+
+OP_FCMP_ULE
+	Floating-point compare unordered less-than-or-equal
+
+OP_FCMP_UGE
+	Floating-point compare unordered greater-or-equal
+
+OP_FCMP_ULT
+	Floating-point compare unordered less-than
+
+OP_FCMP_UGT
+	Floating-point compare unordered greater-than
+
+
+OP_FCMP_ORD
+	Floating-point compare ordered: return true if both operands are ordered
+	(none of the operands are a NaN) and false otherwise.
+
+OP_FCMP_UNO
+	Floating-point compare unordered: return false if no operands is ordered
+	and true otherwise.
+
+Unary ops
+---------
+OP_NOT
+	Logical not.
+
+	* .src: operand (type must be compatible with .target)
+	* .target: result of the operation
+	* .type: type of .target, must be an integral type
+
+OP_NEG
+	Integer negation.
+
+	* .src: operand (type must be compatible with .target)
+	* .target: result of the operation (must be an integral type)
+	* .type: type of .target
+
+OP_FNEG
+	Floating-point negation.
+
+	* .src: operand (type must be compatible with .target)
+	* .target: result of the operation (must be a floating-point type)
+	* .type: type of .target
+
+OP_COPY
+	Copy (only needed after out-of-SSA).
+
+	* .src: operand (type must be compatible with .target)
+	* .target: result of the operation
+	* .type: type of .target
+
+Type conversions
+----------------
+They all have the following signature:
+	* .src: source value
+	* .orig_type: type of .src
+	* .target: result value
+	* .type: type of .target
+
+OP_CAST
+	Cast to unsigned integer (and to void pointer).
+
+OP_SCAST
+	Cast to signed integer.
+
+OP_FPCAST
+	Cast to floating-point.
+
+OP_PTRCAST
+	Cast to pointer.
+
+Ternary ops
+-----------
+OP_SEL
+	* .src1: condition, must be of integral type
+	* .src2, .src3: operands (types must be compatible with .target)
+	* .target: result of the operation
+	* .type: type of .target
+
+OP_RANGE
+	Range/bounds checking (only used for an unused sparse extension).
+
+	* .src1: value to be checked
+	* .src2, src3: bound of the value (must be constants?)
+	* .type: type of .src[123]?
+
+Memory ops
+----------
+OP_LOAD
+	Load.
+
+	* .src: base address to load from
+	* .offset: address offset
+	* .target: loaded value
+	* .type: type of .target
+
+OP_STORE
+	Store.
+
+	* .src: base address to store to
+	* .offset: address offset
+	* .target: value to be stored
+	* .type: type of .target
+
+Others
+------
+OP_SYMADDR
+	Create a pseudo corresponding to the address of a symbol.
+
+	* .symbol: (pseudo_t) input symbol (alias .src)
+	* .target: symbol's address
+
+OP_SETFVAL
+	Create a pseudo corresponding to a floating-point literal.
+
+	* .fvalue: the literal's value (long double)
+	* .target: the corresponding pseudo
+	* .type: type of the literal & .target
+
+OP_SETVAL
+	Create a pseudo corresponding to a string literal or a label-as-value.
+	The value is given as an expression EXPR_STRING or EXPR_LABEL.
+
+	* .val: (expression) input expression
+	* .target: the resulting value
+	* .type: type of .target, the value
+
+OP_PHI
+	Phi-node (for SSA form).
+
+	* .phi_list: phi-operands (type must be compatible with .target)
+	* .target: "result"
+	* .type: type of .target
+
+OP_PHISOURCE
+	Phi-node source.
+	Like OP_COPY but exclusively used to give a defining instructions
+	(and thus also a type) to *all* OP_PHI operands.
+	* .phi_src: operand (type must be compatible with .target, alias .src)
+	* .target: the "result" PSEUDO_PHI
+	* .type: type of .target
+	* .phi_users: list of phi instructions using the target pseudo
+
+OP_CALL
+	Function call.
+
+	* .func: (pseudo_t) the function (can be a symbol or a "register",
+	  alias .src))
+	* .arguments: (pseudo_list) list of the associated arguments
+	* .target: function return value (if any)
+	* .type: type of .target
+	* .fntypes: (symbol_list) list of the function's types: the first
+	  entry is the full function type, the next ones are the type of
+	  each arguments
+
+OP_INLINED_CALL
+	Only used as an annotation to show that the instructions just above
+	correspond to a function that have been inlined.
+	* .func: (pseudo_t) the function (must be a symbol, alias .src))
+	* .arguments: list of pseudos that where the function's arguments
+	* .target: function return value (if any)
+	* .type: type of .target
+
+OP_SLICE
+	Extract a "slice" from an aggregate.
+
+	* .base: (pseudo_t) aggregate (alias .src)
+	* .from, .len: offet & size of the "slice" within the aggregate
+	* .target: result
+	* .type: type of .target
+
+OP_ASM
+	Inlined assembly code.
+
+	* .string: asm template
+	* .asm_rules: asm constraints, rules
+
+Sparse tagging (line numbers, context, whatever)
+------------------------------------------------
+OP_CONTEXT
+	Currently only used for lock/unlock tracking.
+
+	* .context_expr: unused
+	* .increment: (1 for locking, -1 for unlocking)
+	* .check: (ignore the instruction if 0)
+
+Misc ops
+--------
+OP_ENTRY
+	Function entry point (no associated semantic).
+
+OP_BADOP
+	Invalid operation (should never be generated).
+
+OP_NOP
+	No-op (should never be generated).
+
+OP_DEATHNOTE
+	Annotation telling the pseudo will be death after the next
+	instruction (other than some other annotation, that is).
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 1/2] give a position to end-of-input ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 1/2] give a position to end-of-input
Date: Sun, 27 May 2018 00:08:54 +0000
Message-ID: <20180527000855.19459-2-luc.vanoostenryck () gmail ! com>
--------------------
If an error occurs at the end of the input, for example because
a missing terminating ';' or '}', the error message is like:
	builtin:0:0: error: ...
IOW, the stream name & position is not displayed because the
because the current token is eof_token_entry which has no position.
This can be confusing and for sure doesn't point where the error is.

Fix this by giving to eof_token_entry the end-of-stream position.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 tokenize.c                        |  1 +
 validation/check_byte_count-ice.c |  4 ++--
 validation/error-at-eof.c         | 10 ++++++++++
 3 files changed, 13 insertions(+), 2 deletions(-)
 create mode 100644 validation/error-at-eof.c

diff --git a/tokenize.c b/tokenize.c
index aa4dc1840..1118786f6 100644
--- a/tokenize.c
+++ b/tokenize.c
@@ -444,6 +444,7 @@ static struct token *mark_eof(stream_t *stream)
 	struct token *end;
 
 	end = alloc_token(stream);
+	eof_token_entry.pos = end->pos;
 	token_type(end) = TOKEN_STREAMEND;
 	end->pos.newline = 1;
 
diff --git a/validation/check_byte_count-ice.c b/validation/check_byte_count-ice.c
index 7b85b9631..dae40c676 100644
--- a/validation/check_byte_count-ice.c
+++ b/validation/check_byte_count-ice.c
@@ -12,8 +12,8 @@ check_byte_count-ice.c:6:0: warning: Newline in string or character constant
 check_byte_count-ice.c:5:23: warning: multi-character character constant
 check_byte_count-ice.c:6:1: error: Expected ) in function call
 check_byte_count-ice.c:6:1: error: got }
-builtin:0:0: error: Expected } at end of function
-builtin:0:0: error: got end-of-input
+check_byte_count-ice.c:20:0: error: Expected } at end of function
+check_byte_count-ice.c:20:0: error: got end-of-input
 check_byte_count-ice.c:5:15: error: not enough arguments for function memset
  * check-error-end
  */
diff --git a/validation/error-at-eof.c b/validation/error-at-eof.c
new file mode 100644
index 000000000..7d9336511
--- /dev/null
+++ b/validation/error-at-eof.c
@@ -0,0 +1,10 @@
+/*
+ * check-name: error-at-eof
+ *
+ * check-error-start
+error-at-eof.c:11:0: error: Expected ; at end of declaration
+error-at-eof.c:11:0: error: got end-of-input
+ * check-error-end
+ */
+
+int a
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 1/2] label: add testcase for label redefinition ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 1/2] label: add testcase for label redefinition
Date: Sat, 26 May 2018 18:42:54 +0000
Message-ID: <20180526184255.53856-2-luc.vanoostenryck () gmail ! com>
--------------------
Redefined labels create inconsistencies in BB processing.
Add a testcase for it.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/label-redefined.c | 18 ++++++++++++++++++
 1 file changed, 18 insertions(+)
 create mode 100644 validation/label-redefined.c

diff --git a/validation/label-redefined.c b/validation/label-redefined.c
new file mode 100644
index 000000000..5e0a51b41
--- /dev/null
+++ b/validation/label-redefined.c
@@ -0,0 +1,18 @@
+extern void fun(void);
+
+static void foo(int p)
+{
+l:
+	if (p)
+l:
+		fun();
+}
+
+/*
+ * check-name: label-redefined
+ * check-known-to-fail
+ *
+ * check-error-start
+label-redefined.c:7:1: error: label 'l' redefined
+ * check-error-end
+ */
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 1/2] testsuite: add check-cp-if ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 1/2] testsuite: add check-cp-if
Date: Mon, 30 Apr 2018 13:21:31 +0000
Message-ID: <20180430132132.62055-2-luc.vanoostenryck () gmail ! com>
--------------------
Ideally, the testcases should be universal but it happen than
some of them need to test some specificities or are meaningless
or plainly wrong in some situations. In such cases, these tests
must but ignored.

Currently, the only the only mechanisms a test are:
1) ignoring the tests depending on a tool which cannot be compiled
   (like, for example, those using sparse-llvm when LLVM is not
   installed.
2) some rather corse criteria using the name of the arch used
   to run the tests.

Allow more flexibility by allowing to exclude some tests based on
the evaluation of some pre-processor expression at test-time.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/test-suite |  4 ++++
 validation/test-suite    | 14 ++++++++++++++
 2 files changed, 18 insertions(+)

diff --git a/Documentation/test-suite b/Documentation/test-suite
index 1315dbd67..626bc3fe7 100644
--- a/Documentation/test-suite
+++ b/Documentation/test-suite
@@ -27,6 +27,10 @@ check-arch-only: <arch[|...]>
 	Ignore the test if the current architecture (as returned by 'uname -m')
 	match or not one of the archs given in the pattern.
 
+check-cpp-if: <condition>
+	Ignore the test if the given condition is false when evaluated
+	by sparse's pre-processor.
+
 check-exit-value: <value>
 	The expected exit value of check-command. It defaults to 0.
 
diff --git a/validation/test-suite b/validation/test-suite
index 4fdc9e9fa..6c6d34b28 100755
--- a/validation/test-suite
+++ b/validation/test-suite
@@ -79,6 +79,7 @@ get_tag_value()
 	check_output_pattern=0
 	check_arch_ignore=""
 	check_arch_only=""
+	check_cpp_if=""
 
 	lines=$(grep 'check-[a-z-]*' $1 | \
 		sed -e 's/^.*\(check-[a-z-]*:*\) *\(.*\)$/\1 \2/')
@@ -102,6 +103,7 @@ get_tag_value()
 					check_arch_ignore="$val" ;;
 		check-arch-only:)	arch=$(uname -m)
 					check_arch_only="$val" ;;
+		check-cpp-if:)		check_cpp_if="$val" ;;
 
 		check-description:)	;;	# ignore
 		check-note:)		;;	# ignore
@@ -301,6 +303,18 @@ do_test()
 			return 3
 		fi
 	fi
+	if [ "$check_cpp_if" != "" ]; then
+		res=$(../sparse -E - 2>/dev/null <<- EOF
+			#if !($check_cpp_if)
+			fail
+			#endif
+			EOF
+		)
+		if [ "$res" != "" ]; then
+			disable "$test_name" "$file"
+			return 3
+		fi
+	fi
 
 	if [ -z "$vquiet" ]; then
 		echo "  TEST    $test_name ($file)"
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 1/3] add testcase for restricted enum ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 1/3] add testcase for restricted enum
Date: Wed, 21 Feb 2018 22:39:06 +0000
Message-ID: <20180221223908.38904-2-luc.vanoostenryck () gmail ! com>
--------------------
Asked-by: Matthew Wilcox <willy@infradead.org>
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/enum-restricted.c | 22 ++++++++++++++++++++++
 1 file changed, 22 insertions(+)
 create mode 100644 validation/enum-restricted.c

diff --git a/validation/enum-restricted.c b/validation/enum-restricted.c
new file mode 100644
index 000000000..b58964172
--- /dev/null
+++ b/validation/enum-restricted.c
@@ -0,0 +1,22 @@
+#ifdef __CHECKER__
+#define __bitwise	__attribute__((bitwise))
+#define __force		__attribute__((force))
+#else
+#define __bitwise
+#define __force
+#endif
+
+typedef enum __bitwise foobar {
+        FOO = 0,
+        BAR = 1,
+} fb_t;
+
+static void foo(void)
+{
+	fb_t v = BAR;
+}
+
+/*
+ * check-name: enum-restricted
+ * check-known-to-fail
+ */
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 1/3] fix show typename of enums ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 1/3] fix show typename of enums
Date: Wed, 18 Apr 2018 15:39:57 +0000
Message-ID: <20180418153959.33271-2-luc.vanoostenryck () gmail ! com>
--------------------
Currently when displaying the typename of an enum this
typename is first prefixed with the enum base type (most
often 'int'). So displaying the type for 'enum num' will
give 'int enum num'. This is a bit weird, we expect to have
just the type which is 'enum num'.

Change this by stopping to display the base type.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 show-parse.c               | 2 +-
 validation/enum-mismatch.c | 4 ++--
 2 files changed, 3 insertions(+), 3 deletions(-)

diff --git a/show-parse.c b/show-parse.c
index 72d3f3854..ccd60a26e 100644
--- a/show-parse.c
+++ b/show-parse.c
@@ -342,7 +342,7 @@ deeper:
 
 	case SYM_ENUM:
 		prepend(name, "enum %s ", show_ident(sym->ident));
-		break;
+		goto out;
 
 	case SYM_NODE:
 		append(name, "%s", show_ident(sym->ident));
diff --git a/validation/enum-mismatch.c b/validation/enum-mismatch.c
index 9a929d24c..9d61f5816 100644
--- a/validation/enum-mismatch.c
+++ b/validation/enum-mismatch.c
@@ -13,7 +13,7 @@ static enum eb foo(enum ea a)
  *
  * check-error-start
 enum-mismatch.c:7:16: warning: mixing different enum types
-enum-mismatch.c:7:16:     int enum ea  versus
-enum-mismatch.c:7:16:     int enum eb 
+enum-mismatch.c:7:16:     enum ea  versus
+enum-mismatch.c:7:16:     enum eb 
  * check-error-end
  */
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 1/3] initial parsing of __attribute__((format)) ===

From: Ben Dooks <ben.dooks () codethink ! co ! uk>
To: linux-sparse
Subject: [PATCH 1/3] initial parsing of __attribute__((format))
Date: Fri, 26 Oct 2018 15:26:30 +0000
Message-ID: <20181026152632.30318-2-ben.dooks () codethink ! co ! uk>
--------------------
---
 parse.c  | 75 +++++++++++++++++++++++++++++++++++++++++++++++++++++++-
 symbol.h |  2 ++
 2 files changed, 76 insertions(+), 1 deletion(-)

diff --git a/parse.c b/parse.c
index 02a55a7..bb0545c 100644
--- a/parse.c
+++ b/parse.c
@@ -84,7 +84,7 @@ static attr_t
 	attribute_address_space, attribute_context,
 	attribute_designated_init,
 	attribute_transparent_union, ignore_attribute,
-	attribute_mode, attribute_force;
+	attribute_mode, attribute_force, attribute_format;
 
 typedef struct symbol *to_mode_t(struct symbol *);
 
@@ -353,6 +353,10 @@ static struct symbol_op attr_force_op = {
 	.attribute = attribute_force,
 };
 
+static struct symbol_op attr_format = {
+	.attribute = attribute_format,
+};
+
 static struct symbol_op address_space_op = {
 	.attribute = attribute_address_space,
 };
@@ -407,6 +411,10 @@ static struct symbol_op mode_word_op = {
 	.to_mode = to_word_mode
 };
 
+static struct symbol_op attr_printf_op = {
+	.type	= KW_FORMAT,
+};
+
 /* Using NS_TYPEDEF will also make the keyword a reserved one */
 static struct init_keyword {
 	const char *name;
@@ -513,6 +521,8 @@ static struct init_keyword {
 	{ "bitwise",	NS_KEYWORD,	MOD_BITWISE,	.op = &attr_bitwise_op },
 	{ "__bitwise__",NS_KEYWORD,	MOD_BITWISE,	.op = &attr_bitwise_op },
 	{ "address_space",NS_KEYWORD,	.op = &address_space_op },
+	{ "format",	NS_KEYWORD,	.op = &attr_format },
+	{ "printf",	NS_KEYWORD,	.op = &attr_printf_op },
 	{ "mode",	NS_KEYWORD,	.op = &mode_op },
 	{ "context",	NS_KEYWORD,	.op = &context_op },
 	{ "designated_init",	NS_KEYWORD,	.op = &designated_init_op },
@@ -1051,6 +1061,69 @@ static struct token *attribute_address_space(struct token *token, struct symbol
 	return token;
 }
 
+static struct token *attribute_format(struct token *token, struct symbol *attr, struct decl_state *ctx)
+{
+	struct expression *args[3];
+	struct symbol *fmt_sym = NULL;
+	int argc = 0;
+
+	/* expecting format ( type, start, va_args at) */
+
+	token = expect(token, '(', "after format attribute");
+	while (!match_op(token, ')')) {
+		struct expression *expr = NULL;
+
+		if (argc == 0) {
+			if (token_type(token) == TOKEN_IDENT)
+				fmt_sym = lookup_keyword(token->ident, NS_KEYWORD);
+
+			if (!fmt_sym || !fmt_sym->op ||
+			    fmt_sym->op != &attr_printf_op) {
+				sparse_error(token->pos,
+					     "unknown format type '%s'\n",
+					     show_ident(token->ident));
+				fmt_sym = NULL;
+			}
+		}
+
+		token = conditional_expression(token, &expr);
+		if (!expr)
+			break;
+		if (argc < 3)
+			args[argc++] = expr;
+		if (!match_op(token, ','))
+			break;
+		token = token->next;
+	}
+
+	if (argc != 3 || !fmt_sym)
+		warning(token->pos,
+			"expected format type and start/position values");
+	else {
+		struct symbol *base_type = ctx->ctype.base_type;
+		long long start, at;
+
+		start = get_expression_value(args[2]);
+		at = get_expression_value(args[1]);
+
+		if (start <= 0 || at <= 0)
+			warning(token->pos, "bad format positions");
+		else if (!base_type)
+			sparse_error(token->pos, "no base context for format");
+		else if (base_type->type != SYM_FN)
+			warning(token->pos, "attribute format can only be used on functions");
+		else if (!base_type->variadic)
+			warning(token->pos, "attribute format used on non variadic function");
+		else {
+			base_type->printf_va_start = start;
+			base_type->printf_msg = at;
+		}
+	}
+
+	token = expect(token, ')', "after format attribute");
+	return token;
+}
+
 static struct symbol *to_QI_mode(struct symbol *ctype)
 {
 	if (ctype->ctype.base_type != &int_type)
diff --git a/symbol.h b/symbol.h
index 3274496..5f1b85d 100644
--- a/symbol.h
+++ b/symbol.h
@@ -86,6 +86,7 @@ enum keyword {
 	KW_SHORT	= 1 << 7,
 	KW_LONG		= 1 << 8,
 	KW_EXACT	= 1 << 9,
+	KW_FORMAT	= 1 << 10,
 };
 
 struct context {
@@ -185,6 +186,7 @@ struct symbol {
 			struct entrypoint *ep;
 			long long value;		/* Initial value */
 			struct symbol *definition;
+			long long	printf_va_start, printf_msg;
 		};
 	};
 	union /* backend */ {
-- 
2.19.1

================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH 1/3] initial parsing of __attribute__((format))
Date: Fri, 26 Oct 2018 21:31:11 +0000
Message-ID: <20181026213110.vbwyzu7a7songnbp () ltop ! local>
--------------------
On Fri, Oct 26, 2018 at 04:26:30PM +0100, Ben Dooks wrote:
> ---
>  parse.c  | 75 +++++++++++++++++++++++++++++++++++++++++++++++++++++++-
>  symbol.h |  2 ++
>  2 files changed, 76 insertions(+), 1 deletion(-)
> 
> diff --git a/parse.c b/parse.c
> index 02a55a7..bb0545c 100644
> --- a/parse.c
> +++ b/parse.c
> @@ -1051,6 +1061,69 @@ static struct token *attribute_address_space(struct token *token, struct symbol
>  	return token;
>  }
>  
> +static struct token *attribute_format(struct token *token, struct symbol *attr, struct decl_state *ctx)
> +{
> +	struct expression *args[3];
> +	struct symbol *fmt_sym = NULL;
> +	int argc = 0;
> +
> +	/* expecting format ( type, start, va_args at) */
> +
> +	token = expect(token, '(', "after format attribute");
> +	while (!match_op(token, ')')) {
> +		struct expression *expr = NULL;
> +
> +		if (argc == 0) {
> +			if (token_type(token) == TOKEN_IDENT)
> +				fmt_sym = lookup_keyword(token->ident, NS_KEYWORD);
> +
> +			if (!fmt_sym || !fmt_sym->op ||
> +			    fmt_sym->op != &attr_printf_op) {

Better to check for op->type == KW_FORMAT so it can later be easily extended.

> +				sparse_error(token->pos,
> +					     "unknown format type '%s'\n",
> +					     show_ident(token->ident));
> +				fmt_sym = NULL;
> +			}
> +		}
> +
> +		token = conditional_expression(token, &expr);
> +		if (!expr)
> +			break;
> +		if (argc < 3)
> +			args[argc++] = expr;
> +		if (!match_op(token, ','))
> +			break;
> +		token = token->next;
> +	}
> +
> +	if (argc != 3 || !fmt_sym)
> +		warning(token->pos,
> +			"expected format type and start/position values");
> +	else {
> +		struct symbol *base_type = ctx->ctype.base_type;
> +		long long start, at;
> +
> +		start = get_expression_value(args[2]);
> +		at = get_expression_value(args[1]);
> +
> +		if (start <= 0 || at <= 0)
> +			warning(token->pos, "bad format positions");
> +		else if (!base_type)
> +			sparse_error(token->pos, "no base context for format");
> +		else if (base_type->type != SYM_FN)
> +			warning(token->pos, "attribute format can only be used on functions");
> +		else if (!base_type->variadic)
> +			warning(token->pos, "attribute format used on non variadic function");
> +		else {
> +			base_type->printf_va_start = start;
> +			base_type->printf_msg = at;
> +		}

Just a stylistic nit and as such not really imprtant but I would prefer
in the next version that chained ifs either:
* all of them take only a single line and then doesn't take braces
* otherwise that they all take the braces
This is the same as done in the kernel.

> +	}
> +
> +	token = expect(token, ')', "after format attribute");
> +	return token;
> +}
> +
>  static struct symbol *to_QI_mode(struct symbol *ctype)
>  {
>  	if (ctype->ctype.base_type != &int_type)
> diff --git a/symbol.h b/symbol.h
> index 3274496..5f1b85d 100644
> --- a/symbol.h
> +++ b/symbol.h
> @@ -86,6 +86,7 @@ enum keyword {
>  	KW_SHORT	= 1 << 7,
>  	KW_LONG		= 1 << 8,
>  	KW_EXACT	= 1 << 9,
> +	KW_FORMAT	= 1 << 10,
>  };

If not using KW_FORMAT in the test here above, it's not really used and needed..

>  struct context {
> @@ -185,6 +186,7 @@ struct symbol {
>  			struct entrypoint *ep;
>  			long long value;		/* Initial value */
>  			struct symbol *definition;
> +			long long	printf_va_start, printf_msg;

This is annoying. This struct is the most used one and is already
quite large and thus uses a significant amount of the allocated memory
which represent a non-negligeable amount of the processing time.
There is several options:
* a long long is certainly not needed for these, 32 bits 16 or even 8
  should be enough
* they could be 'unionized' with some of the other fields
* it could be moved elsewhere ...
More fundamentally these two fields should not belong to struct symbol
but to struct ctype (for example next to 'as' make thme each 8 bit witdh).

Thanks,
-- Luc
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH 1/3] initial parsing of __attribute__((format))
Date: Fri, 26 Oct 2018 23:13:34 +0000
Message-ID: <20181026231333.jev4vrah6kieu34m () ltop ! local>
--------------------
On Fri, Oct 26, 2018 at 04:26:30PM +0100, Ben Dooks wrote:
> ---
>  parse.c  | 75 +++++++++++++++++++++++++++++++++++++++++++++++++++++++-
>  symbol.h |  2 ++
>  2 files changed, 76 insertions(+), 1 deletion(-)
> 
> @@ -1051,6 +1061,69 @@ static struct token *attribute_address_space(struct token *token, struct symbol
>  	return token;
>  }
>  
> +static struct token *attribute_format(struct token *token, struct symbol *attr, struct decl_state *ctx)
> +{
> +	struct expression *args[3];
> +	struct symbol *fmt_sym = NULL;
> +	int argc = 0;
> +
> +	/* expecting format ( type, start, va_args at) */
> +
> +	token = expect(token, '(', "after format attribute");
> +	while (!match_op(token, ')')) {
> +		struct expression *expr = NULL;
> +
> +		if (argc == 0) {
> +			if (token_type(token) == TOKEN_IDENT)
> +				fmt_sym = lookup_keyword(token->ident, NS_KEYWORD);
> +
> +			if (!fmt_sym || !fmt_sym->op ||
> +			    fmt_sym->op != &attr_printf_op) {
> +				sparse_error(token->pos,
> +					     "unknown format type '%s'\n",
> +					     show_ident(token->ident));
> +				fmt_sym = NULL;
> +			}
> +		}
> +
> +		token = conditional_expression(token, &expr);
> +		if (!expr)
> +			break;
> +		if (argc < 3)
> +			args[argc++] = expr;
> +		if (!match_op(token, ','))
> +			break;
> +		token = token->next;
> +	}
> +
> +	if (argc != 3 || !fmt_sym)
> +		warning(token->pos,
> +			"expected format type and start/position values");
> +	else {
> +		struct symbol *base_type = ctx->ctype.base_type;
> +		long long start, at;
> +
> +		start = get_expression_value(args[2]);
> +		at = get_expression_value(args[1]);
> +
> +		if (start <= 0 || at <= 0)
> +			warning(token->pos, "bad format positions");
> +		else if (!base_type)
> +			sparse_error(token->pos, "no base context for format");
> +		else if (base_type->type != SYM_FN)
> +			warning(token->pos, "attribute format can only be used on functions");
> +		else if (!base_type->variadic)
> +			warning(token->pos, "attribute format used on non variadic function");
> +		else {
> +			base_type->printf_va_start = start;
> +			base_type->printf_msg = at;
> +		}

Another reason why printf_va_start & printf_msg need to move (to ctx->ctype) is
that the attribute can be placed before the function. For example:
	extern __attribute__((format(printf, 1, 2))) void printk(const char *, ...);
is legit but gives here the error "no base context for format".

-- Luc
================================================================================


################################################################################

=== Thread: [PATCH 1/4] initial parsing of __attribute__((format)) ===

From: Ben Dooks <ben.dooks () codethink ! co ! uk>
To: linux-sparse
Subject: Re: [PATCH 1/4] initial parsing of __attribute__((format))
Date: Fri, 26 Oct 2018 15:23:00 +0000
Message-ID: <74b9e686-1752-2d21-b3b0-bb695348ca87 () codethink ! co ! uk>
--------------------
Apologies, this escaped before I realised the rebase I was working on
wasn't finished. Will re-send with cover note.

-- 
Ben Dooks				http://www.codethink.co.uk/
Senior Engineer				Codethink - Providing Genius

https://www.codethink.co.uk/privacy.html
================================================================================


################################################################################

=== Thread: [PATCH 1/5] add copy_ptr_list() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 1/5] add copy_ptr_list()
Date: Tue, 24 Jul 2018 20:34:54 +0000
Message-ID: <20180724203458.46429-2-luc.vanoostenryck () gmail ! com>
--------------------
When an instruction can be replaced by a pseudo, the user list of
this pseudo and the instruction's target must be merged.

Currently this is done by concat_ptr_list() which copy the elements
of one list into the other using add_ptr_list(). This incurs quite
a bit overhead.

Add a new more efficient ptrlist function: copy_ptr_list() which
copy the element by block by looping over both list in parallel.

This gives a speedup up to 26% on some pathological workloads.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 flow.c    |  2 +-
 ptrlist.c | 46 ++++++++++++++++++++++++++++++++++++++++++++++
 ptrlist.h |  1 +
 3 files changed, 48 insertions(+), 1 deletion(-)

diff --git a/flow.c b/flow.c
index f928c2684..9483938fb 100644
--- a/flow.c
+++ b/flow.c
@@ -278,7 +278,7 @@ int simplify_flow(struct entrypoint *ep)
 
 static inline void concat_user_list(struct pseudo_user_list *src, struct pseudo_user_list **dst)
 {
-	concat_ptr_list((struct ptr_list *)src, (struct ptr_list **)dst);
+	copy_ptr_list((struct ptr_list **)dst, (struct ptr_list *)src);
 }
 
 void convert_instruction_target(struct instruction *insn, pseudo_t src)
diff --git a/ptrlist.c b/ptrlist.c
index c7ebf5a3f..234433033 100644
--- a/ptrlist.c
+++ b/ptrlist.c
@@ -340,6 +340,52 @@ void concat_ptr_list(struct ptr_list *a, struct ptr_list **b)
 	} END_FOR_EACH_PTR(entry);
 }
 
+///
+// copy the elements of a list at the end of another list.
+// @listp: a pointer to the destination list.
+// @src: the head of the source list.
+void copy_ptr_list(struct ptr_list **listp, struct ptr_list *src)
+{
+	struct ptr_list *head, *tail;
+	struct ptr_list *cur = src;
+	int idx;
+
+	if (!src)
+		return;
+	if (!(head = *listp))
+		return (void) (*listp = src);
+
+	tail = head->prev;
+	idx = tail->nr;
+	do {
+		struct ptr_list *next;
+		int nr = cur->nr;
+		int i;
+		for (i = 0; i < nr;) {
+			void *ptr = cur->list[i++];
+			if (!ptr)
+				continue;
+			if (idx >= LIST_NODE_NR) {
+				struct ptr_list *next = __alloc_ptrlist(0);
+				tail->next = next;
+				next->prev = tail;
+				tail->nr = idx;
+				idx = 0;
+				tail = next;
+			}
+			tail->list[idx++] = ptr;
+		}
+
+		next = cur->next;
+		__free_ptrlist(cur);
+		cur = next;
+	} while (cur != src);
+
+	tail->nr = idx;
+	head->prev = tail;
+	tail->next = head;
+}
+
 ///
 // free a ptrlist
 // @listp: a pointer to the list
diff --git a/ptrlist.h b/ptrlist.h
index e97cdda31..46a9baee2 100644
--- a/ptrlist.h
+++ b/ptrlist.h
@@ -35,6 +35,7 @@ int replace_ptr_list_entry(struct ptr_list **, void *old, void *new, int);
 extern void sort_list(struct ptr_list **, int (*)(const void *, const void *));
 
 extern void concat_ptr_list(struct ptr_list *a, struct ptr_list **b);
+extern void copy_ptr_list(struct ptr_list **h, struct ptr_list *t);
 extern int ptr_list_size(struct ptr_list *);
 extern int linearize_ptr_list(struct ptr_list *, void **, int);
 extern void *first_ptr_list(struct ptr_list *);
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Ramsay Jones <ramsay () ramsayjones ! plus ! com>
To: linux-sparse
Subject: Re: [PATCH 1/5] add copy_ptr_list()
Date: Tue, 24 Jul 2018 22:56:01 +0000
Message-ID: <66b59292-8cbf-eaef-df1b-c406a213784d () ramsayjones ! plus ! com>
--------------------


On 24/07/18 21:34, Luc Van Oostenryck wrote:
> When an instruction can be replaced by a pseudo, the user list of
> this pseudo and the instruction's target must be merged.
> 
> Currently this is done by concat_ptr_list() which copy the elements
> of one list into the other using add_ptr_list(). This incurs quite
> a bit overhead.
> 
> Add a new more efficient ptrlist function: copy_ptr_list() which
> copy the element by block by looping over both list in parallel.
> 
> This gives a speedup up to 26% on some pathological workloads.
> 
> Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> ---
>  flow.c    |  2 +-
>  ptrlist.c | 46 ++++++++++++++++++++++++++++++++++++++++++++++
>  ptrlist.h |  1 +
>  3 files changed, 48 insertions(+), 1 deletion(-)
> 
> diff --git a/flow.c b/flow.c
> index f928c2684..9483938fb 100644
> --- a/flow.c
> +++ b/flow.c
> @@ -278,7 +278,7 @@ int simplify_flow(struct entrypoint *ep)
>  
>  static inline void concat_user_list(struct pseudo_user_list *src, struct pseudo_user_list **dst)
>  {
> -	concat_ptr_list((struct ptr_list *)src, (struct ptr_list **)dst);
> +	copy_ptr_list((struct ptr_list **)dst, (struct ptr_list *)src);
>  }
>  
>  void convert_instruction_target(struct instruction *insn, pseudo_t src)
> diff --git a/ptrlist.c b/ptrlist.c
> index c7ebf5a3f..234433033 100644
> --- a/ptrlist.c
> +++ b/ptrlist.c
> @@ -340,6 +340,52 @@ void concat_ptr_list(struct ptr_list *a, struct ptr_list **b)
>  	} END_FOR_EACH_PTR(entry);
>  }
>  
> +///
> +// copy the elements of a list at the end of another list.
> +// @listp: a pointer to the destination list.
> +// @src: the head of the source list.
> +void copy_ptr_list(struct ptr_list **listp, struct ptr_list *src)
> +{
> +	struct ptr_list *head, *tail;
> +	struct ptr_list *cur = src;
> +	int idx;
> +
> +	if (!src)
> +		return;
> +	if (!(head = *listp))
> +		return (void) (*listp = src);

cute, but I don't like this. I would prefer:

	head = *listp;
	if (!head) {
		*listp = src;
		return;
	}

... but it's your choice. :-)

> +
> +	tail = head->prev;
> +	idx = tail->nr;
> +	do {
> +		struct ptr_list *next;

first 'next' (which could be initialised with cur->next).

> +		int nr = cur->nr;
> +		int i;
> +		for (i = 0; i < nr;) {
> +			void *ptr = cur->list[i++];
> +			if (!ptr)
> +				continue;
> +			if (idx >= LIST_NODE_NR) {
> +				struct ptr_list *next = __alloc_ptrlist(0);

second 'next', which is actually the 'new_tail' (or if you
want to be short and cute: 'newt'!)

> +				tail->next = next;
> +				next->prev = tail;
> +				tail->nr = idx;
> +				idx = 0;
> +				tail = next;

rename all 'next' as above.

> +			}
> +			tail->list[idx++] = ptr;
> +		}
> +
> +		next = cur->next;

possibly remove, see above.

> +		__free_ptrlist(cur);> +		cur = next;
> +	} while (cur != src);

Hmm, hasn't 'src' been freed? (via __free_ptrlist(cur)).

Maybe free the entire list after the loop, with
__free_ptr_list(&src)? (or should that use the macro wrapper
'free_ptr_list' instead?).

ATB,
Ramsay Jones

> +
> +	tail->nr = idx;
> +	head->prev = tail;
> +	tail->next = head;
> +}
> +
>  ///
>  // free a ptrlist
>  // @listp: a pointer to the list
> diff --git a/ptrlist.h b/ptrlist.h
> index e97cdda31..46a9baee2 100644
> --- a/ptrlist.h
> +++ b/ptrlist.h
> @@ -35,6 +35,7 @@ int replace_ptr_list_entry(struct ptr_list **, void *old, void *new, int);
>  extern void sort_list(struct ptr_list **, int (*)(const void *, const void *));
>  
>  extern void concat_ptr_list(struct ptr_list *a, struct ptr_list **b);
> +extern void copy_ptr_list(struct ptr_list **h, struct ptr_list *t);
>  extern int ptr_list_size(struct ptr_list *);
>  extern int linearize_ptr_list(struct ptr_list *, void **, int);
>  extern void *first_ptr_list(struct ptr_list *);
> 
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 1/5] testcase for SET{EQ,NE}([SZ]EXT(x, N),{0,1})'s simplification ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 1/5] testcase for SET{EQ,NE}([SZ]EXT(x, N),{0,1})'s simplification
Date: Wed, 25 Jul 2018 20:38:24 +0000
Message-ID: <20180725203828.91410-2-luc.vanoostenryck () gmail ! com>
--------------------
Add some basic testcase for these relatively common
simplification opportunities.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/optim/bool-sext-test.c | 13 +++++++++++++
 validation/optim/bool-zext-test.c | 13 +++++++++++++
 2 files changed, 26 insertions(+)
 create mode 100644 validation/optim/bool-sext-test.c
 create mode 100644 validation/optim/bool-zext-test.c

diff --git a/validation/optim/bool-sext-test.c b/validation/optim/bool-sext-test.c
new file mode 100644
index 000000000..0ca3dea9a
--- /dev/null
+++ b/validation/optim/bool-sext-test.c
@@ -0,0 +1,13 @@
+_Bool eqs0(  signed char a) { return a == 0; }
+_Bool eqs1(  signed char a) { return a == 1; }
+_Bool nes0(  signed char a) { return a != 0; }
+_Bool nes1(  signed char a) { return a != 1; }
+
+/*
+ * check-name: bool-sext-test
+ * check-command: test-linearize -Wno-decl $file
+ * check-known-to-fail
+ *
+ * check-output-ignore
+ * check-output-excludes: sext\\.
+ */
diff --git a/validation/optim/bool-zext-test.c b/validation/optim/bool-zext-test.c
new file mode 100644
index 000000000..f837ace20
--- /dev/null
+++ b/validation/optim/bool-zext-test.c
@@ -0,0 +1,13 @@
+_Bool equ0(unsigned char a) { return a == 0; }
+_Bool equ1(unsigned char a) { return a == 1; }
+_Bool neu0(unsigned char a) { return a != 0; }
+_Bool neu1(unsigned char a) { return a != 1; }
+
+/*
+ * check-name: bool-zext-test
+ * check-command: test-linearize -Wno-decl $file
+ * check-known-to-fail
+ *
+ * check-output-ignore
+ * check-output-excludes: zext\\.
+ */
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 1/5] tokenize: check show_string() for NULL pointer ===

From: Ben Dooks <ben.dooks () codethink ! co ! uk>
To: linux-sparse
Subject: [PATCH 1/5] tokenize: check show_string() for NULL pointer
Date: Mon, 29 Oct 2018 15:39:48 +0000
Message-ID: <20181029153952.13927-2-ben.dooks () codethink ! co ! uk>
--------------------
Fix issue where show_string() being passed a NULL pointer by accident.
This only happened during debugging, but would be a useful addition to
the checks in this function.

Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
---
 tokenize.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/tokenize.c b/tokenize.c
index 99b9580..c32f8c7 100644
--- a/tokenize.c
+++ b/tokenize.c
@@ -124,7 +124,7 @@ const char *show_string(const struct string *string)
 	char *ptr;
 	int i;
 
-	if (!string->length)
+	if (!string || !string->length)
 		return "<bad_string>";
 	ptr = buffer;
 	*ptr++ = '"';
-- 
2.19.1

================================================================================


################################################################################

=== Thread: [PATCH 1/8] move DEF_OPCODE() to header file ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 1/8] move DEF_OPCODE() to header file
Date: Thu, 30 Aug 2018 22:26:09 +0000
Message-ID: <20180830222616.47360-2-luc.vanoostenryck () gmail ! com>
--------------------
as it will be needed before it was defined in simplify.c and
can be useful in other files too.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 linearize.h | 8 ++++++++
 simplify.c  | 7 -------
 2 files changed, 8 insertions(+), 7 deletions(-)

diff --git a/linearize.h b/linearize.h
index 413bf0132..66ca37667 100644
--- a/linearize.h
+++ b/linearize.h
@@ -264,6 +264,14 @@ enum opcode {
 	OP_LAST,			/* keep this one last! */
 };
 
+//
+// return the opcode of the instruction defining ``SRC`` if existing
+// and OP_BADOP if not. It also assigns the defining instruction
+// to ``DEF``.
+#define DEF_OPCODE(DEF, SRC)	\
+	(((SRC)->type == PSEUDO_REG && (DEF = (SRC)->def)) ? DEF->opcode : OP_BADOP)
+
+
 struct basic_block_list;
 struct instruction_list;
 
diff --git a/simplify.c b/simplify.c
index 52910876c..e9995a546 100644
--- a/simplify.c
+++ b/simplify.c
@@ -417,13 +417,6 @@ static inline int def_opcode(pseudo_t p)
 	return p->def->opcode;
 }
 
-//
-// return the opcode of the instruction defining ``SRC`` if existing
-// and OP_BADOP if not. It also assigns the defining instruction
-// to ``DEF``.
-#define DEF_OPCODE(DEF, SRC)	\
-	(((SRC)->type == PSEUDO_REG && (DEF = (SRC)->def)) ? DEF->opcode : OP_BADOP)
-
 static unsigned int value_size(long long value)
 {
 	value >>= 8;
-- 
2.18.0

================================================================================


################################################################################

=== Thread: [PATCH 10/10] bool: remove OP_{AND,OR}_BOOL instructions ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 10/10] bool: remove OP_{AND,OR}_BOOL instructions
Date: Tue, 26 Jun 2018 06:00:02 +0000
Message-ID: <20180626060002.35753-11-luc.vanoostenryck () gmail ! com>
--------------------
Now that these instructions are not generated anymore,
we can remove all related code, defines and doc.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/IR.rst |  8 --------
 cse.c                |  4 +---
 example.c            |  3 ---
 linearize.c          |  2 --
 linearize.h          |  4 +---
 simplify.c           | 23 -----------------------
 sparse-llvm.c        | 24 ------------------------
 7 files changed, 2 insertions(+), 66 deletions(-)

diff --git a/Documentation/IR.rst b/Documentation/IR.rst
index ff0ccdf24..0419ac411 100644
--- a/Documentation/IR.rst
+++ b/Documentation/IR.rst
@@ -125,14 +125,6 @@ They all follow the same signature:
 .. op:: OP_XOR
 	Logical XOR
 
-Boolean ops
------------
-.. op:: OP_AND_BOOL
-	Boolean AND
-
-.. op:: OP_OR_BOOL
-	Boolean OR
-
 Integer compares
 ----------------
 They all have the following signature:
diff --git a/cse.c b/cse.c
index 231b7e869..848ac5125 100644
--- a/cse.c
+++ b/cse.c
@@ -54,8 +54,7 @@ void cse_collect(struct instruction *insn)
 	case OP_AND: case OP_OR:
 
 	/* Binary logical */
-	case OP_XOR: case OP_AND_BOOL:
-	case OP_OR_BOOL:
+	case OP_XOR:
 
 	/* Binary comparison */
 	case OP_SET_EQ: case OP_SET_NE:
@@ -175,7 +174,6 @@ static int insn_compare(const void *_i1, const void *_i2)
 	/* commutative binop */
 	case OP_ADD:
 	case OP_MUL:
-	case OP_AND_BOOL: case OP_OR_BOOL:
 	case OP_AND: case OP_OR:
 	case OP_XOR:
 	case OP_SET_EQ: case OP_SET_NE:
diff --git a/example.c b/example.c
index 6b645211f..8a2b1ab46 100644
--- a/example.c
+++ b/example.c
@@ -43,8 +43,6 @@ static const char *opcodes[] = {
 	[OP_AND] = "and",
 	[OP_OR] = "or",
 	[OP_XOR] = "xor",
-	[OP_AND_BOOL] = "and-bool",
-	[OP_OR_BOOL] = "or-bool",
 
 	/* Binary comparison */
 	[OP_SET_EQ] = "seteq",
@@ -1402,7 +1400,6 @@ static void generate_one_insn(struct instruction *insn, struct bb_state *state)
 
 	case OP_ADD: case OP_MUL:
 	case OP_AND: case OP_OR: case OP_XOR:
-	case OP_AND_BOOL: case OP_OR_BOOL:
 		generate_commutative_binop(state, insn);
 		break;
 
diff --git a/linearize.c b/linearize.c
index bd9914507..194afe664 100644
--- a/linearize.c
+++ b/linearize.c
@@ -213,8 +213,6 @@ static const char *opcodes[] = {
 	[OP_AND] = "and",
 	[OP_OR] = "or",
 	[OP_XOR] = "xor",
-	[OP_AND_BOOL] = "and-bool",
-	[OP_OR_BOOL] = "or-bool",
 
 	/* Binary comparison */
 	[OP_SET_EQ] = "seteq",
diff --git a/linearize.h b/linearize.h
index 242cefb8f..092e1ac23 100644
--- a/linearize.h
+++ b/linearize.h
@@ -178,9 +178,7 @@ enum opcode {
 	OP_AND,
 	OP_OR,
 	OP_XOR,
-	OP_AND_BOOL,
-	OP_OR_BOOL,
-	OP_BINARY_END = OP_OR_BOOL,
+	OP_BINARY_END = OP_XOR,
 
 	/* floating-point comparison */
 	OP_FPCMP,
diff --git a/simplify.c b/simplify.c
index 89532646a..512df6310 100644
--- a/simplify.c
+++ b/simplify.c
@@ -489,12 +489,6 @@ static pseudo_t eval_insn(struct instruction *insn)
 	case OP_XOR:
 		res = left ^ right;
 		break;
-	case OP_AND_BOOL:
-		res = left && right;
-		break;
-	case OP_OR_BOOL:
-		res = left || right;
-		break;
 
 	/* Binary comparison */
 	case OP_SET_EQ:
@@ -644,11 +638,6 @@ static int simplify_constant_rightside(struct instruction *insn)
 	long long bits = sbit | (sbit - 1);
 
 	switch (insn->opcode) {
-	case OP_OR_BOOL:
-		if (value == 1)
-			return replace_with_pseudo(insn, insn->src2);
-		goto case_neutral_zero;
-
 	case OP_OR:
 		if ((value & bits) == bits)
 			return replace_with_pseudo(insn, insn->src2);
@@ -687,10 +676,6 @@ static int simplify_constant_rightside(struct instruction *insn)
 	case OP_MUL:
 		return simplify_mul_div(insn, value);
 
-	case OP_AND_BOOL:
-		if (value == 1)
-			return replace_with_pseudo(insn, insn->src1);
-	/* Fall through */
 	case OP_AND:
 		if (!value)
 			return replace_with_pseudo(insn, insn->src2);
@@ -760,13 +745,6 @@ static int simplify_binop_same_args(struct instruction *insn, pseudo_t arg)
 	case OP_OR:
 		return replace_with_pseudo(insn, arg);
 
-	case OP_AND_BOOL:
-	case OP_OR_BOOL:
-		remove_usage(arg, &insn->src2);
-		insn->src2 = value_pseudo(0);
-		insn->opcode = OP_SET_NE;
-		return REPEAT_CSE;
-
 	default:
 		break;
 	}
@@ -1204,7 +1182,6 @@ int simplify_instruction(struct instruction *insn)
 	switch (insn->opcode) {
 	case OP_ADD: case OP_MUL:
 	case OP_AND: case OP_OR: case OP_XOR:
-	case OP_AND_BOOL: case OP_OR_BOOL:
 		canonicalize_commutative(insn);
 		if (simplify_binop(insn))
 			return REPEAT_CSE;
diff --git a/sparse-llvm.c b/sparse-llvm.c
index 937f4490c..d28eb6e7a 100644
--- a/sparse-llvm.c
+++ b/sparse-llvm.c
@@ -601,30 +601,6 @@ static void output_op_binary(struct function *fn, struct instruction *insn)
 		assert(!is_float_type(insn->type));
 		target = LLVMBuildXor(fn->builder, lhs, rhs, target_name);
 		break;
-	case OP_AND_BOOL: {
-		LLVMValueRef lhs_nz, rhs_nz;
-		LLVMTypeRef dst_type;
-
-		lhs_nz = LLVMBuildIsNotNull(fn->builder, lhs, LLVMGetValueName(lhs));
-		rhs_nz = LLVMBuildIsNotNull(fn->builder, rhs, LLVMGetValueName(rhs));
-		target = LLVMBuildAnd(fn->builder, lhs_nz, rhs_nz, target_name);
-
-		dst_type = insn_symbol_type(insn);
-		target = LLVMBuildZExt(fn->builder, target, dst_type, target_name);
-		break;
-	}
-	case OP_OR_BOOL: {
-		LLVMValueRef lhs_nz, rhs_nz;
-		LLVMTypeRef dst_type;
-
-		lhs_nz = LLVMBuildIsNotNull(fn->builder, lhs, LLVMGetValueName(lhs));
-		rhs_nz = LLVMBuildIsNotNull(fn->builder, rhs, LLVMGetValueName(rhs));
-		target = LLVMBuildOr(fn->builder, lhs_nz, rhs_nz, target_name);
-
-		dst_type = insn_symbol_type(insn);
-		target = LLVMBuildZExt(fn->builder, target, dst_type, target_name);
-		break;
-	}
 	default:
 		assert(0);
 		break;
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 10/10] doc: fix weirdness with option lists ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 10/10] doc: fix weirdness with option lists
Date: Sat, 24 Feb 2018 01:47:17 +0000
Message-ID: <20180224014717.56552-11-luc.vanoostenryck () gmail ! com>
--------------------
sphinx considers an option '-fsomething' as the option'f' with
parameter 'something'. This is often the right interpretation
but here the output in the manpage is really too strange.

Fix this with a little bit of sed magic.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/Makefile | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/Documentation/Makefile b/Documentation/Makefile
index 2e8cbdfd7..3955892ef 100644
--- a/Documentation/Makefile
+++ b/Documentation/Makefile
@@ -21,6 +21,6 @@ $(targets): conf.py Makefile
 	@$(SPHINXBUILD) -M  $@  "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS)
 
 %.1: %.rst man
-	@mv build/man/$@ $@
+	@sed '/^\.BI \\-[a-zA-Z]\\fB /s/\\fB //' < build/man/$@ > $@
 
 .PHONY: Makefile	# avoid circular deps with the catch-all rule
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/2] avoid multiple error message after parsing error ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/2] avoid multiple error message after parsing error
Date: Sun, 27 May 2018 00:08:55 +0000
Message-ID: <20180527000855.19459-3-luc.vanoostenryck () gmail ! com>
--------------------
Parsing error triggered by expect() insert bad_token as the
current token. This allows to skip other errors with expect().
So far so good but this token is a fake token which has no
position set, if another parsing error is detected by something
else than expect(), the error message will use the position of
this bad token which will be displayed like:
	builtin:0:0: ...
which is confusing.

Since the concerned error message are secondary ones, the primary
one have been already be repported by expect(), the best is to
simply skip all these secondary error messages.

Do this by creating a new token type (TOKEN_BAD) and use it for
bad_token, then filter error messages based on this token type.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 lib.c   | 9 ++++++++-
 token.h | 1 +
 2 files changed, 9 insertions(+), 1 deletion(-)

diff --git a/lib.c b/lib.c
index a05d04744..c451a88ce 100644
--- a/lib.c
+++ b/lib.c
@@ -70,7 +70,7 @@ struct token *skip_to(struct token *token, int op)
 	return token;
 }
 
-static struct token bad_token;
+static struct token bad_token = { .pos.type = TOKEN_BAD };
 struct token *expect(struct token *token, int op, const char *where)
 {
 	if (!match_op(token, op)) {
@@ -123,6 +123,10 @@ static void do_warn(const char *type, struct position pos, const char * fmt, va_
 	static char buffer[512];
 	const char *name;
 
+	/* Shut up warnings if position is bad_token.pos */
+	if (pos.type == TOKEN_BAD)
+		return;
+
 	vsprintf(buffer, fmt, args);	
 	name = stream_name(pos.stream);
 		
@@ -150,6 +154,9 @@ static void do_error(struct position pos, const char * fmt, va_list args)
 	static int errors = 0;
         die_if_error = 1;
 	show_info = 1;
+	/* Shut up warnings if position is bad_token.pos */
+	if (pos.type == TOKEN_BAD)
+		return;
 	/* Shut up warnings after an error */
 	has_error |= ERROR_CURR_PHASE;
 	if (errors > 100) {
diff --git a/token.h b/token.h
index 847fdf4d0..292db167e 100644
--- a/token.h
+++ b/token.h
@@ -79,6 +79,7 @@ struct ident {
 
 enum token_type {
 	TOKEN_EOF,
+	TOKEN_BAD,
 	TOKEN_ERROR,
 	TOKEN_IDENT,
 	TOKEN_ZERO_IDENT,
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/2] doc: add sphinx domain for IR instruction indexation ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/2] doc: add sphinx domain for IR instruction indexation
Date: Mon, 21 May 2018 01:52:59 +0000
Message-ID: <20180521015259.57316-3-luc.vanoostenryck () gmail ! com>
--------------------
The doc for IR instructions used definition lists to structure
the instructions names and their description. This is simple
to write and present well but it doesn't put the instructions'
name in the index.

Fix this by adding a new domain, with a single directive 'op'
which takes the instruction name, inserts an index entry and uses
it for the definition.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/IR.rst        | 154 +++++++++++++++++++-----------------
 Documentation/conf.py       |   1 +
 Documentation/doc-guide.rst |   4 +
 Documentation/sphinx/ir.py  |  75 ++++++++++++++++++
 4 files changed, 160 insertions(+), 74 deletions(-)
 create mode 100755 Documentation/sphinx/ir.py

diff --git a/Documentation/IR.rst b/Documentation/IR.rst
index ec297eed0..67ef06a5d 100644
--- a/Documentation/IR.rst
+++ b/Documentation/IR.rst
@@ -1,3 +1,5 @@
+.. default-domain:: ir
+
 Sparse's Intermediate Representation
 ====================================
 
@@ -20,32 +22,32 @@ The common ones are:
 
 Terminators
 -----------
-OP_RET
+.. op:: OP_RET
 	Return from subroutine.
 
 	* .src : returned value (NULL if void)
 	* .type: type of .src
 
-OP_BR
+.. op:: OP_BR
 	Unconditional branch
 
 	* .bb_true: destination basic block
 
-OP_CBR
+.. op:: OP_CBR
 	Conditional branch
 
 	* .cond: condition
 	* .type: type of .cond, must be an integral type
 	* .bb_true, .bb_false: destination basic blocks
 
-OP_SWITCH
+.. op:: OP_SWITCH
 	Switch / multi-branch
 
 	* .cond: condition
 	* .type: type of .cond, must be an integral type
 	* .multijmp_list: pairs of case-value - destination basic block
 
-OP_COMPUTEDGOTO
+.. op:: OP_COMPUTEDGOTO
 	Computed goto / branch to register
 
 	* .src: address to branch to (void*)
@@ -58,34 +60,34 @@ They all follow the same signature:
 	* .target: result of the operation (must be an integral type)
 	* .type: type of .target
 
-OP_ADD
+.. op:: OP_ADD
 	Integer addition.
 
-OP_SUB
+.. op:: OP_SUB
 	Integer subtraction.
 
-OP_MUL
+.. op:: OP_MUL
 	Integer multiplication.
 
-OP_DIVU
+.. op:: OP_DIVU
 	Integer unsigned division.
 
-OP_DIVS
+.. op:: OP_DIVS
 	Integer signed division.
 
-OP_MODU
+.. op:: OP_MODU
 	Integer unsigned remainder.
 
-OP_MODS
+.. op:: OP_MODS
 	Integer signed remainder.
 
-OP_SHL
+.. op:: OP_SHL
 	Shift left (integer only)
 
-OP_LSR
+.. op:: OP_LSR
 	Logical Shift right (integer only)
 
-OP_ASR
+.. op:: OP_ASR
 	Arithmetic Shift right (integer only)
 
 Floating-point binops
@@ -95,16 +97,16 @@ They all follow the same signature:
 	* .target: result of the operation (must be a floating-point type)
 	* .type: type of .target
 
-OP_FADD
+.. op:: OP_FADD
 	Floating-point addition.
 
-OP_FSUB
+.. op:: OP_FSUB
 	Floating-point subtraction.
 
-OP_FMUL
+.. op:: OP_FMUL
 	Floating-point multiplication.
 
-OP_FDIV
+.. op:: OP_FDIV
 	Floating-point division.
 
 Logical ops
@@ -114,21 +116,21 @@ They all follow the same signature:
 	* .target: result of the operation
 	* .type: type of .target, must be an integral type
 
-OP_AND
+.. op:: OP_AND
 	Logical AND
 
-OP_OR
+.. op:: OP_OR
 	Logical OR
 
-OP_XOR
+.. op:: OP_XOR
 	Logical XOR
 
 Boolean ops
 -----------
-OP_AND_BOOL
+.. op:: OP_AND_BOOL
 	Boolean AND
 
-OP_OR_BOOL
+.. op:: OP_OR_BOOL
 	Boolean OR
 
 Integer compares
@@ -138,34 +140,34 @@ They all have the following signature:
 	* .target: result of the operation (0/1 valued integer)
 	* .type: type of .target, must be an integral type
 
-OP_SET_EQ
+.. op:: OP_SET_EQ
 	Compare equal.
 
-OP_SET_NE
+.. op:: OP_SET_NE
 	Compare not-equal.
 
-OP_SET_LE
+.. op:: OP_SET_LE
 	Compare less-than-or-equal (signed).
 
-OP_SET_GE
+.. op:: OP_SET_GE
 	Compare greater-than-or-equal (signed).
 
-OP_SET_LT
+.. op:: OP_SET_LT
 	Compare less-than (signed).
 
-OP_SET_GT
+.. op:: OP_SET_GT
 	Compare greater-than (signed).
 
-OP_SET_B
+.. op:: OP_SET_B
 	Compare less-than (unsigned).
 
-OP_SET_A
+.. op:: OP_SET_A
 	Compare greater-than (unsigned).
 
-OP_SET_BE
+.. op:: OP_SET_BE
 	Compare less-than-or-equal (unsigned).
 
-OP_SET_AE
+.. op:: OP_SET_AE
 	Compare greater-than-or-equal (unsigned).
 
 Floating-point compares
@@ -178,76 +180,76 @@ NaN and if it is the case the ordered compares return false
 and then unordered return true, otherwise the result of the
 comparison, now guaranteed to be done on non-NaNs, is returned.
 
-OP_FCMP_OEQ
+.. op:: OP_FCMP_OEQ
 	Floating-point compare ordered equal
 
-OP_FCMP_ONE
+.. op:: OP_FCMP_ONE
 	Floating-point compare ordered not-equal
 
-OP_FCMP_OLE
+.. op:: OP_FCMP_OLE
 	Floating-point compare ordered less-than-or-equal
 
-OP_FCMP_OGE
+.. op:: OP_FCMP_OGE
 	Floating-point compare ordered greater-or-equal
 
-OP_FCMP_OLT
+.. op:: OP_FCMP_OLT
 	Floating-point compare ordered less-than
 
-OP_FCMP_OGT
+.. op:: OP_FCMP_OGT
 	Floating-point compare ordered greater-than
 
 
-OP_FCMP_UEQ
+.. op:: OP_FCMP_UEQ
 	Floating-point compare unordered equal
 
-OP_FCMP_UNE
+.. op:: OP_FCMP_UNE
 	Floating-point compare unordered not-equal
 
-OP_FCMP_ULE
+.. op:: OP_FCMP_ULE
 	Floating-point compare unordered less-than-or-equal
 
-OP_FCMP_UGE
+.. op:: OP_FCMP_UGE
 	Floating-point compare unordered greater-or-equal
 
-OP_FCMP_ULT
+.. op:: OP_FCMP_ULT
 	Floating-point compare unordered less-than
 
-OP_FCMP_UGT
+.. op:: OP_FCMP_UGT
 	Floating-point compare unordered greater-than
 
 
-OP_FCMP_ORD
+.. op:: OP_FCMP_ORD
 	Floating-point compare ordered: return true if both operands are ordered
 	(none of the operands are a NaN) and false otherwise.
 
-OP_FCMP_UNO
+.. op:: OP_FCMP_UNO
 	Floating-point compare unordered: return false if no operands is ordered
 	and true otherwise.
 
 Unary ops
 ---------
-OP_NOT
+.. op:: OP_NOT
 	Logical not.
 
 	* .src: operand (type must be compatible with .target)
 	* .target: result of the operation
 	* .type: type of .target, must be an integral type
 
-OP_NEG
+.. op:: OP_NEG
 	Integer negation.
 
 	* .src: operand (type must be compatible with .target)
 	* .target: result of the operation (must be an integral type)
 	* .type: type of .target
 
-OP_FNEG
+.. op:: OP_FNEG
 	Floating-point negation.
 
 	* .src: operand (type must be compatible with .target)
 	* .target: result of the operation (must be a floating-point type)
 	* .type: type of .target
 
-OP_COPY
+.. op:: OP_COPY
 	Copy (only needed after out-of-SSA).
 
 	* .src: operand (type must be compatible with .target)
@@ -262,27 +264,27 @@ They all have the following signature:
 	* .target: result value
 	* .type: type of .target
 
-OP_CAST
+.. op:: OP_CAST
 	Cast to unsigned integer (and to void pointer).
 
-OP_SCAST
+.. op:: OP_SCAST
 	Cast to signed integer.
 
-OP_FPCAST
+.. op:: OP_FPCAST
 	Cast to floating-point.
 
-OP_PTRCAST
+.. op:: OP_PTRCAST
 	Cast to pointer.
 
 Ternary ops
 -----------
-OP_SEL
+.. op:: OP_SEL
 	* .src1: condition, must be of integral type
 	* .src2, .src3: operands (types must be compatible with .target)
 	* .target: result of the operation
 	* .type: type of .target
 
-OP_RANGE
+.. op:: OP_RANGE
 	Range/bounds checking (only used for an unused sparse extension).
 
 	* .src1: value to be checked
@@ -291,7 +293,7 @@ OP_RANGE
 
 Memory ops
 ----------
-OP_LOAD
+.. op:: OP_LOAD
 	Load.
 
 	* .src: base address to load from
@@ -299,7 +301,7 @@ OP_LOAD
 	* .target: loaded value
 	* .type: type of .target
 
-OP_STORE
+.. op:: OP_STORE
 	Store.
 
 	* .src: base address to store to
@@ -309,20 +311,20 @@ OP_STORE
 
 Others
 ------
-OP_SYMADDR
+.. op:: OP_SYMADDR
 	Create a pseudo corresponding to the address of a symbol.
 
 	* .symbol: (pseudo_t) input symbol (alias .src)
 	* .target: symbol's address
 
-OP_SETFVAL
+.. op:: OP_SETFVAL
 	Create a pseudo corresponding to a floating-point literal.
 
 	* .fvalue: the literal's value (long double)
 	* .target: the corresponding pseudo
 	* .type: type of the literal & .target
 
-OP_SETVAL
+.. op:: OP_SETVAL
 	Create a pseudo corresponding to a string literal or a label-as-value.
 	The value is given as an expression EXPR_STRING or EXPR_LABEL.
 
@@ -330,23 +332,24 @@ OP_SETVAL
 	* .target: the resulting value
 	* .type: type of .target, the value
 
-OP_PHI
+.. op:: OP_PHI
 	Phi-node (for SSA form).
 
 	* .phi_list: phi-operands (type must be compatible with .target)
 	* .target: "result"
 	* .type: type of .target
 
-OP_PHISOURCE
+.. op:: OP_PHISOURCE
 	Phi-node source.
 	Like OP_COPY but exclusively used to give a defining instructions
 	(and thus also a type) to *all* OP_PHI operands.
+
 	* .phi_src: operand (type must be compatible with .target, alias .src)
 	* .target: the "result" PSEUDO_PHI
 	* .type: type of .target
 	* .phi_users: list of phi instructions using the target pseudo
 
-OP_CALL
+.. op:: OP_CALL
 	Function call.
 
 	* .func: (pseudo_t) the function (can be a symbol or a "register",
@@ -358,15 +361,16 @@ OP_CALL
 	  entry is the full function type, the next ones are the type of
 	  each arguments
 
-OP_INLINED_CALL
+.. op:: OP_INLINED_CALL
 	Only used as an annotation to show that the instructions just above
 	correspond to a function that have been inlined.
+
 	* .func: (pseudo_t) the function (must be a symbol, alias .src))
 	* .arguments: list of pseudos that where the function's arguments
 	* .target: function return value (if any)
 	* .type: type of .target
 
-OP_SLICE
+.. op:: OP_SLICE
 	Extract a "slice" from an aggregate.
 
 	* .base: (pseudo_t) aggregate (alias .src)
@@ -374,7 +378,7 @@ OP_SLICE
 	* .target: result
 	* .type: type of .target
 
-OP_ASM
+.. op:: OP_ASM
 	Inlined assembly code.
 
 	* .string: asm template
@@ -382,7 +386,7 @@ OP_ASM
 
 Sparse tagging (line numbers, context, whatever)
 ------------------------------------------------
-OP_CONTEXT
+.. op:: OP_CONTEXT
 	Currently only used for lock/unlock tracking.
 
 	* .context_expr: unused
@@ -391,15 +395,17 @@ OP_CONTEXT
 
 Misc ops
 --------
-OP_ENTRY
+.. op:: OP_ENTRY
 	Function entry point (no associated semantic).
 
-OP_BADOP
+.. op:: OP_BADOP
 	Invalid operation (should never be generated).
 
-OP_NOP
+.. op:: OP_NOP
 	No-op (should never be generated).
 
-OP_DEATHNOTE
+.. op:: OP_DEATHNOTE
 	Annotation telling the pseudo will be death after the next
 	instruction (other than some other annotation, that is).
+
+.. # vim: tabstop=4
diff --git a/Documentation/conf.py b/Documentation/conf.py
index f7a680147..aae9d39bb 100644
--- a/Documentation/conf.py
+++ b/Documentation/conf.py
@@ -29,6 +29,7 @@ needs_sphinx = '1.3'
 sys.path.insert(0, os.path.abspath('sphinx'))
 extensions = [
 	'cdoc'
+	, 'ir'
 ]
 
 # support .md with python2 & python3
diff --git a/Documentation/doc-guide.rst b/Documentation/doc-guide.rst
index 80ec82c25..8133cb3a5 100644
--- a/Documentation/doc-guide.rst
+++ b/Documentation/doc-guide.rst
@@ -149,3 +149,7 @@ will be displayed like this:
 	It's strongly encouraged to use this
 	function instead of open coding a simple
 	``++``.
+
+Intermediate Representation
+---------------------------
+.. c:autodoc:: Documentation/sphinx/ir.py
diff --git a/Documentation/sphinx/ir.py b/Documentation/sphinx/ir.py
new file mode 100755
index 000000000..3028200a6
--- /dev/null
+++ b/Documentation/sphinx/ir.py
@@ -0,0 +1,75 @@
+#!/usr/bin/env python
+# SPDX_License-Identifier: MIT
+#
+# Copyright (C) 2018 Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
+#
+
+"""
+///
+// To document the instructions used in the intermediate representation
+// a new domain is defined: 'ir' with a directive::
+//
+//	.. op: <OP_NAME>
+//		<description of OP_NAME>
+//		...
+//
+// This is equivalent to using a definition list but with the name
+// also placed in the index (with 'IR instruction' as descriptions).
+
+"""
+
+import docutils
+import sphinx
+
+class IROpDirective(docutils.parsers.rst.Directive):
+
+	# use the first line of content as the argument, this allow
+	# to not have to write a blanck line after the directive
+	final_argument_whitespace = True
+	required_argument = 0
+	#optional_arguments = 0
+	has_content = True
+
+	objtype = None
+
+	def run(self):
+		self.env = self.state.document.settings.env
+
+		source = self.state.document
+		lineno = self.lineno
+		text = self.content
+		name = text[0]
+
+		node = docutils.nodes.section()
+		node['ids'].append(name)
+		node.document = source
+
+		index = '.. index:: pair: %s; IR instruction' % name
+		content = docutils.statemachine.ViewList()
+		content.append(index, source, lineno)
+		content.append(''   , source, lineno)
+		content.append(name , source, lineno)
+		content.append(''   , source, lineno)
+		self.state.nested_parse(content, self.content_offset, node)
+
+		defnode = docutils.nodes.definition()
+		self.state.nested_parse(text[1:], self.content_offset, defnode)
+		node.append(defnode)
+
+		return [node]
+
+class IRDomain(sphinx.domains.Domain):
+
+    """IR domain."""
+    name = 'ir'
+
+def setup(app):
+	app.add_domain(IRDomain)
+	app.add_directive_to_domain('ir', 'op', IROpDirective)
+
+	return {
+		'version': '1.0',
+		'parallel_read_safe': True,
+	}
+
+# vim: tabstop=4
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/2] fix dead dominator ===

From: Christopher Li <sparse () chrisli ! org>
To: linux-sparse
Subject: Re: [PATCH 2/2] fix dead dominator
Date: Sat, 24 Feb 2018 22:05:07 +0000
Message-ID: <CANeU7Qkc7OgE-wSLJjD-RR26U6RwTSjbbCkn5OZsAV4cznrZ_Q () mail ! gmail ! com>
--------------------
On Sat, Feb 10, 2018 at 5:29 AM, Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
> During simplify_one_symbol(), if possible, loads are replaced by
> an OP_PHI and the corresponding OP_PHISOURCE. To simplify things
> firther, If all the phisrcs correspond to an unique pseudo (often
> because there is only a single phisrc), then it's useless to
> create the OP_PHI: the created OP_PHISOURCEs can be removed and
> the initial load can be converted to the unique pseudo.
>
> However, if the unique pseudo was never used, the removal of
> the OP_PHISOURCEs, done *before* the load conversion, will
> kill the defining load (at this point the only user of the
> pseudo was the OP_PHISOURCEs) which will then erroneously make
> a VOID from the pseudo.
>
> Fix this by doing the load conversion before removing the
> unneeded OP_PHISOURCEs.

That make sense.

Thanks

Chris
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/2] label: avoid multiple definitions ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/2] label: avoid multiple definitions
Date: Sat, 26 May 2018 18:42:55 +0000
Message-ID: <20180526184255.53856-3-luc.vanoostenryck () gmail ! com>
--------------------
If a label is defined several times, an error is issued about it.
Nevertheless, the label is used as is and once the code is linearized
several BB are created for the same label and this create
inconsistencies. For example, some code will trigger assertion failures
in rewrite_parent_branch().

Avoid the consistencies by ignoring redefined labels.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 parse.c                      | 9 ++++++---
 validation/label-redefined.c | 1 -
 2 files changed, 6 insertions(+), 4 deletions(-)

diff --git a/parse.c b/parse.c
index d94c53a22..1f5fa3a41 100644
--- a/parse.c
+++ b/parse.c
@@ -2395,12 +2395,15 @@ static struct token *statement(struct token *token, struct statement **tree)
 
 		if (match_op(token->next, ':')) {
 			struct symbol *s = label_symbol(token);
+			token = skip_attributes(token->next->next);
+			if (s->stmt) {
+				sparse_error(stmt->pos, "label '%s' redefined", show_ident(s->ident));
+				// skip the label to avoid multiple definitions
+				return statement(token, tree);
+			}
 			stmt->type = STMT_LABEL;
 			stmt->label_identifier = s;
-			if (s->stmt)
-				sparse_error(stmt->pos, "label '%s' redefined", show_ident(token->ident));
 			s->stmt = stmt;
-			token = skip_attributes(token->next->next);
 			return statement(token, &stmt->label_statement);
 		}
 	}
diff --git a/validation/label-redefined.c b/validation/label-redefined.c
index 5e0a51b41..c98e815c1 100644
--- a/validation/label-redefined.c
+++ b/validation/label-redefined.c
@@ -10,7 +10,6 @@ l:
 
 /*
  * check-name: label-redefined
- * check-known-to-fail
  *
  * check-error-start
 label-redefined.c:7:1: error: label 'l' redefined
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/2] testcase: ignore -vcompound test on 32 bit archs ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/2] testcase: ignore -vcompound test on 32 bit archs
Date: Mon, 30 Apr 2018 16:10:05 +0000
Message-ID: <20180430161005.92480-3-luc.vanoostenryck () gmail ! com>
--------------------
The -vcompound flag is used to output the size & aligments of
compound types. Its correctness is thus dependent of the alignment
of the types used, like 'uint64_t' which alignment is expected
to be 8.

This is a problem because this type is only so aligned on 64 bit
archs and not on 32 ones, like i386 or arm.

Fix this by explicitly ignoring the test if the alignment is not met.
This solution avoids to make assumptions on the alignment of archs
by directly testing the root condition.

CC: Randy Dunlap <rdunlap@infradead.org>
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/compound-sizes.c | 1 +
 1 file changed, 1 insertion(+)

diff --git a/validation/compound-sizes.c b/validation/compound-sizes.c
index 3d36f3eda..d8ccf6052 100644
--- a/validation/compound-sizes.c
+++ b/validation/compound-sizes.c
@@ -76,6 +76,7 @@ int main(void)
 /*
  * check-name: compound-sizes
  * check-command: sparse -vcompound $file
+ * check-assert: _Alignof(long long) == 8
  *
  * check-error-start
 compound-sizes.c:39:17: union un static [toplevel] un: compound size 192, alignment 8
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/2] testsuite: add check-assert ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/2] testsuite: add check-assert
Date: Mon, 30 Apr 2018 13:21:32 +0000
Message-ID: <20180430132132.62055-3-luc.vanoostenryck () gmail ! com>
--------------------
Ideally, the testcases should be universal but it happen than
some of them need to test some specificities or are meaningless
or plainly wrong in some situations. In such cases, these tests
must but ignored.

Currently, the only the only mechanisms a test are:
1) ignoring the tests depending on a tool which cannot be compiled
   (like, for example, those using sparse-llvm when LLVM is not
   installed.
2) some rather corse criteria using the name of the arch used
   to run the tests.

Allow more flexibility by allowing to exclude some tests based on
the success or failure of an arbitrary condition via _Static_assert().

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/test-suite |  4 ++++
 validation/test-suite    | 12 ++++++++++++
 2 files changed, 16 insertions(+)

diff --git a/Documentation/test-suite b/Documentation/test-suite
index 626bc3fe7..bf4518a2b 100644
--- a/Documentation/test-suite
+++ b/Documentation/test-suite
@@ -27,6 +27,10 @@ check-arch-only: <arch[|...]>
 	Ignore the test if the current architecture (as returned by 'uname -m')
 	match or not one of the archs given in the pattern.
 
+check-assert: <condition>
+	Ignore the test if the given condition is false when evaluated as a
+	static assertion (_Static_assert).
+
 check-cpp-if: <condition>
 	Ignore the test if the given condition is false when evaluated
 	by sparse's pre-processor.
diff --git a/validation/test-suite b/validation/test-suite
index 6c6d34b28..04607a3e8 100755
--- a/validation/test-suite
+++ b/validation/test-suite
@@ -79,6 +79,7 @@ get_tag_value()
 	check_output_pattern=0
 	check_arch_ignore=""
 	check_arch_only=""
+	check_assert=""
 	check_cpp_if=""
 
 	lines=$(grep 'check-[a-z-]*' $1 | \
@@ -103,6 +104,7 @@ get_tag_value()
 					check_arch_ignore="$val" ;;
 		check-arch-only:)	arch=$(uname -m)
 					check_arch_only="$val" ;;
+		check-assert:)		check_assert="$val" ;;
 		check-cpp-if:)		check_cpp_if="$val" ;;
 
 		check-description:)	;;	# ignore
@@ -303,6 +305,16 @@ do_test()
 			return 3
 		fi
 	fi
+	if [ "$check_assert" != "" ]; then
+		res=$(../sparse - 2>&1 >/dev/null <<- EOF
+			_Static_assert($check_assert, "$check_assert");
+			EOF
+		)
+		if [ "$res" != "" ]; then
+			disable "$test_name" "$file"
+			return 3
+		fi
+	fi
 	if [ "$check_cpp_if" != "" ]; then
 		res=$(../sparse -E - 2>/dev/null <<- EOF
 			#if !($check_cpp_if)
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/3] add support for mode __pointer__ ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/3] add support for mode __pointer__
Date: Mon, 30 Jul 2018 16:34:39 +0000
Message-ID: <20180730163440.2303-3-luc.vanoostenryck () gmail ! com>
--------------------
sparse support GCC's modes like __SI__, __DI__, and __word__
but GCC also supports a mode __pointer__ which is used by Xen.

Add support for this missing __pointer__ mode.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 parse.c | 18 +++++++++++++++++-
 1 file changed, 17 insertions(+), 1 deletion(-)

diff --git a/parse.c b/parse.c
index 0cafb4e6c..845b2c167 100644
--- a/parse.c
+++ b/parse.c
@@ -91,7 +91,8 @@ static attr_t
 typedef struct symbol *to_mode_t(struct symbol *);
 
 static to_mode_t
-	to_QI_mode, to_HI_mode, to_SI_mode, to_DI_mode, to_TI_mode, to_word_mode;
+	to_QI_mode, to_HI_mode, to_SI_mode, to_DI_mode, to_TI_mode;
+static to_mode_t to_pointer_mode, to_word_mode;
 
 enum {
 	Set_T = 1,
@@ -410,6 +411,11 @@ static struct symbol_op mode_TI_op = {
 	.to_mode = to_TI_mode
 };
 
+static struct symbol_op mode_pointer_op = {
+	.type = KW_MODE,
+	.to_mode = to_pointer_mode
+};
+
 static struct symbol_op mode_word_op = {
 	.type = KW_MODE,
 	.to_mode = to_word_mode
@@ -548,6 +554,8 @@ static struct init_keyword {
 	{ "__DI__",	NS_KEYWORD,	.op = &mode_DI_op },
 	{ "TI",		NS_KEYWORD,	.op = &mode_TI_op },
 	{ "__TI__",	NS_KEYWORD,	.op = &mode_TI_op },
+	{ "pointer",	NS_KEYWORD,	.op = &mode_pointer_op },
+	{ "__pointer__",NS_KEYWORD,	.op = &mode_pointer_op },
 	{ "word",	NS_KEYWORD,	.op = &mode_word_op },
 	{ "__word__",	NS_KEYWORD,	.op = &mode_word_op },
 };
@@ -1105,6 +1113,14 @@ static struct symbol *to_TI_mode(struct symbol *ctype)
 						     : &slllong_ctype;
 }
 
+static struct symbol *to_pointer_mode(struct symbol *ctype)
+{
+	if (ctype->ctype.base_type != &int_type)
+		return NULL;
+	return ctype->ctype.modifiers & MOD_UNSIGNED ? uintptr_ctype
+						     :  intptr_ctype;
+}
+
 static struct symbol *to_word_mode(struct symbol *ctype)
 {
 	if (ctype->ctype.base_type != &int_type)
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/3] add testcase for enum / int type difference ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/3] add testcase for enum / int type difference
Date: Wed, 18 Apr 2018 15:39:58 +0000
Message-ID: <20180418153959.33271-3-luc.vanoostenryck () gmail ! com>
--------------------
Currently, type_difference() doesn't make a distinction between
enums & ints.

Add some testcases for this and mark the test as 'known-to-fail'.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/typediff-enum.c | 83 ++++++++++++++++++++++++++++++++++++++
 1 file changed, 83 insertions(+)
 create mode 100644 validation/typediff-enum.c

diff --git a/validation/typediff-enum.c b/validation/typediff-enum.c
new file mode 100644
index 000000000..4c97bcf6c
--- /dev/null
+++ b/validation/typediff-enum.c
@@ -0,0 +1,83 @@
+enum num { ZERO, ONE, MANY, };
+typedef enum num num;
+
+extern int v;
+num v = 0;
+
+extern num w;
+int w = 0;
+
+int foo(void);
+num foo(void) { return ZERO; }
+
+num bar(void);
+int bar(void) { return ZERO; }
+
+void baz(int a);
+void baz(num a) { }
+
+void qux(num a);
+void qux(int a) { }
+
+void pin(int *a);
+void pin(num *a) { }
+
+void pni(num *a);
+void pni(int *a) { }
+
+
+static num oprn(void) { return ZERO; }
+static int opri(void) { return ZERO; }
+
+static void opai(int a) { };
+static void opan(num a) { };
+
+
+struct ops {
+	int  (*opri)(void);
+	num  (*oprn)(void);
+	void (*opai)(int);
+	void (*opan)(num);
+};
+
+static struct ops ops_ok = {
+	.opri = opri,
+	.oprn = oprn,
+	.opai = opai,
+	.opan = opan,
+};
+
+static struct ops ops_ko = {
+	.opri = oprn,
+	.oprn = opri,
+	.opai = opan,
+	.opan = opai,
+};
+
+/*
+ * check-name: typediff-enum
+ * check-known-to-fail
+ *
+ * check-error-start
+typediff-enum.c:5:5: error: symbol 'v' redeclared with different type (originally declared at typediff-enum.c:4) - different base types
+typediff-enum.c:8:5: error: symbol 'w' redeclared with different type (originally declared at typediff-enum.c:7) - different base types
+typediff-enum.c:11:5: error: symbol 'foo' redeclared with different type (originally declared at typediff-enum.c:10) - different base types
+typediff-enum.c:14:5: error: symbol 'bar' redeclared with different type (originally declared at typediff-enum.c:13) - different base types
+typediff-enum.c:17:6: error: symbol 'baz' redeclared with different type (originally declared at typediff-enum.c:16) - incompatible argument 1 (different base types)
+typediff-enum.c:20:6: error: symbol 'qux' redeclared with different type (originally declared at typediff-enum.c:19) - incompatible argument 1 (different base types)
+typediff-enum.c:23:6: error: symbol 'pin' redeclared with different type (originally declared at typediff-enum.c:22) - incompatible argument 1 (different base types)
+typediff-enum.c:26:6: error: symbol 'pni' redeclared with different type (originally declared at typediff-enum.c:25) - incompatible argument 1 (different base types)
+typediff-enum.c:51:17: warning: incorrect type in initializer (different base types)
+typediff-enum.c:51:17:    expected int ( *opri )( ... )
+typediff-enum.c:51:17:    got enum num ( *<noident> )( ... )
+typediff-enum.c:52:17: warning: incorrect type in initializer (different base types)
+typediff-enum.c:52:17:    expected enum num ( *oprn )( ... )
+typediff-enum.c:52:17:    got int ( *<noident> )( ... )
+typediff-enum.c:53:17: warning: incorrect type in initializer (incompatible argument 1 (different base types))
+typediff-enum.c:53:17:    expected void ( *opai )( ... )
+typediff-enum.c:53:17:    got void ( *<noident> )( ... )
+typediff-enum.c:54:17: warning: incorrect type in initializer (incompatible argument 1 (different base types))
+typediff-enum.c:54:17:    expected void ( *opan )( ... )
+typediff-enum.c:54:17:    got void ( *<noident> )( ... )
+ * check-error-end
+ */
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/3] extract apply_bitwise() from declaration_specifiers() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/3] extract apply_bitwise() from declaration_specifiers()
Date: Wed, 21 Feb 2018 22:39:07 +0000
Message-ID: <20180221223908.38904-3-luc.vanoostenryck () gmail ! com>
--------------------
No functional changes here, just preparing the next patch.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 parse.c | 41 ++++++++++++++++++++++++++---------------
 1 file changed, 26 insertions(+), 15 deletions(-)

diff --git a/parse.c b/parse.c
index bb504d21a..ce96289c5 100644
--- a/parse.c
+++ b/parse.c
@@ -641,6 +641,29 @@ static void apply_modifiers(struct position pos, struct decl_state *ctx)
 	
 }
 
+static struct symbol *apply_bitwise(struct position pos, struct ctype *ctx)
+{
+	struct symbol *old = ctx->base_type;
+	struct symbol *type;
+
+	if (!(ctx->modifiers & MOD_BITWISE))
+		return old;
+
+	ctx->modifiers &= ~MOD_BITWISE;
+	if (!is_int_type(old)) {
+		sparse_error(pos, "invalid modifier");
+		return old;
+	}
+	type = alloc_symbol(pos, SYM_BASETYPE);
+	*type = *ctx->base_type;
+	type->ctype.modifiers &= ~MOD_SPECIFIER;
+	type->ctype.base_type = old;
+	type->type = SYM_RESTRICT;
+	ctx->base_type = type;
+	create_fouled(type);
+	return type;
+}
+
 static struct symbol * alloc_indirect_symbol(struct position pos, struct ctype *ctype, int type)
 {
 	struct symbol *sym = alloc_symbol(pos, type);
@@ -1543,21 +1566,9 @@ static struct token *declaration_specifiers(struct token *token, struct decl_sta
 		ctx->ctype.base_type = base;
 	}
 
-	if (ctx->ctype.modifiers & MOD_BITWISE) {
-		struct symbol *type;
-		ctx->ctype.modifiers &= ~MOD_BITWISE;
-		if (!is_int_type(ctx->ctype.base_type)) {
-			sparse_error(token->pos, "invalid modifier");
-			return token;
-		}
-		type = alloc_symbol(token->pos, SYM_BASETYPE);
-		*type = *ctx->ctype.base_type;
-		type->ctype.modifiers &= ~MOD_SPECIFIER;
-		type->ctype.base_type = ctx->ctype.base_type;
-		type->type = SYM_RESTRICT;
-		ctx->ctype.base_type = type;
-		create_fouled(type);
-	}
+	if (ctx->ctype.modifiers & MOD_BITWISE)
+		apply_bitwise(token->pos, &ctx->ctype);
+
 	return token;
 }
 
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/3] ir-validate: add validation branch to dead BB ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/3] ir-validate: add validation branch to dead BB
Date: Thu, 30 Aug 2018 23:10:14 +0000
Message-ID: <20180830231015.48281-3-luc.vanoostenryck () gmail ! com>
--------------------
All branches must of course be done to existing BBs.

Validate that it is the case for BR, CBR & SWITCH
(COMPUTEDGOTO is left aside for the moment).

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 ir.c        | 37 ++++++++++++++++++++++++++++++++++---
 linearize.h |  6 ++++++
 2 files changed, 40 insertions(+), 3 deletions(-)

diff --git a/ir.c b/ir.c
index 1890eb13c..533e8e238 100644
--- a/ir.c
+++ b/ir.c
@@ -82,7 +82,29 @@ static int check_user(struct instruction *insn, pseudo_t pseudo)
 	return 0;
 }
 
-static int validate_insn(struct instruction *insn)
+static int check_branch(struct entrypoint *ep, struct instruction *insn, struct basic_block *bb)
+{
+	if (bb->ep && lookup_bb(ep->bbs, bb))
+		return 0;
+	sparse_error(insn->pos, "branch to dead BB: %s", show_instruction(insn));
+	return 1;
+}
+
+static int check_switch(struct entrypoint *ep, struct instruction *insn)
+{
+	struct multijmp *jmp;
+	int err = 0;
+
+	FOR_EACH_PTR(insn->multijmp_list, jmp) {
+		err = check_branch(ep, insn, jmp->target);
+		if (err)
+			return err;
+	} END_FOR_EACH_PTR(jmp);
+
+	return err;
+}
+
+static int validate_insn(struct entrypoint *ep, struct instruction *insn)
 {
 	int err = 0;
 
@@ -104,6 +126,9 @@ static int validate_insn(struct instruction *insn)
 		break;
 
 	case OP_CBR:
+		err += check_branch(ep, insn, insn->bb_true);
+		err += check_branch(ep, insn, insn->bb_false);
+		/* fall through */
 	case OP_COMPUTEDGOTO:
 		err += check_user(insn, insn->cond);
 		break;
@@ -124,8 +149,14 @@ static int validate_insn(struct instruction *insn)
 		err += check_user(insn, insn->src);
 		break;
 
-	case OP_ENTRY:
 	case OP_BR:
+		err += check_branch(ep, insn, insn->bb_true);
+		break;
+	case OP_SWITCH:
+		err += check_switch(ep, insn);
+		break;
+
+	case OP_ENTRY:
 	case OP_SETVAL:
 	default:
 		break;
@@ -147,7 +178,7 @@ int ir_validate(struct entrypoint *ep)
 		FOR_EACH_PTR(bb->insns, insn) {
 			if (!insn->bb)
 				continue;
-			err += validate_insn(insn);
+			err += validate_insn(ep, insn);
 		} END_FOR_EACH_PTR(insn);
 	} END_FOR_EACH_PTR(bb);
 
diff --git a/linearize.h b/linearize.h
index 413bf0132..25fc4ce98 100644
--- a/linearize.h
+++ b/linearize.h
@@ -335,6 +335,12 @@ static inline int bb_reachable(struct basic_block *bb)
 	return bb != NULL;
 }
 
+static inline int lookup_bb(struct basic_block_list *list, struct basic_block *bb)
+{
+	return lookup_ptr_list_entry((struct ptr_list *)list, bb);
+}
+
+
 static inline void add_pseudo_user_ptr(struct pseudo_user *user, struct pseudo_user_list **list)
 {
 	add_ptr_list(list, user);
-- 
2.18.0

================================================================================


################################################################################

=== Thread: [PATCH 2/3] shift: simplify LSR(LSR(x,N),N') & friends ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/3] shift: simplify LSR(LSR(x,N),N') & friends
Date: Mon, 23 Jul 2018 21:15:16 +0000
Message-ID: <20180723211517.84113-3-luc.vanoostenryck () gmail ! com>
--------------------
A shift of a shift (of the same kind) is equivalent to a single
shitf with a count equal to the sum of the two initial counts.
In addition:
* for LSRs & SHLs, if the sum is >= to the instruction size, then
  the result is zero.
* for ASRs, if the sum is >= to the instruction size, then
  the result is the same as a shift of a count of size - 1.

Implement these simplifications if both shift counts are in range.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c                     |  37 +++++++-
 validation/optim/lsr-asr.c     |  46 ++++++++++
 validation/optim/shift-shift.c | 149 +++++++++++++++++++++++++++++++++
 3 files changed, 231 insertions(+), 1 deletion(-)
 create mode 100644 validation/optim/lsr-asr.c
 create mode 100644 validation/optim/shift-shift.c

diff --git a/simplify.c b/simplify.c
index 014466889..7475d9e6c 100644
--- a/simplify.c
+++ b/simplify.c
@@ -577,7 +577,10 @@ static long long check_shift_count(struct instruction *insn, unsigned long long
 
 static int simplify_shift(struct instruction *insn, pseudo_t pseudo, long long value)
 {
+	struct instruction *def;
+	unsigned long long nval;
 	unsigned int size;
+	pseudo_t src2;
 
 	if (!value)
 		return replace_with_pseudo(insn, pseudo);
@@ -590,15 +593,47 @@ static int simplify_shift(struct instruction *insn, pseudo_t pseudo, long long v
 	case OP_ASR:
 		if (value >= size)
 			return 0;
+		switch(DEF_OPCODE(def, pseudo)) {
+		case OP_ASR:
+			src2 = def->src2;
+			if (src2->type != PSEUDO_VAL)
+				break;
+			nval = src2->value;
+			if (nval > insn->size)
+				break;
+			value += nval;
+			if (value >= size)
+				value = size - 1;
+			goto new_value;
+		}
 		break;
 	case OP_LSR:
 		size = operand_size(insn, pseudo);
 		/* fall through */
 	case OP_SHL:
 		if (value >= size)
-			return replace_with_pseudo(insn, value_pseudo(0));
+			goto zero;
+		if (DEF_OPCODE(def, pseudo) == insn->opcode) {
+			src2 = def->src2;
+			if (src2->type != PSEUDO_VAL)
+				break;
+			nval = src2->value;
+			if (nval > insn->size)
+				break;
+			value += nval;
+			goto new_value;
+		}
+		break;
 	}
 	return 0;
+
+new_value:
+	if (value < size) {
+		insn->src2 = value_pseudo(value);
+		return replace_pseudo(insn, &insn->src1, pseudo->def->src1);
+	}
+zero:
+	return replace_with_pseudo(insn, value_pseudo(0));
 }
 
 static int simplify_mul_div(struct instruction *insn, long long value)
diff --git a/validation/optim/lsr-asr.c b/validation/optim/lsr-asr.c
new file mode 100644
index 000000000..2e4f8323f
--- /dev/null
+++ b/validation/optim/lsr-asr.c
@@ -0,0 +1,46 @@
+int lsrasr0(unsigned int x)
+{
+	return ((int) (x >> 15)) >> 15;
+}
+
+int lsrasr1(unsigned int x)
+{
+	return ((int) (x >> 16)) >> 15;
+}
+
+int lsrasr2(unsigned int x)
+{
+	return ((int) (x >> 16)) >> 16;
+}
+
+/*
+ * check-name: lsr-asr
+ * check-command: test-linearize -Wno-decl $file
+ *
+ * check-output-start
+lsrasr0:
+.L0:
+	<entry-point>
+	lsr.32      %r2 <- %arg1, $15
+	asr.32      %r3 <- %r2, $15
+	ret.32      %r3
+
+
+lsrasr1:
+.L2:
+	<entry-point>
+	lsr.32      %r6 <- %arg1, $16
+	asr.32      %r7 <- %r6, $15
+	ret.32      %r7
+
+
+lsrasr2:
+.L4:
+	<entry-point>
+	lsr.32      %r10 <- %arg1, $16
+	asr.32      %r11 <- %r10, $16
+	ret.32      %r11
+
+
+ * check-output-end
+ */
diff --git a/validation/optim/shift-shift.c b/validation/optim/shift-shift.c
new file mode 100644
index 000000000..12a4b7d4c
--- /dev/null
+++ b/validation/optim/shift-shift.c
@@ -0,0 +1,149 @@
+unsigned int shl0(unsigned int x)
+{
+	return x << 15 << 15;
+}
+
+unsigned int shl1(unsigned int x)
+{
+	return x << 16 << 15;
+}
+
+unsigned int shl2(unsigned int x)
+{
+	return x << 16 << 16;
+}
+
+unsigned int shl3(unsigned int x)
+{
+	return x << 12 << 10 << 10;
+}
+
+
+unsigned int lsr0(unsigned int x)
+{
+	return x >> 15 >> 15;
+}
+
+unsigned int lsr1(unsigned int x)
+{
+	return x >> 16 >> 15;
+}
+
+unsigned int lsr2(unsigned int x)
+{
+	return x >> 16 >> 16;
+}
+
+unsigned int lsr3(unsigned int x)
+{
+	return x >> 12 >> 10 >> 10;
+}
+
+
+int asr0(int x)
+{
+	return x >> 15 >> 15;
+}
+
+int asr1(int x)
+{
+	return x >> 16 >> 15;
+}
+
+int asr2(int x)
+{
+	return x >> 16 >> 16;
+}
+
+int asr3(int x)
+{
+	return x >> 12 >> 10 >> 10;
+}
+
+/*
+ * check-name: shift-shift
+ * check-command: test-linearize -Wno-decl $file
+ *
+ * check-output-start
+shl0:
+.L0:
+	<entry-point>
+	shl.32      %r3 <- %arg1, $30
+	ret.32      %r3
+
+
+shl1:
+.L2:
+	<entry-point>
+	shl.32      %r7 <- %arg1, $31
+	ret.32      %r7
+
+
+shl2:
+.L4:
+	<entry-point>
+	ret.32      $0
+
+
+shl3:
+.L6:
+	<entry-point>
+	ret.32      $0
+
+
+lsr0:
+.L8:
+	<entry-point>
+	lsr.32      %r20 <- %arg1, $30
+	ret.32      %r20
+
+
+lsr1:
+.L10:
+	<entry-point>
+	lsr.32      %r24 <- %arg1, $31
+	ret.32      %r24
+
+
+lsr2:
+.L12:
+	<entry-point>
+	ret.32      $0
+
+
+lsr3:
+.L14:
+	<entry-point>
+	ret.32      $0
+
+
+asr0:
+.L16:
+	<entry-point>
+	asr.32      %r37 <- %arg1, $30
+	ret.32      %r37
+
+
+asr1:
+.L18:
+	<entry-point>
+	asr.32      %r41 <- %arg1, $31
+	ret.32      %r41
+
+
+asr2:
+.L20:
+	<entry-point>
+	asr.32      %r45 <- %arg1, $31
+	ret.32      %r45
+
+
+asr3:
+.L22:
+	<entry-point>
+	asr.32      %r50 <- %arg1, $31
+	ret.32      %r50
+
+
+ * check-output-end
+ */
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/3] tokenize: check if string is not NULL ===

From: Ben Dooks <ben.dooks () codethink ! co ! uk>
To: linux-sparse
Subject: Re: [PATCH 2/3] tokenize: check if string is not NULL
Date: Mon, 29 Oct 2018 14:11:29 +0000
Message-ID: <9ab30201-e637-f2b0-7244-c232c76ee5ff () codethink ! co ! uk>
--------------------
On 29/10/18 13:55, Luc Van Oostenryck wrote:
> On Mon, Oct 29, 2018 at 01:40:06PM +0000, Ben Dooks wrote:
>> On 26/10/18 22:33, Luc Van Oostenryck wrote:
>>> On Fri, Oct 26, 2018 at 04:26:31PM +0100, Ben Dooks wrote:
>>>> ---
>>>>    tokenize.c | 2 +-
>>>>    1 file changed, 1 insertion(+), 1 deletion(-)
>>>>
>>>> diff --git a/tokenize.c b/tokenize.c
>>>> index 99b9580..c32f8c7 100644
>>>> --- a/tokenize.c
>>>> +++ b/tokenize.c
>>>> @@ -124,7 +124,7 @@ const char *show_string(const struct string *string)
>>>>    	char *ptr;
>>>>    	int i;
>>>> -	if (!string->length)
>>>> +	if (!string || !string->length)
>>>>    		return "<bad_string>";
>>>
>>> I don't understand this change. What about a zero-length string ("")?
>>
>>
>> The original checks for string->length but not if the string itself
>> was NULL. I've eliminated the original mistake that triggered this.
> 
> Oh yes, sorry. I didn't noticed that the original test checked the length
> and not the pointer.
> 
> I'll investigate why the length was tested.
> 
> Have you an example/testcase where it crashed?

I was using it in earlier code, but I removed the call to show_string
I think (debugging).

-- 
Ben Dooks				http://www.codethink.co.uk/
Senior Engineer				Codethink - Providing Genius

https://www.codethink.co.uk/privacy.html
================================================================================


################################################################################

=== Thread: [PATCH 2/4] has-attr: move 'mode' next to '__mode__' ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/4] has-attr: move 'mode' next to '__mode__'
Date: Fri, 31 Aug 2018 23:16:33 +0000
Message-ID: <20180831231635.90772-3-luc.vanoostenryck () gmail ! com>
--------------------
In the list of keywords '__mode__' was just before the entries for
modes but 'mode' was lost in the middle of some other attributes.

Move 'mode' justbefore '__mode__'.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 parse.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/parse.c b/parse.c
index 270ab7bfd..5a4743314 100644
--- a/parse.c
+++ b/parse.c
@@ -525,7 +525,6 @@ static struct init_keyword {
 	{ "bitwise",	NS_KEYWORD,	MOD_BITWISE,	.op = &attr_bitwise_op },
 	{ "__bitwise__",NS_KEYWORD,	MOD_BITWISE,	.op = &attr_bitwise_op },
 	{ "address_space",NS_KEYWORD,	.op = &address_space_op },
-	{ "mode",	NS_KEYWORD,	.op = &mode_op },
 	{ "context",	NS_KEYWORD,	.op = &context_op },
 	{ "designated_init",	NS_KEYWORD,	.op = &designated_init_op },
 	{ "__transparent_union__",	NS_KEYWORD,	.op = &transparent_union_op },
@@ -537,6 +536,7 @@ static struct init_keyword {
 	{"__const",	NS_KEYWORD,	MOD_PURE,	.op = &attr_mod_op },
 	{"__const__",	NS_KEYWORD,	MOD_PURE,	.op = &attr_mod_op },
 
+	{ "mode",	NS_KEYWORD,	.op = &mode_op },
 	{ "__mode__",	NS_KEYWORD,	.op = &mode_op },
 	{ "QI",		NS_KEYWORD,	MOD_CHAR,	.op = &mode_QI_op },
 	{ "__QI__",	NS_KEYWORD,	MOD_CHAR,	.op = &mode_QI_op },
-- 
2.18.0

================================================================================


################################################################################

=== Thread: [PATCH 2/4] testsuite: add a few more tests catching quadratic behaviour ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/4] testsuite: add a few more tests catching quadratic behaviour
Date: Mon, 19 Mar 2018 00:54:16 +0000
Message-ID: <20180319005418.18548-3-luc.vanoostenryck () gmail ! com>
--------------------
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/mem2reg/quadra01.c | 27 +++++++++++++++++++++++++++
 validation/mem2reg/quadra02.c | 18 ++++++++++++++++++
 validation/repeat.h           | 24 ++++++++++++++++++++++++
 3 files changed, 69 insertions(+)
 create mode 100644 validation/mem2reg/quadra01.c
 create mode 100644 validation/mem2reg/quadra02.c
 create mode 100644 validation/repeat.h

diff --git a/validation/mem2reg/quadra01.c b/validation/mem2reg/quadra01.c
new file mode 100644
index 000000000..b71f46969
--- /dev/null
+++ b/validation/mem2reg/quadra01.c
@@ -0,0 +1,27 @@
+#include "repeat.h"
+
+void use(void *, void *, void *, void *);
+void *def(void);
+
+#define BLOCK(n) {				\
+	void *label;				\
+	use(&&w##n, &&x##n, &&y##n, &&z##n);	\
+w##n:	label = def(); goto *label;		\
+x##n:	label = def(); goto *label;		\
+y##n:	label = def(); goto *label;		\
+z##n:	label = def(); goto *label;		\
+}
+
+static void foo(void) {
+	REPEAT2(5, BLOCK)
+}
+
+/*
+ * check-name: quadratic @ liveness
+ * check-command: test-linearize -I. $file
+ * check-timeout:
+ *
+ * check-output-ignore
+ * check-output-excludes: phi\\.
+ * check-output-excludes: phisrc\\.
+ */
diff --git a/validation/mem2reg/quadra02.c b/validation/mem2reg/quadra02.c
new file mode 100644
index 000000000..6475c7802
--- /dev/null
+++ b/validation/mem2reg/quadra02.c
@@ -0,0 +1,18 @@
+#include "repeat.h"
+
+#define	PAT(X)	int a##X = X;
+static void foo(void)
+{
+	REPEAT2(12, PAT)
+}
+
+/*
+ * check-name: quadratic vars
+ * check-command: test-linearize -I. $file
+ * check-timeout:
+ *
+ * check-output-ignore
+ * check-output-excludes: phi\\.
+ * check-output-excludes: phisrc\\.
+ * check-output-excludes: store\\.
+ */
diff --git a/validation/repeat.h b/validation/repeat.h
new file mode 100644
index 000000000..83433b2a2
--- /dev/null
+++ b/validation/repeat.h
@@ -0,0 +1,24 @@
+#define  R0(P, S)  P(S)
+#define  R1(P, S)  R0(P,S##0)  R0(P,S##1)
+#define  R2(P, S)  R0(P,S##0)  R0(P,S##1)  R0(P,S##2)  R0(P,S##3)
+#define  R3(P, S)  R0(P,S##0)  R0(P,S##1)  R0(P,S##2)  R0(P,S##3)  R0(P,S##4)  R0(P,S##5)  R0(P,S##6)  R0(P,S##7)
+#define  R4(P, S)  R3(P,S##0)  R3(P,S##1)
+#define  R5(P, S)  R3(P,S##0)  R3(P,S##1)  R3(P,S##2)  R3(P,S##3)
+#define  R6(P, S)  R3(P,S##0)  R3(P,S##1)  R3(P,S##2)  R3(P,S##3)  R3(P,S##4)  R3(P,S##5)  R3(P,S##6)  R3(P,S##7)
+#define  R7(P, S)  R6(P,S##0)  R6(P,S##1)
+#define  R8(P, S)  R6(P,S##0)  R6(P,S##1)  R6(P,S##2)  R6(P,S##3)
+#define  R9(P, S)  R6(P,S##0)  R6(P,S##1)  R6(P,S##2)  R6(P,S##3)  R6(P,S##4)  R6(P,S##5)  R6(P,S##6)  R6(P,S##7)
+#define R10(P, S)  R9(P,S##0)  R9(P,S##1)
+#define R11(P, S)  R9(P,S##0)  R9(P,S##1)  R9(P,S##2)  R9(P,S##3)
+#define R12(P, S)  R9(P,S##0)  R9(P,S##1)  R9(P,S##2)  R9(P,S##3)  R9(P,S##4)  R9(P,S##5)  R9(P,S##6)  R9(P,S##7)
+#define R13(P, S) R12(P,S##0) R12(P,S##1)
+#define R14(P, S) R12(P,S##0) R12(P,S##1) R12(P,S##2) R12(P,S##3)
+#define R15(P, S) R12(P,S##0) R12(P,S##1) R12(P,S##2) R12(P,S##3) R12(P,S##4) R12(P,S##5) R12(P,S##6) R12(P,S##7)
+#define R16(P, S) R15(P,S##0) R15(P,S##1)
+#define R17(P, S) R15(P,S##0) R15(P,S##1) R15(P,S##2) R15(P,S##3)
+#define R18(P, S) R15(P,S##0) R15(P,S##1) R15(P,S##2) R15(P,S##3) R15(P,S##4) R15(P,S##5) R15(P,S##6) R15(P,S##7)
+#define R19(P, S) R18(P,S##0) R18(P,S##1)
+#define R20(P, S) R18(P,S##0) R18(P,S##1) R18(P,S##2) R18(P,S##3)
+
+#define REPEAT_(RN, P)	RN(P,)
+#define REPEAT2(N, P)	REPEAT_(R##N,P)
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/5] add ptr_ctype_noderef for use with prinft code ===

From: Ben Dooks <ben.dooks () codethink ! co ! uk>
To: linux-sparse
Subject: [PATCH 2/5] add ptr_ctype_noderef for use with prinft code
Date: Mon, 29 Oct 2018 15:39:49 +0000
Message-ID: <20181029153952.13927-3-ben.dooks () codethink ! co ! uk>
--------------------
Add a specific type for a pointer that shouldn't be dereferenced.
This is used by the printf format parser for the %p case.

Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
---
 symbol.c | 4 +++-
 symbol.h | 2 +-
 2 files changed, 4 insertions(+), 2 deletions(-)

diff --git a/symbol.c b/symbol.c
index 26906ec..b1e7db1 100644
--- a/symbol.c
+++ b/symbol.c
@@ -674,7 +674,8 @@ struct symbol	bool_ctype, void_ctype, type_ctype,
 		llong_ctype, sllong_ctype, ullong_ctype,
 		lllong_ctype, slllong_ctype, ulllong_ctype,
 		float_ctype, double_ctype, ldouble_ctype,
-		string_ctype, ptr_ctype, lazy_ptr_ctype,
+		string_ctype,
+		ptr_ctype, ptr_ctype_noderef, lazy_ptr_ctype,
 		incomplete_ctype, label_ctype, bad_ctype,
 		null_ctype;
 
@@ -740,6 +741,7 @@ static const struct ctype_declare {
 
 	{ &string_ctype,    SYM_PTR,	  0,			    &bits_in_pointer,        &pointer_alignment, &char_ctype },
 	{ &ptr_ctype,	    SYM_PTR,	  0,			    &bits_in_pointer,        &pointer_alignment, &void_ctype },
+	{ &ptr_ctype_noderef,  SYM_PTR,	  MOD_NODEREF,		    &bits_in_pointer,        &pointer_alignment, &void_ctype },
 	{ &null_ctype,	    SYM_PTR,	  0,			    &bits_in_pointer,        &pointer_alignment, &void_ctype },
 	{ &label_ctype,	    SYM_PTR,	  0,			    &bits_in_pointer,        &pointer_alignment, &void_ctype },
 	{ &lazy_ptr_ctype,  SYM_PTR,	  0,			    &bits_in_pointer,        &pointer_alignment, &void_ctype },
diff --git a/symbol.h b/symbol.h
index 3274496..1f338f5 100644
--- a/symbol.h
+++ b/symbol.h
@@ -266,7 +266,7 @@ extern struct symbol	bool_ctype, void_ctype, type_ctype,
 			llong_ctype, sllong_ctype, ullong_ctype,
 			lllong_ctype, slllong_ctype, ulllong_ctype,
 			float_ctype, double_ctype, ldouble_ctype,
-			string_ctype, ptr_ctype, lazy_ptr_ctype,
+			string_ctype, ptr_ctype, ptr_ctype_noderef, lazy_ptr_ctype,
 			incomplete_ctype, label_ctype, bad_ctype,
 			null_ctype;
 
-- 
2.19.1

================================================================================


################################################################################

=== Thread: [PATCH 2/5] add ptr_list_empty() ===

From: Ramsay Jones <ramsay () ramsayjones ! plus ! com>
To: linux-sparse
Subject: Re: [PATCH 2/5] add ptr_list_empty()
Date: Tue, 24 Jul 2018 23:16:01 +0000
Message-ID: <c55308eb-9d03-290d-1ac8-93c346f2777a () ramsayjones ! plus ! com>
--------------------


On 24/07/18 21:34, Luc Van Oostenryck wrote:
> Sometimes it's needed to know if a list is empty, for example,
> to know if a pseudo has some users or not. Currently this is
> done using ptr_list_size() which needs to walk the whole list
> while this is often not needed if the list is empty.

Sometimes we need to know if a list is empty, for example, in
order to determine if a pseudo has some users or not. Currently,
this is done using ptr_list_size(), which always walks the whole
list. It is often possible to short-circuit this walk when the
list is empty.

(or something like that! - you may have noticed that I am not
that good at writing commit messages :D ).

> 
> Add the helper ptr_list_empty() and use it for has_users().
> 
> This gives a speedup up to 18% on some pathological workloads.
> 
> Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> ---
>  linearize.h |  7 ++++++-
>  ptrlist.c   | 19 +++++++++++++++++++
>  ptrlist.h   |  2 ++
>  simplify.c  |  2 +-
>  4 files changed, 28 insertions(+), 2 deletions(-)
> 
> diff --git a/linearize.h b/linearize.h
> index 092e1ac23..de42e718d 100644
> --- a/linearize.h
> +++ b/linearize.h
> @@ -333,9 +333,14 @@ static inline int pseudo_user_list_size(struct pseudo_user_list *list)
>  	return ptr_list_size((struct ptr_list *)list);
>  }
>  
> +static inline bool pseudo_user_list_empty(struct pseudo_user_list *list)
> +{
> +	return ptr_list_empty((struct ptr_list *)list);
> +}
> +
>  static inline int has_users(pseudo_t p)
>  {
> -	return pseudo_user_list_size(p->users) != 0;
> +	return !pseudo_user_list_empty(p->users);
>  }
>  
>  static inline struct pseudo_user *alloc_pseudo_user(struct instruction *insn, pseudo_t *pp)
> diff --git a/ptrlist.c b/ptrlist.c
> index 234433033..40a671000 100644
> --- a/ptrlist.c
> +++ b/ptrlist.c
> @@ -36,6 +36,25 @@ int ptr_list_size(struct ptr_list *head)
>  	return nr;
>  }
>  
> +///
> +// test if a list is empty
> +// @head: the head of the list
> +// @return: ``true`` if the list is empty, '``false`` otherwise.

extraneous ' mark just before ``false``?

ATB,
Ramsay Jones
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/5] cleanup of simplify_seteq_setne(): remove tmp vars ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/5] cleanup of simplify_seteq_setne(): remove tmp vars
Date: Wed, 25 Jul 2018 20:38:25 +0000
Message-ID: <20180725203828.91410-3-luc.vanoostenryck () gmail ! com>
--------------------
No functional changes, just remove unneeded temporary variables.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c | 7 ++-----
 1 file changed, 2 insertions(+), 5 deletions(-)

diff --git a/simplify.c b/simplify.c
index 741b1272c..b6c07ad9f 100644
--- a/simplify.c
+++ b/simplify.c
@@ -578,7 +578,6 @@ static int simplify_seteq_setne(struct instruction *insn, long long value)
 {
 	pseudo_t old = insn->src1;
 	struct instruction *def;
-	pseudo_t src1, src2;
 	int inverse;
 	int opcode;
 
@@ -602,11 +601,9 @@ static int simplify_seteq_setne(struct instruction *insn, long long value)
 		//	setcc.n	%t <- %a, %b
 		//	setcc.m %r <- %a, $b
 		// and similar for setne/eq ... 0/1
-		src1 = def->src1;
-		src2 = def->src2;
 		insn->opcode = inverse ? opcode_table[opcode].negate : opcode;
-		use_pseudo(insn, src1, &insn->src1);
-		use_pseudo(insn, src2, &insn->src2);
+		use_pseudo(insn, def->src1, &insn->src1);
+		use_pseudo(insn, def->src2, &insn->src2);
 		remove_usage(old, &insn->src1);
 		return REPEAT_CSE;
 
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/6] context: fix parsing of attribute 'context' ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/6] context: fix parsing of attribute 'context'
Date: Sat, 26 May 2018 15:14:23 +0000
Message-ID: <20180526151427.30789-3-luc.vanoostenryck () gmail ! com>
--------------------
Currently the parsing of the attribute 'context' is rather
complex and uses a loop which allows 1, 2, 3 or more arguments.
But the the real syntax is only correct for 2 or 3 arguments.

Furthermore the parsing mixes calls to expect() with its own
error reporting. This is a problem because if the error has first
been reported by expect(), the returned token is 'bad_token'
which has no position so you can have error logs like:
	test.c:1:43: error: Expected ( after context attribute
	test.c:1:43: error: got )
	builtin:0:0: error: expected context input/output values
But the 'builtin:0.0' should really be 'test.c:1.43' or, even better,
there shouldn't be a double error reporting.

Fix this by simplifying the parsing and only support 2 or 3 args.
Also, make the error messages slightly more explicit about the
nature of the error.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 parse.c                   | 45 +++++++++++----------------------------
 validation/attr-context.c | 40 ++++++++++++++++++++++++++++++++++
 2 files changed, 53 insertions(+), 32 deletions(-)
 create mode 100644 validation/attr-context.c

diff --git a/parse.c b/parse.c
index d94c53a22..68cdeb226 100644
--- a/parse.c
+++ b/parse.c
@@ -1133,43 +1133,24 @@ static struct token *attribute_context(struct token *token, struct symbol *attr,
 {
 	struct context *context = alloc_context();
 	struct expression *args[3];
-	int argc = 0;
+	int idx = 0;
 
 	token = expect(token, '(', "after context attribute");
-	while (!match_op(token, ')')) {
-		struct expression *expr = NULL;
-		token = conditional_expression(token, &expr);
-		if (!expr)
-			break;
-		if (argc < 3)
-			args[argc++] = expr;
-		if (!match_op(token, ','))
-			break;
+	token = conditional_expression(token, &args[0]);
+	token = expect(token, ',', "after context 1st argument");
+	token = conditional_expression(token, &args[1]);
+	if (match_op(token, ',')) {
 		token = token->next;
-	}
-
-	switch(argc) {
-	case 0:
-		sparse_error(token->pos, "expected context input/output values");
-		break;
-	case 1:
-		context->in = get_expression_value(args[0]);
-		break;
-	case 2:
-		context->in = get_expression_value(args[0]);
-		context->out = get_expression_value(args[1]);
-		break;
-	case 3:
+		token = conditional_expression(token, &args[2]);
+		token = expect(token, ')', "after context 3rd argument");
 		context->context = args[0];
-		context->in = get_expression_value(args[1]);
-		context->out = get_expression_value(args[2]);
-		break;
+		idx++;
+	} else {
+		token = expect(token, ')', "after context 2nd argument");
 	}
-
-	if (argc)
-		add_ptr_list(&ctx->ctype.contexts, context);
-
-	token = expect(token, ')', "after context attribute");
+	context->in =  get_expression_value(args[idx++]);
+	context->out = get_expression_value(args[idx++]);
+	add_ptr_list(&ctx->ctype.contexts, context);
 	return token;
 }
 
diff --git a/validation/attr-context.c b/validation/attr-context.c
new file mode 100644
index 000000000..00e54c668
--- /dev/null
+++ b/validation/attr-context.c
@@ -0,0 +1,40 @@
+static void a(void) __attribute__((context));		// KO
+static void b(void) __attribute__((context()));		// KO
+static void c(void) __attribute__((context 1));		// KO
+static void d(void) __attribute__((context 1,2));	// KO
+static void e(void) __attribute__((context (1)));	// !!!!
+static void f(void) __attribute__((context(0)));	// !!!!
+static void g(void) __attribute__((context(0,1,2,3)));	// KO
+
+static void h(void) __attribute__((context (1,2)));	// OK
+static void i(void) __attribute__((context(0,1)));	// OK
+static void j(void) __attribute__((context(0,1,2)));	// OK
+
+extern int u, v;
+static void x(void) __attribute__((context(0,1,v)));
+static void y(void) __attribute__((context(0,u,1)));
+static void z(void) __attribute__((context(0,u)));
+
+/*
+ * check-name: attr-context
+ *
+ * check-error-start
+attr-context.c:1:43: error: Expected ( after context attribute
+attr-context.c:1:43: error: got )
+attr-context.c:2:44: error: Expected , after context 1st argument
+attr-context.c:2:44: error: got )
+attr-context.c:3:44: error: Expected ( after context attribute
+attr-context.c:3:44: error: got 1
+attr-context.c:4:44: error: Expected ( after context attribute
+attr-context.c:4:44: error: got 1
+attr-context.c:5:46: error: Expected , after context 1st argument
+attr-context.c:5:46: error: got )
+attr-context.c:6:45: error: Expected , after context 1st argument
+attr-context.c:6:45: error: got )
+attr-context.c:7:49: error: Expected ) after context 3rd argument
+attr-context.c:7:49: error: got ,
+attr-context.c:14:48: error: bad constant expression
+attr-context.c:15:46: error: bad constant expression
+attr-context.c:16:46: error: bad constant expression
+ * check-error-end
+ */
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 2/8] trivial-phi: add testcase for unneeded trivial phi-nodes ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 2/8] trivial-phi: add testcase for unneeded trivial phi-nodes
Date: Thu, 30 Aug 2018 22:26:10 +0000
Message-ID: <20180830222616.47360-3-luc.vanoostenryck () gmail ! com>
--------------------
Trivial phi-nodes are phi-nodes having an unique possible outcome.
So, there is nothing to join and the phi-node target can be replaced
by the unique value.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/optim/trivial-phis.c | 15 +++++++++++++++
 1 file changed, 15 insertions(+)
 create mode 100644 validation/optim/trivial-phis.c

diff --git a/validation/optim/trivial-phis.c b/validation/optim/trivial-phis.c
new file mode 100644
index 000000000..754affb73
--- /dev/null
+++ b/validation/optim/trivial-phis.c
@@ -0,0 +1,15 @@
+void foo(int a)
+{
+	while (1)
+		a ^= 0;
+}
+
+/*
+ * check-name: trivial phis
+ * check-command: test-linearize -Wno-decl $file
+ * check-known-to-fail
+ *
+ * check-output-ignore
+ * check-output-excludes: phi\\.
+ * check-output-excludes: phisrc\\.
+ */
-- 
2.18.0

================================================================================


################################################################################

=== Thread: [PATCH 3/3] add support for mode __byte__ ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 3/3] add support for mode __byte__
Date: Mon, 30 Jul 2018 16:34:40 +0000
Message-ID: <20180730163440.2303-4-luc.vanoostenryck () gmail ! com>
--------------------
sparse support GCC's modes like __SI__, __DI__, as well as
__word__ & __pointer__ but GCC also supports a mode __byte__.

For completeness, add this mode as an alias for mode __QI__.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 parse.c | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/parse.c b/parse.c
index 845b2c167..950da3646 100644
--- a/parse.c
+++ b/parse.c
@@ -554,6 +554,8 @@ static struct init_keyword {
 	{ "__DI__",	NS_KEYWORD,	.op = &mode_DI_op },
 	{ "TI",		NS_KEYWORD,	.op = &mode_TI_op },
 	{ "__TI__",	NS_KEYWORD,	.op = &mode_TI_op },
+	{ "byte",	NS_KEYWORD,	.op = &mode_QI_op },
+	{ "__byte__",	NS_KEYWORD,	.op = &mode_QI_op },
 	{ "pointer",	NS_KEYWORD,	.op = &mode_pointer_op },
 	{ "__pointer__",NS_KEYWORD,	.op = &mode_pointer_op },
 	{ "word",	NS_KEYWORD,	.op = &mode_word_op },
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 3/3] fix enum typing ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 3/3] fix enum typing
Date: Wed, 18 Apr 2018 15:39:59 +0000
Message-ID: <20180418153959.33271-4-luc.vanoostenryck () gmail ! com>
--------------------
Currently, type_difference() doesn't make a distinction between
enums & ints with the result being that sparse is not only way
too permissive regarding the typing of enums but simply wrong.

Fix this in type_difference() by:
*) stop to strip SYM_ENUM as it is (and must be) done with SYM_NODE.
*) be as strict with SYM_ENUMs as with SYM_STRUCTs & SYM_UNIONs.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 evaluate.c                 | 5 +++--
 validation/typediff-enum.c | 1 -
 2 files changed, 3 insertions(+), 3 deletions(-)

diff --git a/evaluate.c b/evaluate.c
index 0d50220a6..b7be8a409 100644
--- a/evaluate.c
+++ b/evaluate.c
@@ -695,7 +695,7 @@ const char *type_difference(struct ctype *c1, struct ctype *c2,
 		if (!t1 || !t2)
 			return "different types";
 
-		if (t1->type == SYM_NODE || t1->type == SYM_ENUM) {
+		if (t1->type == SYM_NODE) {
 			t1 = base1;
 			move1 = 1;
 			if (!t1)
@@ -703,7 +703,7 @@ const char *type_difference(struct ctype *c1, struct ctype *c2,
 			continue;
 		}
 
-		if (t2->type == SYM_NODE || t2->type == SYM_ENUM) {
+		if (t2->type == SYM_NODE) {
 			t2 = base2;
 			move2 = 1;
 			if (!t2)
@@ -724,6 +724,7 @@ const char *type_difference(struct ctype *c1, struct ctype *c2,
 			return "bad types";
 		case SYM_RESTRICT:
 			return "different base types";
+		case SYM_ENUM:
 		case SYM_UNION:
 		case SYM_STRUCT:
 			/* allow definition of incomplete structs and unions */
diff --git a/validation/typediff-enum.c b/validation/typediff-enum.c
index 4c97bcf6c..6c7b37acf 100644
--- a/validation/typediff-enum.c
+++ b/validation/typediff-enum.c
@@ -56,7 +56,6 @@ static struct ops ops_ko = {
 
 /*
  * check-name: typediff-enum
- * check-known-to-fail
  *
  * check-error-start
 typediff-enum.c:5:5: error: symbol 'v' redeclared with different type (originally declared at typediff-enum.c:4) - different base types
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 3/3] initial variadic argument code ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH 3/3] initial variadic argument code
Date: Fri, 26 Oct 2018 23:58:56 +0000
Message-ID: <20181026235854.7gaydrlevkjcrarx () ltop ! local>
--------------------
On Fri, Oct 26, 2018 at 04:26:32PM +0100, Ben Dooks wrote:
> @@ -2259,7 +2402,18 @@ static int evaluate_arguments(struct symbol *fn, struct expression_list *head)
>  		if (!ctype)
>  			return 0;
>  
> -		target = argtype;
> +		if (i == fn->printf_msg) {
> +			int ret = evaluate_format_printf(fn, *p, &variadic);
> +			if (ret < 0)
> +				warning((*p)->pos, "cannot parse format");
> +			else if (ret > 0)
> +				variadic_limit = fn->printf_va_start + ret;
> +		}
> +
> +		if (i >= fn->printf_va_start && i <= variadic_limit)

There is an off-by-one error here. The test should be:
+		if (i >= fn->printf_va_start && i < variadic_limit)

But even better to replace this with a struct symbol_list.

-- Luc
================================================================================


################################################################################

=== Thread: [PATCH 3/3] shift: simplify ASR(LSR(x,N),N') ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH 3/3] shift: simplify ASR(LSR(x,N),N')
Date: Tue, 24 Jul 2018 05:39:00 +0000
Message-ID: <20180724053858.smvgjthj4cqbaixy () ltop ! local>
--------------------
On Tue, Jul 24, 2018 at 02:28:45AM +0100, Ramsay Jones wrote:
> 
> 
> On 23/07/18 22:15, Luc Van Oostenryck wrote:
> > Since an LSR with an in-range shift count will insert a zero in
> 
> I'm assuming that the LSR shift count N is non-zero (otherwise
> LSR(x,0) => x, which means ASR(LSR(x,0),N') != LSR(x,N')), right?

Yes. This is normally guaranted at all time.
But yes, known that I'm thinking about it, since a zero LSR is not
simplified away at expansion time, there is a very small possibility
that this simplification is done with an LSR of 0 (at the first run
of the simplification loop, if the LSR and the ASR are in two different
Basic Blocks and for some reasons the ASR's BB is processed before
the LSR's BB (which normally doesn't occurs)).

Thank for catching that this!

-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 3/3] teach sparse about restricted enums ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 3/3] teach sparse about restricted enums
Date: Wed, 21 Feb 2018 22:39:08 +0000
Message-ID: <20180221223908.38904-4-luc.vanoostenryck () gmail ! com>
--------------------
Too ugly to describe.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 parse.c                      | 20 +++++++++++++++++---
 validation/enum-restricted.c |  1 -
 2 files changed, 17 insertions(+), 4 deletions(-)

diff --git a/parse.c b/parse.c
index ce96289c5..b58ae0a3f 100644
--- a/parse.c
+++ b/parse.c
@@ -696,10 +696,11 @@ static struct token *struct_union_enum_specifier(enum type type,
 	struct token *token, struct decl_state *ctx,
 	struct token *(*parse)(struct token *, struct symbol *))
 {
+	struct decl_state type_attr = { };
 	struct symbol *sym;
 	struct position *repos;
 
-	token = handle_attributes(token, ctx, KW_ATTRIBUTE);
+	token = handle_attributes(token, &type_attr, KW_ATTRIBUTE);
 	if (token_type(token) == TOKEN_IDENT) {
 		sym = lookup_symbol(token->ident, NS_STRUCT);
 		if (!sym ||
@@ -712,6 +713,12 @@ static struct token *struct_union_enum_specifier(enum type type,
 		}
 		if (sym->type != type)
 			error_die(token->pos, "invalid tag applied to %s", show_typename (sym));
+
+		// FIXME: should apply all modifiers
+		type_attr.ctype.base_type = sym;
+		if (type == SYM_ENUM)	// FIXME: see is_int_type()
+			sym->ctype.base_type = &int_ctype;
+		sym = apply_bitwise(token->pos, &type_attr.ctype);
 		ctx->ctype.base_type = sym;
 		repos = &token->pos;
 		token = token->next;
@@ -860,8 +867,11 @@ static struct token *parse_enum_declaration(struct token *token, struct symbol *
 	struct symbol *ctype = NULL, *base_type = NULL;
 	Num upper = {-1, 0}, lower = {1, 0};
 
-	parent->examined = 1;
-	parent->ctype.base_type = &int_ctype;
+	if (parent->type == SYM_RESTRICT)
+		parent->ctype.base_type->ctype.base_type = &int_ctype;
+	else
+		parent->ctype.base_type = &int_ctype;
+	examine_symbol_type(parent);
 	while (token_type(token) == TOKEN_IDENT) {
 		struct expression *expr = NULL;
 		struct token *next = token->next;
@@ -886,6 +896,8 @@ static struct token *parse_enum_declaration(struct token *token, struct symbol *
 			expr->value = lastval;
 			expr->ctype = ctype;
 		}
+		if (parent->type == SYM_RESTRICT)
+			expr->ctype = parent;
 
 		sym = alloc_symbol(token->pos, SYM_NODE);
 		bind_symbol(sym, token->ident, NS_SYMBOL);
@@ -898,6 +910,8 @@ static struct token *parse_enum_declaration(struct token *token, struct symbol *
 		if (base_type != &bad_ctype) {
 			if (ctype->type == SYM_NODE)
 				ctype = ctype->ctype.base_type;
+			if (ctype->type == SYM_RESTRICT)
+				ctype = ctype->ctype.base_type;
 			if (ctype->type == SYM_ENUM) {
 				if (ctype == parent)
 					ctype = base_type;
diff --git a/validation/enum-restricted.c b/validation/enum-restricted.c
index b58964172..39fd90673 100644
--- a/validation/enum-restricted.c
+++ b/validation/enum-restricted.c
@@ -18,5 +18,4 @@ static void foo(void)
 
 /*
  * check-name: enum-restricted
- * check-known-to-fail
  */
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 3/4] has-attr: add __designated_init__ & transparent_union ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 3/4] has-attr: add __designated_init__ & transparent_union
Date: Fri, 31 Aug 2018 23:16:34 +0000
Message-ID: <20180831231635.90772-4-luc.vanoostenryck () gmail ! com>
--------------------
Attributes can be used with the plain keyword or squeezed
between a pair of double underscrore.

For some reasons, 'designated_init' was not allowed with its
underscores and '__transparent_union__' wasn't without them.

So, allow '__designated_init__' & 'transparent_union'.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 parse.c | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/parse.c b/parse.c
index 5a4743314..ce71c1c1b 100644
--- a/parse.c
+++ b/parse.c
@@ -527,6 +527,8 @@ static struct init_keyword {
 	{ "address_space",NS_KEYWORD,	.op = &address_space_op },
 	{ "context",	NS_KEYWORD,	.op = &context_op },
 	{ "designated_init",	NS_KEYWORD,	.op = &designated_init_op },
+	{ "__designated_init__",	NS_KEYWORD,	.op = &designated_init_op },
+	{ "transparent_union",	NS_KEYWORD,	.op = &transparent_union_op },
 	{ "__transparent_union__",	NS_KEYWORD,	.op = &transparent_union_op },
 	{ "noreturn",	NS_KEYWORD,	MOD_NORETURN,	.op = &attr_mod_op },
 	{ "__noreturn__",	NS_KEYWORD,	MOD_NORETURN,	.op = &attr_mod_op },
-- 
2.18.0

================================================================================


################################################################################

=== Thread: [PATCH 3/4] testsuite: improve mem2reg testcases ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 3/4] testsuite: improve mem2reg testcases
Date: Mon, 19 Mar 2018 00:54:17 +0000
Message-ID: <20180319005418.18548-4-luc.vanoostenryck () gmail ! com>
--------------------
A few tests are added, some have been renamed to better
refect their purposes. Finally, some checks have been added
or tweaked.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/crash-select.c                     | 18 ++++++++++++++
 validation/{ => mem2reg}/alias-distinct.c     |  0
 validation/{ => mem2reg}/alias-mixed.c        |  0
 validation/{ => mem2reg}/alias-same.c         |  0
 validation/mem2reg/cond-expr.c                |  1 +
 validation/mem2reg/cond-expr5.c               |  6 ++++-
 validation/mem2reg/init-global-array.c        | 12 ++++++----
 validation/mem2reg/init-local-array.c         | 13 ++++++----
 validation/{ => mem2reg}/kill-casts.c         |  0
 validation/mem2reg/loop02-global.c            |  2 +-
 validation/mem2reg/missing-return.c           | 34 +++++++++++++++++++++++++++
 validation/{ => mem2reg}/reload-aliasing.c    |  0
 validation/mem2reg/store-deadborn.c           |  9 +++++++
 validation/{linear => mem2reg}/stray-phisrc.c |  0
 validation/mem2reg/struct.c                   | 32 +++++++++++++++++++++++++
 validation/mem2reg/unused-var.c               | 23 ++++++++++++++++++
 validation/{mem2reg => optim}/killed-insn.c   |  7 +++---
 validation/optim/null-phi.c                   |  9 +++++++
 18 files changed, 151 insertions(+), 15 deletions(-)
 create mode 100644 validation/crash-select.c
 rename validation/{ => mem2reg}/alias-distinct.c (100%)
 rename validation/{ => mem2reg}/alias-mixed.c (100%)
 rename validation/{ => mem2reg}/alias-same.c (100%)
 rename validation/{ => mem2reg}/kill-casts.c (100%)
 create mode 100644 validation/mem2reg/missing-return.c
 rename validation/{ => mem2reg}/reload-aliasing.c (100%)
 create mode 100644 validation/mem2reg/store-deadborn.c
 rename validation/{linear => mem2reg}/stray-phisrc.c (100%)
 create mode 100644 validation/mem2reg/struct.c
 create mode 100644 validation/mem2reg/unused-var.c
 rename validation/{mem2reg => optim}/killed-insn.c (53%)
 create mode 100644 validation/optim/null-phi.c

diff --git a/validation/crash-select.c b/validation/crash-select.c
new file mode 100644
index 000000000..cec00baf8
--- /dev/null
+++ b/validation/crash-select.c
@@ -0,0 +1,18 @@
+struct s {
+	void *b;
+	long c;
+};
+
+long d(void);
+static long f(void)
+{
+	struct s s;
+	s.c = d();
+	if (s.c)
+		s.c = 2;
+	return s.c;
+}
+
+/*
+ * check-name: crash-select
+ */
diff --git a/validation/alias-distinct.c b/validation/mem2reg/alias-distinct.c
similarity index 100%
rename from validation/alias-distinct.c
rename to validation/mem2reg/alias-distinct.c
diff --git a/validation/alias-mixed.c b/validation/mem2reg/alias-mixed.c
similarity index 100%
rename from validation/alias-mixed.c
rename to validation/mem2reg/alias-mixed.c
diff --git a/validation/alias-same.c b/validation/mem2reg/alias-same.c
similarity index 100%
rename from validation/alias-same.c
rename to validation/mem2reg/alias-same.c
diff --git a/validation/mem2reg/cond-expr.c b/validation/mem2reg/cond-expr.c
index f38564ef8..8acb00ac9 100644
--- a/validation/mem2reg/cond-expr.c
+++ b/validation/mem2reg/cond-expr.c
@@ -10,4 +10,5 @@ int foo(int a, int b, int c)
  * check-command: test-linearize -Wno-decl -fdump-ir=mem2reg $file
  * check-output-ignore
  * check-output-pattern(2): phi\\.
+ * check-output-pattern(3): phisrc\\.
  */
diff --git a/validation/mem2reg/cond-expr5.c b/validation/mem2reg/cond-expr5.c
index 6c1e1c34d..62ac6c15d 100644
--- a/validation/mem2reg/cond-expr5.c
+++ b/validation/mem2reg/cond-expr5.c
@@ -11,8 +11,12 @@ int foo(int p, int q, int a)
 /*
  * check-name: cond-expr5
  * check-command: test-linearize -Wno-decl -fdump-ir=mem2reg $file
+ * check-known-to-fail
+ *
  * check-output-ignore
  * check-output-excludes: load\\.
  * check-output-excludes: store\\.
- * check-output-pattern(2): phi\\.
+ * check-output-excludes: phi\\..*, .*, .*
+ * check-output-pattern(3): phi\\.
+ * check-output-pattern(5): phisrc\\.
  */
diff --git a/validation/mem2reg/init-global-array.c b/validation/mem2reg/init-global-array.c
index aea4135ae..51ca50e35 100644
--- a/validation/mem2reg/init-global-array.c
+++ b/validation/mem2reg/init-global-array.c
@@ -1,8 +1,11 @@
-struct {
+struct s {
 	int a[2];
-} s;
+};
 
-int sarray(void)
+
+static struct s s;
+
+static int sarray(void)
 {
 	s.a[1] = 1;
 	return s.a[1];
@@ -10,8 +13,9 @@ int sarray(void)
 
 /*
  * check-name: init global array
- * check-command: test-linearize -Wno-decl -fdump-ir=mem2reg $file
+ * check-command: test-linearize $file
  * check-output-ignore
  * check-output-excludes: load\\.
  * check-output-pattern(1): store\\.
+ * check-output-pattern(1): ret.32 *\\$1
  */
diff --git a/validation/mem2reg/init-local-array.c b/validation/mem2reg/init-local-array.c
index 2ac53bc77..639a74f10 100644
--- a/validation/mem2reg/init-local-array.c
+++ b/validation/mem2reg/init-local-array.c
@@ -1,25 +1,28 @@
-int array(void)
+static int array(void)
 {
 	int a[2];
 
 	a[1] = 1;
+	a[0] = 0;
 	return a[1];
 }
 
-int sarray(void)
+static int sarray(void)
 {
 	struct {
 		int a[2];
 	} s;
 
 	s.a[1] = 1;
+	s.a[0] = 0;
 	return s.a[1];
 }
 
 /*
  * check-name: init local array
- * check-command: test-linearize -Wno-decl -fdump-ir=mem2reg $file
+ * check-command: test-linearize $file
  * check-output-ignore
- * check-output-excludes: load\\.
- * check-output-excludes: store\\.
+ * check-output-excludes: load
+ * check-output-excludes: store
+ * check-output-pattern(2): ret.32 *\\$1
  */
diff --git a/validation/kill-casts.c b/validation/mem2reg/kill-casts.c
similarity index 100%
rename from validation/kill-casts.c
rename to validation/mem2reg/kill-casts.c
diff --git a/validation/mem2reg/loop02-global.c b/validation/mem2reg/loop02-global.c
index a0a8b42b0..b627b33d1 100644
--- a/validation/mem2reg/loop02-global.c
+++ b/validation/mem2reg/loop02-global.c
@@ -16,7 +16,7 @@ int foo(void)
 
 /*
  * check-name: loop02 global
- * check-command: test-linearize -Wno-decl -fdump-ir=mem2reg $file
+ * check-command: test-linearize -Wno-decl $file
  * check-output-ignore
  * check-output-excludes: load\\.
  */
diff --git a/validation/mem2reg/missing-return.c b/validation/mem2reg/missing-return.c
new file mode 100644
index 000000000..06f6e4d52
--- /dev/null
+++ b/validation/mem2reg/missing-return.c
@@ -0,0 +1,34 @@
+int f1(void)
+{
+	if (1)
+		return 1;
+}
+
+int f0(void)
+{
+	if (0)
+		return 0;
+}
+
+int fx(int p)
+{
+	if (p)
+		return 0;
+}
+
+int bar(int p)
+{
+	if (p)
+		return 0;
+	p++;
+}
+
+/*
+ * check-name: missing-return
+ * check-command: test-linearize -m32 -fdump-ir=mem2reg -Wno-decl $file
+ * check-known-to-fail
+ *
+ * check-output-ignore
+ * check-output-pattern(1): ret.32 *\\$1
+ * check-output-pattern(3): ret.32 *UNDEF
+ */
diff --git a/validation/reload-aliasing.c b/validation/mem2reg/reload-aliasing.c
similarity index 100%
rename from validation/reload-aliasing.c
rename to validation/mem2reg/reload-aliasing.c
diff --git a/validation/mem2reg/store-deadborn.c b/validation/mem2reg/store-deadborn.c
new file mode 100644
index 000000000..cca34d592
--- /dev/null
+++ b/validation/mem2reg/store-deadborn.c
@@ -0,0 +1,9 @@
+static void foo(int a)
+{
+	return;
+	a = 0;
+}
+
+/*
+ * check-name: store-deadborn
+ */
diff --git a/validation/linear/stray-phisrc.c b/validation/mem2reg/stray-phisrc.c
similarity index 100%
rename from validation/linear/stray-phisrc.c
rename to validation/mem2reg/stray-phisrc.c
diff --git a/validation/mem2reg/struct.c b/validation/mem2reg/struct.c
new file mode 100644
index 000000000..13962a8e6
--- /dev/null
+++ b/validation/mem2reg/struct.c
@@ -0,0 +1,32 @@
+struct s {
+	int a;
+	int b;
+};
+
+int f0(void)
+{
+	struct s s;
+
+	s.a = 0;
+	s.b = 1;
+
+	return s.a;
+}
+
+int f1(void)
+{
+	struct s s;
+
+	s.a = 1;
+	s.b = 0;
+
+	return s.b;
+}
+
+/*
+ * check-name: struct
+ * check-command: test-linearize -Wno-decl $file
+ *
+ * check-output-ignore
+ * check-output-pattern(2): ret.32 *\\$0
+ */
diff --git a/validation/mem2reg/unused-var.c b/validation/mem2reg/unused-var.c
new file mode 100644
index 000000000..ac3945820
--- /dev/null
+++ b/validation/mem2reg/unused-var.c
@@ -0,0 +1,23 @@
+int foo(int a)
+{
+	switch (a) {
+		int u = 1;
+
+	default:
+		return a;
+	}
+}
+
+/*
+ * check-name: unused-var
+ * check-command: test-linearize -Wno-decl $file
+ *
+ * check-output-start
+foo:
+.L0:
+	<entry-point>
+	ret.32      %arg1
+
+
+ * check-output-end
+ */
diff --git a/validation/mem2reg/killed-insn.c b/validation/optim/killed-insn.c
similarity index 53%
rename from validation/mem2reg/killed-insn.c
rename to validation/optim/killed-insn.c
index adbef9805..d1cdd02ee 100644
--- a/validation/mem2reg/killed-insn.c
+++ b/validation/optim/killed-insn.c
@@ -1,14 +1,13 @@
-static int g;
-static void foo(void)
+static void foo(int v)
 {
 	int a[2] = { };
 	a;
-	a[1] = g;
+	a[1] = v;
 }
 
 /*
  * check-name: killed-insn
- * check-command: test-linearize -fdump-ir=mem2reg $file
+ * check-command: test-linearize $file
  *
  * check-output-ignore
  * check-output-excludes: store\\.
diff --git a/validation/optim/null-phi.c b/validation/optim/null-phi.c
new file mode 100644
index 000000000..1f9de4d5b
--- /dev/null
+++ b/validation/optim/null-phi.c
@@ -0,0 +1,9 @@
+static int foo(void)
+{
+	if (0)
+		return 0;
+}
+
+/*
+ * check-name: null-phi
+ */
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 3/5] add ptr_list_multiple() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 3/5] add ptr_list_multiple()
Date: Tue, 24 Jul 2018 20:34:56 +0000
Message-ID: <20180724203458.46429-4-luc.vanoostenryck () gmail ! com>
--------------------
When doing IR simplification, it's often needed to check if the
result of a instructions is used by a single instruction in which
case it can be destructively modified together with its user.

Currently this is done using ptr_list_size() which needs to walk
the whole list which is relatively costly when the list is long
while knowing if the list contains more than 1 element can often
be answered cheaply by inspecting only the first block.

Add the helpers ptr_list_multiple() and multi_users().

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 linearize.h |  5 +++++
 ptrlist.c   | 21 +++++++++++++++++++++
 ptrlist.h   |  1 +
 simplify.c  |  2 +-
 4 files changed, 28 insertions(+), 1 deletion(-)

diff --git a/linearize.h b/linearize.h
index de42e718d..b067b3e84 100644
--- a/linearize.h
+++ b/linearize.h
@@ -343,6 +343,11 @@ static inline int has_users(pseudo_t p)
 	return !pseudo_user_list_empty(p->users);
 }
 
+static inline bool multi_users(pseudo_t p)
+{
+	return ptr_list_multiple((struct ptr_list *)(p->users));
+}
+
 static inline struct pseudo_user *alloc_pseudo_user(struct instruction *insn, pseudo_t *pp)
 {
 	struct pseudo_user *user = __alloc_pseudo_user(0);
diff --git a/ptrlist.c b/ptrlist.c
index 40a671000..4cd3baca9 100644
--- a/ptrlist.c
+++ b/ptrlist.c
@@ -55,6 +55,27 @@ bool ptr_list_empty(const struct ptr_list *head)
 	return true;
 }
 
+///
+// test is a list contains more than one element
+// @head: the head of the list
+// @return: ``true`` if the list has more than 1 element, '``false`` otherwise.
+bool ptr_list_multiple(const struct ptr_list *head)
+{
+	const struct ptr_list *list = head;
+	int nr = 0;
+
+	if (!head)
+		return false;
+
+	do {
+		nr += list->nr - list->rm;
+		if (nr > 1)
+			return true;
+	} while ((list = list->next) != head);
+
+	return false;
+}
+
 ///
 // get the first element of a ptrlist
 // @head: the head of the list
diff --git a/ptrlist.h b/ptrlist.h
index f145bc5f1..176bb0712 100644
--- a/ptrlist.h
+++ b/ptrlist.h
@@ -39,6 +39,7 @@ extern void concat_ptr_list(struct ptr_list *a, struct ptr_list **b);
 extern void copy_ptr_list(struct ptr_list **h, struct ptr_list *t);
 extern int ptr_list_size(struct ptr_list *);
 extern bool ptr_list_empty(const struct ptr_list *head);
+extern bool ptr_list_multiple(const struct ptr_list *head);
 extern int linearize_ptr_list(struct ptr_list *, void **, int);
 extern void *first_ptr_list(struct ptr_list *);
 extern void *last_ptr_list(struct ptr_list *);
diff --git a/simplify.c b/simplify.c
index 4dc24a505..65f29de0a 100644
--- a/simplify.c
+++ b/simplify.c
@@ -824,7 +824,7 @@ static int simplify_associative_binop(struct instruction *insn)
 		return 0;
 	if (!simple_pseudo(def->src2))
 		return 0;
-	if (pseudo_user_list_size(def->target->users) != 1)
+	if (multi_users(def->target))
 		return 0;
 	switch_pseudo(def, &def->src1, insn, &insn->src2);
 	return REPEAT_CSE;
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH 3/5] add ptr_list_multiple()
Date: Wed, 25 Jul 2018 08:31:30 +0000
Message-ID: <20180725083128.5x4h73gbmwwtjl6u () ltop ! local>
--------------------
On Wed, Jul 25, 2018 at 12:22:21AM +0100, Ramsay Jones wrote:
> >  
> > +///
> > +// test is a list contains more than one element
> > +// @head: the head of the list
> > +// @return: ``true`` if the list has more than 1 element, '``false``otherwise.
> 
> Hmm, so the ' is maybe deliberate - or cut-n-paste?

The later.

Thanks for noticing.
-- Luc 
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 3/5] initial parsing of __attribute__((format)) ===

From: Ben Dooks <ben.dooks () codethink ! co ! uk>
To: linux-sparse
Subject: [PATCH 3/5] initial parsing of __attribute__((format))
Date: Mon, 29 Oct 2018 15:39:50 +0000
Message-ID: <20181029153952.13927-4-ben.dooks () codethink ! co ! uk>
--------------------
Add code to parse the __attribute__((format)) used to indicate that
a variadic function takes a printf-style format string and where
those are. Save the data in ctype ready for checking when such an
function is encoutered.

Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
---
Fixes since v1:
- moved to using ctype in base_type to store infromation
- fixed formatting issues
- updated check for bad format arguments
- reduced the line count to unsigned short to save space

Notes:
- What to do when base_type is not set... current code doesn't work
---
 parse.c  | 74 ++++++++++++++++++++++++++++++++++++++++++++++++++++++--
 symbol.h |  2 ++
 2 files changed, 74 insertions(+), 2 deletions(-)

diff --git a/parse.c b/parse.c
index 02a55a7..9b0d40e 100644
--- a/parse.c
+++ b/parse.c
@@ -84,7 +84,7 @@ static attr_t
 	attribute_address_space, attribute_context,
 	attribute_designated_init,
 	attribute_transparent_union, ignore_attribute,
-	attribute_mode, attribute_force;
+	attribute_mode, attribute_force, attribute_format;
 
 typedef struct symbol *to_mode_t(struct symbol *);
 
@@ -353,6 +353,10 @@ static struct symbol_op attr_force_op = {
 	.attribute = attribute_force,
 };
 
+static struct symbol_op attr_format = {
+	.attribute = attribute_format,
+};
+
 static struct symbol_op address_space_op = {
 	.attribute = attribute_address_space,
 };
@@ -407,6 +411,10 @@ static struct symbol_op mode_word_op = {
 	.to_mode = to_word_mode
 };
 
+static struct symbol_op attr_printf_op = {
+	.type	= KW_FORMAT,
+};
+
 /* Using NS_TYPEDEF will also make the keyword a reserved one */
 static struct init_keyword {
 	const char *name;
@@ -513,6 +521,8 @@ static struct init_keyword {
 	{ "bitwise",	NS_KEYWORD,	MOD_BITWISE,	.op = &attr_bitwise_op },
 	{ "__bitwise__",NS_KEYWORD,	MOD_BITWISE,	.op = &attr_bitwise_op },
 	{ "address_space",NS_KEYWORD,	.op = &address_space_op },
+	{ "format",	NS_KEYWORD,	.op = &attr_format },
+	{ "printf",	NS_KEYWORD,	.op = &attr_printf_op },
 	{ "mode",	NS_KEYWORD,	.op = &mode_op },
 	{ "context",	NS_KEYWORD,	.op = &context_op },
 	{ "designated_init",	NS_KEYWORD,	.op = &designated_init_op },
@@ -1051,6 +1061,67 @@ static struct token *attribute_address_space(struct token *token, struct symbol
 	return token;
 }
 
+static struct token *attribute_format(struct token *token, struct symbol *attr, struct decl_state *ctx)
+{
+	struct expression *args[3];
+	struct symbol *fmt_sym = NULL;
+	int argc = 0;
+
+	/* expecting format ( type, start, va_args at) */
+
+	token = expect(token, '(', "after format attribute");
+	while (!match_op(token, ')')) {
+		struct expression *expr = NULL;
+
+		if (argc == 0) {
+			if (token_type(token) == TOKEN_IDENT)
+				fmt_sym = lookup_keyword(token->ident, NS_KEYWORD);
+
+			if (!fmt_sym || !fmt_sym->op ||
+			    fmt_sym->op != &attr_printf_op) {
+				sparse_error(token->pos,
+					     "unknown format type '%s'\n",
+					     show_ident(token->ident));
+				fmt_sym = NULL;
+			}
+		}
+
+		token = conditional_expression(token, &expr);
+		if (!expr)
+			break;
+		if (argc < 3)
+			args[argc++] = expr;
+		if (!match_op(token, ','))
+			break;
+		token = token->next;
+	}
+
+	if (argc != 3 || !fmt_sym) {
+		warning(token->pos, "incorrect format attribute");
+	} else {
+		long long start, at;
+
+		start = get_expression_value(args[2]);
+		at = get_expression_value(args[1]);
+
+		if (start <= 0 || at <= 0 || (start == at && start > 0)) {
+			warning(token->pos, "bad format positions");
+		} else if (start < at) {
+			warning(token->pos, "format cannot be after va_args");
+		} else if (!ctx->ctype.base_type) {
+			warning(token->pos, "no base type to set");
+			ctx->ctype.printf_va_start = start;
+			ctx->ctype.printf_msg = at;
+		} else {
+			ctx->ctype.base_type->ctype.printf_va_start = start;
+			ctx->ctype.base_type->ctype.printf_msg = at;
+		}
+	}
+
+	token = expect(token, ')', "after format attribute");
+	return token;
+}
+
 static struct symbol *to_QI_mode(struct symbol *ctype)
 {
 	if (ctype->ctype.base_type != &int_type)
@@ -2102,7 +2173,6 @@ static struct statement *start_function(struct symbol *sym)
 
 	// Currently parsed symbol for __func__/__FUNCTION__/__PRETTY_FUNCTION__
 	current_fn = sym;
-
 	return stmt;
 }
 
diff --git a/symbol.h b/symbol.h
index 1f338f5..4cd8d61 100644
--- a/symbol.h
+++ b/symbol.h
@@ -86,6 +86,7 @@ enum keyword {
 	KW_SHORT	= 1 << 7,
 	KW_LONG		= 1 << 8,
 	KW_EXACT	= 1 << 9,
+	KW_FORMAT	= 1 << 10,
 };
 
 struct context {
@@ -103,6 +104,7 @@ struct ctype {
 	struct context_list *contexts;
 	unsigned int as;
 	struct symbol *base_type;
+	unsigned short printf_va_start, printf_msg;
 };
 
 struct decl_state {
-- 
2.19.1

================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH 3/5] initial parsing of __attribute__((format))
Date: Mon, 29 Oct 2018 20:41:52 +0000
Message-ID: <20181029204151.zpl55dxv2qoxynz7 () ltop ! local>
--------------------
On Mon, Oct 29, 2018 at 03:39:50PM +0000, Ben Dooks wrote:
> Add code to parse the __attribute__((format)) used to indicate that
> a variadic function takes a printf-style format string and where
> those are. Save the data in ctype ready for checking when such an
> function is encoutered.
> 
> Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
> ---
> Fixes since v1:
> - moved to using ctype in base_type to store infromation
> - fixed formatting issues
> - updated check for bad format arguments
> - reduced the line count to unsigned short to save space
> 
> Notes:
> - What to do when base_type is not set... current code doesn't work

Yes, I saw this. See here below.

> ---
>  parse.c  | 74 ++++++++++++++++++++++++++++++++++++++++++++++++++++++--
>  symbol.h |  2 ++
>  2 files changed, 74 insertions(+), 2 deletions(-)
> 
> diff --git a/parse.c b/parse.c
> index 02a55a7..9b0d40e 100644
> --- a/parse.c
> +++ b/parse.c
> @@ -1051,6 +1061,67 @@ static struct token *attribute_address_space(struct token *token, struct symbol
>  	return token;
>  }
>  
> +static struct token *attribute_format(struct token *token, struct symbol *attr, struct decl_state *ctx)
> +{
> +	struct expression *args[3];
> +	struct symbol *fmt_sym = NULL;
> +	int argc = 0;
> +
> +	/* expecting format ( type, start, va_args at) */
> +
> +	token = expect(token, '(', "after format attribute");
> +	while (!match_op(token, ')')) {
> +		struct expression *expr = NULL;
> +
> +		if (argc == 0) {
> +			if (token_type(token) == TOKEN_IDENT)
> +				fmt_sym = lookup_keyword(token->ident, NS_KEYWORD);
> +
> +			if (!fmt_sym || !fmt_sym->op ||
> +			    fmt_sym->op != &attr_printf_op) {
> +				sparse_error(token->pos,
> +					     "unknown format type '%s'\n",
> +					     show_ident(token->ident));
> +				fmt_sym = NULL;
> +			}
> +		}
> +
> +		token = conditional_expression(token, &expr);
> +		if (!expr)
> +			break;
> +		if (argc < 3)
> +			args[argc++] = expr;
> +		if (!match_op(token, ','))
> +			break;
> +		token = token->next;
> +	}
> +
> +	if (argc != 3 || !fmt_sym) {
> +		warning(token->pos, "incorrect format attribute");
> +	} else {
> +		long long start, at;
> +
> +		start = get_expression_value(args[2]);
> +		at = get_expression_value(args[1]);
> +
> +		if (start <= 0 || at <= 0 || (start == at && start > 0)) {
> +			warning(token->pos, "bad format positions");
> +		} else if (start < at) {
> +			warning(token->pos, "format cannot be after va_args");
> +		} else if (!ctx->ctype.base_type) {
> +			warning(token->pos, "no base type to set");
> +			ctx->ctype.printf_va_start = start;
> +			ctx->ctype.printf_msg = at;
> +		} else {
> +			ctx->ctype.base_type->ctype.printf_va_start = start;
> +			ctx->ctype.base_type->ctype.printf_msg = at;
> +		}

This makes me realize that only a few attributes are handled and can be
handled. The last lines should simply be written as:
+		} else if (start < at) {
+			warning(token->pos, "format cannot be after va_args");
+		} else {
+			ctx->ctype.printf_va_start = start;
+			ctx->ctype.printf_msg = at;
+		}

In other words, this parsing should not poke into the base type. But as-is
the two args will not be propagated into the function because ctx is for the
whole declaration. You will need to add something like:
	diff --git a/parse.c b/parse.c
	index 83bca24b3..4e273b743 100644
	--- a/parse.c
	+++ b/parse.c
	@@ -2979,6 +2979,10 @@ struct token *external_declaration(struct token *token, struct symbol_list **lis
	 
	 		if (!(decl->ctype.modifiers & MOD_STATIC))
	 			decl->ctype.modifiers |= MOD_EXTERN;
	+
	+		// apply non-modifier function attributes
	+		base_type->ctype.printf_msg = decl->ctype.printf_msg;
	+		base_type->ctype.printf_va_start = decl->ctype.printf_va_start;
	 	} else if (base_type == &void_ctype && !(decl->ctype.modifiers & MOD_EXTERN)) {
	 		sparse_error(token->pos, "void declaration");
	 	}

and at some later stage, I'll make it more generic.


Kind regards,
-- Luc
================================================================================

From: Ben Dooks <ben.dooks () codethink ! co ! uk>
To: linux-sparse
Subject: Re: [PATCH 3/5] initial parsing of __attribute__((format))
Date: Tue, 30 Oct 2018 09:05:22 +0000
Message-ID: <28d37a8a-a9d8-62ea-b658-af3175020de3 () codethink ! co ! uk>
--------------------
On 29/10/18 20:41, Luc Van Oostenryck wrote:
> On Mon, Oct 29, 2018 at 03:39:50PM +0000, Ben Dooks wrote:
>> Add code to parse the __attribute__((format)) used to indicate that
>> a variadic function takes a printf-style format string and where
>> those are. Save the data in ctype ready for checking when such an
>> function is encoutered.
>>

> In other words, this parsing should not poke into the base type. But as-is
> the two args will not be propagated into the function because ctx is for the
> whole declaration. You will need to add something like:
> 	diff --git a/parse.c b/parse.c
> 	index 83bca24b3..4e273b743 100644
> 	--- a/parse.c
> 	+++ b/parse.c
> 	@@ -2979,6 +2979,10 @@ struct token *external_declaration(struct token *token, struct symbol_list **lis
> 	
> 	 		if (!(decl->ctype.modifiers & MOD_STATIC))
> 	 			decl->ctype.modifiers |= MOD_EXTERN;
> 	+
> 	+		// apply non-modifier function attributes
> 	+		base_type->ctype.printf_msg = decl->ctype.printf_msg;
> 	+		base_type->ctype.printf_va_start = decl->ctype.printf_va_start;
> 	 	} else if (base_type == &void_ctype && !(decl->ctype.modifiers & MOD_EXTERN)) {
> 	 		sparse_error(token->pos, "void declaration");
> 	 	}
> 
> and at some later stage, I'll make it more generic.

Ok, thanks. That seems to have worked.


-- 
Ben Dooks				http://www.codethink.co.uk/
Senior Engineer				Codethink - Providing Genius

https://www.codethink.co.uk/privacy.html
================================================================================


################################################################################

=== Thread: [PATCH 3/5] simplify SET{EQ,NE}(ZEXT(x, N),{0,1}) ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 3/5] simplify SET{EQ,NE}(ZEXT(x, N),{0,1})
Date: Wed, 25 Jul 2018 20:38:26 +0000
Message-ID: <20180725203828.91410-4-luc.vanoostenryck () gmail ! com>
--------------------
A zero-extension has no effect on the result of a comparison with 0 or 1.

Optimize away the extension.

Note: this simplification was already done but only when the
      original size was 1 but it can be done for all sizes.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c                        | 17 +++++++----------
 validation/optim/bool-zext-test.c |  1 -
 2 files changed, 7 insertions(+), 11 deletions(-)

diff --git a/simplify.c b/simplify.c
index b6c07ad9f..fb4bb3429 100644
--- a/simplify.c
+++ b/simplify.c
@@ -608,16 +608,13 @@ static int simplify_seteq_setne(struct instruction *insn, long long value)
 		return REPEAT_CSE;
 
 	case OP_ZEXT:
-		if (def->orig_type->bit_size == 1) {
-			// Convert:
-			//	zext.m	%s <- (1) %a
-			//	setne.1 %r <- %s, $0
-			// into:
-			//	setne.1 %s <- %a, $0
-			// and same for setne/eq ... 0/1
-			return replace_pseudo(insn, &insn->src1, def->src1);
-		}
-		break;
+		// Convert:
+		//	*ext.m	%s <- (1) %a
+		//	setne.1 %r <- %s, $0
+		// into:
+		//	setne.1 %s <- %a, $0
+		// and same for setne/eq ... 0/1
+		return replace_pseudo(insn, &insn->src1, def->src);
 	}
 	return 0;
 }
diff --git a/validation/optim/bool-zext-test.c b/validation/optim/bool-zext-test.c
index f837ace20..138938b0a 100644
--- a/validation/optim/bool-zext-test.c
+++ b/validation/optim/bool-zext-test.c
@@ -6,7 +6,6 @@ _Bool neu1(unsigned char a) { return a != 1; }
 /*
  * check-name: bool-zext-test
  * check-command: test-linearize -Wno-decl $file
- * check-known-to-fail
  *
  * check-output-ignore
  * check-output-excludes: zext\\.
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Ramsay Jones <ramsay () ramsayjones ! plus ! com>
To: linux-sparse
Subject: Re: [PATCH 3/5] simplify SET{EQ,NE}(ZEXT(x, N),{0,1})
Date: Sat, 28 Jul 2018 01:20:34 +0000
Message-ID: <379a5b32-690e-91db-ab12-b0ea80cd94f5 () ramsayjones ! plus ! com>
--------------------


On 25/07/18 21:38, Luc Van Oostenryck wrote:
> A zero-extension has no effect on the result of a comparison with 0 or 1.
> 
> Optimize away the extension.
> 
> Note: this simplification was already done but only when the
>       original size was 1 but it can be done for all sizes.
> 
> Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> ---
>  simplify.c                        | 17 +++++++----------
>  validation/optim/bool-zext-test.c |  1 -
>  2 files changed, 7 insertions(+), 11 deletions(-)
> 
> diff --git a/simplify.c b/simplify.c
> index b6c07ad9f..fb4bb3429 100644
> --- a/simplify.c
> +++ b/simplify.c
> @@ -608,16 +608,13 @@ static int simplify_seteq_setne(struct instruction *insn, long long value)
>  		return REPEAT_CSE;
>  
>  	case OP_ZEXT:
> -		if (def->orig_type->bit_size == 1) {
> -			// Convert:
> -			//	zext.m	%s <- (1) %a
> -			//	setne.1 %r <- %s, $0
> -			// into:
> -			//	setne.1 %s <- %a, $0
> -			// and same for setne/eq ... 0/1
> -			return replace_pseudo(insn, &insn->src1, def->src1);
> -		}
> -		break;
> +		// Convert:
> +		//	*ext.m	%s <- (1) %a

why *ext.m?, this is only zext - Ah, next patch you add sext! ;-)
Hmm, maybe keep it as zext until next patch?

> +		//	setne.1 %r <- %s, $0
> +		// into:
> +		//	setne.1 %s <- %a, $0
> +		// and same for setne/eq ... 0/1
> +		return replace_pseudo(insn, &insn->src1, def->src);

Hmm, so this isn't just removing the conditional. In the context
I can see that the last argument to replace_pseudo was def->src1
rather than def->src used here. I haven't given it much thought,
just reading the patch text, but ...

ATB,
Ramsay Jones

>  	}
>  	return 0;
>  }
> diff --git a/validation/optim/bool-zext-test.c b/validation/optim/bool-zext-test.c
> index f837ace20..138938b0a 100644
> --- a/validation/optim/bool-zext-test.c
> +++ b/validation/optim/bool-zext-test.c
> @@ -6,7 +6,6 @@ _Bool neu1(unsigned char a) { return a != 1; }
>  /*
>   * check-name: bool-zext-test
>   * check-command: test-linearize -Wno-decl $file
> - * check-known-to-fail
>   *
>   * check-output-ignore
>   * check-output-excludes: zext\\.
> 
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH 3/5] simplify SET{EQ,NE}(ZEXT(x, N),{0,1})
Date: Sat, 28 Jul 2018 06:57:53 +0000
Message-ID: <20180728065752.zgnwtgwdd2qiewwq () ltop ! local>
--------------------
On Sat, Jul 28, 2018 at 03:29:52AM +0100, Ramsay Jones wrote:
> 
> 
> On 28/07/18 02:20, Ramsay Jones wrote:
> [snip]
> >> +		//	setne.1 %r <- %s, $0
> >> +		// into:
> >> +		//	setne.1 %s <- %a, $0
> >> +		// and same for setne/eq ... 0/1
> >> +		return replace_pseudo(insn, &insn->src1, def->src);
> > 
> > Hmm, so this isn't just removing the conditional. In the context
> > I can see that the last argument to replace_pseudo was def->src1
> > rather than def->src used here. I haven't given it much thought,
> > just reading the patch text, but ...
> 
> OK, so I already knew that the pseudo_t's src and src1 were
> effectively aliases (since they are stored at the same position
> in the union), but they shouldn't be used interchangeably.
> 
> The difference is the 'arrity' of the instructions; unary
> versus binary. So, this is actually a 'fix' (zext is a unary
> operator)?

I see it more as an automatic thing than a fix. In the doc (IR.rst)
I've documented that src, src1, cond and some others are all aliased
but I try to keep things consistent with the declarations: 'src' for
unops, src1 for binops, ...

The '*ext' is there because I splitted the patch when I begin to
write the patch description and I forgot to change what was once
correctly '*ext' into 'zext'.

Thanks for noticing.

-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 3/6] context: __context__(...) expect a constant expression ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 3/6] context: __context__(...) expect a constant expression
Date: Sat, 26 May 2018 15:14:24 +0000
Message-ID: <20180526151427.30789-4-luc.vanoostenryck () gmail ! com>
--------------------
No check are done if the inc/dec value of context statements
is effectively a compile-time integer value: '0' is silently
used if it is not.

Change that by using get_expression_value() when linearizing
context statements (which has the added advantage to also
slightly simplify the code).

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 linearize.c               |  6 +-----
 validation/context-stmt.c | 19 +++++++++++++++++++
 2 files changed, 20 insertions(+), 5 deletions(-)
 create mode 100644 validation/context-stmt.c

diff --git a/linearize.c b/linearize.c
index 88f6c0949..6284d079b 100644
--- a/linearize.c
+++ b/linearize.c
@@ -1777,12 +1777,8 @@ static pseudo_t linearize_context(struct entrypoint *ep, struct statement *stmt)
 {
 	struct instruction *insn = alloc_instruction(OP_CONTEXT, 0);
 	struct expression *expr = stmt->expression;
-	int value = 0;
 
-	if (expr->type == EXPR_VALUE)
-		value = expr->value;
-
-	insn->increment = value;
+	insn->increment = get_expression_value(expr);
 	insn->context_expr = stmt->context;
 	add_one_insn(ep, insn);
 	return VOID;
diff --git a/validation/context-stmt.c b/validation/context-stmt.c
new file mode 100644
index 000000000..cb85e562b
--- /dev/null
+++ b/validation/context-stmt.c
@@ -0,0 +1,19 @@
+static void foo(int x)
+{
+	__context__(0);		// OK
+	__context__(x, 0);	// OK
+	__context__ (x, 1);	// OK
+
+	__context__(x);		// KO: no const expr
+	__context__(1,x);	// KO: no const expr
+}
+
+/*
+ * check-name: context-stmt
+ * check-command: sparse -Wno-context $file
+ *
+ * check-error-start
+context-stmt.c:7:21: error: bad constant expression
+context-stmt.c:8:23: error: bad constant expression
+ * check-error-end
+ */
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 4/4] has-attr: add support for __has_attribute() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 4/4] has-attr: add support for __has_attribute()
Date: Fri, 31 Aug 2018 23:16:35 +0000
Message-ID: <20180831231635.90772-5-luc.vanoostenryck () gmail ! com>
--------------------
Sparse has support for a subset of GCC's large collection of
attributes. It's not easy to know wich versions support this
or this attributes but since GCC5, there is a good solution
to this problem: the magic macro __has_attribute(<name>)
which evaluates to 1 if <name> is an attribute known to the
compiler and 0 otherwise.

Add support for this __has_attribute() macro by extending the
already existing support for __has_builtin().

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 ident-list.h                            |  1 +
 lib.c                                   |  1 +
 pre-process.c                           | 33 ++++++++++++++++++-------
 validation/preprocessor/has-attribute.c |  1 -
 4 files changed, 26 insertions(+), 10 deletions(-)

diff --git a/ident-list.h b/ident-list.h
index a37a4a1b5..75740b9d9 100644
--- a/ident-list.h
+++ b/ident-list.h
@@ -59,6 +59,7 @@ IDENT_RESERVED(__label__);
  * sparse. */
 IDENT(defined);
 IDENT(once);
+IDENT(__has_attribute);
 IDENT(__has_builtin);
 __IDENT(pragma_ident, "__pragma__", 0);
 __IDENT(__VA_ARGS___ident, "__VA_ARGS__", 0);
diff --git a/lib.c b/lib.c
index db456a63e..48c6dc983 100644
--- a/lib.c
+++ b/lib.c
@@ -1287,6 +1287,7 @@ static void create_builtin_stream(void)
 	add_pre_buffer("#add_system \"%s/include-fixed\"\n", gcc_base_dir);
 
 	add_pre_buffer("#define __has_builtin(x) 0\n");
+	add_pre_buffer("#define __has_attribute(x) 0\n");
 	add_pre_buffer("#define __builtin_stdarg_start(a,b) ((a) = (__builtin_va_list)(&(b)))\n");
 	add_pre_buffer("#define __builtin_va_start(a,b) ((a) = (__builtin_va_list)(&(b)))\n");
 	add_pre_buffer("#define __builtin_ms_va_start(a,b) ((a) = (__builtin_ms_va_list)(&(b)))\n");
diff --git a/pre-process.c b/pre-process.c
index da4b7acdb..bf4b8e766 100644
--- a/pre-process.c
+++ b/pre-process.c
@@ -165,6 +165,12 @@ static void replace_with_has_builtin(struct token *token)
 	replace_with_bool(token, sym && sym->builtin);
 }
 
+static void replace_with_has_attribute(struct token *token)
+{
+	struct symbol *sym = lookup_symbol(token->ident, NS_KEYWORD);
+	replace_with_bool(token, sym && sym->op && sym->op->attribute);
+}
+
 static void expand_line(struct token *token)
 {
 	replace_with_integer(token, token->pos.line);
@@ -1592,6 +1598,10 @@ static int expression_value(struct token **where)
 				state = 4;
 				beginning = list;
 				break;
+			} else if (p->ident == &__has_attribute_ident) {
+				state = 6;
+				beginning = list;
+				break;
 			}
 			if (!expand_one_symbol(list))
 				continue;
@@ -1623,29 +1633,34 @@ static int expression_value(struct token **where)
 			*list = p->next;
 			continue;
 
-		// __has_builtin(xyz)
-		case 4:
+		// __has_builtin(x) or __has_attribute(x)
+		case 4: case 6:
 			if (match_op(p, '(')) {
-				state = 5;
+				state++;
 			} else {
-				sparse_error(p->pos, "missing '(' after \"__has_builtin\"");
+				sparse_error(p->pos, "missing '(' after \"__has_%s\"",
+					state == 4 ? "builtin" : "attribute");
 				state = 0;
 			}
 			*beginning = p;
 			break;
-		case 5:
+		case 5: case 7:
 			if (token_type(p) != TOKEN_IDENT) {
 				sparse_error(p->pos, "identifier expected");
 				state = 0;
 				break;
 			}
 			if (!match_op(p->next, ')'))
-				sparse_error(p->pos, "missing ')' after \"__has_builtin\"");
-			state = 6;
-			replace_with_has_builtin(p);
+				sparse_error(p->pos, "missing ')' after \"__has_%s\"",
+					state == 5 ? "builtin" : "attribute");
+			if (state == 5)
+				replace_with_has_builtin(p);
+			else
+				replace_with_has_attribute(p);
+			state = 8;
 			*beginning = p;
 			break;
-		case 6:
+		case 8:
 			state = 0;
 			*list = p->next;
 			continue;
diff --git a/validation/preprocessor/has-attribute.c b/validation/preprocessor/has-attribute.c
index ec8dbb06e..3149cbfa6 100644
--- a/validation/preprocessor/has-attribute.c
+++ b/validation/preprocessor/has-attribute.c
@@ -44,7 +44,6 @@ __has_attribute()??? Quesako?
 /*
  * check-name: has-attribute
  * check-command: sparse -E $file
- * check-known-to-fail
  *
  * check-output-start
 
-- 
2.18.0

================================================================================


################################################################################

=== Thread: [PATCH 4/4] testsuite: remove useless test for loop-linearization ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 4/4] testsuite: remove useless test for loop-linearization
Date: Mon, 19 Mar 2018 00:54:18 +0000
Message-ID: <20180319005418.18548-5-luc.vanoostenryck () gmail ! com>
--------------------
This testcase was added a bit too quickly in order to have
minimal testing of loop's linearization.

However, such test just comparing the raw output of test-linearize
is a big PITA because it's so sensible to things like pseudos' name
themselves depending very much on details about the linearization
and simplification. Also, this test didn't really tested anything,
it only allowed to track changes.

Remove it as it has no testing value.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/loop-linearization.c | 136 ----------------------------------------
 1 file changed, 136 deletions(-)
 delete mode 100644 validation/loop-linearization.c

diff --git a/validation/loop-linearization.c b/validation/loop-linearization.c
deleted file mode 100644
index 25c6dfb87..000000000
--- a/validation/loop-linearization.c
+++ /dev/null
@@ -1,136 +0,0 @@
-extern int p(int);
-
-static int ffor(void)
-{
-	int i;
-	for (int i = 0; i < 10; i++) {
-		if (!p(i))
-			return 0;
-	}
-	return 1;
-}
-
-static int fwhile(void)
-{
-	int i = 0;
-	while (i < 10) {
-		if (!p(i))
-			return 0;
-		i++;
-	}
-	return 1;
-}
-
-static int fdo(void)
-{
-	int i = 0;
-	do {
-		if (!p(i))
-			return 0;
-	} while (i++ < 10);
-	return 1;
-}
-
-/*
- * check-name: loop-linearization
- * check-command: test-linearize $file
- *
- * check-output-start
-ffor:
-.L0:
-	<entry-point>
-	phisrc.32   %phi5(i) <- $0
-	br          .L4
-
-.L4:
-	phi.32      %r1(i) <- %phi5(i), %phi6(i)
-	setlt.32    %r2 <- %r1(i), $10
-	cbr         %r2, .L1, .L3
-
-.L1:
-	call.32     %r4 <- p, %r1(i)
-	cbr         %r4, .L2, .L5
-
-.L5:
-	phisrc.32   %phi1(return) <- $0
-	br          .L7
-
-.L2:
-	add.32      %r7 <- %r1(i), $1
-	phisrc.32   %phi6(i) <- %r7
-	br          .L4
-
-.L3:
-	phisrc.32   %phi2(return) <- $1
-	br          .L7
-
-.L7:
-	phi.32      %r5 <- %phi1(return), %phi2(return)
-	ret.32      %r5
-
-
-fwhile:
-.L8:
-	<entry-point>
-	phisrc.32   %phi11(i) <- $0
-	br          .L12
-
-.L12:
-	phi.32      %r8(i) <- %phi11(i), %phi12(i)
-	setlt.32    %r9 <- %r8(i), $10
-	cbr         %r9, .L9, .L11
-
-.L9:
-	call.32     %r11 <- p, %r8(i)
-	cbr         %r11, .L14, .L13
-
-.L13:
-	phisrc.32   %phi7(return) <- $0
-	br          .L15
-
-.L14:
-	add.32      %r14 <- %r8(i), $1
-	phisrc.32   %phi12(i) <- %r14
-	br          .L12
-
-.L11:
-	phisrc.32   %phi8(return) <- $1
-	br          .L15
-
-.L15:
-	phi.32      %r12 <- %phi7(return), %phi8(return)
-	ret.32      %r12
-
-
-fdo:
-.L16:
-	<entry-point>
-	phisrc.32   %phi16(i) <- $0
-	br          .L17
-
-.L17:
-	phi.32      %r15(i) <- %phi16(i), %phi17(i)
-	call.32     %r16 <- p, %r15(i)
-	cbr         %r16, .L18, .L20
-
-.L20:
-	phisrc.32   %phi13(return) <- $0
-	br          .L22
-
-.L18:
-	add.32      %r19 <- %r15(i), $1
-	setlt.32    %r20 <- %r15(i), $10
-	phisrc.32   %phi17(i) <- %r19
-	cbr         %r20, .L17, .L19
-
-.L19:
-	phisrc.32   %phi14(return) <- $1
-	br          .L22
-
-.L22:
-	phi.32      %r17 <- %phi13(return), %phi14(return)
-	ret.32      %r17
-
-
- * check-output-end
- */
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 4/6] context: fix crashes while parsing '__context__;' or '__context__(;' ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 4/6] context: fix crashes while parsing '__context__;' or '__context__(;'
Date: Sat, 26 May 2018 15:14:25 +0000
Message-ID: <20180526151427.30789-5-luc.vanoostenryck () gmail ! com>
--------------------
The expected syntax for the __context__ statement is:
	__context__(<inc/dec value>);
or
	__context__(<context>, <inc/dec value>);

The distinction between the two formats is made by checking if
the expression is a PREOP with '(' as op and with an comma
expression as inner expression.

However, code like:
	__context__;
or
	__context__(;
crashes while trying to test the non-existing expression
(after PREOP or after the comma expression).

Fix this by testing if the expression is non-null before
dereferencing it.

Note: this fix has the merit to directly address the problem
      but doesn't let a diagnostic to be issued for the case
	__context__;
      which is considered as perfectly valid.
      The next patch will take care of this.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 parse.c                   | 4 +++-
 validation/context-stmt.c | 7 +++++++
 2 files changed, 10 insertions(+), 1 deletion(-)

diff --git a/parse.c b/parse.c
index 68cdeb226..42b3fd20a 100644
--- a/parse.c
+++ b/parse.c
@@ -2339,8 +2339,10 @@ static struct token *parse_context_statement(struct token *token, struct stateme
 {
 	stmt->type = STMT_CONTEXT;
 	token = parse_expression(token->next, &stmt->expression);
-	if (stmt->expression->type == EXPR_PREOP
+	if (stmt->expression
+	    && stmt->expression->type == EXPR_PREOP
 	    && stmt->expression->op == '('
+	    && stmt->expression->unop
 	    && stmt->expression->unop->type == EXPR_COMMA) {
 		struct expression *expr;
 		expr = stmt->expression->unop;
diff --git a/validation/context-stmt.c b/validation/context-stmt.c
index cb85e562b..1f02c3a67 100644
--- a/validation/context-stmt.c
+++ b/validation/context-stmt.c
@@ -6,6 +6,9 @@ static void foo(int x)
 
 	__context__(x);		// KO: no const expr
 	__context__(1,x);	// KO: no const expr
+
+	__context__;		// KO: no expression at all
+	__context__(;		// KO: no expression at all
 }
 
 /*
@@ -13,7 +16,11 @@ static void foo(int x)
  * check-command: sparse -Wno-context $file
  *
  * check-error-start
+context-stmt.c:11:21: error: an expression is expected before ')'
+context-stmt.c:11:21: error: Expected ) in expression
+context-stmt.c:11:21: error: got ;
 context-stmt.c:7:21: error: bad constant expression
 context-stmt.c:8:23: error: bad constant expression
+context-stmt.c:11:20: error: bad constant expression type
  * check-error-end
  */
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 4/8] trivial-phi: extract trivial_phi() from clean_up_phi() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 4/8] trivial-phi: extract trivial_phi() from clean_up_phi()
Date: Thu, 30 Aug 2018 22:26:12 +0000
Message-ID: <20180830222616.47360-5-luc.vanoostenryck () gmail ! com>
--------------------
This will allow us, at a later step, to recursivaely test
the operands.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c | 25 ++++++++++++++++++++++---
 1 file changed, 22 insertions(+), 3 deletions(-)

diff --git a/simplify.c b/simplify.c
index f6471a4ef..b9f13f655 100644
--- a/simplify.c
+++ b/simplify.c
@@ -172,7 +172,16 @@ static int if_convert_phi(struct instruction *insn)
 	return REPEAT_CSE;
 }
 
-static int clean_up_phi(struct instruction *insn)
+///
+// detect trivial phi-nodes
+// @insn: the phi-node
+// @pseudo: a pointer to the resulting pseudo
+// @return: 1 if the phi-node is trivial, 0 otherwise
+//
+// A phi-node is trivial if it has a single possible result:
+//	# all operands are the same
+// Since the result is unique, these phi-nodes can be removed.
+static int trivial_phi(pseudo_t *pseudo, struct instruction *insn)
 {
 	pseudo_t phi;
 	struct instruction *last;
@@ -196,8 +205,18 @@ static int clean_up_phi(struct instruction *insn)
 		same = 0;
 	} END_FOR_EACH_PTR(phi);
 
-	if (same) {
-		pseudo_t pseudo = last ? last->phi_src : VOID;
+	if (same)
+		*pseudo = last ? last->phi_src : NULL;
+	return same;
+}
+
+static int clean_up_phi(struct instruction *insn)
+{
+	pseudo_t pseudo;
+
+	if (trivial_phi(&pseudo, insn)) {
+		if (!pseudo)
+			pseudo = VOID;
 		convert_instruction_target(insn, pseudo);
 		kill_instruction(insn);
 		return REPEAT_CSE;
-- 
2.18.0

================================================================================


################################################################################

=== Thread: [PATCH 5/5] evaluate: stifle check if one pointer is a noderef ===

From: Ben Dooks <ben.dooks () codethink ! co ! uk>
To: linux-sparse
Subject: [PATCH 5/5] evaluate: stifle check if one pointer is a noderef
Date: Mon, 29 Oct 2018 15:39:52 +0000
Message-ID: <20181029153952.13927-6-ben.dooks () codethink ! co ! uk>
--------------------
If both t1 and t2 are ptr_ctype_noderef then it should be ok to ignore
the address-space of both (in the case of passing to printf style
functions).

Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
---
 evaluate.c | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/evaluate.c b/evaluate.c
index 2a98a9b..0d5b8ad 100644
--- a/evaluate.c
+++ b/evaluate.c
@@ -718,7 +718,8 @@ const char *type_difference(struct ctype *c1, struct ctype *c2,
 			/* XXX: we ought to compare sizes */
 			break;
 		case SYM_PTR:
-			if (as1 != as2)
+			if (as1 != as2 && (t1 != &ptr_ctype_noderef &&
+					   t2 != &ptr_ctype_noderef))
 				return "different address spaces";
 			/* MOD_SPECIFIER is due to idiocy in parse.c */
 			if ((mod1 ^ mod2) & ~MOD_IGNORE & ~MOD_SPECIFIER)
-- 
2.19.1

================================================================================


################################################################################

=== Thread: [PATCH 5/5] no VOID test in convert_instruction_target() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 5/5] no VOID test in convert_instruction_target()
Date: Tue, 24 Jul 2018 20:34:58 +0000
Message-ID: <20180724203458.46429-6-luc.vanoostenryck () gmail ! com>
--------------------
In convert_instruction_target(), when replacing the pseudo
in the target user list, it's first checked if the old pseudo
is not VOID and nothing is done otherwise. But this test is
not needed because:
2) the only case where VOID is stored in the user list is when
   a BB is killed and a killed instruction wouln't be converted
3) this test used to be needed when OP_PHIs were converted during
   CSE (meaning that the pseudo stored there have been removed
   from the list) but OP_PHIs are not CSEed anymore.

So, removed this unneeded test.

This gives a speedup up to 9% in some pathological workloads.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 flow.c | 5 +----
 1 file changed, 1 insertion(+), 4 deletions(-)

diff --git a/flow.c b/flow.c
index 9483938fb..0fdbdf44d 100644
--- a/flow.c
+++ b/flow.c
@@ -292,10 +292,7 @@ void convert_instruction_target(struct instruction *insn, pseudo_t src)
 	if (target == src)
 		return;
 	FOR_EACH_PTR(target->users, pu) {
-		if (*pu->userp != VOID) {
-			assert(*pu->userp == target);
-			*pu->userp = src;
-		}
+		*pu->userp = src;
 	} END_FOR_EACH_PTR(pu);
 	if (has_use_list(src))
 		concat_user_list(target->users, &src->users);
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Ramsay Jones <ramsay () ramsayjones ! plus ! com>
To: linux-sparse
Subject: Re: [PATCH 5/5] no VOID test in convert_instruction_target()
Date: Tue, 24 Jul 2018 23:28:32 +0000
Message-ID: <a5fa07e1-b094-7025-b7e7-a319a28ecda2 () ramsayjones ! plus ! com>
--------------------


On 24/07/18 21:34, Luc Van Oostenryck wrote:
> In convert_instruction_target(), when replacing the pseudo
> in the target user list, it's first checked if the old pseudo
> is not VOID and nothing is done otherwise. But this test is
> not needed because:

What happened to 1)? ;-)

> 2) the only case where VOID is stored in the user list is when
>    a BB is killed and a killed instruction wouln't be converted
> 3) this test used to be needed when OP_PHIs were converted during
>    CSE (meaning that the pseudo stored there have been removed
>    from the list) but OP_PHIs are not CSEed anymore.
> 
> So, removed this unneeded test.
> 
> This gives a speedup up to 9% in some pathological workloads.
> 
> Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> ---
>  flow.c | 5 +----
>  1 file changed, 1 insertion(+), 4 deletions(-)
> 
> diff --git a/flow.c b/flow.c
> index 9483938fb..0fdbdf44d 100644
> --- a/flow.c
> +++ b/flow.c
> @@ -292,10 +292,7 @@ void convert_instruction_target(struct instruction *insn, pseudo_t src)
>  	if (target == src)
>  		return;
>  	FOR_EACH_PTR(target->users, pu) {
> -		if (*pu->userp != VOID) {
> -			assert(*pu->userp == target);
> -			*pu->userp = src;
> -		}
> +		*pu->userp = src;
>  	} END_FOR_EACH_PTR(pu);
>  	if (has_use_list(src))
>  		concat_user_list(target->users, &src->users);
> 

ATB,
Ramsay Jones

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH 5/5] no VOID test in convert_instruction_target()
Date: Wed, 25 Jul 2018 08:30:02 +0000
Message-ID: <20180725083000.qqr5ugyregby5psp () ltop ! local>
--------------------
On Wed, Jul 25, 2018 at 12:28:32AM +0100, Ramsay Jones wrote:
> 
> 
> On 24/07/18 21:34, Luc Van Oostenryck wrote:
> > In convert_instruction_target(), when replacing the pseudo
> > in the target user list, it's first checked if the old pseudo
> > is not VOID and nothing is done otherwise. But this test is
> > not needed because:
> 
> What happened to 1)? ;-)

Good question ;)
 
-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 5/5] simplify 'x != 0' or 'x == 1' to 'x' ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 5/5] simplify 'x != 0' or 'x == 1' to 'x'
Date: Wed, 25 Jul 2018 20:38:28 +0000
Message-ID: <20180725203828.91410-6-luc.vanoostenryck () gmail ! com>
--------------------
These two comparisons are no-ops when the operand is boolean.
Simplify them away.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c                         |  8 +++++
 validation/optim/bool-context-fp.c |  6 ++--
 validation/optim/bool-simplify2.c  | 50 ++++++++++--------------------
 3 files changed, 27 insertions(+), 37 deletions(-)

diff --git a/simplify.c b/simplify.c
index db2fba7ff..412bfb6a9 100644
--- a/simplify.c
+++ b/simplify.c
@@ -591,6 +591,14 @@ static int simplify_seteq_setne(struct instruction *insn, long long value)
 		return 0;
 
 	inverse = (insn->opcode == OP_SET_NE) == value;
+	if (!inverse && def->size == 1) {
+		// Replace:
+		//	setne   %r <- %s, $0
+		// or:
+		//	seteq   %r <- %s, $1
+		// by %s when boolean
+		return replace_with_pseudo(insn, old);
+	}
 	opcode = def->opcode;
 	switch (opcode) {
 	case OP_FPCMP ... OP_BINCMP_END:
diff --git a/validation/optim/bool-context-fp.c b/validation/optim/bool-context-fp.c
index 50e968251..6325ee365 100644
--- a/validation/optim/bool-context-fp.c
+++ b/validation/optim/bool-context-fp.c
@@ -54,8 +54,7 @@ bfior:
 	fcmpune.1   %r20 <- %arg1, %r19
 	fcmpune.1   %r23 <- %arg2, %r19
 	or.1        %r24 <- %r20, %r23
-	setne.1     %r26 <- %r24, $0
-	ret.1       %r26
+	ret.1       %r24
 
 
 ifior:
@@ -76,8 +75,7 @@ bfand:
 	fcmpune.1   %r39 <- %arg1, %r38
 	fcmpune.1   %r42 <- %arg2, %r38
 	and.1       %r43 <- %r39, %r42
-	setne.1     %r45 <- %r43, $0
-	ret.1       %r45
+	ret.1       %r43
 
 
 ifand:
diff --git a/validation/optim/bool-simplify2.c b/validation/optim/bool-simplify2.c
index c94b44123..a089fe625 100644
--- a/validation/optim/bool-simplify2.c
+++ b/validation/optim/bool-simplify2.c
@@ -27,7 +27,7 @@ static bool babbb(bool a, bool b, bool c) { return a && b && c; }
  * check-name: bool-simplify2
  * check-command: test-linearize $file
  *
- * check-output-pattern(36): setne\\.
+ * check-output-pattern(20): setne\\.
  * check-output-pattern(4):  seteq\\.
  * check-output-pattern(8): zext\\.
  * check-output-pattern(12): and
@@ -75,8 +75,7 @@ boii:
 	setne.1     %r23 <- %arg1, $0
 	setne.1     %r25 <- %arg2, $0
 	or.1        %r26 <- %r23, %r25
-	setne.1     %r28 <- %r26, $0
-	ret.1       %r28
+	ret.1       %r26
 
 
 baii:
@@ -85,8 +84,7 @@ baii:
 	setne.1     %r31 <- %arg1, $0
 	setne.1     %r33 <- %arg2, $0
 	and.1       %r34 <- %r31, %r33
-	setne.1     %r36 <- %r34, $0
-	ret.1       %r36
+	ret.1       %r34
 
 
 ioiii:
@@ -95,9 +93,8 @@ ioiii:
 	setne.1     %r39 <- %arg1, $0
 	setne.1     %r41 <- %arg2, $0
 	or.1        %r42 <- %r39, %r41
-	setne.1     %r44 <- %r42, $0
 	setne.1     %r46 <- %arg3, $0
-	or.1        %r47 <- %r44, %r46
+	or.1        %r47 <- %r42, %r46
 	zext.32     %r48 <- (1) %r47
 	ret.32      %r48
 
@@ -108,9 +105,8 @@ iaiii:
 	setne.1     %r51 <- %arg1, $0
 	setne.1     %r53 <- %arg2, $0
 	and.1       %r54 <- %r51, %r53
-	setne.1     %r56 <- %r54, $0
 	setne.1     %r58 <- %arg3, $0
-	and.1       %r59 <- %r56, %r58
+	and.1       %r59 <- %r54, %r58
 	zext.32     %r60 <- (1) %r59
 	ret.32      %r60
 
@@ -121,11 +117,9 @@ boiii:
 	setne.1     %r63 <- %arg1, $0
 	setne.1     %r65 <- %arg2, $0
 	or.1        %r66 <- %r63, %r65
-	setne.1     %r68 <- %r66, $0
 	setne.1     %r70 <- %arg3, $0
-	or.1        %r71 <- %r68, %r70
-	setne.1     %r73 <- %r71, $0
-	ret.1       %r73
+	or.1        %r71 <- %r66, %r70
+	ret.1       %r71
 
 
 baiii:
@@ -134,11 +128,9 @@ baiii:
 	setne.1     %r76 <- %arg1, $0
 	setne.1     %r78 <- %arg2, $0
 	and.1       %r79 <- %r76, %r78
-	setne.1     %r81 <- %r79, $0
 	setne.1     %r83 <- %arg3, $0
-	and.1       %r84 <- %r81, %r83
-	setne.1     %r86 <- %r84, $0
-	ret.1       %r86
+	and.1       %r84 <- %r79, %r83
+	ret.1       %r84
 
 
 inb:
@@ -175,24 +167,21 @@ bobb:
 .L28:
 	<entry-point>
 	or.1        %r107 <- %arg1, %arg2
-	setne.1     %r109 <- %r107, $0
-	ret.1       %r109
+	ret.1       %r107
 
 
 babb:
 .L30:
 	<entry-point>
 	and.1       %r113 <- %arg1, %arg2
-	setne.1     %r115 <- %r113, $0
-	ret.1       %r115
+	ret.1       %r113
 
 
 iobbb:
 .L32:
 	<entry-point>
 	or.1        %r119 <- %arg1, %arg2
-	setne.1     %r121 <- %r119, $0
-	or.1        %r123 <- %r121, %arg3
+	or.1        %r123 <- %r119, %arg3
 	zext.32     %r124 <- (1) %r123
 	ret.32      %r124
 
@@ -201,8 +190,7 @@ iabbb:
 .L34:
 	<entry-point>
 	and.1       %r128 <- %arg1, %arg2
-	setne.1     %r130 <- %r128, $0
-	and.1       %r132 <- %r130, %arg3
+	and.1       %r132 <- %r128, %arg3
 	zext.32     %r133 <- (1) %r132
 	ret.32      %r133
 
@@ -211,20 +199,16 @@ bobbb:
 .L36:
 	<entry-point>
 	or.1        %r137 <- %arg1, %arg2
-	setne.1     %r139 <- %r137, $0
-	or.1        %r141 <- %r139, %arg3
-	setne.1     %r143 <- %r141, $0
-	ret.1       %r143
+	or.1        %r141 <- %r137, %arg3
+	ret.1       %r141
 
 
 babbb:
 .L38:
 	<entry-point>
 	and.1       %r147 <- %arg1, %arg2
-	setne.1     %r149 <- %r147, $0
-	and.1       %r151 <- %r149, %arg3
-	setne.1     %r153 <- %r151, $0
-	ret.1       %r153
+	and.1       %r151 <- %r147, %arg3
+	ret.1       %r151
 
 
  * check-output-end
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 5/6] context: stricter syntax for __context__ statement ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 5/6] context: stricter syntax for __context__ statement
Date: Sat, 26 May 2018 15:14:26 +0000
Message-ID: <20180526151427.30789-6-luc.vanoostenryck () gmail ! com>
--------------------
The expected syntax for the __context__ statement is:
	__context__(<expression>);
or
	__context__(<context>, <expression>);
but originally it was just:
	__context__ <expression>
In other words the parenthesis were not needed and are
still not needed when no context is given.

One problem with the current way to parse these statements is
that very few validation is done. For example, code like:
	__context__;
is silently accepted, as is:
	__context__ a, b;
which is of course not the same as:
	__context__(a,b);
And code like:
	__context__(,1);
results in a confusing error message:
	error: an expression is expected before ')'
	error: Expected ) in expression
	error: got ,

So, given that:
* the kernel has always used the syntax with parenthesis,
* the two arguments form requires the parenthesis and thus
  a function-like syntax
use a more direct, robust and simpl parsing which enforce
the function-like syntax for both forms.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 parse.c                   | 18 ++++++++----------
 validation/context-stmt.c | 25 ++++++++++++++++++++++---
 2 files changed, 30 insertions(+), 13 deletions(-)

diff --git a/parse.c b/parse.c
index 42b3fd20a..cdf034dea 100644
--- a/parse.c
+++ b/parse.c
@@ -2338,17 +2338,15 @@ static struct token *parse_goto_statement(struct token *token, struct statement
 static struct token *parse_context_statement(struct token *token, struct statement *stmt)
 {
 	stmt->type = STMT_CONTEXT;
-	token = parse_expression(token->next, &stmt->expression);
-	if (stmt->expression
-	    && stmt->expression->type == EXPR_PREOP
-	    && stmt->expression->op == '('
-	    && stmt->expression->unop
-	    && stmt->expression->unop->type == EXPR_COMMA) {
-		struct expression *expr;
-		expr = stmt->expression->unop;
-		stmt->context = expr->left;
-		stmt->expression = expr->right;
+	token = token->next;
+	token = expect(token, '(', "after __context__ statement");
+	token = assignment_expression(token, &stmt->expression);
+	if (match_op(token, ',')) {
+		token = token->next;
+		stmt->context = stmt->expression;
+		token = assignment_expression(token, &stmt->expression);
 	}
+	token = expect(token, ')', "at end of __context__ statement");
 	return expect(token, ';', "at end of statement");
 }
 
diff --git a/validation/context-stmt.c b/validation/context-stmt.c
index 1f02c3a67..8cea6b5f2 100644
--- a/validation/context-stmt.c
+++ b/validation/context-stmt.c
@@ -9,6 +9,13 @@ static void foo(int x)
 
 	__context__;		// KO: no expression at all
 	__context__(;		// KO: no expression at all
+
+	__context__ 0;		// KO: need parens
+	__context__ x, 0;	// KO: need parens
+	__context__(x, 0;	// KO: unmatched parens
+	__context__ x, 0);	// KO: unmatched parens
+	__context__(0;		// KO: unmatched parens
+	__context__ 0);		// KO: unmatched parens
 }
 
 /*
@@ -16,11 +23,23 @@ static void foo(int x)
  * check-command: sparse -Wno-context $file
  *
  * check-error-start
-context-stmt.c:11:21: error: an expression is expected before ')'
-context-stmt.c:11:21: error: Expected ) in expression
+context-stmt.c:10:20: error: Expected ( after __context__ statement
+context-stmt.c:10:20: error: got ;
+context-stmt.c:11:21: error: Expected ) at end of __context__ statement
 context-stmt.c:11:21: error: got ;
+context-stmt.c:13:21: error: Expected ( after __context__ statement
+context-stmt.c:13:21: error: got 0
+context-stmt.c:14:21: error: Expected ( after __context__ statement
+context-stmt.c:14:21: error: got x
+context-stmt.c:15:25: error: Expected ) at end of __context__ statement
+context-stmt.c:15:25: error: got ;
+context-stmt.c:16:21: error: Expected ( after __context__ statement
+context-stmt.c:16:21: error: got x
+context-stmt.c:17:22: error: Expected ) at end of __context__ statement
+context-stmt.c:17:22: error: got ;
+context-stmt.c:18:21: error: Expected ( after __context__ statement
+context-stmt.c:18:21: error: got 0
 context-stmt.c:7:21: error: bad constant expression
 context-stmt.c:8:23: error: bad constant expression
-context-stmt.c:11:20: error: bad constant expression type
  * check-error-end
  */
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 5/6] move inner optimization loop into optimize.c ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 5/6] move inner optimization loop into optimize.c
Date: Sun, 18 Feb 2018 16:00:23 +0000
Message-ID: <20180218160024.58173-6-luc.vanoostenryck () gmail ! com>
--------------------
The code for the inner optimization loop was in the same file
than the one for CSE. Now that the CSE have a well defined
interface, we can move this inner loop together with
the main optimization loop in optimize.c

This move make the code better structured and make it easier
to understand the optimization logic and make any experiment
or needed changes to it.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 cse.c      | 41 +++--------------------------------------
 cse.h      | 11 +++++++++++
 flow.h     |  1 -
 optimize.c | 37 +++++++++++++++++++++++++++++++++++++
 4 files changed, 51 insertions(+), 39 deletions(-)
 create mode 100644 cse.h

diff --git a/cse.c b/cse.c
index 6469359f1..4a2675e75 100644
--- a/cse.c
+++ b/cse.c
@@ -16,12 +16,11 @@
 #include "expression.h"
 #include "linearize.h"
 #include "flow.h"
+#include "cse.h"
 
 #define INSN_HASH_SIZE 256
 static struct instruction_list *insn_hash_table[INSN_HASH_SIZE];
 
-int repeat_phase;
-
 static int phi_compare(pseudo_t phi1, pseudo_t phi2)
 {
 	const struct instruction *def1 = phi1->def;
@@ -35,7 +34,7 @@ static int phi_compare(pseudo_t phi1, pseudo_t phi2)
 }
 
 
-static void cse_collect(struct instruction *insn)
+void cse_collect(struct instruction *insn)
 {
 	unsigned long hash;
 
@@ -131,22 +130,6 @@ static void cse_collect(struct instruction *insn)
 	add_instruction(insn_hash_table + hash, insn);
 }
 
-static void clean_up_insns(struct entrypoint *ep)
-{
-	struct basic_block *bb;
-
-	FOR_EACH_PTR(ep->bbs, bb) {
-		struct instruction *insn;
-		FOR_EACH_PTR(bb->insns, insn) {
-			repeat_phase |= simplify_instruction(insn);
-			if (!insn->bb)
-				continue;
-			assert(insn->bb == bb);
-			cse_collect(insn);
-		} END_FOR_EACH_PTR(insn);
-	} END_FOR_EACH_PTR(bb);
-}
-
 /* Compare two (sorted) phi-lists */
 static int phi_list_compare(struct pseudo_list *l1, struct pseudo_list *l2)
 {
@@ -381,7 +364,7 @@ static struct instruction * try_to_cse(struct entrypoint *ep, struct instruction
 	return i1;
 }
 
-static void cse_eliminate(struct entrypoint *ep)
+void cse_eliminate(struct entrypoint *ep)
 {
 	int i;
 
@@ -408,21 +391,3 @@ static void cse_eliminate(struct entrypoint *ep)
 		}
 	}
 }
-
-void cleanup_and_cse(struct entrypoint *ep)
-{
-	simplify_memops(ep);
-repeat:
-	repeat_phase = 0;
-	clean_up_insns(ep);
-	if (repeat_phase & REPEAT_CFG_CLEANUP)
-		kill_unreachable_bbs(ep);
-
-	cse_eliminate(ep);
-
-	if (repeat_phase & REPEAT_SYMBOL_CLEANUP)
-		simplify_memops(ep);
-
-	if (repeat_phase & REPEAT_CSE)
-		goto repeat;
-}
diff --git a/cse.h b/cse.h
new file mode 100644
index 000000000..29c97ea9d
--- /dev/null
+++ b/cse.h
@@ -0,0 +1,11 @@
+#ifndef CSE_H
+#define CSE_H
+
+struct instruction;
+struct entrypoint;
+
+/* cse.c */
+void cse_collect(struct instruction *insn);
+void cse_eliminate(struct entrypoint *ep);
+
+#endif
diff --git a/flow.h b/flow.h
index 8e96f62fb..611aed7af 100644
--- a/flow.h
+++ b/flow.h
@@ -19,7 +19,6 @@ extern void simplify_memops(struct entrypoint *ep);
 extern void pack_basic_blocks(struct entrypoint *ep);
 
 extern void convert_instruction_target(struct instruction *insn, pseudo_t src);
-extern void cleanup_and_cse(struct entrypoint *ep);
 extern int simplify_instruction(struct instruction *);
 
 extern void kill_bb(struct basic_block *);
diff --git a/optimize.c b/optimize.c
index 8cf243517..6435d0c87 100644
--- a/optimize.c
+++ b/optimize.c
@@ -8,7 +8,9 @@
 #include "optimize.h"
 #include "linearize.h"
 #include "flow.h"
+#include "cse.h"
 
+int repeat_phase;
 
 static void clear_symbol_pseudos(struct entrypoint *ep)
 {
@@ -19,6 +21,41 @@ static void clear_symbol_pseudos(struct entrypoint *ep)
 	} END_FOR_EACH_PTR(pseudo);
 }
 
+
+static void clean_up_insns(struct entrypoint *ep)
+{
+	struct basic_block *bb;
+
+	FOR_EACH_PTR(ep->bbs, bb) {
+		struct instruction *insn;
+		FOR_EACH_PTR(bb->insns, insn) {
+			repeat_phase |= simplify_instruction(insn);
+			if (!insn->bb)
+				continue;
+			assert(insn->bb == bb);
+			cse_collect(insn);
+		} END_FOR_EACH_PTR(insn);
+	} END_FOR_EACH_PTR(bb);
+}
+
+static void cleanup_and_cse(struct entrypoint *ep)
+{
+	simplify_memops(ep);
+repeat:
+	repeat_phase = 0;
+	clean_up_insns(ep);
+	if (repeat_phase & REPEAT_CFG_CLEANUP)
+		kill_unreachable_bbs(ep);
+
+	cse_eliminate(ep);
+
+	if (repeat_phase & REPEAT_SYMBOL_CLEANUP)
+		simplify_memops(ep);
+
+	if (repeat_phase & REPEAT_CSE)
+		goto repeat;
+}
+
 void optimize(struct entrypoint *ep)
 {
 	if (fdump_ir & PASS_LINEARIZE)
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 6/6] context: extra warning for __context__() & friends ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 6/6] context: extra warning for __context__() & friends
Date: Sat, 26 May 2018 15:14:27 +0000
Message-ID: <20180526151427.30789-7-luc.vanoostenryck () gmail ! com>
--------------------
Statements with an empty expression, like:
	__context__();
or
	__context__(x,);
are silently accepted. Worse, since NULL expressions are usually
ignored because it is assumed they have already been properly
diagnosticated, no warnings of any kind are given at some later
stage.

Fix this by explicitly checking after empty expressions and
emit an error message if needed.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 parse.c                   |  4 ++++
 validation/context-stmt.c | 17 +++++++++++++++++
 2 files changed, 21 insertions(+)

diff --git a/parse.c b/parse.c
index cdf034dea..b4a7fef8a 100644
--- a/parse.c
+++ b/parse.c
@@ -2341,10 +2341,14 @@ static struct token *parse_context_statement(struct token *token, struct stateme
 	token = token->next;
 	token = expect(token, '(', "after __context__ statement");
 	token = assignment_expression(token, &stmt->expression);
+	if (!stmt->expression)
+		unexpected(token, "expression expected after '('");
 	if (match_op(token, ',')) {
 		token = token->next;
 		stmt->context = stmt->expression;
 		token = assignment_expression(token, &stmt->expression);
+		if (!stmt->expression)
+			unexpected(token, "expression expected after ','");
 	}
 	token = expect(token, ')', "at end of __context__ statement");
 	return expect(token, ';', "at end of statement");
diff --git a/validation/context-stmt.c b/validation/context-stmt.c
index 8cea6b5f2..2884a8a21 100644
--- a/validation/context-stmt.c
+++ b/validation/context-stmt.c
@@ -16,6 +16,11 @@ static void foo(int x)
 	__context__ x, 0);	// KO: unmatched parens
 	__context__(0;		// KO: unmatched parens
 	__context__ 0);		// KO: unmatched parens
+
+	__context__();		// KO: no expression at all
+	__context__(,0);	// KO: no expression at all
+	__context__(x,);	// KO: no expression at all
+	__context__(,);		// KO: no expression at all
 }
 
 /*
@@ -25,6 +30,8 @@ static void foo(int x)
  * check-error-start
 context-stmt.c:10:20: error: Expected ( after __context__ statement
 context-stmt.c:10:20: error: got ;
+context-stmt.c:11:21: error: expression expected after '('
+context-stmt.c:11:21: error: got ;
 context-stmt.c:11:21: error: Expected ) at end of __context__ statement
 context-stmt.c:11:21: error: got ;
 context-stmt.c:13:21: error: Expected ( after __context__ statement
@@ -39,6 +46,16 @@ context-stmt.c:17:22: error: Expected ) at end of __context__ statement
 context-stmt.c:17:22: error: got ;
 context-stmt.c:18:21: error: Expected ( after __context__ statement
 context-stmt.c:18:21: error: got 0
+context-stmt.c:20:21: error: expression expected after '('
+context-stmt.c:20:21: error: got )
+context-stmt.c:21:21: error: expression expected after '('
+context-stmt.c:21:21: error: got ,
+context-stmt.c:22:23: error: expression expected after ','
+context-stmt.c:22:23: error: got )
+context-stmt.c:23:21: error: expression expected after '('
+context-stmt.c:23:21: error: got ,
+context-stmt.c:23:22: error: expression expected after ','
+context-stmt.c:23:22: error: got )
 context-stmt.c:7:21: error: bad constant expression
 context-stmt.c:8:23: error: bad constant expression
  * check-error-end
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 6/6] move the inner optimization loop into the main loop ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 6/6] move the inner optimization loop into the main loop
Date: Sun, 18 Feb 2018 16:00:24 +0000
Message-ID: <20180218160024.58173-7-luc.vanoostenryck () gmail ! com>
--------------------
Moving the inner optimization loop into the main one
help to see the real structure of the optimization logic
and facilitate experiments there and any needed changes.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 optimize.c | 31 ++++++++++++-------------------
 1 file changed, 12 insertions(+), 19 deletions(-)

diff --git a/optimize.c b/optimize.c
index 6435d0c87..127a0c42b 100644
--- a/optimize.c
+++ b/optimize.c
@@ -38,24 +38,6 @@ static void clean_up_insns(struct entrypoint *ep)
 	} END_FOR_EACH_PTR(bb);
 }
 
-static void cleanup_and_cse(struct entrypoint *ep)
-{
-	simplify_memops(ep);
-repeat:
-	repeat_phase = 0;
-	clean_up_insns(ep);
-	if (repeat_phase & REPEAT_CFG_CLEANUP)
-		kill_unreachable_bbs(ep);
-
-	cse_eliminate(ep);
-
-	if (repeat_phase & REPEAT_SYMBOL_CLEANUP)
-		simplify_memops(ep);
-
-	if (repeat_phase & REPEAT_CSE)
-		goto repeat;
-}
-
 void optimize(struct entrypoint *ep)
 {
 	if (fdump_ir & PASS_LINEARIZE)
@@ -83,7 +65,18 @@ repeat:
 	 * the rest.
 	 */
 	do {
-		cleanup_and_cse(ep);
+		simplify_memops(ep);
+		do {
+			repeat_phase = 0;
+			clean_up_insns(ep);
+			if (repeat_phase & REPEAT_CFG_CLEANUP)
+				kill_unreachable_bbs(ep);
+
+			cse_eliminate(ep);
+
+			if (repeat_phase & REPEAT_SYMBOL_CLEANUP)
+				simplify_memops(ep);
+		} while (repeat_phase & REPEAT_CSE);
 		pack_basic_blocks(ep);
 	} while (repeat_phase & REPEAT_CSE);
 
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH 6/8] trivial-phi: use a temp var for the real source ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 6/8] trivial-phi: use a temp var for the real source
Date: Thu, 30 Aug 2018 22:26:14 +0000
Message-ID: <20180830222616.47360-7-luc.vanoostenryck () gmail ! com>
--------------------
By design, all operands of a phi-node are defined by a OP_PHISRC.
So, this phi-source need to be dereferenced to get the real source.

Since this value is used in several tests, use a temoparary variable
for it.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c | 9 +++++++--
 1 file changed, 7 insertions(+), 2 deletions(-)

diff --git a/simplify.c b/simplify.c
index 5413e1dd3..9a2f6c50d 100644
--- a/simplify.c
+++ b/simplify.c
@@ -191,16 +191,21 @@ static int trivial_phi(pseudo_t *pseudo, struct instruction *insn)
 	same = 1;
 	FOR_EACH_PTR(insn->phi_list, phi) {
 		struct instruction *def;
+		pseudo_t src;
+
 		if (phi == VOID)
 			continue;
 		def = phi->def;
-		if (def->phi_src == VOID || !def->bb)
+		if (!def->bb)
+			continue;
+		src = def->phi_src; // bypass OP_PHISRC & get the real source
+		if (src == VOID)
 			continue;
 		if (!last) {
 			last = def;
 			continue;
 		}
-		if (last->phi_src == def->phi_src)
+		if (last->phi_src == src)
 			continue;
 		return 0;
 	} END_FOR_EACH_PTR(phi);
-- 
2.18.0

================================================================================


################################################################################

=== Thread: [PATCH 7/8] trivial-phi: directly return the unique value ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 7/8] trivial-phi: directly return the unique value
Date: Thu, 30 Aug 2018 22:26:15 +0000
Message-ID: <20180830222616.47360-8-luc.vanoostenryck () gmail ! com>
--------------------
In trivial_phi(), the fact that the phi-node is trivial or not
is returned as an int and, if trivial, the unique value is
returned via the pointer given as first argument.

But these two results can easily be combined in a single one
by returning the unique value if trivial and NULL otherwise.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c | 26 +++++++++-----------------
 1 file changed, 9 insertions(+), 17 deletions(-)

diff --git a/simplify.c b/simplify.c
index 9a2f6c50d..1fa46548a 100644
--- a/simplify.c
+++ b/simplify.c
@@ -175,20 +175,16 @@ static int if_convert_phi(struct instruction *insn)
 ///
 // detect trivial phi-nodes
 // @insn: the phi-node
-// @pseudo: a pointer to the resulting pseudo
-// @return: 1 if the phi-node is trivial, 0 otherwise
+// @pseudo: the candidate resulting pseudo (NULL when starting)
+// @return: the unique result if the phi-node is trivial, NULL otherwise
 //
 // A phi-node is trivial if it has a single possible result:
 //	# all operands are the same
 // Since the result is unique, these phi-nodes can be removed.
-static int trivial_phi(pseudo_t *pseudo, struct instruction *insn)
+static pseudo_t trivial_phi(pseudo_t pseudo, struct instruction *insn)
 {
 	pseudo_t phi;
-	struct instruction *last;
-	int same;
 
-	last = NULL;
-	same = 1;
 	FOR_EACH_PTR(insn->phi_list, phi) {
 		struct instruction *def;
 		pseudo_t src;
@@ -201,27 +197,23 @@ static int trivial_phi(pseudo_t *pseudo, struct instruction *insn)
 		src = def->phi_src; // bypass OP_PHISRC & get the real source
 		if (src == VOID)
 			continue;
-		if (!last) {
-			last = def;
+		if (!pseudo) {
+			pseudo = src;
 			continue;
 		}
-		if (last->phi_src == src)
+		if (src == pseudo)
 			continue;
-		return 0;
+		return NULL;
 	} END_FOR_EACH_PTR(phi);
 
-	if (same)
-		*pseudo = last ? last->phi_src : NULL;
-	return same;
+	return pseudo ? pseudo : VOID;
 }
 
 static int clean_up_phi(struct instruction *insn)
 {
 	pseudo_t pseudo;
 
-	if (trivial_phi(&pseudo, insn)) {
-		if (!pseudo)
-			pseudo = VOID;
+	if ((pseudo = trivial_phi(NULL, insn))) {
 		convert_instruction_target(insn, pseudo);
 		kill_instruction(insn);
 		return REPEAT_CSE;
-- 
2.18.0

================================================================================


################################################################################

=== Thread: [PATCH 8/8] trivial-phi: remove more complex trivial phi-nodes ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH 8/8] trivial-phi: remove more complex trivial phi-nodes
Date: Thu, 30 Aug 2018 22:26:16 +0000
Message-ID: <20180830222616.47360-9-luc.vanoostenryck () gmail ! com>
--------------------
In a set of related phi-nodes and phi-sources if all phi-sources
but one correspond to the target of one of the phi-sources, then
no phi-nodes is needed and all %phis can be replaced by the unique
source.
For example, code like:
	int test(void);
	int foo(int a)
	{
		while (test())
			a ^= 0;
		return a;
	}

used to produce an IR with a phi-node for 'a', like:
	foo:
		phisrc.32   %phi2(a) <- %arg1
		br          .L4
	.L4:
		phi.32      %r7(a) <- %phi2(a), %phi3(a)
		call.32     %r1 <- test
		cbr         %r1, .L2, .L5
	.L2:
		phisrc.32   %phi3(a) <- %r7(a)
		br          .L4
	.L5:
		ret.32      %r7(a)

but since 'a ^= 0' is a no-op, the value of 'a' is in fact
never mofified. This can be seen in the phi-node where its
second operand (%phi3) is the same as its target (%r7). So
the only possible value for 'a' is the one from the first
operand, its initial value (%arg1).

Once this trivial phi-nodes is removed, the IR is the expected:
	foo:
		br          .L4
	.L4:
		call.32     %r1 <- test
		cbr         %r1, .L4, .L5
	.L5:
		ret.32      %arg1

Removing these trivial phi-nodes will usually trigger other
simplifications, especially those concerning the CFG.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c                      | 19 +++++++++++++++++--
 validation/optim/trivial-phis.c |  1 -
 2 files changed, 17 insertions(+), 3 deletions(-)

diff --git a/simplify.c b/simplify.c
index 1fa46548a..696d9d587 100644
--- a/simplify.c
+++ b/simplify.c
@@ -180,11 +180,17 @@ static int if_convert_phi(struct instruction *insn)
 //
 // A phi-node is trivial if it has a single possible result:
 //	# all operands are the same
+//	# the operands are themselves defined by a chain or cycle of phi-nodes
+//		and the set of all operands involved contains a single value
+//		not defined by these phi-nodes
 // Since the result is unique, these phi-nodes can be removed.
-static pseudo_t trivial_phi(pseudo_t pseudo, struct instruction *insn)
+static pseudo_t trivial_phi(pseudo_t pseudo, struct instruction *insn, struct pseudo_list **list)
 {
+	pseudo_t target = insn->target;
 	pseudo_t phi;
 
+	add_pseudo(list, target);
+
 	FOR_EACH_PTR(insn->phi_list, phi) {
 		struct instruction *def;
 		pseudo_t src;
@@ -203,6 +209,14 @@ static pseudo_t trivial_phi(pseudo_t pseudo, struct instruction *insn)
 		}
 		if (src == pseudo)
 			continue;
+		if (src == target)
+			continue;
+		if (DEF_OPCODE(def, src) == OP_PHI) {
+			if (pseudo_in_list(*list, src))
+				continue;
+			if ((pseudo = trivial_phi(pseudo, def, list)))
+				continue;
+		}
 		return NULL;
 	} END_FOR_EACH_PTR(phi);
 
@@ -211,9 +225,10 @@ static pseudo_t trivial_phi(pseudo_t pseudo, struct instruction *insn)
 
 static int clean_up_phi(struct instruction *insn)
 {
+	struct pseudo_list *list = NULL;
 	pseudo_t pseudo;
 
-	if ((pseudo = trivial_phi(NULL, insn))) {
+	if ((pseudo = trivial_phi(NULL, insn, &list))) {
 		convert_instruction_target(insn, pseudo);
 		kill_instruction(insn);
 		return REPEAT_CSE;
diff --git a/validation/optim/trivial-phis.c b/validation/optim/trivial-phis.c
index 754affb73..8af093c1a 100644
--- a/validation/optim/trivial-phis.c
+++ b/validation/optim/trivial-phis.c
@@ -7,7 +7,6 @@ void foo(int a)
 /*
  * check-name: trivial phis
  * check-command: test-linearize -Wno-decl $file
- * check-known-to-fail
  *
  * check-output-ignore
  * check-output-excludes: phi\\.
-- 
2.18.0

================================================================================


################################################################################

=== Thread: [PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-mm
Subject: [PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
Date: Tue, 18 Dec 2018 13:30:33 +0000
Message-ID: <706da77adfceb0c324e824d03b52d58a752577ea.1545139710.git.andreyknvl () google ! com>
--------------------
Instead of changing cache->align to be aligned to KASAN_SHADOW_SCALE_SIZE
in kasan_cache_create() we can reuse the ARCH_SLAB_MINALIGN macro.

Suggested-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/kasan.h | 4 ++++
 mm/kasan/common.c              | 2 --
 2 files changed, 4 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/kasan.h b/arch/arm64/include/asm/kasan.h
index b52aacd2c526..ba26150d578d 100644
--- a/arch/arm64/include/asm/kasan.h
+++ b/arch/arm64/include/asm/kasan.h
@@ -36,6 +36,10 @@
 #define KASAN_SHADOW_OFFSET     (KASAN_SHADOW_END - (1ULL << \
 					(64 - KASAN_SHADOW_SCALE_SHIFT)))
 
+#ifdef CONFIG_KASAN_SW_TAGS
+#define ARCH_SLAB_MINALIGN	(1ULL << KASAN_SHADOW_SCALE_SHIFT)
+#endif
+
 void kasan_init(void);
 void kasan_copy_shadow(pgd_t *pgdir);
 asmlinkage void kasan_early_init(void);
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 03d5d1374ca7..44390392d4c9 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -298,8 +298,6 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 		return;
 	}
 
-	cache->align = round_up(cache->align, KASAN_SHADOW_SCALE_SIZE);
-
 	*flags |= SLAB_KASAN;
 }
 
-- 
2.20.0.405.gbc1bbc6f85-goog

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-arm-kernel
Subject: [PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
Date: Tue, 18 Dec 2018 13:30:33 +0000
Message-ID: <706da77adfceb0c324e824d03b52d58a752577ea.1545139710.git.andreyknvl () google ! com>
--------------------
Instead of changing cache->align to be aligned to KASAN_SHADOW_SCALE_SIZE
in kasan_cache_create() we can reuse the ARCH_SLAB_MINALIGN macro.

Suggested-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/kasan.h | 4 ++++
 mm/kasan/common.c              | 2 --
 2 files changed, 4 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/kasan.h b/arch/arm64/include/asm/kasan.h
index b52aacd2c526..ba26150d578d 100644
--- a/arch/arm64/include/asm/kasan.h
+++ b/arch/arm64/include/asm/kasan.h
@@ -36,6 +36,10 @@
 #define KASAN_SHADOW_OFFSET     (KASAN_SHADOW_END - (1ULL << \
 					(64 - KASAN_SHADOW_SCALE_SHIFT)))
 
+#ifdef CONFIG_KASAN_SW_TAGS
+#define ARCH_SLAB_MINALIGN	(1ULL << KASAN_SHADOW_SCALE_SHIFT)
+#endif
+
 void kasan_init(void);
 void kasan_copy_shadow(pgd_t *pgdir);
 asmlinkage void kasan_early_init(void);
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 03d5d1374ca7..44390392d4c9 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -298,8 +298,6 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 		return;
 	}
 
-	cache->align = round_up(cache->align, KASAN_SHADOW_SCALE_SIZE);
-
 	*flags |= SLAB_KASAN;
 }
 
-- 
2.20.0.405.gbc1bbc6f85-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kernel
Subject: [PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
Date: Tue, 18 Dec 2018 13:30:33 +0000
Message-ID: <706da77adfceb0c324e824d03b52d58a752577ea.1545139710.git.andreyknvl () google ! com>
--------------------
Instead of changing cache->align to be aligned to KASAN_SHADOW_SCALE_SIZE
in kasan_cache_create() we can reuse the ARCH_SLAB_MINALIGN macro.

Suggested-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/kasan.h | 4 ++++
 mm/kasan/common.c              | 2 --
 2 files changed, 4 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/kasan.h b/arch/arm64/include/asm/kasan.h
index b52aacd2c526..ba26150d578d 100644
--- a/arch/arm64/include/asm/kasan.h
+++ b/arch/arm64/include/asm/kasan.h
@@ -36,6 +36,10 @@
 #define KASAN_SHADOW_OFFSET     (KASAN_SHADOW_END - (1ULL << \
 					(64 - KASAN_SHADOW_SCALE_SHIFT)))
 
+#ifdef CONFIG_KASAN_SW_TAGS
+#define ARCH_SLAB_MINALIGN	(1ULL << KASAN_SHADOW_SCALE_SHIFT)
+#endif
+
 void kasan_init(void);
 void kasan_copy_shadow(pgd_t *pgdir);
 asmlinkage void kasan_early_init(void);
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 03d5d1374ca7..44390392d4c9 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -298,8 +298,6 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 		return;
 	}
 
-	cache->align = round_up(cache->align, KASAN_SHADOW_SCALE_SIZE);
-
 	*flags |= SLAB_KASAN;
 }
 
-- 
2.20.0.405.gbc1bbc6f85-goog

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kbuild
Subject: [PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
Date: Tue, 18 Dec 2018 13:30:33 +0000
Message-ID: <706da77adfceb0c324e824d03b52d58a752577ea.1545139710.git.andreyknvl () google ! com>
--------------------
Instead of changing cache->align to be aligned to KASAN_SHADOW_SCALE_SIZE
in kasan_cache_create() we can reuse the ARCH_SLAB_MINALIGN macro.

Suggested-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/kasan.h | 4 ++++
 mm/kasan/common.c              | 2 --
 2 files changed, 4 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/kasan.h b/arch/arm64/include/asm/kasan.h
index b52aacd2c526..ba26150d578d 100644
--- a/arch/arm64/include/asm/kasan.h
+++ b/arch/arm64/include/asm/kasan.h
@@ -36,6 +36,10 @@
 #define KASAN_SHADOW_OFFSET     (KASAN_SHADOW_END - (1ULL << \
 					(64 - KASAN_SHADOW_SCALE_SHIFT)))
 
+#ifdef CONFIG_KASAN_SW_TAGS
+#define ARCH_SLAB_MINALIGN	(1ULL << KASAN_SHADOW_SCALE_SHIFT)
+#endif
+
 void kasan_init(void);
 void kasan_copy_shadow(pgd_t *pgdir);
 asmlinkage void kasan_early_init(void);
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 03d5d1374ca7..44390392d4c9 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -298,8 +298,6 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 		return;
 	}
 
-	cache->align = round_up(cache->align, KASAN_SHADOW_SCALE_SIZE);
-
 	*flags |= SLAB_KASAN;
 }
 
-- 
2.20.0.405.gbc1bbc6f85-goog

================================================================================

From: Andrew Morton <akpm () linux-foundation ! org>
To: linux-arm-kernel
Subject: Re: [PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
Date: Tue, 18 Dec 2018 20:54:53 +0000
Message-ID: <20181218125453.4c5e6c056d31ccaa3a73d4a5 () linux-foundation ! org>
--------------------
On Tue, 18 Dec 2018 14:30:33 +0100 Andrey Konovalov <andreyknvl@google.com> wrote:

> Instead of changing cache->align to be aligned to KASAN_SHADOW_SCALE_SIZE
> in kasan_cache_create() we can reuse the ARCH_SLAB_MINALIGN macro.
> 
> ...
>
> --- a/arch/arm64/include/asm/kasan.h
> +++ b/arch/arm64/include/asm/kasan.h
> @@ -36,6 +36,10 @@
>  #define KASAN_SHADOW_OFFSET     (KASAN_SHADOW_END - (1ULL << \
>  					(64 - KASAN_SHADOW_SCALE_SHIFT)))
>  
> +#ifdef CONFIG_KASAN_SW_TAGS
> +#define ARCH_SLAB_MINALIGN	(1ULL << KASAN_SHADOW_SCALE_SHIFT)
> +#endif
> +
>  void kasan_init(void);
>  void kasan_copy_shadow(pgd_t *pgdir);
>  asmlinkage void kasan_early_init(void);

This looks unreliable.  include/linux/slab.h has

/*
 * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
 * Intended for arches that get misalignment faults even for 64 bit integer
 * aligned buffers.
 */
#ifndef ARCH_SLAB_MINALIGN
#define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
#endif

so if a .c file includes arch/arm64/include/asm/kasan.h after
include/linux/slab.h, it can get a macro-redefined warning.  If the .c
file includes those headers in the other order, ARCH_SLAB_MINALIGN will
get a different value compared to other .c files.

Or something like that.

Different architectures define ARCH_SLAB_MINALIGN in different place:

./arch/microblaze/include/asm/page.h:#define ARCH_SLAB_MINALIGN L1_CACHE_BYTES
./arch/arm/include/asm/cache.h:#define ARCH_SLAB_MINALIGN 8
./arch/sh/include/asm/page.h:#define ARCH_SLAB_MINALIGN 8
./arch/c6x/include/asm/cache.h:#define ARCH_SLAB_MINALIGN       L1_CACHE_BYTES
./arch/sparc/include/asm/cache.h:#define ARCH_SLAB_MINALIGN     __alignof__(unsigned long long)
./arch/xtensa/include/asm/processor.h:#define ARCH_SLAB_MINALIGN STACK_ALIGN

which is rather bad of us.

But still.  I think your definition should occur in an arch header file
which is reliably included from slab.h.  And kasan code should get its
definition of ARCH_SLAB_MINALIGN by including slab.h.


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Andrew Morton <akpm () linux-foundation ! org>
To: linux-doc
Subject: Re: [PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
Date: Tue, 18 Dec 2018 20:54:53 +0000
Message-ID: <20181218125453.4c5e6c056d31ccaa3a73d4a5 () linux-foundation ! org>
--------------------
On Tue, 18 Dec 2018 14:30:33 +0100 Andrey Konovalov <andreyknvl@google.com> wrote:

> Instead of changing cache->align to be aligned to KASAN_SHADOW_SCALE_SIZE
> in kasan_cache_create() we can reuse the ARCH_SLAB_MINALIGN macro.
> 
> ...
>
> --- a/arch/arm64/include/asm/kasan.h
> +++ b/arch/arm64/include/asm/kasan.h
> @@ -36,6 +36,10 @@
>  #define KASAN_SHADOW_OFFSET     (KASAN_SHADOW_END - (1ULL << \
>  					(64 - KASAN_SHADOW_SCALE_SHIFT)))
>  
> +#ifdef CONFIG_KASAN_SW_TAGS
> +#define ARCH_SLAB_MINALIGN	(1ULL << KASAN_SHADOW_SCALE_SHIFT)
> +#endif
> +
>  void kasan_init(void);
>  void kasan_copy_shadow(pgd_t *pgdir);
>  asmlinkage void kasan_early_init(void);

This looks unreliable.  include/linux/slab.h has

/*
 * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
 * Intended for arches that get misalignment faults even for 64 bit integer
 * aligned buffers.
 */
#ifndef ARCH_SLAB_MINALIGN
#define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
#endif

so if a .c file includes arch/arm64/include/asm/kasan.h after
include/linux/slab.h, it can get a macro-redefined warning.  If the .c
file includes those headers in the other order, ARCH_SLAB_MINALIGN will
get a different value compared to other .c files.

Or something like that.

Different architectures define ARCH_SLAB_MINALIGN in different place:

./arch/microblaze/include/asm/page.h:#define ARCH_SLAB_MINALIGN L1_CACHE_BYTES
./arch/arm/include/asm/cache.h:#define ARCH_SLAB_MINALIGN 8
./arch/sh/include/asm/page.h:#define ARCH_SLAB_MINALIGN 8
./arch/c6x/include/asm/cache.h:#define ARCH_SLAB_MINALIGN       L1_CACHE_BYTES
./arch/sparc/include/asm/cache.h:#define ARCH_SLAB_MINALIGN     __alignof__(unsigned long long)
./arch/xtensa/include/asm/processor.h:#define ARCH_SLAB_MINALIGN STACK_ALIGN

which is rather bad of us.

But still.  I think your definition should occur in an arch header file
which is reliably included from slab.h.  And kasan code should get its
definition of ARCH_SLAB_MINALIGN by including slab.h.

================================================================================

From: Andrew Morton <akpm () linux-foundation ! org>
To: linux-sparse
Subject: Re: [PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
Date: Tue, 18 Dec 2018 20:54:53 +0000
Message-ID: <20181218125453.4c5e6c056d31ccaa3a73d4a5 () linux-foundation ! org>
--------------------
On Tue, 18 Dec 2018 14:30:33 +0100 Andrey Konovalov <andreyknvl@google.com> wrote:

> Instead of changing cache->align to be aligned to KASAN_SHADOW_SCALE_SIZE
> in kasan_cache_create() we can reuse the ARCH_SLAB_MINALIGN macro.
> 
> ...
>
> --- a/arch/arm64/include/asm/kasan.h
> +++ b/arch/arm64/include/asm/kasan.h
> @@ -36,6 +36,10 @@
>  #define KASAN_SHADOW_OFFSET     (KASAN_SHADOW_END - (1ULL << \
>  					(64 - KASAN_SHADOW_SCALE_SHIFT)))
>  
> +#ifdef CONFIG_KASAN_SW_TAGS
> +#define ARCH_SLAB_MINALIGN	(1ULL << KASAN_SHADOW_SCALE_SHIFT)
> +#endif
> +
>  void kasan_init(void);
>  void kasan_copy_shadow(pgd_t *pgdir);
>  asmlinkage void kasan_early_init(void);

This looks unreliable.  include/linux/slab.h has

/*
 * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
 * Intended for arches that get misalignment faults even for 64 bit integer
 * aligned buffers.
 */
#ifndef ARCH_SLAB_MINALIGN
#define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
#endif

so if a .c file includes arch/arm64/include/asm/kasan.h after
include/linux/slab.h, it can get a macro-redefined warning.  If the .c
file includes those headers in the other order, ARCH_SLAB_MINALIGN will
get a different value compared to other .c files.

Or something like that.

Different architectures define ARCH_SLAB_MINALIGN in different place:

./arch/microblaze/include/asm/page.h:#define ARCH_SLAB_MINALIGN L1_CACHE_BYTES
./arch/arm/include/asm/cache.h:#define ARCH_SLAB_MINALIGN 8
./arch/sh/include/asm/page.h:#define ARCH_SLAB_MINALIGN 8
./arch/c6x/include/asm/cache.h:#define ARCH_SLAB_MINALIGN       L1_CACHE_BYTES
./arch/sparc/include/asm/cache.h:#define ARCH_SLAB_MINALIGN     __alignof__(unsigned long long)
./arch/xtensa/include/asm/processor.h:#define ARCH_SLAB_MINALIGN STACK_ALIGN

which is rather bad of us.

But still.  I think your definition should occur in an arch header file
which is reliably included from slab.h.  And kasan code should get its
definition of ARCH_SLAB_MINALIGN by including slab.h.

================================================================================

From: Andrew Morton <akpm () linux-foundation ! org>
To: linux-mm
Subject: Re: [PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
Date: Tue, 18 Dec 2018 20:54:53 +0000
Message-ID: <20181218125453.4c5e6c056d31ccaa3a73d4a5 () linux-foundation ! org>
--------------------
On Tue, 18 Dec 2018 14:30:33 +0100 Andrey Konovalov <andreyknvl@google.com> wrote:

> Instead of changing cache->align to be aligned to KASAN_SHADOW_SCALE_SIZE
> in kasan_cache_create() we can reuse the ARCH_SLAB_MINALIGN macro.
> 
> ...
>
> --- a/arch/arm64/include/asm/kasan.h
> +++ b/arch/arm64/include/asm/kasan.h
> @@ -36,6 +36,10 @@
>  #define KASAN_SHADOW_OFFSET     (KASAN_SHADOW_END - (1ULL << \
>  					(64 - KASAN_SHADOW_SCALE_SHIFT)))
>  
> +#ifdef CONFIG_KASAN_SW_TAGS
> +#define ARCH_SLAB_MINALIGN	(1ULL << KASAN_SHADOW_SCALE_SHIFT)
> +#endif
> +
>  void kasan_init(void);
>  void kasan_copy_shadow(pgd_t *pgdir);
>  asmlinkage void kasan_early_init(void);

This looks unreliable.  include/linux/slab.h has

/*
 * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
 * Intended for arches that get misalignment faults even for 64 bit integer
 * aligned buffers.
 */
#ifndef ARCH_SLAB_MINALIGN
#define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
#endif

so if a .c file includes arch/arm64/include/asm/kasan.h after
include/linux/slab.h, it can get a macro-redefined warning.  If the .c
file includes those headers in the other order, ARCH_SLAB_MINALIGN will
get a different value compared to other .c files.

Or something like that.

Different architectures define ARCH_SLAB_MINALIGN in different place:

./arch/microblaze/include/asm/page.h:#define ARCH_SLAB_MINALIGN L1_CACHE_BYTES
./arch/arm/include/asm/cache.h:#define ARCH_SLAB_MINALIGN 8
./arch/sh/include/asm/page.h:#define ARCH_SLAB_MINALIGN 8
./arch/c6x/include/asm/cache.h:#define ARCH_SLAB_MINALIGN       L1_CACHE_BYTES
./arch/sparc/include/asm/cache.h:#define ARCH_SLAB_MINALIGN     __alignof__(unsigned long long)
./arch/xtensa/include/asm/processor.h:#define ARCH_SLAB_MINALIGN STACK_ALIGN

which is rather bad of us.

But still.  I think your definition should occur in an arch header file
which is reliably included from slab.h.  And kasan code should get its
definition of ARCH_SLAB_MINALIGN by including slab.h.

================================================================================

From: Andrew Morton <akpm () linux-foundation ! org>
To: linux-kernel
Subject: Re: [PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
Date: Tue, 18 Dec 2018 20:54:53 +0000
Message-ID: <20181218125453.4c5e6c056d31ccaa3a73d4a5 () linux-foundation ! org>
--------------------
On Tue, 18 Dec 2018 14:30:33 +0100 Andrey Konovalov <andreyknvl@google.com> wrote:

> Instead of changing cache->align to be aligned to KASAN_SHADOW_SCALE_SIZE
> in kasan_cache_create() we can reuse the ARCH_SLAB_MINALIGN macro.
> 
> ...
>
> --- a/arch/arm64/include/asm/kasan.h
> +++ b/arch/arm64/include/asm/kasan.h
> @@ -36,6 +36,10 @@
>  #define KASAN_SHADOW_OFFSET     (KASAN_SHADOW_END - (1ULL << \
>  					(64 - KASAN_SHADOW_SCALE_SHIFT)))
>  
> +#ifdef CONFIG_KASAN_SW_TAGS
> +#define ARCH_SLAB_MINALIGN	(1ULL << KASAN_SHADOW_SCALE_SHIFT)
> +#endif
> +
>  void kasan_init(void);
>  void kasan_copy_shadow(pgd_t *pgdir);
>  asmlinkage void kasan_early_init(void);

This looks unreliable.  include/linux/slab.h has

/*
 * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
 * Intended for arches that get misalignment faults even for 64 bit integer
 * aligned buffers.
 */
#ifndef ARCH_SLAB_MINALIGN
#define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
#endif

so if a .c file includes arch/arm64/include/asm/kasan.h after
include/linux/slab.h, it can get a macro-redefined warning.  If the .c
file includes those headers in the other order, ARCH_SLAB_MINALIGN will
get a different value compared to other .c files.

Or something like that.

Different architectures define ARCH_SLAB_MINALIGN in different place:

./arch/microblaze/include/asm/page.h:#define ARCH_SLAB_MINALIGN L1_CACHE_BYTES
./arch/arm/include/asm/cache.h:#define ARCH_SLAB_MINALIGN 8
./arch/sh/include/asm/page.h:#define ARCH_SLAB_MINALIGN 8
./arch/c6x/include/asm/cache.h:#define ARCH_SLAB_MINALIGN       L1_CACHE_BYTES
./arch/sparc/include/asm/cache.h:#define ARCH_SLAB_MINALIGN     __alignof__(unsigned long long)
./arch/xtensa/include/asm/processor.h:#define ARCH_SLAB_MINALIGN STACK_ALIGN

which is rather bad of us.

But still.  I think your definition should occur in an arch header file
which is reliably included from slab.h.  And kasan code should get its
definition of ARCH_SLAB_MINALIGN by including slab.h.

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-arm-kernel
Subject: Re: [PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
Date: Thu, 20 Dec 2018 13:02:45 +0000
Message-ID: <CAAeHK+yggnKfkycdUdTHG4MvWBMq_XK70m0rQuH873DZU+RnGQ () mail ! gmail ! com>
--------------------
On Tue, Dec 18, 2018 at 9:55 PM Andrew Morton <akpm@linux-foundation.org> wrote:
>
> On Tue, 18 Dec 2018 14:30:33 +0100 Andrey Konovalov <andreyknvl@google.com> wrote:
>
> > Instead of changing cache->align to be aligned to KASAN_SHADOW_SCALE_SIZE
> > in kasan_cache_create() we can reuse the ARCH_SLAB_MINALIGN macro.
> >
> > ...
> >
> > --- a/arch/arm64/include/asm/kasan.h
> > +++ b/arch/arm64/include/asm/kasan.h
> > @@ -36,6 +36,10 @@
> >  #define KASAN_SHADOW_OFFSET     (KASAN_SHADOW_END - (1ULL << \
> >                                       (64 - KASAN_SHADOW_SCALE_SHIFT)))
> >
> > +#ifdef CONFIG_KASAN_SW_TAGS
> > +#define ARCH_SLAB_MINALIGN   (1ULL << KASAN_SHADOW_SCALE_SHIFT)
> > +#endif
> > +
> >  void kasan_init(void);
> >  void kasan_copy_shadow(pgd_t *pgdir);
> >  asmlinkage void kasan_early_init(void);
>
> This looks unreliable.  include/linux/slab.h has
>
> /*
>  * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
>  * Intended for arches that get misalignment faults even for 64 bit integer
>  * aligned buffers.
>  */
> #ifndef ARCH_SLAB_MINALIGN
> #define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
> #endif
>
> so if a .c file includes arch/arm64/include/asm/kasan.h after
> include/linux/slab.h, it can get a macro-redefined warning.  If the .c
> file includes those headers in the other order, ARCH_SLAB_MINALIGN will
> get a different value compared to other .c files.
>
> Or something like that.
>
> Different architectures define ARCH_SLAB_MINALIGN in different place:
>
> ./arch/microblaze/include/asm/page.h:#define ARCH_SLAB_MINALIGN L1_CACHE_BYTES
> ./arch/arm/include/asm/cache.h:#define ARCH_SLAB_MINALIGN 8
> ./arch/sh/include/asm/page.h:#define ARCH_SLAB_MINALIGN 8
> ./arch/c6x/include/asm/cache.h:#define ARCH_SLAB_MINALIGN       L1_CACHE_BYTES
> ./arch/sparc/include/asm/cache.h:#define ARCH_SLAB_MINALIGN     __alignof__(unsigned long long)
> ./arch/xtensa/include/asm/processor.h:#define ARCH_SLAB_MINALIGN STACK_ALIGN
>
> which is rather bad of us.
>
> But still.  I think your definition should occur in an arch header file
> which is reliably included from slab.h.  And kasan code should get its
> definition of ARCH_SLAB_MINALIGN by including slab.h.
>

KASAN code doesn't use this macro directly, so I don't think it needs
to get it's definition.

What do you think about adding #include <linux/kasan.h> into
linux/slab.h? Perhaps with a comment that this is needed to get
definition of ARCH_SLAB_MINALIGN?

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-mm
Subject: Re: [PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
Date: Thu, 20 Dec 2018 13:02:45 +0000
Message-ID: <CAAeHK+yggnKfkycdUdTHG4MvWBMq_XK70m0rQuH873DZU+RnGQ () mail ! gmail ! com>
--------------------
On Tue, Dec 18, 2018 at 9:55 PM Andrew Morton <akpm@linux-foundation.org> wrote:
>
> On Tue, 18 Dec 2018 14:30:33 +0100 Andrey Konovalov <andreyknvl@google.com> wrote:
>
> > Instead of changing cache->align to be aligned to KASAN_SHADOW_SCALE_SIZE
> > in kasan_cache_create() we can reuse the ARCH_SLAB_MINALIGN macro.
> >
> > ...
> >
> > --- a/arch/arm64/include/asm/kasan.h
> > +++ b/arch/arm64/include/asm/kasan.h
> > @@ -36,6 +36,10 @@
> >  #define KASAN_SHADOW_OFFSET     (KASAN_SHADOW_END - (1ULL << \
> >                                       (64 - KASAN_SHADOW_SCALE_SHIFT)))
> >
> > +#ifdef CONFIG_KASAN_SW_TAGS
> > +#define ARCH_SLAB_MINALIGN   (1ULL << KASAN_SHADOW_SCALE_SHIFT)
> > +#endif
> > +
> >  void kasan_init(void);
> >  void kasan_copy_shadow(pgd_t *pgdir);
> >  asmlinkage void kasan_early_init(void);
>
> This looks unreliable.  include/linux/slab.h has
>
> /*
>  * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
>  * Intended for arches that get misalignment faults even for 64 bit integer
>  * aligned buffers.
>  */
> #ifndef ARCH_SLAB_MINALIGN
> #define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
> #endif
>
> so if a .c file includes arch/arm64/include/asm/kasan.h after
> include/linux/slab.h, it can get a macro-redefined warning.  If the .c
> file includes those headers in the other order, ARCH_SLAB_MINALIGN will
> get a different value compared to other .c files.
>
> Or something like that.
>
> Different architectures define ARCH_SLAB_MINALIGN in different place:
>
> ./arch/microblaze/include/asm/page.h:#define ARCH_SLAB_MINALIGN L1_CACHE_BYTES
> ./arch/arm/include/asm/cache.h:#define ARCH_SLAB_MINALIGN 8
> ./arch/sh/include/asm/page.h:#define ARCH_SLAB_MINALIGN 8
> ./arch/c6x/include/asm/cache.h:#define ARCH_SLAB_MINALIGN       L1_CACHE_BYTES
> ./arch/sparc/include/asm/cache.h:#define ARCH_SLAB_MINALIGN     __alignof__(unsigned long long)
> ./arch/xtensa/include/asm/processor.h:#define ARCH_SLAB_MINALIGN STACK_ALIGN
>
> which is rather bad of us.
>
> But still.  I think your definition should occur in an arch header file
> which is reliably included from slab.h.  And kasan code should get its
> definition of ARCH_SLAB_MINALIGN by including slab.h.
>

KASAN code doesn't use this macro directly, so I don't think it needs
to get it's definition.

What do you think about adding #include <linux/kasan.h> into
linux/slab.h? Perhaps with a comment that this is needed to get
definition of ARCH_SLAB_MINALIGN?

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kernel
Subject: Re: [PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
Date: Thu, 20 Dec 2018 13:02:45 +0000
Message-ID: <CAAeHK+yggnKfkycdUdTHG4MvWBMq_XK70m0rQuH873DZU+RnGQ () mail ! gmail ! com>
--------------------
On Tue, Dec 18, 2018 at 9:55 PM Andrew Morton <akpm@linux-foundation.org> wrote:
>
> On Tue, 18 Dec 2018 14:30:33 +0100 Andrey Konovalov <andreyknvl@google.com> wrote:
>
> > Instead of changing cache->align to be aligned to KASAN_SHADOW_SCALE_SIZE
> > in kasan_cache_create() we can reuse the ARCH_SLAB_MINALIGN macro.
> >
> > ...
> >
> > --- a/arch/arm64/include/asm/kasan.h
> > +++ b/arch/arm64/include/asm/kasan.h
> > @@ -36,6 +36,10 @@
> >  #define KASAN_SHADOW_OFFSET     (KASAN_SHADOW_END - (1ULL << \
> >                                       (64 - KASAN_SHADOW_SCALE_SHIFT)))
> >
> > +#ifdef CONFIG_KASAN_SW_TAGS
> > +#define ARCH_SLAB_MINALIGN   (1ULL << KASAN_SHADOW_SCALE_SHIFT)
> > +#endif
> > +
> >  void kasan_init(void);
> >  void kasan_copy_shadow(pgd_t *pgdir);
> >  asmlinkage void kasan_early_init(void);
>
> This looks unreliable.  include/linux/slab.h has
>
> /*
>  * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
>  * Intended for arches that get misalignment faults even for 64 bit integer
>  * aligned buffers.
>  */
> #ifndef ARCH_SLAB_MINALIGN
> #define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
> #endif
>
> so if a .c file includes arch/arm64/include/asm/kasan.h after
> include/linux/slab.h, it can get a macro-redefined warning.  If the .c
> file includes those headers in the other order, ARCH_SLAB_MINALIGN will
> get a different value compared to other .c files.
>
> Or something like that.
>
> Different architectures define ARCH_SLAB_MINALIGN in different place:
>
> ./arch/microblaze/include/asm/page.h:#define ARCH_SLAB_MINALIGN L1_CACHE_BYTES
> ./arch/arm/include/asm/cache.h:#define ARCH_SLAB_MINALIGN 8
> ./arch/sh/include/asm/page.h:#define ARCH_SLAB_MINALIGN 8
> ./arch/c6x/include/asm/cache.h:#define ARCH_SLAB_MINALIGN       L1_CACHE_BYTES
> ./arch/sparc/include/asm/cache.h:#define ARCH_SLAB_MINALIGN     __alignof__(unsigned long long)
> ./arch/xtensa/include/asm/processor.h:#define ARCH_SLAB_MINALIGN STACK_ALIGN
>
> which is rather bad of us.
>
> But still.  I think your definition should occur in an arch header file
> which is reliably included from slab.h.  And kasan code should get its
> definition of ARCH_SLAB_MINALIGN by including slab.h.
>

KASAN code doesn't use this macro directly, so I don't think it needs
to get it's definition.

What do you think about adding #include <linux/kasan.h> into
linux/slab.h? Perhaps with a comment that this is needed to get
definition of ARCH_SLAB_MINALIGN?
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kbuild
Subject: Re: [PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
Date: Thu, 20 Dec 2018 13:02:45 +0000
Message-ID: <CAAeHK+yggnKfkycdUdTHG4MvWBMq_XK70m0rQuH873DZU+RnGQ () mail ! gmail ! com>
--------------------
On Tue, Dec 18, 2018 at 9:55 PM Andrew Morton <akpm@linux-foundation.org> wrote:
>
> On Tue, 18 Dec 2018 14:30:33 +0100 Andrey Konovalov <andreyknvl@google.com> wrote:
>
> > Instead of changing cache->align to be aligned to KASAN_SHADOW_SCALE_SIZE
> > in kasan_cache_create() we can reuse the ARCH_SLAB_MINALIGN macro.
> >
> > ...
> >
> > --- a/arch/arm64/include/asm/kasan.h
> > +++ b/arch/arm64/include/asm/kasan.h
> > @@ -36,6 +36,10 @@
> >  #define KASAN_SHADOW_OFFSET     (KASAN_SHADOW_END - (1ULL << \
> >                                       (64 - KASAN_SHADOW_SCALE_SHIFT)))
> >
> > +#ifdef CONFIG_KASAN_SW_TAGS
> > +#define ARCH_SLAB_MINALIGN   (1ULL << KASAN_SHADOW_SCALE_SHIFT)
> > +#endif
> > +
> >  void kasan_init(void);
> >  void kasan_copy_shadow(pgd_t *pgdir);
> >  asmlinkage void kasan_early_init(void);
>
> This looks unreliable.  include/linux/slab.h has
>
> /*
>  * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
>  * Intended for arches that get misalignment faults even for 64 bit integer
>  * aligned buffers.
>  */
> #ifndef ARCH_SLAB_MINALIGN
> #define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
> #endif
>
> so if a .c file includes arch/arm64/include/asm/kasan.h after
> include/linux/slab.h, it can get a macro-redefined warning.  If the .c
> file includes those headers in the other order, ARCH_SLAB_MINALIGN will
> get a different value compared to other .c files.
>
> Or something like that.
>
> Different architectures define ARCH_SLAB_MINALIGN in different place:
>
> ./arch/microblaze/include/asm/page.h:#define ARCH_SLAB_MINALIGN L1_CACHE_BYTES
> ./arch/arm/include/asm/cache.h:#define ARCH_SLAB_MINALIGN 8
> ./arch/sh/include/asm/page.h:#define ARCH_SLAB_MINALIGN 8
> ./arch/c6x/include/asm/cache.h:#define ARCH_SLAB_MINALIGN       L1_CACHE_BYTES
> ./arch/sparc/include/asm/cache.h:#define ARCH_SLAB_MINALIGN     __alignof__(unsigned long long)
> ./arch/xtensa/include/asm/processor.h:#define ARCH_SLAB_MINALIGN STACK_ALIGN
>
> which is rather bad of us.
>
> But still.  I think your definition should occur in an arch header file
> which is reliably included from slab.h.  And kasan code should get its
> definition of ARCH_SLAB_MINALIGN by including slab.h.
>

KASAN code doesn't use this macro directly, so I don't think it needs
to get it's definition.

What do you think about adding #include <linux/kasan.h> into
linux/slab.h? Perhaps with a comment that this is needed to get
definition of ARCH_SLAB_MINALIGN?
================================================================================


################################################################################

=== Thread: [PATCH sparse] parse: shifting by full number of bits is undefined ===

From: "Jason A. Donenfeld" <Jason () zx2c4 ! com>
To: linux-kernel
Subject: [PATCH sparse] parse: shifting by full number of bits is undefined
Date: Fri, 26 Oct 2018 03:17:00 +0000
Message-ID: <20181026031700.12310-1-Jason () zx2c4 ! com>
--------------------
The type checker wasn't identifying upper bounds for huge unsigned
64-bit numbers, because the right shift turned into a no-op:

zx2c4@thinkpad /tmp $ cat sparse.c
enum { sparse_does_not_like_this = 0x8000000000000003ULL };
zx2c4@thinkpad /tmp $ sparse sparse.c
sparse.c:1:36: warning: cast truncates bits from constant value (8000000000000003 becomes 3)

This works around the issue by detecting when we're going to shift by
the size of the variable and treat that as always zero.

Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
---
 parse.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/parse.c b/parse.c
index 02a55a7..02d0615 100644
--- a/parse.c
+++ b/parse.c
@@ -788,7 +788,7 @@ static int type_is_ok(struct symbol *type, Num *upper, Num *lower)
 
 	if (!is_unsigned)
 		shift--;
-	if (upper->x == 0 && upper->y >> shift)
+	if (upper->x == 0 && (shift < (sizeof(upper->y) << 3)) && upper->y >> shift)
 		return 0;
 	if (lower->x == 0 || (!is_unsigned && (~lower->y >> shift) == 0))
 		return 1;
-- 
2.19.1

================================================================================

From: "Jason A. Donenfeld" <Jason () zx2c4 ! com>
To: linux-sparse
Subject: [PATCH sparse] parse: shifting by full number of bits is undefined
Date: Fri, 26 Oct 2018 03:17:00 +0000
Message-ID: <20181026031700.12310-1-Jason () zx2c4 ! com>
--------------------
The type checker wasn't identifying upper bounds for huge unsigned
64-bit numbers, because the right shift turned into a no-op:

zx2c4@thinkpad /tmp $ cat sparse.c
enum { sparse_does_not_like_this = 0x8000000000000003ULL };
zx2c4@thinkpad /tmp $ sparse sparse.c
sparse.c:1:36: warning: cast truncates bits from constant value (8000000000000003 becomes 3)

This works around the issue by detecting when we're going to shift by
the size of the variable and treat that as always zero.

Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
---
 parse.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/parse.c b/parse.c
index 02a55a7..02d0615 100644
--- a/parse.c
+++ b/parse.c
@@ -788,7 +788,7 @@ static int type_is_ok(struct symbol *type, Num *upper, Num *lower)
 
 	if (!is_unsigned)
 		shift--;
-	if (upper->x == 0 && upper->y >> shift)
+	if (upper->x == 0 && (shift < (sizeof(upper->y) << 3)) && upper->y >> shift)
 		return 0;
 	if (lower->x == 0 || (!is_unsigned && (~lower->y >> shift) == 0))
 		return 1;
-- 
2.19.1

================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-kernel
Subject: Re: [PATCH sparse] parse: shifting by full number of bits is undefined
Date: Fri, 26 Oct 2018 09:01:37 +0000
Message-ID: <20181026090136.2fidq6kicv4szwjd () ltop ! local>
--------------------
On Fri, Oct 26, 2018 at 05:17:00AM +0200, Jason A. Donenfeld wrote:
> The type checker wasn't identifying upper bounds for huge unsigned
> 64-bit numbers, because the right shift turned into a no-op:
> 
> zx2c4@thinkpad /tmp $ cat sparse.c
> enum { sparse_does_not_like_this = 0x8000000000000003ULL };
> zx2c4@thinkpad /tmp $ sparse sparse.c
> sparse.c:1:36: warning: cast truncates bits from constant value (8000000000000003 becomes 3)

Hi Jason,

This is already fixed in the development tree:
   git://github.com/lucvoo/sparse.git
together with some other fixes & changes regarding enums.

I hope to be able to push this to the official tree real soon now.


Kind regards,
-- Luc
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH sparse] parse: shifting by full number of bits is undefined
Date: Fri, 26 Oct 2018 09:01:37 +0000
Message-ID: <20181026090136.2fidq6kicv4szwjd () ltop ! local>
--------------------
On Fri, Oct 26, 2018 at 05:17:00AM +0200, Jason A. Donenfeld wrote:
> The type checker wasn't identifying upper bounds for huge unsigned
> 64-bit numbers, because the right shift turned into a no-op:
> 
> zx2c4@thinkpad /tmp $ cat sparse.c
> enum { sparse_does_not_like_this = 0x8000000000000003ULL };
> zx2c4@thinkpad /tmp $ sparse sparse.c
> sparse.c:1:36: warning: cast truncates bits from constant value (8000000000000003 becomes 3)

Hi Jason,

This is already fixed in the development tree:
   git://github.com/lucvoo/sparse.git
together with some other fixes & changes regarding enums.

I hope to be able to push this to the official tree real soon now.


Kind regards,
-- Luc
================================================================================


################################################################################

=== Thread: [PATCH v1 00/18] SSA conversion, the classical way ===

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: Re: [PATCH v1 00/18] SSA conversion, the classical way
Date: Tue, 20 Mar 2018 21:07:47 +0000
Message-ID: <CACXZuxf7iONUzqm1aV9F7NSpnTs7q=hZq9fRr=6OVbRrJ6cvYg () mail ! gmail ! com>
--------------------
Hi Luc,

On 20 March 2018 at 00:52, Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
> This series replace the current SSA conversion by one using
> the 'classical' way, via the iterated dominance frontier.
>

Thank you for posting this patch series. I would like to test it. I
will apply my LLVM fixes against your tree so that I can run the set
of tests I have prepared. This will take a bit of time but I will
report back here on results.

Regards
Dibyendu
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v1 00/18] SSA conversion, the classical way
Date: Tue, 20 Mar 2018 21:49:12 +0000
Message-ID: <20180320214910.h2tl4iqf6o66nihx () ltop ! local>
--------------------
On Tue, Mar 20, 2018 at 09:07:47PM +0000, Dibyendu Majumdar wrote:
> Hi Luc,
> 
> On 20 March 2018 at 00:52, Luc Van Oostenryck
> <luc.vanoostenryck@gmail.com> wrote:
> > This series replace the current SSA conversion by one using
> > the 'classical' way, via the iterated dominance frontier.
> >
> 
> Thank you for posting this patch series. I would like to test it. I
> will apply my LLVM fixes against your tree so that I can run the set
> of tests I have prepared. This will take a bit of time but I will
> report back here on results.

Sure. Thank you.
I must warn you of two things though:
- most series I posted the last months and especially the last
  two weeks are somehow needed to use and test this one.
- don't forget that for the reasons explained in the cover letter
  (and a few others, less important), the SSA is still not correct
  or at least is broken during simplify_loads() and branch rewritting.

So, to stay 100% meaningful, you should drop all optimizations after
ssa_convert() and insure that you don't have unreachable code in your
input files.
 
-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: Re: [PATCH v1 00/18] SSA conversion, the classical way
Date: Tue, 20 Mar 2018 21:52:57 +0000
Message-ID: <CACXZuxct8zXMM0Aa-Qg+WSJLDQ98GNmDF=NGEApghEfqdZoqKw () mail ! gmail ! com>
--------------------
On 20 March 2018 at 21:49, Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
> On Tue, Mar 20, 2018 at 09:07:47PM +0000, Dibyendu Majumdar wrote:
>> Hi Luc,
>>
>> On 20 March 2018 at 00:52, Luc Van Oostenryck
>> <luc.vanoostenryck@gmail.com> wrote:
>> > This series replace the current SSA conversion by one using
>> > the 'classical' way, via the iterated dominance frontier.
>> >
>>
>> Thank you for posting this patch series. I would like to test it. I
>> will apply my LLVM fixes against your tree so that I can run the set
>> of tests I have prepared. This will take a bit of time but I will
>> report back here on results.
>
> Sure. Thank you.
> I must warn you of two things though:
> - most series I posted the last months and especially the last
>   two weeks are somehow needed to use and test this one.

I will apply my patches LLVM against your tree (not dmr_C) so all your
changes are present already in the tree I presume?

> - don't forget that for the reasons explained in the cover letter
>   (and a few others, less important), the SSA is still not correct
>   or at least is broken during simplify_loads() and branch rewritting.
>
> So, to stay 100% meaningful, you should drop all optimizations after
> ssa_convert() and insure that you don't have unreachable code in your
> input files.
>

Okay noted; I will report back anyway what the results are.

Thanks and Regards
Dibyendu
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Christopher Li <sparse () chrisli ! org>
To: linux-sparse
Subject: Re: [PATCH v1 00/18] SSA conversion, the classical way
Date: Sat, 24 Mar 2018 17:04:33 +0000
Message-ID: <CANeU7Q=ksCcvRtndH8w63v4YF2J239bwSDCGm2HY-Xxb+Carwg () mail ! gmail ! com>
--------------------
On Mon, Mar 19, 2018 at 5:52 PM, Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
> This series replace the current SSA conversion by one using
> the 'classical' way, via the iterated dominance frontier.

Hi, Congratulations on the SSA conversion.

I always want to try a traditional SSA conversion you beats
me to it.

Chris
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: Re: [PATCH v1 00/18] SSA conversion, the classical way
Date: Sun, 25 Mar 2018 21:43:53 +0000
Message-ID: <CACXZuxdfxX3GLOdAvjrGTttqzE1=x=7JoM9FUi-28YGw6u4hUQ () mail ! gmail ! com>
--------------------
On 20 March 2018 at 21:52, Dibyendu Majumdar <mobile@majumdar.org.uk> wrote:
> On 20 March 2018 at 21:49, Luc Van Oostenryck
> <luc.vanoostenryck@gmail.com> wrote:
>> On Tue, Mar 20, 2018 at 09:07:47PM +0000, Dibyendu Majumdar wrote:
>>> On 20 March 2018 at 00:52, Luc Van Oostenryck
>>> <luc.vanoostenryck@gmail.com> wrote:
>>> > This series replace the current SSA conversion by one using
>>> > the 'classical' way, via the iterated dominance frontier.
>>> >
>>>
>>> Thank you for posting this patch series. I would like to test it. I
>>> will apply my LLVM fixes against your tree so that I can run the set
>>> of tests I have prepared. This will take a bit of time but I will
>>> report back here on results.
>>

I have done some initial work on this. My changes are in:
https://github.com/dibyendumajumdar/sparse-dev/commits/ssa-v1. Some
more work is needed as this is still preliminary.

So far 65 out of 85 tests in
(https://github.com/dibyendumajumdar/sparse-testing) pass. I haven't
yet investigated the failures - they could be due to issues in my
merge.

In terms of my experience so far:

a) The series has new opcodes for floating point values compared to
the Sparse master tree.
b) The solution for variadic calls is different compared to the
value-size patch. I opted not to use the mechanism in this tree as I
think the value-size patch is a better approach. Right now therefore
this is probably broken.
c) In dmrC the macros for var args are disabled in code gen mode - I
need to do a similar fix here else any code using var args will crash.

I will report further results as I test this more and investigate the
test failures.

Thanks and Regards
Dibyendu
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: Re: [PATCH v1 00/18] SSA conversion, the classical way
Date: Sun, 25 Mar 2018 22:03:19 +0000
Message-ID: <CACXZuxcDYqs0WqCTNOLRzZd4R=-3sW0v9TivnVsomMRZSytiHg () mail ! gmail ! com>
--------------------
On 25 March 2018 at 22:43, Dibyendu Majumdar <mobile@majumdar.org.uk> wrote:
> On 20 March 2018 at 21:52, Dibyendu Majumdar <mobile@majumdar.org.uk> wrote:
>> On 20 March 2018 at 21:49, Luc Van Oostenryck
>> <luc.vanoostenryck@gmail.com> wrote:
>>> On Tue, Mar 20, 2018 at 09:07:47PM +0000, Dibyendu Majumdar wrote:
>>>> On 20 March 2018 at 00:52, Luc Van Oostenryck
>>>> <luc.vanoostenryck@gmail.com> wrote:
>>>> > This series replace the current SSA conversion by one using
>>>> > the 'classical' way, via the iterated dominance frontier.
>>>> >
>>>>
>>>> Thank you for posting this patch series. I would like to test it. I
>>>> will apply my LLVM fixes against your tree so that I can run the set
>>>> of tests I have prepared. This will take a bit of time but I will
>>>> report back here on results.
>>>
>
> I have done some initial work on this. My changes are in:
> https://github.com/dibyendumajumdar/sparse-dev/commits/ssa-v1. Some
> more work is needed as this is still preliminary.
>
> So far 65 out of 85 tests in
> (https://github.com/dibyendumajumdar/sparse-testing) pass. I haven't
> yet investigated the failures - they could be due to issues in my
> merge.
>

Improved to 70 out of 85 after removing the store instruction for
initializing aggregates.

Regards
Dibyendu
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: Re: [PATCH v1 00/18] SSA conversion, the classical way
Date: Sun, 25 Mar 2018 22:13:43 +0000
Message-ID: <CACXZuxcc0BOMQMeoEb7Sk6h99DDxPD5PVPUKs3AongY9aQaX-w () mail ! gmail ! com>
--------------------
On 25 March 2018 at 23:03, Dibyendu Majumdar <mobile@majumdar.org.uk> wrote:
> On 25 March 2018 at 22:43, Dibyendu Majumdar <mobile@majumdar.org.uk> wrote:
>> On 20 March 2018 at 21:52, Dibyendu Majumdar <mobile@majumdar.org.uk> wrote:
>>> On 20 March 2018 at 21:49, Luc Van Oostenryck
>>> <luc.vanoostenryck@gmail.com> wrote:
>>>> On Tue, Mar 20, 2018 at 09:07:47PM +0000, Dibyendu Majumdar wrote:
>>>>> On 20 March 2018 at 00:52, Luc Van Oostenryck
>>>>> <luc.vanoostenryck@gmail.com> wrote:
>>>>> > This series replace the current SSA conversion by one using
>>>>> > the 'classical' way, via the iterated dominance frontier.
>>>>> >
>>>>>
>>>>> Thank you for posting this patch series. I would like to test it. I
>>>>> will apply my LLVM fixes against your tree so that I can run the set
>>>>> of tests I have prepared. This will take a bit of time but I will
>>>>> report back here on results.
>>>>
>>
>> I have done some initial work on this. My changes are in:
>> https://github.com/dibyendumajumdar/sparse-dev/commits/ssa-v1. Some
>> more work is needed as this is still preliminary.
>>
>> So far 65 out of 85 tests in
>> (https://github.com/dibyendumajumdar/sparse-testing) pass. I haven't
>> yet investigated the failures - they could be due to issues in my
>> merge.
>>
>
> Improved to 70 out of 85 after removing the store instruction for
> initializing aggregates.
>


Luc,

BTW you have said several times that you don't care about the LLVM
backend .... I do, and if you merge my version into your tree, I will
be happy to maintain it.

Thanks and Regards
Dibyendu
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v1 01/18] graph: build the CFG reverse postorder traversal ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v1 01/18] graph: build the CFG reverse postorder traversal
Date: Tue, 20 Mar 2018 00:52:39 +0000
Message-ID: <20180320005256.53284-2-luc.vanoostenryck () gmail ! com>
--------------------
Do a DFS on the CFG and record the (reverse) postorder.
Use this order for the normal BB traversal (ep->bbs).

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Makefile    |  1 +
 flowgraph.c | 65 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 flowgraph.h |  8 ++++++++
 linearize.h |  5 ++++-
 4 files changed, 78 insertions(+), 1 deletion(-)
 create mode 100644 flowgraph.c
 create mode 100644 flowgraph.h

diff --git a/Makefile b/Makefile
index 5548f8483..329e792ff 100644
--- a/Makefile
+++ b/Makefile
@@ -41,6 +41,7 @@ LIB_OBJS += evaluate.o
 LIB_OBJS += expand.o
 LIB_OBJS += expression.o
 LIB_OBJS += flow.o
+LIB_OBJS += flowgraph.o
 LIB_OBJS += inline.o
 LIB_OBJS += lib.o
 LIB_OBJS += linearize.o
diff --git a/flowgraph.c b/flowgraph.c
new file mode 100644
index 000000000..89897fa48
--- /dev/null
+++ b/flowgraph.c
@@ -0,0 +1,65 @@
+// SPDX-License-Identifier: MIT
+//
+// Various utilities for flowgraphs.
+//
+// Copyright (c) 2017 Luc Van Oostenryck.
+//
+
+#include "flowgraph.h"
+#include "linearize.h"
+#include "flow.h"			// for bb_generation
+
+
+struct cfg_info {
+	struct basic_block_list *list;
+	unsigned long gen;
+	unsigned int nr;
+};
+
+
+static void label_postorder(struct basic_block *bb, struct cfg_info *info)
+{
+	struct basic_block *child;
+
+	if (bb->generation == info->gen)
+		return;
+
+	bb->generation = info->gen;
+	FOR_EACH_PTR_REVERSE(bb->children, child) {
+		label_postorder(child, info);
+	} END_FOR_EACH_PTR_REVERSE(child);
+
+	bb->postorder_nr = info->nr++;
+	add_bb(&info->list, bb);
+}
+
+static void reverse_bbs(struct basic_block_list **dst, struct basic_block_list *src)
+{
+	struct basic_block *bb;
+	FOR_EACH_PTR_REVERSE(src, bb) {
+		add_bb(dst, bb);
+	} END_FOR_EACH_PTR_REVERSE(bb);
+}
+
+//
+// cfg_postorder - Set the BB's reverse postorder links
+//
+// Do a postorder DFS walk and set the links
+// (which will do the reverse part).
+//
+int cfg_postorder(struct entrypoint *ep)
+{
+	struct cfg_info info = {
+		.gen = ++bb_generation,
+	};
+
+	label_postorder(ep->entry->bb, &info);
+
+	// OK, now info.list contains the node in postorder
+	// Reuse ep->bbs for the reverse postorder.
+	free_ptr_list(&ep->bbs);
+	ep->bbs = NULL;
+	reverse_bbs(&ep->bbs, info.list);
+	free_ptr_list(&info.list);
+	return info.nr;
+}
diff --git a/flowgraph.h b/flowgraph.h
new file mode 100644
index 000000000..676c5b2d8
--- /dev/null
+++ b/flowgraph.h
@@ -0,0 +1,8 @@
+#ifndef FLOWGRAPH_H
+#define FLOWGRAPH_H
+
+struct entrypoint;
+
+int cfg_postorder(struct entrypoint *ep);
+
+#endif
diff --git a/linearize.h b/linearize.h
index 8790b7e58..50738a9d8 100644
--- a/linearize.h
+++ b/linearize.h
@@ -257,7 +257,10 @@ struct instruction_list;
 struct basic_block {
 	struct position pos;
 	unsigned long generation;
-	int context;
+	union {
+		int context;
+		int postorder_nr;	/* postorder number */
+	};
 	struct entrypoint *ep;
 	struct basic_block_list *parents; /* sources */
 	struct basic_block_list *children; /* destinations */
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v1 03/18] dom: calculate the dominance tree ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v1 03/18] dom: calculate the dominance tree
Date: Tue, 20 Mar 2018 00:52:41 +0000
Message-ID: <20180320005256.53284-4-luc.vanoostenryck () gmail ! com>
--------------------
Build the CFG's dominance tree and for each BB record:
- the immediate dominator as bb->idom (is null for entry BB).
- the list of immediately dominated BB as bb->doms.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 flowgraph.c | 103 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 flowgraph.h |   1 +
 linearize.h |   4 +++
 3 files changed, 108 insertions(+)

diff --git a/flowgraph.c b/flowgraph.c
index 40a75e825..d5551908e 100644
--- a/flowgraph.c
+++ b/flowgraph.c
@@ -8,7 +8,9 @@
 #include "flowgraph.h"
 #include "linearize.h"
 #include "flow.h"			// for bb_generation
+#include <assert.h>
 #include <stdio.h>
+#include <stdlib.h>
 
 
 struct cfg_info {
@@ -76,3 +78,104 @@ int cfg_postorder(struct entrypoint *ep)
 		debug_postorder(ep);
 	return info.nr;
 }
+
+//
+// Calculate the dominance tree following:
+//	"A simple, fast dominance algorithm"
+//	by K. D. Cooper, T. J. Harvey, and K. Kennedy.
+//	cfr. http://www.cs.rice.edu/â¼keith/EMBED/dom.pdf
+//
+static struct basic_block *intersect_dom(struct basic_block *doms[],
+		struct basic_block *b1, struct basic_block *b2)
+{
+	int f1 = b1->postorder_nr, f2 = b2->postorder_nr;
+	while (f1 != f2) {
+		while (f1 < f2) {
+			b1 = doms[f1];
+			f1 = b1->postorder_nr;
+		}
+		while (f2 < f1) {
+			b2 = doms[f2];
+			f2 = b2->postorder_nr;
+		}
+	}
+	return b1;
+}
+
+void domtree_build(struct entrypoint *ep)
+{
+	struct basic_block *entry = ep->entry->bb;
+	struct basic_block **doms;
+	struct basic_block *bb;
+	unsigned int size;
+	int max_level = 0;
+	int changed;
+
+	// First calculate the (reverse) postorder.
+	// This will give use us:
+	//	- the links to do a reverse postorder traversal
+	//	- the order number for each block
+	size = cfg_postorder(ep);
+
+	// initialize the dominators array
+	doms = calloc(size, sizeof(*doms));
+	assert(entry->postorder_nr == size-1);
+	doms[size-1] = entry;
+
+	do {
+		struct basic_block *b;
+
+		changed = 0;
+		FOR_EACH_PTR(ep->bbs, b) {
+			struct basic_block *p;
+			int bnr = b->postorder_nr;
+			struct basic_block *new_idom = NULL;
+
+			if (b == entry)
+				continue;	// ignore entry node
+
+			FOR_EACH_PTR(b->parents, p) {
+				unsigned int pnr = p->postorder_nr;
+				if (!doms[pnr])
+					continue;
+				if (!new_idom) {
+					new_idom = p;
+					continue;
+				}
+
+				new_idom = intersect_dom(doms, p, new_idom);
+			} END_FOR_EACH_PTR(p);
+
+			assert(new_idom);
+			if (doms[bnr] != new_idom) {
+				doms[bnr] = new_idom;
+				changed = 1;
+			}
+		} END_FOR_EACH_PTR(b);
+	} while (changed);
+
+	// set the idom links
+	FOR_EACH_PTR(ep->bbs, bb) {
+		struct basic_block *idom = doms[bb->postorder_nr];
+
+		if (bb == entry)
+			continue;	// ignore entry node
+
+		bb->idom = idom;
+		add_bb(&idom->doms, bb);
+	} END_FOR_EACH_PTR(bb);
+	entry->idom = NULL;
+
+	// set the dominance levels
+	FOR_EACH_PTR(ep->bbs, bb) {
+		struct basic_block *idom = bb->idom;
+		int level = idom ? idom->dom_level + 1 : 0;
+
+		bb->dom_level = level;
+		if (max_level < level)
+			max_level = level;
+	} END_FOR_EACH_PTR(bb);
+	ep->dom_levels = max_level + 1;
+
+	free(doms);
+}
diff --git a/flowgraph.h b/flowgraph.h
index 676c5b2d8..8e42f865d 100644
--- a/flowgraph.h
+++ b/flowgraph.h
@@ -4,5 +4,6 @@
 struct entrypoint;
 
 int cfg_postorder(struct entrypoint *ep);
+void domtree_build(struct entrypoint *ep);
 
 #endif
diff --git a/linearize.h b/linearize.h
index 50738a9d8..486cf5979 100644
--- a/linearize.h
+++ b/linearize.h
@@ -260,11 +260,14 @@ struct basic_block {
 	union {
 		int context;
 		int postorder_nr;	/* postorder number */
+		int dom_level;		/* level in the dominance tree */
 	};
 	struct entrypoint *ep;
 	struct basic_block_list *parents; /* sources */
 	struct basic_block_list *children; /* destinations */
 	struct instruction_list *insns;	/* Linear list of instructions */
+	struct basic_block *idom;	/* link to the immediate dominator */
+	struct basic_block_list *doms;	/* list of BB idominated by this one */
 	struct pseudo_list *needs, *defines;
 	union {
 		unsigned int nr;	/* unique id for label's names */
@@ -366,6 +369,7 @@ struct entrypoint {
 	struct basic_block_list *bbs;
 	struct basic_block *active;
 	struct instruction *entry;
+	unsigned int dom_levels;	/* max levels in the dom tree */
 };
 
 extern void insert_select(struct basic_block *bb, struct instruction *br, struct instruction *phi, pseudo_t if_true, pseudo_t if_false);
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v1 04/18] dom: add some debugging for the dominance tree ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v1 04/18] dom: add some debugging for the dominance tree
Date: Tue, 20 Mar 2018 00:52:42 +0000
Message-ID: <20180320005256.53284-5-luc.vanoostenryck () gmail ! com>
--------------------
So, it's possible to use the flag '-vdomtree' to dump the
domonance tree.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/dev-options.md |  1 +
 flowgraph.c                  | 14 ++++++++++++++
 lib.c                        |  2 ++
 lib.h                        |  1 +
 4 files changed, 18 insertions(+)

diff --git a/Documentation/dev-options.md b/Documentation/dev-options.md
index b288d904c..882a566aa 100644
--- a/Documentation/dev-options.md
+++ b/Documentation/dev-options.md
@@ -31,5 +31,6 @@ document options only useful for development on sparse itself.
 
   Add or display some debug info. The flag can be one of:
   * 'dead': annotate dead pseudos.
+  * 'domtree': dump the dominance tree after its calculation.
   * 'entry': dump the IR after all optimization passes.
   * 'postorder': dump the reverse postorder traversal of the CFG.
diff --git a/flowgraph.c b/flowgraph.c
index d5551908e..b2d95893c 100644
--- a/flowgraph.c
+++ b/flowgraph.c
@@ -102,6 +102,18 @@ static struct basic_block *intersect_dom(struct basic_block *doms[],
 	return b1;
 }
 
+static void debug_domtree(struct entrypoint *ep)
+{
+	struct basic_block *bb = ep->entry->bb;
+
+	printf("%s's idoms:\n", show_ident(ep->name->ident));
+	FOR_EACH_PTR(ep->bbs, bb) {
+		if (bb == ep->entry->bb)
+			continue;	// entry node has no idom
+		printf("\t%s	<- %s\n", show_label(bb), show_label(bb->idom));
+	} END_FOR_EACH_PTR(bb);
+}
+
 void domtree_build(struct entrypoint *ep)
 {
 	struct basic_block *entry = ep->entry->bb;
@@ -178,4 +190,6 @@ void domtree_build(struct entrypoint *ep)
 	ep->dom_levels = max_level + 1;
 
 	free(doms);
+	if (dbg_domtree)
+		debug_domtree(ep);
 }
diff --git a/lib.c b/lib.c
index 26645b156..904008b43 100644
--- a/lib.c
+++ b/lib.c
@@ -258,6 +258,7 @@ int dump_macro_defs = 0;
 
 int dbg_entry = 0;
 int dbg_dead = 0;
+int dbg_domtree = 0;
 int dbg_postorder = 0;
 
 unsigned long fdump_ir;
@@ -718,6 +719,7 @@ static char **handle_switch_W(char *arg, char **next)
 static struct flag debugs[] = {
 	{ "entry", &dbg_entry},
 	{ "dead", &dbg_dead},
+	{ "domtree", &dbg_domtree},
 	{ "postorder", &dbg_postorder},
 };
 
diff --git a/lib.h b/lib.h
index 321655485..3eb8adc73 100644
--- a/lib.h
+++ b/lib.h
@@ -167,6 +167,7 @@ extern int dump_macro_defs;
 
 extern int dbg_entry;
 extern int dbg_dead;
+extern int dbg_domtree;
 extern int dbg_postorder;
 
 extern unsigned int fmax_warnings;
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v1 06/18] dom: build the domtree before optimization ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v1 06/18] dom: build the domtree before optimization
Date: Tue, 20 Mar 2018 00:52:44 +0000
Message-ID: <20180320005256.53284-7-luc.vanoostenryck () gmail ! com>
--------------------
Now that can build the dominance tree, let's effectivey
build it.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 optimize.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/optimize.c b/optimize.c
index 5cde6a58c..2f9c7910d 100644
--- a/optimize.c
+++ b/optimize.c
@@ -7,6 +7,7 @@
 
 #include <assert.h>
 #include "optimize.h"
+#include "flowgraph.h"
 #include "linearize.h"
 #include "liveness.h"
 #include "flow.h"
@@ -51,6 +52,8 @@ void optimize(struct entrypoint *ep)
 	 */
 	kill_unreachable_bbs(ep);
 
+	domtree_build(ep);
+
 	/*
 	 * Turn symbols into pseudos
 	 */
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v1 08/18] sset: add implementation of sparse sets ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v1 08/18] sset: add implementation of sparse sets
Date: Tue, 20 Mar 2018 00:52:46 +0000
Message-ID: <20180320005256.53284-9-luc.vanoostenryck () gmail ! com>
--------------------
Sparse set implements set operations like add, remove & test in O(1).
More importantly it also allow to reset the set as a whole in O(1).

It's very handy for a few things.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Makefile |  1 +
 sset.c   | 28 ++++++++++++++++++++++++++++
 sset.h   | 56 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 3 files changed, 85 insertions(+)
 create mode 100644 sset.c
 create mode 100644 sset.h

diff --git a/Makefile b/Makefile
index 329e792ff..dae6a07b1 100644
--- a/Makefile
+++ b/Makefile
@@ -56,6 +56,7 @@ LIB_OBJS += scope.o
 LIB_OBJS += show-parse.o
 LIB_OBJS += simplify.o
 LIB_OBJS += sort.o
+LIB_OBJS += sset.o
 LIB_OBJS += stats.o
 LIB_OBJS += storage.o
 LIB_OBJS += symbol.o
diff --git a/sset.c b/sset.c
new file mode 100644
index 000000000..e9681e00d
--- /dev/null
+++ b/sset.c
@@ -0,0 +1,28 @@
+// SPDX-License-Identifier: MIT
+//
+// sset.c - an all O(1) implementation of sparse sets as presented in:
+//	"An Efficient Representation for Sparse Sets"
+//	by Preston Briggs and Linda Torczon
+//
+// Copyright (C) 2017 - Luc Van Oostenryck
+
+#include "sset.h"
+#include "lib.h"
+#include <stdlib.h>
+
+
+struct sset *sset_init(unsigned int first, unsigned int last)
+{
+	unsigned int size = last - first + 1;
+	struct sset *s = malloc(sizeof(*s) + size * 2 * sizeof(s->sets[0]));
+
+	s->size = size;
+	s->off = first;
+	s->nbr = 0;
+	return s;
+}
+
+void sset_free(struct sset *s)
+{
+	free(s);
+}
diff --git a/sset.h b/sset.h
new file mode 100644
index 000000000..69cee18a2
--- /dev/null
+++ b/sset.h
@@ -0,0 +1,56 @@
+// SPDX-License-Identifier: MIT
+
+#ifndef SSET_H
+#define SSET_H
+
+/*
+ * sset.h - an all O(1) implementation of sparse sets as presented in:
+ *	"An Efficient Representation for Sparse Sets"
+ *	by Preston Briggs and Linda Torczon
+ *
+ * Copyright (C) 2017 - Luc Van Oostenryck
+ */
+
+#include <stdbool.h>
+
+struct sset {
+	unsigned int nbr;
+	unsigned int off;
+	unsigned int size;
+	unsigned int sets[0];
+};
+
+extern struct sset *sset_init(unsigned int size, unsigned int off);
+extern void sset_free(struct sset *s);
+
+
+static inline void sset_reset(struct sset *s)
+{
+	s->nbr = 0;
+}
+
+static inline void sset_add(struct sset *s, unsigned int idx)
+{
+	unsigned int __idx = idx - s->off;
+	unsigned int n = s->nbr++;
+	s->sets[__idx] = n;
+	s->sets[s->size + n] = __idx;
+}
+
+static inline bool sset_test(struct sset *s, unsigned int idx)
+{
+	unsigned int __idx = idx - s->off;
+	unsigned int n = s->sets[__idx];
+
+	return (n < s->nbr) && (s->sets[s->size + n] == __idx);
+}
+
+static inline bool sset_testset(struct sset *s, unsigned int idx)
+{
+	if (sset_test(s, idx))
+		return true;
+	sset_add(s, idx);
+	return false;
+}
+
+#endif
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v1 10/18] idf: add test/debug/example ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v1 10/18] idf: add test/debug/example
Date: Tue, 20 Mar 2018 00:52:48 +0000
Message-ID: <20180320005256.53284-11-luc.vanoostenryck () gmail ! com>
--------------------
This patch add a small silly function which dump the iterated
dominance frontier of each individual node in the CFG.

It's just there to show how to use the IDF API.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 dominate.c | 27 +++++++++++++++++++++++++++
 1 file changed, 27 insertions(+)

diff --git a/dominate.c b/dominate.c
index d7808119b..8085171d0 100644
--- a/dominate.c
+++ b/dominate.c
@@ -15,6 +15,7 @@
 #include "flow.h"
 #include <assert.h>
 #include <stdlib.h>
+#include <stdio.h>
 
 
 struct piggy {
@@ -124,3 +125,29 @@ void idf_compute(struct entrypoint *ep, struct basic_block_list **idf, struct ba
 
 	bank_free(bank, levels);
 }
+
+void idf_dump(struct entrypoint *ep)
+{
+	struct basic_block *bb;
+
+	domtree_build(ep);
+
+	printf("%s's IDF:\n", show_ident(ep->name->ident));
+	FOR_EACH_PTR(ep->bbs, bb) {
+		struct basic_block_list *alpha = NULL;
+		struct basic_block_list *idf = NULL;
+		struct basic_block *df;
+
+		add_bb(&alpha, bb);
+		idf_compute(ep, &idf, alpha);
+
+		printf("\t%s\t<-", show_label(bb));
+		FOR_EACH_PTR(idf, df) {
+			printf(" %s", show_label(df));
+		} END_FOR_EACH_PTR(df);
+		printf("\n");
+
+		free_ptr_list(&idf);
+		free_ptr_list(&alpha);
+	} END_FOR_EACH_PTR(bb);
+}
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v1 11/18] add new helper: is_integral_type() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v1 11/18] add new helper: is_integral_type()
Date: Tue, 20 Mar 2018 00:52:49 +0000
Message-ID: <20180320005256.53284-12-luc.vanoostenryck () gmail ! com>
--------------------
Some optimizations must only be done on integral types:
all kinds of integers and any pointers type.

Add a new helper for this.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 symbol.h | 18 ++++++++++++++++++
 1 file changed, 18 insertions(+)

diff --git a/symbol.h b/symbol.h
index dc6de8fa4..5901a85aa 100644
--- a/symbol.h
+++ b/symbol.h
@@ -420,6 +420,24 @@ static inline int is_scalar_type(struct symbol *type)
 	return 0;
 }
 
+/// return true for integer & pointer types
+static inline bool is_integral_type(struct symbol *type)
+{
+	if (type->type == SYM_NODE)
+		type = type->ctype.base_type;
+	switch (type->type) {
+	case SYM_ENUM:
+	case SYM_PTR:
+	case SYM_RESTRICT:	// OK, always integer types
+		return 1;
+	default:
+		break;
+	}
+	if (type->ctype.base_type == &int_type)
+		return 1;
+	return 0;
+}
+
 static inline int is_function(struct symbol *type)
 {
 	return type && type->type == SYM_FN;
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v1 13/18] add insert_phi_node() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v1 13/18] add insert_phi_node()
Date: Tue, 20 Mar 2018 00:52:51 +0000
Message-ID: <20180320005256.53284-14-luc.vanoostenryck () gmail ! com>
--------------------
This helper is used later during the SSA construction and is,
as its name suggest, used to insert phi-nodes in the
instruction stream.

More exactly, the phi-node will be put at the begining of the
specified BB, just after the others phi-nodes but before
any other instructions, as required for their semantics
(although, it's less important for sparse since each operand
correspond first to a phi-source, so no phi-node directly
depending on themselves in sparse).

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 linearize.c | 36 ++++++++++++++++++++++++++++++++++++
 linearize.h |  3 +++
 2 files changed, 39 insertions(+)

diff --git a/linearize.c b/linearize.c
index 4cd2c75eb..d35842b1f 100644
--- a/linearize.c
+++ b/linearize.c
@@ -871,6 +871,42 @@ pseudo_t alloc_phi(struct basic_block *source, pseudo_t pseudo, struct symbol *t
 	return insn->target;
 }
 
+struct instruction *alloc_phi_node(struct basic_block *bb, struct symbol *type, struct ident *ident)
+{
+	struct instruction *phi_node = alloc_typed_instruction(OP_PHI, type);
+	pseudo_t phi;
+
+	phi = alloc_pseudo(phi_node);
+	phi->ident = ident;
+	phi->def = phi_node;
+	phi_node->target = phi;
+	phi_node->bb = bb;
+	return phi_node;
+}
+
+void add_phi_node(struct basic_block *bb, struct instruction *phi_node)
+{
+	struct instruction *insn;
+
+	FOR_EACH_PTR(bb->insns, insn) {
+		enum opcode op = insn->opcode;
+		if (op == OP_PHI)
+			continue;
+		INSERT_CURRENT(phi_node, insn);
+		return;
+	} END_FOR_EACH_PTR(insn);
+
+	// FIXME
+	add_instruction(&bb->insns, phi_node);
+}
+
+struct instruction *insert_phi_node(struct basic_block *bb, struct symbol *var)
+{
+	struct instruction *phi_node = alloc_phi_node(bb, var, var->ident);
+	add_phi_node(bb, phi_node);
+	return phi_node;
+}
+
 /*
  * We carry the "access_data" structure around for any accesses,
  * which simplifies things a lot. It contains all the access
diff --git a/linearize.h b/linearize.h
index 0285e28d3..0aaf11da5 100644
--- a/linearize.h
+++ b/linearize.h
@@ -377,6 +377,9 @@ extern void insert_select(struct basic_block *bb, struct instruction *br, struct
 extern void insert_branch(struct basic_block *bb, struct instruction *br, struct basic_block *target);
 
 struct instruction *alloc_phisrc(pseudo_t pseudo, struct symbol *type);
+struct instruction *alloc_phi_node(struct basic_block *bb, struct symbol *type, struct ident *ident);
+struct instruction *insert_phi_node(struct basic_block *bb, struct symbol *var);
+void add_phi_node(struct basic_block *bb, struct instruction *phi_node);
 
 pseudo_t alloc_phi(struct basic_block *source, pseudo_t pseudo, struct symbol *type);
 pseudo_t alloc_pseudo(struct instruction *def);
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v1 15/18] ptrmap: add type-safe interface ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v1 15/18] ptrmap: add type-safe interface
Date: Tue, 20 Mar 2018 00:52:53 +0000
Message-ID: <20180320005256.53284-16-luc.vanoostenryck () gmail ! com>
--------------------
Add the external, type-safe, API for the ptrmap implementation.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 ptrmap.h | 16 ++++++++++++++++
 1 file changed, 16 insertions(+)

diff --git a/ptrmap.h b/ptrmap.h
index 070db15e2..cbbb61da9 100644
--- a/ptrmap.h
+++ b/ptrmap.h
@@ -3,6 +3,22 @@
 
 struct ptrmap;
 
+#define DECLARE_PTRMAP(name, ktype, vtype)				\
+	struct name ## _pair { ktype key; vtype val; };			\
+	struct name { struct name ## _pair block[1]; };			\
+	static inline							\
+	void name##_add(struct name **map, ktype k, vtype v) {		\
+		__ptrmap_add((struct ptrmap**)map, k, v);		\
+	}								\
+	static inline							\
+	void name##_update(struct name **map, ktype k, vtype v) {	\
+		__ptrmap_update((struct ptrmap**)map, k, v);		\
+	}								\
+	static inline							\
+	vtype name##_lookup(struct name *map, ktype k) {		\
+		vtype val = __ptrmap_lookup((struct ptrmap*)map, k);	\
+		return val;						\
+	}								\
 
 /* ptrmap.c */
 void __ptrmap_add(struct ptrmap **mapp, void *key, void *val);
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v1 17/18] ssa: remove unused simplify_symbol_usage() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v1 17/18] ssa: remove unused simplify_symbol_usage()
Date: Tue, 20 Mar 2018 00:52:55 +0000
Message-ID: <20180320005256.53284-18-luc.vanoostenryck () gmail ! com>
--------------------
This patch remove simplify_symbol_usage() and the associated
functions, now unneeded with the new SSA conversion.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 flow.c | 278 -----------------------------------------------------------------
 ssa.c  |  33 +++++---
 2 files changed, 20 insertions(+), 291 deletions(-)

diff --git a/flow.c b/flow.c
index 1dade8dd8..f988554e5 100644
--- a/flow.c
+++ b/flow.c
@@ -374,68 +374,6 @@ int dominates(pseudo_t pseudo, struct instruction *insn, struct instruction *dom
 	return 1;
 }
 
-static int phisrc_in_bb(struct pseudo_list *list, struct basic_block *bb)
-{
-	pseudo_t p;
-	FOR_EACH_PTR(list, p) {
-		if (p->def->bb == bb)
-			return 1;
-	} END_FOR_EACH_PTR(p);
-
-	return 0;
-}
-
-static int find_dominating_parents(pseudo_t pseudo, struct instruction *insn,
-	struct basic_block *bb, unsigned long generation, struct pseudo_list **dominators,
-	int local)
-{
-	struct basic_block *parent;
-
-	if (!bb->parents)
-		return !!local;
-
-	FOR_EACH_PTR(bb->parents, parent) {
-		struct instruction *one;
-		struct instruction *br;
-		pseudo_t phi;
-
-		FOR_EACH_PTR_REVERSE(parent->insns, one) {
-			int dominance;
-			if (!one->bb)
-				continue;
-			if (one == insn)
-				goto no_dominance;
-			dominance = dominates(pseudo, insn, one, local);
-			if (dominance < 0) {
-				if (one->opcode == OP_LOAD)
-					continue;
-				return 0;
-			}
-			if (!dominance)
-				continue;
-			goto found_dominator;
-		} END_FOR_EACH_PTR_REVERSE(one);
-no_dominance:
-		if (parent->generation == generation)
-			continue;
-		parent->generation = generation;
-
-		if (!find_dominating_parents(pseudo, insn, parent, generation, dominators, local))
-			return 0;
-		continue;
-
-found_dominator:
-		if (dominators && phisrc_in_bb(*dominators, parent))
-			continue;
-		br = delete_last_instruction(&parent->insns);
-		phi = alloc_phi(parent, one->target, one->type);
-		phi->ident = phi->ident ? : pseudo->ident;
-		add_instruction(&parent->insns, br);
-		use_pseudo(insn, phi, add_pseudo(dominators, phi));
-	} END_FOR_EACH_PTR(parent);
-	return 1;
-}		
-
 /*
  * We should probably sort the phi list just to make it easier to compare
  * later for equality. 
@@ -477,79 +415,6 @@ end:
 	repeat_phase |= REPEAT_SYMBOL_CLEANUP;
 }
 
-static int find_dominating_stores(pseudo_t pseudo, struct instruction *insn,
-	unsigned long generation, int local)
-{
-	struct basic_block *bb = insn->bb;
-	struct instruction *one, *dom = NULL;
-	struct pseudo_list *dominators;
-	int partial;
-
-	/* Unreachable load? Undo it */
-	if (!bb) {
-		kill_use(&insn->src);
-		return 1;
-	}
-
-	partial = 0;
-	FOR_EACH_PTR(bb->insns, one) {
-		int dominance;
-		if (!one->bb)
-			continue;
-		if (one == insn)
-			goto found;
-		dominance = dominates(pseudo, insn, one, local);
-		if (dominance < 0) {
-			/* Ignore partial load dominators */
-			if (one->opcode == OP_LOAD)
-				continue;
-			dom = NULL;
-			partial = 1;
-			continue;
-		}
-		if (!dominance)
-			continue;
-		dom = one;
-		partial = 0;
-	} END_FOR_EACH_PTR(one);
-	/* Whaa? */
-	warning(pseudo->sym->pos, "unable to find symbol read");
-	return 0;
-found:
-	if (partial)
-		return 0;
-
-	if (dom) {
-		convert_load_instruction(insn, dom->target);
-		return 1;
-	}
-
-	/* OK, go find the parents */
-	bb->generation = generation;
-
-	dominators = NULL;
-	if (!find_dominating_parents(pseudo, insn, bb, generation, &dominators, local))
-		return 0;
-
-	/* This happens with initial assignments to structures etc.. */
-	if (!dominators) {
-		if (!local)
-			return 0;
-		check_access(insn);
-		convert_load_instruction(insn, value_pseudo(0));
-		return 1;
-	}
-
-	/*
-	 * If we find just one dominating instruction, we
-	 * can turn it into a direct thing. Otherwise we'll
-	 * have to turn the load into a phi-node of the
-	 * dominators.
-	 */
-	rewrite_load_instruction(insn, dominators);
-	return 1;
-}
-
 /* Kill a pseudo that is dead on exit from the bb */
 // The context is:
 // * the variable is not global but may have its address used (local/non-local)
@@ -604,59 +469,6 @@ static void kill_dead_stores_bb(pseudo_t pseudo, unsigned long generation, struc
 	} END_FOR_EACH_PTR(parent);
 }
 
-/*
- * This should see if the "insn" trivially dominates some previous store, and kill the
- * store if unnecessary.
- */
-static void kill_dominated_stores(pseudo_t pseudo, struct instruction *insn, 
-	unsigned long generation, struct basic_block *bb, int local, int found)
-{
-	struct instruction *one;
-	struct basic_block *parent;
-
-	/* Unreachable store? Undo it */
-	if (!bb) {
-		kill_instruction_force(insn);
-		return;
-	}
-	if (bb->generation == generation)
-		return;
-	bb->generation = generation;
-	FOR_EACH_PTR_REVERSE(bb->insns, one) {
-		int dominance;
-		if (!one->bb)
-			continue;
-		if (!found) {
-			if (one != insn)
-				continue;
-			found = 1;
-			continue;
-		}
-		dominance = dominates(pseudo, insn, one, local);
-		if (!dominance)
-			continue;
-		if (dominance < 0)
-			return;
-		if (one->opcode == OP_LOAD)
-			return;
-		kill_instruction_force(one);
-	} END_FOR_EACH_PTR_REVERSE(one);
-
-	if (!found) {
-		warning(bb->pos, "Unable to find instruction");
-		return;
-	}
-
-	FOR_EACH_PTR(bb->parents, parent) {
-		struct basic_block *child;
-		FOR_EACH_PTR(parent->children, child) {
-			if (child && child != bb)
-				return;
-		} END_FOR_EACH_PTR(child);
-		kill_dominated_stores(pseudo, insn, generation, parent, local, found);
-	} END_FOR_EACH_PTR(parent);
-}
-
 void check_access(struct instruction *insn)
 {
 	pseudo_t pseudo = insn->src;
@@ -677,96 +489,6 @@ void check_access(struct instruction *insn)
 	}
 }
 
-static void simplify_one_symbol(struct entrypoint *ep, struct symbol *sym)
-{
-	pseudo_t pseudo;
-	struct pseudo_user *pu;
-	unsigned long mod;
-	int all;
-
-	/* Never used as a symbol? */
-	pseudo = sym->pseudo;
-	if (!pseudo)
-		return;
-
-	/* We don't do coverage analysis of volatiles.. */
-	if (sym->ctype.modifiers & MOD_VOLATILE)
-		return;
-
-	/* ..and symbols with external visibility need more care */
-	mod = sym->ctype.modifiers & (MOD_NONLOCAL | MOD_STATIC | MOD_ADDRESSABLE);
-	if (mod)
-		goto external_visibility;
-
-	FOR_EACH_PTR(pseudo->users, pu) {
-		/* We know that the symbol-pseudo use is the "src" in the instruction */
-		struct instruction *insn = pu->insn;
-
-		switch (insn->opcode) {
-		case OP_STORE:
-			break;
-		case OP_LOAD:
-			break;
-		case OP_SYMADDR:
-			if (!insn->bb)
-				continue;
-			mod |= MOD_ADDRESSABLE;
-			goto external_visibility;
-		case OP_NOP:
-		case OP_PHI:
-			continue;
-		default:
-			warning(sym->pos, "symbol '%s' pseudo used in unexpected way", show_ident(sym->ident));
-		}
-	} END_FOR_EACH_PTR(pu);
-
-external_visibility:
-	all = 1;
-	FOR_EACH_PTR_REVERSE(pseudo->users, pu) {
-		struct instruction *insn = pu->insn;
-		if (insn->opcode == OP_LOAD)
-			all &= find_dominating_stores(pseudo, insn, ++bb_generation, !mod);
-	} END_FOR_EACH_PTR_REVERSE(pu);
-
-	/* If we converted all the loads, remove the stores. They are dead */
-	if (all && !mod) {
-		FOR_EACH_PTR(pseudo->users, pu) {
-			struct instruction *insn = pu->insn;
-			if (insn->opcode == OP_STORE)
-				kill_instruction_force(insn);
-		} END_FOR_EACH_PTR(pu);
-	} else {
-		/*
-		 * If we couldn't take the shortcut, see if we can at least kill some
-		 * of them..
-		 */
-		FOR_EACH_PTR(pseudo->users, pu) {
-			struct instruction *insn = pu->insn;
-			if (insn->opcode == OP_STORE)
-				kill_dominated_stores(pseudo, insn, ++bb_generation, insn->bb, !mod, 0);
-		} END_FOR_EACH_PTR(pu);
-
-		if (!(mod & (MOD_NONLOCAL | MOD_STATIC))) {
-			struct basic_block *bb;
-			FOR_EACH_PTR(ep->bbs, bb) {
-				if (!bb->children)
-					kill_dead_stores_bb(pseudo, ++bb_generation, bb, !mod);
-			} END_FOR_EACH_PTR(bb);
-		}
-	}
-			
-	return;
-}
-
-void simplify_symbol_usage(struct entrypoint *ep)
-{
-	pseudo_t pseudo;
-
-	FOR_EACH_PTR(ep->accesses, pseudo) {
-		simplify_one_symbol(ep, pseudo->sym);
-	} END_FOR_EACH_PTR(pseudo);
-}
-
 static struct pseudo_user *first_user(pseudo_t p)
 {
 	struct pseudo_user *pu;
diff --git a/ssa.c b/ssa.c
index 85f8acde2..34d922798 100644
--- a/ssa.c
+++ b/ssa.c
@@ -299,7 +299,21 @@ static void ssa_rename_insn(struct basic_block *bb, struct instruction *insn)
 	}
 }
 
-static void ssa_rename_phi(struct basic_block *bb, struct instruction *insn)
+static void ssa_rename_insns(struct entrypoint *ep)
+{
+	struct basic_block *bb;
+
+	FOR_EACH_PTR(ep->bbs, bb) {
+		struct instruction *insn;
+		FOR_EACH_PTR(bb->insns, insn) {
+			if (!insn->bb)
+				continue;
+			ssa_rename_insn(bb, insn);
+		} END_FOR_EACH_PTR(insn);
+	} END_FOR_EACH_PTR(bb);
+}
+
+static void ssa_rename_phi(struct instruction *insn)
 {
 	struct basic_block *par;
 	struct symbol *var;
@@ -309,7 +323,7 @@ static void ssa_rename_phi(struct basic_block *bb, struct instruction *insn)
 	var = insn->phi_var->sym;
 	if (!var->torename)
 		return;
-	FOR_EACH_PTR(bb->parents, par) {
+	FOR_EACH_PTR(insn->bb->parents, par) {
 		struct instruction *term = delete_last_instruction(&par->insns);
 		pseudo_t val = lookup_var(par, var);
 		pseudo_t phi = alloc_phi(par, val, var);
@@ -319,25 +333,17 @@ static void ssa_rename_phi(struct basic_block *bb, struct instruction *insn)
 	} END_FOR_EACH_PTR(par);
 }
 
-static void ssa_rename_vars(struct entrypoint *ep)
+static void ssa_rename_phis(struct entrypoint *ep)
 {
 	struct basic_block *bb;
 
-	FOR_EACH_PTR(ep->bbs, bb) {
-		struct instruction *insn;
-		FOR_EACH_PTR(bb->insns, insn) {
-			if (!insn->bb)
-				continue;
-			ssa_rename_insn(bb, insn);
-		} END_FOR_EACH_PTR(insn);
-	} END_FOR_EACH_PTR(bb);
 
 	FOR_EACH_PTR(ep->bbs, bb) {
 		struct instruction *insn;
 		FOR_EACH_PTR(bb->insns, insn) {
 			if (!insn->bb || insn->opcode != OP_PHI)
 				continue;
-			ssa_rename_phi(bb, insn);
+			ssa_rename_phi(insn);
 		} END_FOR_EACH_PTR(insn);
 	} END_FOR_EACH_PTR(bb);
 }
@@ -365,5 +371,6 @@ void ssa_convert(struct entrypoint *ep)
 	} END_FOR_EACH_PTR(pseudo);
 
 	// rename the converted accesses
-	ssa_rename_vars(ep);
+	ssa_rename_insns(ep);
+	ssa_rename_phis(ep);
 }
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v1 18/18] ssa: phi worklist ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v1 18/18] ssa: phi worklist
Date: Tue, 20 Mar 2018 00:52:56 +0000
Message-ID: <20180320005256.53284-19-luc.vanoostenryck () gmail ! com>
--------------------
This patch optimize the very simple implementation the phi-node
renaming at the end of the SSA conversion.

It avoids to have to rescan the whole function to find the phi-nodes
by using a worklist to put the phi-nodes during the renaming
of non-phi nodes instructions.

This optimization avoids some O(n^2) behaviour in some pathological
cases.

Note: A lot of optimizations can be done for the renaming.
      For the moment, things are kept as simplest as possible,
      the goal being to to have correctness first.
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 linearize.h                   |  1 +
 ssa.c                         | 43 ++++++++++++++++++++++++++++++++++---------
 validation/mem2reg/quadra01.c |  1 -
 3 files changed, 35 insertions(+), 10 deletions(-)

diff --git a/linearize.h b/linearize.h
index 5592b2c52..a5b06559d 100644
--- a/linearize.h
+++ b/linearize.h
@@ -104,6 +104,7 @@ struct instruction {
 		struct /* phi_node */ {
 			pseudo_t phi_var;		// used for SSA conversion
 			struct pseudo_list *phi_list;
+			unsigned int used:1;
 		};
 		struct /* phi source */ {
 			pseudo_t phi_src;
diff --git a/ssa.c b/ssa.c
index 34d922798..d67c8dede 100644
--- a/ssa.c
+++ b/ssa.c
@@ -263,6 +263,9 @@ static pseudo_t lookup_var(struct basic_block *bb, struct symbol *var)
 	return undef_pseudo();
 }
 
+static struct instruction_list *phis_all;
+static struct instruction_list *phis_used;
+
 static void ssa_rename_insn(struct basic_block *bb, struct instruction *insn)
 {
 	struct symbol *var;
@@ -295,6 +298,7 @@ static void ssa_rename_insn(struct basic_block *bb, struct instruction *insn)
 		if (!var || !var->torename)
 			break;
 		phi_map_update(&bb->phi_map, var, insn->target);
+		add_instruction(&phis_all, insn);
 		break;
 	}
 }
@@ -313,6 +317,21 @@ static void ssa_rename_insns(struct entrypoint *ep)
 	} END_FOR_EACH_PTR(bb);
 }
 
+static void mark_phi_used(pseudo_t val)
+{
+	struct instruction *node;
+
+	if (val->type != PSEUDO_REG)
+		return;
+	node = val->def;
+	if (node->opcode != OP_PHI)
+		return;
+	if (node->used)
+		return;
+	node->used = 1;
+	add_instruction(&phis_used, node);
+}
+
 static void ssa_rename_phi(struct instruction *insn)
 {
 	struct basic_block *par;
@@ -330,22 +349,27 @@ static void ssa_rename_phi(struct instruction *insn)
 		phi->ident = var->ident;
 		add_instruction(&par->insns, term);
 		use_pseudo(insn, phi, add_pseudo(&insn->phi_list, phi));
+		mark_phi_used(val);
 	} END_FOR_EACH_PTR(par);
 }
 
 static void ssa_rename_phis(struct entrypoint *ep)
 {
-	struct basic_block *bb;
+	struct instruction *phi;
 
+	phis_used = NULL;
+	FOR_EACH_PTR(phis_all, phi) {
+		if (has_users(phi->target)) {
+			phi->used = 1;
+			add_instruction(&phis_used, phi);
+		}
+	} END_FOR_EACH_PTR(phi);
 
-	FOR_EACH_PTR(ep->bbs, bb) {
-		struct instruction *insn;
-		FOR_EACH_PTR(bb->insns, insn) {
-			if (!insn->bb || insn->opcode != OP_PHI)
-				continue;
-			ssa_rename_phi(insn);
-		} END_FOR_EACH_PTR(insn);
-	} END_FOR_EACH_PTR(bb);
+	FOR_EACH_PTR(phis_used, phi) {
+		if (!phi->bb)
+			continue;
+		ssa_rename_phi(phi);
+	} END_FOR_EACH_PTR(phi);
 }
 
 void ssa_convert(struct entrypoint *ep)
@@ -371,6 +395,7 @@ void ssa_convert(struct entrypoint *ep)
 	} END_FOR_EACH_PTR(pseudo);
 
 	// rename the converted accesses
+	phis_all = phis_used = NULL;
 	ssa_rename_insns(ep);
 	ssa_rename_phis(ep);
 }
diff --git a/validation/mem2reg/quadra01.c b/validation/mem2reg/quadra01.c
index bab337f2b..b71f46969 100644
--- a/validation/mem2reg/quadra01.c
+++ b/validation/mem2reg/quadra01.c
@@ -20,7 +20,6 @@ static void foo(void) {
  * check-name: quadratic @ liveness
  * check-command: test-linearize -I. $file
  * check-timeout:
- * check-known-to-fail
  *
  * check-output-ignore
  * check-output-excludes: phi\\.
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v12 00/25] kasan: add software tag-based mode for arm64 ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 00/25] kasan: add software tag-based mode for arm64
Date: Tue, 27 Nov 2018 16:55:18 +0000
Message-ID: <cover.1543337629.git.andreyknvl () google ! com>
--------------------
This patchset adds a new software tag-based mode to KASAN [1].
(Initially this mode was called KHWASAN, but it got renamed,
 see the naming rationale at the end of this section).

The plan is to implement HWASan [2] for the kernel with the incentive,
that it's going to have comparable to KASAN performance, but in the same
time consume much less memory, trading that off for somewhat imprecise
bug detection and being supported only for arm64.

The underlying ideas of the approach used by software tag-based KASAN are:

1. By using the Top Byte Ignore (TBI) arm64 CPU feature, we can store
   pointer tags in the top byte of each kernel pointer.

2. Using shadow memory, we can store memory tags for each chunk of kernel
   memory.

3. On each memory allocation, we can generate a random tag, embed it into
   the returned pointer and set the memory tags that correspond to this
   chunk of memory to the same value.

4. By using compiler instrumentation, before each memory access we can add
   a check that the pointer tag matches the tag of the memory that is being
   accessed.

5. On a tag mismatch we report an error.

With this patchset the existing KASAN mode gets renamed to generic KASAN,
with the word "generic" meaning that the implementation can be supported
by any architecture as it is purely software.

The new mode this patchset adds is called software tag-based KASAN. The
word "tag-based" refers to the fact that this mode uses tags embedded into
the top byte of kernel pointers and the TBI arm64 CPU feature that allows
to dereference such pointers. The word "software" here means that shadow
memory manipulation and tag checking on pointer dereference is done in
software. As it is the only tag-based implementation right now, "software
tag-based" KASAN is sometimes referred to as simply "tag-based" in this
patchset.

A potential expansion of this mode is a hardware tag-based mode, which would
use hardware memory tagging support (announced by Arm [3]) instead of
compiler instrumentation and manual shadow memory manipulation.

Same as generic KASAN, software tag-based KASAN is strictly a debugging
feature.

[1] https://www.kernel.org/doc/html/latest/dev-tools/kasan.html

[2] http://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.html

[3] https://community.arm.com/processors/b/blog/posts/arm-a-profile-architecture-2018-developments-armv85a


====== Rationale

On mobile devices generic KASAN's memory usage is significant problem. One
of the main reasons to have tag-based KASAN is to be able to perform a
similar set of checks as the generic one does, but with lower memory
requirements.

Comment from Vishwath Mohan <vishwath@google.com>:

I don't have data on-hand, but anecdotally both ASAN and KASAN have proven
problematic to enable for environments that don't tolerate the increased
memory pressure well. This includes,
(a) Low-memory form factors - Wear, TV, Things, lower-tier phones like Go,
(c) Connected components like Pixel's visual core [1].

These are both places I'd love to have a low(er) memory footprint option at
my disposal.

Comment from Evgenii Stepanov <eugenis@google.com>:

Looking at a live Android device under load, slab (according to
/proc/meminfo) + kernel stack take 8-10% available RAM (~350MB). KASAN's
overhead of 2x - 3x on top of it is not insignificant.

Not having this overhead enables near-production use - ex. running
KASAN/KHWASAN kernel on a personal, daily-use device to catch bugs that do
not reproduce in test configuration. These are the ones that often cost
the most engineering time to track down.

CPU overhead is bad, but generally tolerable. RAM is critical, in our
experience. Once it gets low enough, OOM-killer makes your life miserable.

[1] https://www.blog.google/products/pixel/pixel-visual-core-image-processing-and-machine-learning-pixel-2/


====== Technical details

Software tag-based KASAN mode is implemented in a very similar way to the
generic one. This patchset essentially does the following:

1. TCR_TBI1 is set to enable Top Byte Ignore.

2. Shadow memory is used (with a different scale, 1:16, so each shadow
   byte corresponds to 16 bytes of kernel memory) to store memory tags.

3. All slab objects are aligned to shadow scale, which is 16 bytes.

4. All pointers returned from the slab allocator are tagged with a random
   tag and the corresponding shadow memory is poisoned with the same value.

5. Compiler instrumentation is used to insert tag checks. Either by
   calling callbacks or by inlining them (CONFIG_KASAN_OUTLINE and
   CONFIG_KASAN_INLINE flags are reused).

6. When a tag mismatch is detected in callback instrumentation mode
   KASAN simply prints a bug report. In case of inline instrumentation,
   clang inserts a brk instruction, and KASAN has it's own brk handler,
   which reports the bug.

7. The memory in between slab objects is marked with a reserved tag, and
   acts as a redzone.

8. When a slab object is freed it's marked with a reserved tag.

Bug detection is imprecise for two reasons:

1. We won't catch some small out-of-bounds accesses, that fall into the
   same shadow cell, as the last byte of a slab object.

2. We only have 1 byte to store tags, which means we have a 1/256
   probability of a tag match for an incorrect access (actually even
   slightly less due to reserved tag values).

Despite that there's a particular type of bugs that tag-based KASAN can
detect compared to generic KASAN: use-after-free after the object has been
allocated by someone else.


====== Testing

Some kernel developers voiced a concern that changing the top byte of
kernel pointers may lead to subtle bugs that are difficult to discover.
To address this concern deliberate testing has been performed.

It doesn't seem feasible to do some kind of static checking to find
potential issues with pointer tagging, so a dynamic approach was taken.
All pointer comparisons/subtractions have been instrumented in an LLVM
compiler pass and a kernel module that would print a bug report whenever
two pointers with different tags are being compared/subtracted (ignoring
comparisons with NULL pointers and with pointers obtained by casting an
error code to a pointer type) has been used. Then the kernel has been
booted in QEMU and on an Odroid C2 board and syzkaller has been run.

This yielded the following results.

The two places that look interesting are:

is_vmalloc_addr in include/linux/mm.h
is_kernel_rodata in mm/util.c

Here we compare a pointer with some fixed untagged values to make sure
that the pointer lies in a particular part of the kernel address space.
Since tag-based KASAN doesn't add tags to pointers that belong to rodata
or vmalloc regions, this should work as is. To make sure debug checks to
those two functions that check that the result doesn't change whether
we operate on pointers with or without untagging has been added.

A few other cases that don't look that interesting:

Comparing pointers to achieve unique sorting order of pointee objects
(e.g. sorting locks addresses before performing a double lock):

tty_ldisc_lock_pair_timeout in drivers/tty/tty_ldisc.c
pipe_double_lock in fs/pipe.c
unix_state_double_lock in net/unix/af_unix.c
lock_two_nondirectories in fs/inode.c
mutex_lock_double in kernel/events/core.c

ep_cmp_ffd in fs/eventpoll.c
fsnotify_compare_groups fs/notify/mark.c

Nothing needs to be done here, since the tags embedded into pointers
don't change, so the sorting order would still be unique.

Checks that a pointer belongs to some particular allocation:

is_sibling_entry in lib/radix-tree.c
object_is_on_stack in include/linux/sched/task_stack.h

Nothing needs to be done here either, since two pointers can only belong
to the same allocation if they have the same tag.

Overall, since the kernel boots and works, there are no critical bugs.
As for the rest, the traditional kernel testing way (use until fails) is
the only one that looks feasible.

Another point here is that tag-based KASAN is available under a separate
config option that needs to be deliberately enabled. Even though it might
be used in a "near-production" environment to find bugs that are not found
during fuzzing or running tests, it is still a debug tool.


====== Benchmarks

The following numbers were collected on Odroid C2 board. Both generic and
tag-based KASAN were used in inline instrumentation mode.

Boot time [1]:
* ~1.7 sec for clean kernel
* ~5.0 sec for generic KASAN
* ~5.0 sec for tag-based KASAN

Network performance [2]:
* 8.33 Gbits/sec for clean kernel
* 3.17 Gbits/sec for generic KASAN
* 2.85 Gbits/sec for tag-based KASAN

Slab memory usage after boot [3]:
* ~40 kb for clean kernel
* ~105 kb (~260% overhead) for generic KASAN
* ~47 kb (~20% overhead) for tag-based KASAN

KASAN memory overhead consists of three main parts:
1. Increased slab memory usage due to redzones.
2. Shadow memory (the whole reserved once during boot).
3. Quaratine (grows gradually until some preset limit; the more the limit,
   the more the chance to detect a use-after-free).

Comparing tag-based vs generic KASAN for each of these points:
1. 20% vs 260% overhead.
2. 1/16th vs 1/8th of physical memory.
3. Tag-based KASAN doesn't require quarantine.

[1] Time before the ext4 driver is initialized.
[2] Measured as `iperf -s & iperf -c 127.0.0.1 -t 30`.
[3] Measured as `cat /proc/meminfo | grep Slab`.


====== Some notes

A few notes:

1. The patchset can be found here:
   https://github.com/xairy/kasan-prototype/tree/khwasan

2. Building requires a recent Clang version (7.0.0 or later).

3. Stack instrumentation is not supported yet and will be added later.


====== Changes

Changes in v12:
- Rebased onto ef78e5ec (4.20-rc4+).
- Used u64 instead of __u64 in arch/arm64/include/asm/memory.h as it isn't
  a UAPI header.
- Moved the untagged_addr() macro down into the #ifndef __ASSEMBLY__
  block, after we include <linux/bitops.h>, which is necessary for
  sign_extend64().
- New patch: "kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS", that selects
  HAVE_ARCH_KASAN_SW_TAGS for arm64 after all the necessary infrastructure
  code is added.
- Minor style fixes.

Changes in v11:
- Rebased onto 9ff01193 (4.20-rc3).
- Moved KASAN_SHADOW_SCALE_SHIFT definition to arch/arm64/Makefile.
- Added and used CC_HAS_KASAN_GENERIC and CC_HAS_KASAN_SW_TAGS configs to
  detect compiler support.
- New patch: "kasan: rename kasan_zero_page to kasan_early_shadow_page".
- New patch: "arm64: move untagged_addr macro from uaccess.h to memory.h".
- Renamed KASAN_SET_TAG/... macros in arch/arm64/include/asm/memory.h to
  __tag_set/... and reused them later in KASAN core code instead of
  redefining.
- Removed tag reset from the __kimg_to_phys() macro.
- Fixed tagged pointer handling in arm64 fault handling logic.

Changes in v10:
- Rebased onto 65102238 (4.20-rc1).
- Don't ignore kasan_kmalloc() return valued in kmem_cache_alloc_trace()
  and kmem_cache_alloc_node_trace() in include/linux/slab.h.
- New patch: don't ignore kasan_kmalloc return value in
  early_kmem_cache_node_alloc.
- New patch: added __must_check annotations to KASAN hooks that assign
  tags.
- Changed KASAN clang version requirement to 7.0.0 (as we need rL329612).
- Moved __no_sanitize_address definition from compiler_attributes.h to
  compiler-gcc.h and compiler-clang.h.

Changes in v9:
- Fixed kasan_init_slab_obj() hook when KASAN is disabled.
- Added assign_tag() function that preassigns tags for caches with
  constructors.
- Fixed KASAN_TAG_MASK redefinition in include/linux/mm.h vs
  mm/kasan/kasan.h.

Changes in v8:
- Rebased onto 7876320f (4.19-rc4).
- Renamed KHWASAN to software tag-based KASAN (see the top of the cover
  letter for details).
- Explicitly called tag-based KASAN a debug tool.
- Reused kasan_init_slab_obj() callback to preassign tags to caches
  without constructors, remove khwasan_preset_sl(u/a)b_tag().
- Moved move obj_to_index to include/linux/slab_def.h from mm/slab.c.
- Moved cache->s_mem untagging to alloc_slabmgmt() for SLAB.
- Fixed check_memory_region() to correctly handle user memory accesses and
  size == 0 case.
- Merged __no_sanitize_hwaddress into __no_sanitize_address.
- Defined KASAN_SET_TAG and KASAN_RESET_TAG macros for non KASAN builds to
  avoid duplication of __kimg_to_phys, _virt_addr_is_linear and
  page_to_virt macros.
- Fixed and simplified find_first_bad_addr for generic KASAN.
- Use non symbolized example KASAN report in documentation.
- Mention clang version requirements for both KASAN modes in the Kconfig
  options and in the documentation.
- Various small fixes.

Version v7 got accidentally skipped.

Changes in v6:
- Rebased onto 050cdc6c (4.19-rc1+).
- Added notes regarding patchset testing into the cover letter.

Changes in v5:
- Rebased onto 1ffaddd029 (4.18-rc8).
- Preassign tags for objects from caches with constructors and
  SLAB_TYPESAFE_BY_RCU caches.
- Fix SLAB allocator support by untagging page->s_mem in
  kasan_poison_slab().
- Performed dynamic testing to find potential places where pointer tagging
  might result in bugs [1].
- Clarified and fixed memory usage benchmarks in the cover letter.
- Added a rationale for having KHWASAN to the cover letter.

Changes in v4:
- Fixed SPDX comment style in mm/kasan/kasan.h.
- Fixed mm/kasan/kasan.h changes being included in a wrong patch.
- Swapped "khwasan, arm64: fix up fault handling logic" and "khwasan: add
  tag related helper functions" patches order.
- Rebased onto 6f0d349d (4.18-rc2+).

Changes in v3:
- Minor documentation fixes.
- Fixed CFLAGS variable name in KASAN makefile.
- Added a "SPDX-License-Identifier: GPL-2.0" line to all source files
  under mm/kasan.
- Rebased onto 81e97f013 (4.18-rc1+).

Changes in v2:
- Changed kmalloc_large_node_hook to return tagged pointer instead of
  using an output argument.
- Fix checking whether -fsanitize=hwaddress is supported by the compiler.
- Removed duplication of -fno-builtin for KASAN and KHWASAN.
- Removed {} block for one line for_each_possible_cpu loop.
- Made set_track() static inline as it is used only in common.c.
- Moved optimal_redzone() to common.c.
- Fixed using tagged pointer for shadow calculation in
  kasan_unpoison_shadow().
- Restored setting cache->align in kasan_cache_create(), which was
  accidentally lost.
- Simplified __kasan_slab_free(), kasan_alloc_pages() and kasan_kmalloc().
- Removed tagging from kasan_kmalloc_large().
- Added page_kasan_tag_reset() to kasan_poison_slab() and removed
  !PageSlab() check from page_to_virt.
- Reset pointer tag in _virt_addr_is_linear.
- Set page tag for each page when multiple pages are allocated or freed.
- Added a comment as to why we ignore cma allocated pages.

Changes in v1:
- Rebased onto 4.17-rc4.
- Updated benchmarking stats.
- Documented compiler version requirements, memory usage and slowdown.
- Dropped kvm patches, as clang + arm64 + kvm is completely broken [1].

Changes in RFC v3:
- Renamed CONFIG_KASAN_CLASSIC and CONFIG_KASAN_TAGS to
  CONFIG_KASAN_GENERIC and CONFIG_KASAN_HW respectively.
- Switch to -fsanitize=kernel-hwaddress instead of -fsanitize=hwaddress.
- Removed unnecessary excessive shadow initialization.
- Removed khwasan_enabled flag (it's not needed since KHWASAN is
  initialized before any slab caches are used).
- Split out kasan_report.c and khwasan_report.c from report.c.
- Moved more common KASAN and KHWASAN functions to common.c.
- Added tagging to pagealloc.
- Rebased onto 4.17-rc1.
- Temporarily dropped patch that adds kvm support (arm64 + kvm + clang
  combo is broken right now [2]).

Changes in RFC v2:
- Removed explicit casts to u8 * for kasan_mem_to_shadow() calls.
- Introduced KASAN_TCR_FLAGS for setting the TCR_TBI1 flag.
- Added a comment regarding the non-atomic RMW sequence in
  khwasan_random_tag().
- Made all tag related functions accept const void *.
- Untagged pointers in __kimg_to_phys, which is used by virt_to_phys.
- Untagged pointers in show_ptr in fault handling logic.
- Untagged pointers passed to KVM.
- Added two reserved tag values: 0xFF and 0xFE.
- Used the reserved tag 0xFF to disable validity checking (to resolve the
  issue with pointer tag being lost after page_address + kmap usage).
- Used the reserved tag 0xFE to mark redzones and freed objects.
- Added mnemonics for esr manipulation in KHWASAN brk handler.
- Added a comment about the -recover flag.
- Some minor cleanups and fixes.
- Rebased onto 3215b9d5 (4.16-rc6+).
- Tested on real hardware (Odroid C2 board).
- Added better benchmarks.

[1] https://lkml.org/lkml/2018/7/18/765
[2] https://lkml.org/lkml/2018/4/19/775

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>

Andrey Konovalov (25):
  kasan, mm: change hooks signatures
  kasan, slub: handle pointer tags in early_kmem_cache_node_alloc
  kasan: move common generic and tag-based code to common.c
  kasan: rename source files to reflect the new naming scheme
  kasan: add CONFIG_KASAN_GENERIC and CONFIG_KASAN_SW_TAGS
  kasan, arm64: adjust shadow size for tag-based mode
  kasan: rename kasan_zero_page to kasan_early_shadow_page
  kasan: initialize shadow to 0xff for tag-based mode
  arm64: move untagged_addr macro from uaccess.h to memory.h
  kasan: add tag related helper functions
  kasan, arm64: untag address in _virt_addr_is_linear
  kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
  kasan, arm64: fix up fault handling logic
  kasan, arm64: enable top byte ignore for the kernel
  kasan, mm: perform untagged pointers comparison in krealloc
  kasan: split out generic_report.c from report.c
  kasan: add bug reporting routines for tag-based mode
  mm: move obj_to_index to include/linux/slab_def.h
  kasan: add hooks implementation for tag-based mode
  kasan, arm64: add brk handler for inline instrumentation
  kasan, mm, arm64: tag non slab memory allocated via pagealloc
  kasan: add __must_check annotations to kasan hooks
  kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
  kasan: update documentation
  kasan: add SPDX-License-Identifier mark to source files

 Documentation/dev-tools/kasan.rst      | 232 +++++----
 arch/arm64/Kconfig                     |   1 +
 arch/arm64/Makefile                    |  11 +-
 arch/arm64/include/asm/brk-imm.h       |   2 +
 arch/arm64/include/asm/kasan.h         |   8 +-
 arch/arm64/include/asm/memory.h        |  42 +-
 arch/arm64/include/asm/pgtable-hwdef.h |   1 +
 arch/arm64/include/asm/uaccess.h       |   7 -
 arch/arm64/kernel/traps.c              |  68 ++-
 arch/arm64/mm/fault.c                  |  31 +-
 arch/arm64/mm/kasan_init.c             |  56 ++-
 arch/arm64/mm/proc.S                   |   8 +-
 arch/s390/mm/dump_pagetables.c         |  17 +-
 arch/s390/mm/kasan_init.c              |  33 +-
 arch/x86/mm/dump_pagetables.c          |  11 +-
 arch/x86/mm/kasan_init_64.c            |  55 ++-
 arch/xtensa/mm/kasan_init.c            |  18 +-
 include/linux/compiler-clang.h         |   6 +-
 include/linux/compiler-gcc.h           |   6 +
 include/linux/compiler_attributes.h    |  13 -
 include/linux/kasan.h                  | 101 +++-
 include/linux/mm.h                     |  29 ++
 include/linux/page-flags-layout.h      |  10 +
 include/linux/slab.h                   |   4 +-
 include/linux/slab_def.h               |  13 +
 lib/Kconfig.kasan                      |  96 +++-
 mm/cma.c                               |  11 +
 mm/kasan/Makefile                      |  15 +-
 mm/kasan/{kasan.c => common.c}         | 656 +++++++++----------------
 mm/kasan/generic.c                     | 344 +++++++++++++
 mm/kasan/generic_report.c              | 153 ++++++
 mm/kasan/{kasan_init.c => init.c}      |  71 +--
 mm/kasan/kasan.h                       |  59 ++-
 mm/kasan/quarantine.c                  |   1 +
 mm/kasan/report.c                      | 272 +++-------
 mm/kasan/tags.c                        | 161 ++++++
 mm/kasan/tags_report.c                 |  58 +++
 mm/page_alloc.c                        |   1 +
 mm/slab.c                              |  29 +-
 mm/slab.h                              |   2 +-
 mm/slab_common.c                       |   6 +-
 mm/slub.c                              |  51 +-
 scripts/Makefile.kasan                 |  53 +-
 43 files changed, 1825 insertions(+), 997 deletions(-)
 rename mm/kasan/{kasan.c => common.c} (59%)
 create mode 100644 mm/kasan/generic.c
 create mode 100644 mm/kasan/generic_report.c
 rename mm/kasan/{kasan_init.c => init.c} (82%)
 create mode 100644 mm/kasan/tags.c
 create mode 100644 mm/kasan/tags_report.c

-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 01/25] kasan, mm: change hooks signatures ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 01/25] kasan, mm: change hooks signatures
Date: Tue, 27 Nov 2018 16:55:19 +0000
Message-ID: <10068968c5dbdd1913bd60f89262e3c1a50f3d38.1543337629.git.andreyknvl () google ! com>
--------------------
Tag-based KASAN changes the value of the top byte of pointers returned
from the kernel allocation functions (such as kmalloc). This patch updates
KASAN hooks signatures and their usage in SLAB and SLUB code to reflect
that.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 include/linux/kasan.h | 43 +++++++++++++++++++++++++++++--------------
 include/linux/slab.h  |  4 ++--
 mm/kasan/kasan.c      | 30 ++++++++++++++++++------------
 mm/slab.c             | 12 ++++++------
 mm/slab.h             |  2 +-
 mm/slab_common.c      |  4 ++--
 mm/slub.c             | 15 +++++++--------
 7 files changed, 65 insertions(+), 45 deletions(-)

diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index 46aae129917c..52c86a568a4e 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -51,16 +51,16 @@ void kasan_cache_shutdown(struct kmem_cache *cache);
 void kasan_poison_slab(struct page *page);
 void kasan_unpoison_object_data(struct kmem_cache *cache, void *object);
 void kasan_poison_object_data(struct kmem_cache *cache, void *object);
-void kasan_init_slab_obj(struct kmem_cache *cache, const void *object);
+void *kasan_init_slab_obj(struct kmem_cache *cache, const void *object);
 
-void kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags);
+void *kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags);
 void kasan_kfree_large(void *ptr, unsigned long ip);
 void kasan_poison_kfree(void *ptr, unsigned long ip);
-void kasan_kmalloc(struct kmem_cache *s, const void *object, size_t size,
+void *kasan_kmalloc(struct kmem_cache *s, const void *object, size_t size,
 		  gfp_t flags);
-void kasan_krealloc(const void *object, size_t new_size, gfp_t flags);
+void *kasan_krealloc(const void *object, size_t new_size, gfp_t flags);
 
-void kasan_slab_alloc(struct kmem_cache *s, void *object, gfp_t flags);
+void *kasan_slab_alloc(struct kmem_cache *s, void *object, gfp_t flags);
 bool kasan_slab_free(struct kmem_cache *s, void *object, unsigned long ip);
 
 struct kasan_cache {
@@ -105,19 +105,34 @@ static inline void kasan_unpoison_object_data(struct kmem_cache *cache,
 					void *object) {}
 static inline void kasan_poison_object_data(struct kmem_cache *cache,
 					void *object) {}
-static inline void kasan_init_slab_obj(struct kmem_cache *cache,
-				const void *object) {}
+static inline void *kasan_init_slab_obj(struct kmem_cache *cache,
+				const void *object)
+{
+	return (void *)object;
+}
 
-static inline void kasan_kmalloc_large(void *ptr, size_t size, gfp_t flags) {}
+static inline void *kasan_kmalloc_large(void *ptr, size_t size, gfp_t flags)
+{
+	return ptr;
+}
 static inline void kasan_kfree_large(void *ptr, unsigned long ip) {}
 static inline void kasan_poison_kfree(void *ptr, unsigned long ip) {}
-static inline void kasan_kmalloc(struct kmem_cache *s, const void *object,
-				size_t size, gfp_t flags) {}
-static inline void kasan_krealloc(const void *object, size_t new_size,
-				 gfp_t flags) {}
+static inline void *kasan_kmalloc(struct kmem_cache *s, const void *object,
+				size_t size, gfp_t flags)
+{
+	return (void *)object;
+}
+static inline void *kasan_krealloc(const void *object, size_t new_size,
+				 gfp_t flags)
+{
+	return (void *)object;
+}
 
-static inline void kasan_slab_alloc(struct kmem_cache *s, void *object,
-				   gfp_t flags) {}
+static inline void *kasan_slab_alloc(struct kmem_cache *s, void *object,
+				   gfp_t flags)
+{
+	return object;
+}
 static inline bool kasan_slab_free(struct kmem_cache *s, void *object,
 				   unsigned long ip)
 {
diff --git a/include/linux/slab.h b/include/linux/slab.h
index 918f374e7156..351ac48dabc4 100644
--- a/include/linux/slab.h
+++ b/include/linux/slab.h
@@ -444,7 +444,7 @@ static __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,
 {
 	void *ret = kmem_cache_alloc(s, flags);
 
-	kasan_kmalloc(s, ret, size, flags);
+	ret = kasan_kmalloc(s, ret, size, flags);
 	return ret;
 }
 
@@ -455,7 +455,7 @@ kmem_cache_alloc_node_trace(struct kmem_cache *s,
 {
 	void *ret = kmem_cache_alloc_node(s, gfpflags, node);
 
-	kasan_kmalloc(s, ret, size, gfpflags);
+	ret = kasan_kmalloc(s, ret, size, gfpflags);
 	return ret;
 }
 #endif /* CONFIG_TRACING */
diff --git a/mm/kasan/kasan.c b/mm/kasan/kasan.c
index c3bd5209da38..55deff17a4d9 100644
--- a/mm/kasan/kasan.c
+++ b/mm/kasan/kasan.c
@@ -474,20 +474,22 @@ struct kasan_free_meta *get_free_info(struct kmem_cache *cache,
 	return (void *)object + cache->kasan_info.free_meta_offset;
 }
 
-void kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
+void *kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
 {
 	struct kasan_alloc_meta *alloc_info;
 
 	if (!(cache->flags & SLAB_KASAN))
-		return;
+		return (void *)object;
 
 	alloc_info = get_alloc_info(cache, object);
 	__memset(alloc_info, 0, sizeof(*alloc_info));
+
+	return (void *)object;
 }
 
-void kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t flags)
+void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t flags)
 {
-	kasan_kmalloc(cache, object, cache->object_size, flags);
+	return kasan_kmalloc(cache, object, cache->object_size, flags);
 }
 
 static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
@@ -528,7 +530,7 @@ bool kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
 	return __kasan_slab_free(cache, object, ip, true);
 }
 
-void kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
+void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 		   gfp_t flags)
 {
 	unsigned long redzone_start;
@@ -538,7 +540,7 @@ void kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 		quarantine_reduce();
 
 	if (unlikely(object == NULL))
-		return;
+		return NULL;
 
 	redzone_start = round_up((unsigned long)(object + size),
 				KASAN_SHADOW_SCALE_SIZE);
@@ -551,10 +553,12 @@ void kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 
 	if (cache->flags & SLAB_KASAN)
 		set_track(&get_alloc_info(cache, object)->alloc_track, flags);
+
+	return (void *)object;
 }
 EXPORT_SYMBOL(kasan_kmalloc);
 
-void kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
+void *kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
 {
 	struct page *page;
 	unsigned long redzone_start;
@@ -564,7 +568,7 @@ void kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
 		quarantine_reduce();
 
 	if (unlikely(ptr == NULL))
-		return;
+		return NULL;
 
 	page = virt_to_page(ptr);
 	redzone_start = round_up((unsigned long)(ptr + size),
@@ -574,21 +578,23 @@ void kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
 	kasan_unpoison_shadow(ptr, size);
 	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
 		KASAN_PAGE_REDZONE);
+
+	return (void *)ptr;
 }
 
-void kasan_krealloc(const void *object, size_t size, gfp_t flags)
+void *kasan_krealloc(const void *object, size_t size, gfp_t flags)
 {
 	struct page *page;
 
 	if (unlikely(object == ZERO_SIZE_PTR))
-		return;
+		return ZERO_SIZE_PTR;
 
 	page = virt_to_head_page(object);
 
 	if (unlikely(!PageSlab(page)))
-		kasan_kmalloc_large(object, size, flags);
+		return kasan_kmalloc_large(object, size, flags);
 	else
-		kasan_kmalloc(page->slab_cache, object, size, flags);
+		return kasan_kmalloc(page->slab_cache, object, size, flags);
 }
 
 void kasan_poison_kfree(void *ptr, unsigned long ip)
diff --git a/mm/slab.c b/mm/slab.c
index 2a5654bb3b3f..26f60a22e5e0 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -3551,7 +3551,7 @@ void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)
 {
 	void *ret = slab_alloc(cachep, flags, _RET_IP_);
 
-	kasan_slab_alloc(cachep, ret, flags);
+	ret = kasan_slab_alloc(cachep, ret, flags);
 	trace_kmem_cache_alloc(_RET_IP_, ret,
 			       cachep->object_size, cachep->size, flags);
 
@@ -3617,7 +3617,7 @@ kmem_cache_alloc_trace(struct kmem_cache *cachep, gfp_t flags, size_t size)
 
 	ret = slab_alloc(cachep, flags, _RET_IP_);
 
-	kasan_kmalloc(cachep, ret, size, flags);
+	ret = kasan_kmalloc(cachep, ret, size, flags);
 	trace_kmalloc(_RET_IP_, ret,
 		      size, cachep->size, flags);
 	return ret;
@@ -3641,7 +3641,7 @@ void *kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid)
 {
 	void *ret = slab_alloc_node(cachep, flags, nodeid, _RET_IP_);
 
-	kasan_slab_alloc(cachep, ret, flags);
+	ret = kasan_slab_alloc(cachep, ret, flags);
 	trace_kmem_cache_alloc_node(_RET_IP_, ret,
 				    cachep->object_size, cachep->size,
 				    flags, nodeid);
@@ -3660,7 +3660,7 @@ void *kmem_cache_alloc_node_trace(struct kmem_cache *cachep,
 
 	ret = slab_alloc_node(cachep, flags, nodeid, _RET_IP_);
 
-	kasan_kmalloc(cachep, ret, size, flags);
+	ret = kasan_kmalloc(cachep, ret, size, flags);
 	trace_kmalloc_node(_RET_IP_, ret,
 			   size, cachep->size,
 			   flags, nodeid);
@@ -3681,7 +3681,7 @@ __do_kmalloc_node(size_t size, gfp_t flags, int node, unsigned long caller)
 	if (unlikely(ZERO_OR_NULL_PTR(cachep)))
 		return cachep;
 	ret = kmem_cache_alloc_node_trace(cachep, flags, node, size);
-	kasan_kmalloc(cachep, ret, size, flags);
+	ret = kasan_kmalloc(cachep, ret, size, flags);
 
 	return ret;
 }
@@ -3719,7 +3719,7 @@ static __always_inline void *__do_kmalloc(size_t size, gfp_t flags,
 		return cachep;
 	ret = slab_alloc(cachep, flags, caller);
 
-	kasan_kmalloc(cachep, ret, size, flags);
+	ret = kasan_kmalloc(cachep, ret, size, flags);
 	trace_kmalloc(caller, ret,
 		      size, cachep->size, flags);
 
diff --git a/mm/slab.h b/mm/slab.h
index 58c6c1c2a78e..4190c24ef0e9 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -441,7 +441,7 @@ static inline void slab_post_alloc_hook(struct kmem_cache *s, gfp_t flags,
 
 		kmemleak_alloc_recursive(object, s->object_size, 1,
 					 s->flags, flags);
-		kasan_slab_alloc(s, object, flags);
+		p[i] = kasan_slab_alloc(s, object, flags);
 	}
 
 	if (memcg_kmem_enabled())
diff --git a/mm/slab_common.c b/mm/slab_common.c
index 7eb8dc136c1c..5f3504e26d4c 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -1204,7 +1204,7 @@ void *kmalloc_order(size_t size, gfp_t flags, unsigned int order)
 	page = alloc_pages(flags, order);
 	ret = page ? page_address(page) : NULL;
 	kmemleak_alloc(ret, size, 1, flags);
-	kasan_kmalloc_large(ret, size, flags);
+	ret = kasan_kmalloc_large(ret, size, flags);
 	return ret;
 }
 EXPORT_SYMBOL(kmalloc_order);
@@ -1482,7 +1482,7 @@ static __always_inline void *__do_krealloc(const void *p, size_t new_size,
 		ks = ksize(p);
 
 	if (ks >= new_size) {
-		kasan_krealloc((void *)p, new_size, flags);
+		p = kasan_krealloc((void *)p, new_size, flags);
 		return (void *)p;
 	}
 
diff --git a/mm/slub.c b/mm/slub.c
index e3629cd7aff1..fdd4a86aa882 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1372,10 +1372,10 @@ static inline void dec_slabs_node(struct kmem_cache *s, int node,
  * Hooks for other subsystems that check memory allocations. In a typical
  * production configuration these hooks all should produce no code at all.
  */
-static inline void kmalloc_large_node_hook(void *ptr, size_t size, gfp_t flags)
+static inline void *kmalloc_large_node_hook(void *ptr, size_t size, gfp_t flags)
 {
 	kmemleak_alloc(ptr, size, 1, flags);
-	kasan_kmalloc_large(ptr, size, flags);
+	return kasan_kmalloc_large(ptr, size, flags);
 }
 
 static __always_inline void kfree_hook(void *x)
@@ -2768,7 +2768,7 @@ void *kmem_cache_alloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size)
 {
 	void *ret = slab_alloc(s, gfpflags, _RET_IP_);
 	trace_kmalloc(_RET_IP_, ret, size, s->size, gfpflags);
-	kasan_kmalloc(s, ret, size, gfpflags);
+	ret = kasan_kmalloc(s, ret, size, gfpflags);
 	return ret;
 }
 EXPORT_SYMBOL(kmem_cache_alloc_trace);
@@ -2796,7 +2796,7 @@ void *kmem_cache_alloc_node_trace(struct kmem_cache *s,
 	trace_kmalloc_node(_RET_IP_, ret,
 			   size, s->size, gfpflags, node);
 
-	kasan_kmalloc(s, ret, size, gfpflags);
+	ret = kasan_kmalloc(s, ret, size, gfpflags);
 	return ret;
 }
 EXPORT_SYMBOL(kmem_cache_alloc_node_trace);
@@ -3784,7 +3784,7 @@ void *__kmalloc(size_t size, gfp_t flags)
 
 	trace_kmalloc(_RET_IP_, ret, size, s->size, flags);
 
-	kasan_kmalloc(s, ret, size, flags);
+	ret = kasan_kmalloc(s, ret, size, flags);
 
 	return ret;
 }
@@ -3801,8 +3801,7 @@ static void *kmalloc_large_node(size_t size, gfp_t flags, int node)
 	if (page)
 		ptr = page_address(page);
 
-	kmalloc_large_node_hook(ptr, size, flags);
-	return ptr;
+	return kmalloc_large_node_hook(ptr, size, flags);
 }
 
 void *__kmalloc_node(size_t size, gfp_t flags, int node)
@@ -3829,7 +3828,7 @@ void *__kmalloc_node(size_t size, gfp_t flags, int node)
 
 	trace_kmalloc_node(_RET_IP_, ret, size, s->size, flags, node);
 
-	kasan_kmalloc(s, ret, size, flags);
+	ret = kasan_kmalloc(s, ret, size, flags);
 
 	return ret;
 }
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 02/25] kasan, slub: handle pointer tags in early_kmem_cache_node_alloc ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 02/25] kasan, slub: handle pointer tags in early_kmem_cache_node_alloc
Date: Tue, 27 Nov 2018 16:55:20 +0000
Message-ID: <3459b9f5d4daf96668d9579da2cf15eb5523284b.1543337629.git.andreyknvl () google ! com>
--------------------
The previous patch updated KASAN hooks signatures and their usage in SLAB
and SLUB code, except for the early_kmem_cache_node_alloc function. This
patch handles that function separately, as it requires to reorder some of
the initialization code to correctly propagate a tagged pointer in case a
tag is assigned by kasan_kmalloc.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/slub.c | 10 +++++-----
 1 file changed, 5 insertions(+), 5 deletions(-)

diff --git a/mm/slub.c b/mm/slub.c
index fdd4a86aa882..8561a32910dd 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -3364,16 +3364,16 @@ static void early_kmem_cache_node_alloc(int node)
 
 	n = page->freelist;
 	BUG_ON(!n);
-	page->freelist = get_freepointer(kmem_cache_node, n);
-	page->inuse = 1;
-	page->frozen = 0;
-	kmem_cache_node->node[node] = n;
 #ifdef CONFIG_SLUB_DEBUG
 	init_object(kmem_cache_node, n, SLUB_RED_ACTIVE);
 	init_tracking(kmem_cache_node, n);
 #endif
-	kasan_kmalloc(kmem_cache_node, n, sizeof(struct kmem_cache_node),
+	n = kasan_kmalloc(kmem_cache_node, n, sizeof(struct kmem_cache_node),
 		      GFP_KERNEL);
+	page->freelist = get_freepointer(kmem_cache_node, n);
+	page->inuse = 1;
+	page->frozen = 0;
+	kmem_cache_node->node[node] = n;
 	init_kmem_cache_node(n);
 	inc_slabs_node(kmem_cache_node, node, page->objects);
 
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 03/25] kasan: move common generic and tag-based code to common.c ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 03/25] kasan: move common generic and tag-based code to common.c
Date: Tue, 27 Nov 2018 16:55:21 +0000
Message-ID: <c9392610bdef2cbbc4d4c8accd71b551ee32e2c2.1543337629.git.andreyknvl () google ! com>
--------------------
Tag-based KASAN reuses a significant part of the generic KASAN code, so
move the common parts to common.c without any functional changes.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/Makefile |   5 +-
 mm/kasan/common.c | 603 ++++++++++++++++++++++++++++++++++++++++++++++
 mm/kasan/kasan.c  | 570 +------------------------------------------
 mm/kasan/kasan.h  |   5 +
 4 files changed, 614 insertions(+), 569 deletions(-)
 create mode 100644 mm/kasan/common.c

diff --git a/mm/kasan/Makefile b/mm/kasan/Makefile
index 3289db38bc87..a6df14bffb6b 100644
--- a/mm/kasan/Makefile
+++ b/mm/kasan/Makefile
@@ -1,11 +1,14 @@
 # SPDX-License-Identifier: GPL-2.0
 KASAN_SANITIZE := n
+UBSAN_SANITIZE_common.o := n
 UBSAN_SANITIZE_kasan.o := n
 KCOV_INSTRUMENT := n
 
 CFLAGS_REMOVE_kasan.o = -pg
 # Function splitter causes unnecessary splits in __asan_load1/__asan_store1
 # see: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=63533
+
+CFLAGS_common.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 CFLAGS_kasan.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 
-obj-y := kasan.o report.o kasan_init.o quarantine.o
+obj-y := common.o kasan.o report.o kasan_init.o quarantine.o
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
new file mode 100644
index 000000000000..5f68c93734ba
--- /dev/null
+++ b/mm/kasan/common.c
@@ -0,0 +1,603 @@
+/*
+ * This file contains common generic and tag-based KASAN code.
+ *
+ * Copyright (c) 2014 Samsung Electronics Co., Ltd.
+ * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
+ *
+ * Some code borrowed from https://github.com/xairy/kasan-prototype by
+ *        Andrey Konovalov <andreyknvl@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/export.h>
+#include <linux/interrupt.h>
+#include <linux/init.h>
+#include <linux/kasan.h>
+#include <linux/kernel.h>
+#include <linux/kmemleak.h>
+#include <linux/linkage.h>
+#include <linux/memblock.h>
+#include <linux/memory.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/sched/task_stack.h>
+#include <linux/slab.h>
+#include <linux/stacktrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/vmalloc.h>
+#include <linux/bug.h>
+
+#include "kasan.h"
+#include "../slab.h"
+
+static inline int in_irqentry_text(unsigned long ptr)
+{
+	return (ptr >= (unsigned long)&__irqentry_text_start &&
+		ptr < (unsigned long)&__irqentry_text_end) ||
+		(ptr >= (unsigned long)&__softirqentry_text_start &&
+		 ptr < (unsigned long)&__softirqentry_text_end);
+}
+
+static inline void filter_irq_stacks(struct stack_trace *trace)
+{
+	int i;
+
+	if (!trace->nr_entries)
+		return;
+	for (i = 0; i < trace->nr_entries; i++)
+		if (in_irqentry_text(trace->entries[i])) {
+			/* Include the irqentry function into the stack. */
+			trace->nr_entries = i + 1;
+			break;
+		}
+}
+
+static inline depot_stack_handle_t save_stack(gfp_t flags)
+{
+	unsigned long entries[KASAN_STACK_DEPTH];
+	struct stack_trace trace = {
+		.nr_entries = 0,
+		.entries = entries,
+		.max_entries = KASAN_STACK_DEPTH,
+		.skip = 0
+	};
+
+	save_stack_trace(&trace);
+	filter_irq_stacks(&trace);
+	if (trace.nr_entries != 0 &&
+	    trace.entries[trace.nr_entries-1] == ULONG_MAX)
+		trace.nr_entries--;
+
+	return depot_save_stack(&trace, flags);
+}
+
+static inline void set_track(struct kasan_track *track, gfp_t flags)
+{
+	track->pid = current->pid;
+	track->stack = save_stack(flags);
+}
+
+void kasan_enable_current(void)
+{
+	current->kasan_depth++;
+}
+
+void kasan_disable_current(void)
+{
+	current->kasan_depth--;
+}
+
+void kasan_check_read(const volatile void *p, unsigned int size)
+{
+	check_memory_region((unsigned long)p, size, false, _RET_IP_);
+}
+EXPORT_SYMBOL(kasan_check_read);
+
+void kasan_check_write(const volatile void *p, unsigned int size)
+{
+	check_memory_region((unsigned long)p, size, true, _RET_IP_);
+}
+EXPORT_SYMBOL(kasan_check_write);
+
+#undef memset
+void *memset(void *addr, int c, size_t len)
+{
+	check_memory_region((unsigned long)addr, len, true, _RET_IP_);
+
+	return __memset(addr, c, len);
+}
+
+#undef memmove
+void *memmove(void *dest, const void *src, size_t len)
+{
+	check_memory_region((unsigned long)src, len, false, _RET_IP_);
+	check_memory_region((unsigned long)dest, len, true, _RET_IP_);
+
+	return __memmove(dest, src, len);
+}
+
+#undef memcpy
+void *memcpy(void *dest, const void *src, size_t len)
+{
+	check_memory_region((unsigned long)src, len, false, _RET_IP_);
+	check_memory_region((unsigned long)dest, len, true, _RET_IP_);
+
+	return __memcpy(dest, src, len);
+}
+
+/*
+ * Poisons the shadow memory for 'size' bytes starting from 'addr'.
+ * Memory addresses should be aligned to KASAN_SHADOW_SCALE_SIZE.
+ */
+void kasan_poison_shadow(const void *address, size_t size, u8 value)
+{
+	void *shadow_start, *shadow_end;
+
+	shadow_start = kasan_mem_to_shadow(address);
+	shadow_end = kasan_mem_to_shadow(address + size);
+
+	__memset(shadow_start, value, shadow_end - shadow_start);
+}
+
+void kasan_unpoison_shadow(const void *address, size_t size)
+{
+	kasan_poison_shadow(address, size, 0);
+
+	if (size & KASAN_SHADOW_MASK) {
+		u8 *shadow = (u8 *)kasan_mem_to_shadow(address + size);
+		*shadow = size & KASAN_SHADOW_MASK;
+	}
+}
+
+static void __kasan_unpoison_stack(struct task_struct *task, const void *sp)
+{
+	void *base = task_stack_page(task);
+	size_t size = sp - base;
+
+	kasan_unpoison_shadow(base, size);
+}
+
+/* Unpoison the entire stack for a task. */
+void kasan_unpoison_task_stack(struct task_struct *task)
+{
+	__kasan_unpoison_stack(task, task_stack_page(task) + THREAD_SIZE);
+}
+
+/* Unpoison the stack for the current task beyond a watermark sp value. */
+asmlinkage void kasan_unpoison_task_stack_below(const void *watermark)
+{
+	/*
+	 * Calculate the task stack base address.  Avoid using 'current'
+	 * because this function is called by early resume code which hasn't
+	 * yet set up the percpu register (%gs).
+	 */
+	void *base = (void *)((unsigned long)watermark & ~(THREAD_SIZE - 1));
+
+	kasan_unpoison_shadow(base, watermark - base);
+}
+
+/*
+ * Clear all poison for the region between the current SP and a provided
+ * watermark value, as is sometimes required prior to hand-crafted asm function
+ * returns in the middle of functions.
+ */
+void kasan_unpoison_stack_above_sp_to(const void *watermark)
+{
+	const void *sp = __builtin_frame_address(0);
+	size_t size = watermark - sp;
+
+	if (WARN_ON(sp > watermark))
+		return;
+	kasan_unpoison_shadow(sp, size);
+}
+
+void kasan_alloc_pages(struct page *page, unsigned int order)
+{
+	if (likely(!PageHighMem(page)))
+		kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
+}
+
+void kasan_free_pages(struct page *page, unsigned int order)
+{
+	if (likely(!PageHighMem(page)))
+		kasan_poison_shadow(page_address(page),
+				PAGE_SIZE << order,
+				KASAN_FREE_PAGE);
+}
+
+/*
+ * Adaptive redzone policy taken from the userspace AddressSanitizer runtime.
+ * For larger allocations larger redzones are used.
+ */
+static inline unsigned int optimal_redzone(unsigned int object_size)
+{
+	return
+		object_size <= 64        - 16   ? 16 :
+		object_size <= 128       - 32   ? 32 :
+		object_size <= 512       - 64   ? 64 :
+		object_size <= 4096      - 128  ? 128 :
+		object_size <= (1 << 14) - 256  ? 256 :
+		object_size <= (1 << 15) - 512  ? 512 :
+		object_size <= (1 << 16) - 1024 ? 1024 : 2048;
+}
+
+void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
+			slab_flags_t *flags)
+{
+	unsigned int orig_size = *size;
+	int redzone_adjust;
+
+	/* Add alloc meta. */
+	cache->kasan_info.alloc_meta_offset = *size;
+	*size += sizeof(struct kasan_alloc_meta);
+
+	/* Add free meta. */
+	if (cache->flags & SLAB_TYPESAFE_BY_RCU || cache->ctor ||
+	    cache->object_size < sizeof(struct kasan_free_meta)) {
+		cache->kasan_info.free_meta_offset = *size;
+		*size += sizeof(struct kasan_free_meta);
+	}
+	redzone_adjust = optimal_redzone(cache->object_size) -
+		(*size - cache->object_size);
+
+	if (redzone_adjust > 0)
+		*size += redzone_adjust;
+
+	*size = min_t(unsigned int, KMALLOC_MAX_SIZE,
+			max(*size, cache->object_size +
+					optimal_redzone(cache->object_size)));
+
+	/*
+	 * If the metadata doesn't fit, don't enable KASAN at all.
+	 */
+	if (*size <= cache->kasan_info.alloc_meta_offset ||
+			*size <= cache->kasan_info.free_meta_offset) {
+		cache->kasan_info.alloc_meta_offset = 0;
+		cache->kasan_info.free_meta_offset = 0;
+		*size = orig_size;
+		return;
+	}
+
+	*flags |= SLAB_KASAN;
+}
+
+size_t kasan_metadata_size(struct kmem_cache *cache)
+{
+	return (cache->kasan_info.alloc_meta_offset ?
+		sizeof(struct kasan_alloc_meta) : 0) +
+		(cache->kasan_info.free_meta_offset ?
+		sizeof(struct kasan_free_meta) : 0);
+}
+
+struct kasan_alloc_meta *get_alloc_info(struct kmem_cache *cache,
+					const void *object)
+{
+	BUILD_BUG_ON(sizeof(struct kasan_alloc_meta) > 32);
+	return (void *)object + cache->kasan_info.alloc_meta_offset;
+}
+
+struct kasan_free_meta *get_free_info(struct kmem_cache *cache,
+				      const void *object)
+{
+	BUILD_BUG_ON(sizeof(struct kasan_free_meta) > 32);
+	return (void *)object + cache->kasan_info.free_meta_offset;
+}
+
+void kasan_poison_slab(struct page *page)
+{
+	kasan_poison_shadow(page_address(page),
+			PAGE_SIZE << compound_order(page),
+			KASAN_KMALLOC_REDZONE);
+}
+
+void kasan_unpoison_object_data(struct kmem_cache *cache, void *object)
+{
+	kasan_unpoison_shadow(object, cache->object_size);
+}
+
+void kasan_poison_object_data(struct kmem_cache *cache, void *object)
+{
+	kasan_poison_shadow(object,
+			round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE),
+			KASAN_KMALLOC_REDZONE);
+}
+
+void *kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
+{
+	struct kasan_alloc_meta *alloc_info;
+
+	if (!(cache->flags & SLAB_KASAN))
+		return (void *)object;
+
+	alloc_info = get_alloc_info(cache, object);
+	__memset(alloc_info, 0, sizeof(*alloc_info));
+
+	return (void *)object;
+}
+
+void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t flags)
+{
+	return kasan_kmalloc(cache, object, cache->object_size, flags);
+}
+
+static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
+			      unsigned long ip, bool quarantine)
+{
+	s8 shadow_byte;
+	unsigned long rounded_up_size;
+
+	if (unlikely(nearest_obj(cache, virt_to_head_page(object), object) !=
+	    object)) {
+		kasan_report_invalid_free(object, ip);
+		return true;
+	}
+
+	/* RCU slabs could be legally used after free within the RCU period */
+	if (unlikely(cache->flags & SLAB_TYPESAFE_BY_RCU))
+		return false;
+
+	shadow_byte = READ_ONCE(*(s8 *)kasan_mem_to_shadow(object));
+	if (shadow_byte < 0 || shadow_byte >= KASAN_SHADOW_SCALE_SIZE) {
+		kasan_report_invalid_free(object, ip);
+		return true;
+	}
+
+	rounded_up_size = round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE);
+	kasan_poison_shadow(object, rounded_up_size, KASAN_KMALLOC_FREE);
+
+	if (!quarantine || unlikely(!(cache->flags & SLAB_KASAN)))
+		return false;
+
+	set_track(&get_alloc_info(cache, object)->free_track, GFP_NOWAIT);
+	quarantine_put(get_free_info(cache, object), cache);
+	return true;
+}
+
+bool kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
+{
+	return __kasan_slab_free(cache, object, ip, true);
+}
+
+void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
+		   gfp_t flags)
+{
+	unsigned long redzone_start;
+	unsigned long redzone_end;
+
+	if (gfpflags_allow_blocking(flags))
+		quarantine_reduce();
+
+	if (unlikely(object == NULL))
+		return NULL;
+
+	redzone_start = round_up((unsigned long)(object + size),
+				KASAN_SHADOW_SCALE_SIZE);
+	redzone_end = round_up((unsigned long)object + cache->object_size,
+				KASAN_SHADOW_SCALE_SIZE);
+
+	kasan_unpoison_shadow(object, size);
+	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
+		KASAN_KMALLOC_REDZONE);
+
+	if (cache->flags & SLAB_KASAN)
+		set_track(&get_alloc_info(cache, object)->alloc_track, flags);
+
+	return (void *)object;
+}
+EXPORT_SYMBOL(kasan_kmalloc);
+
+void *kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
+{
+	struct page *page;
+	unsigned long redzone_start;
+	unsigned long redzone_end;
+
+	if (gfpflags_allow_blocking(flags))
+		quarantine_reduce();
+
+	if (unlikely(ptr == NULL))
+		return NULL;
+
+	page = virt_to_page(ptr);
+	redzone_start = round_up((unsigned long)(ptr + size),
+				KASAN_SHADOW_SCALE_SIZE);
+	redzone_end = (unsigned long)ptr + (PAGE_SIZE << compound_order(page));
+
+	kasan_unpoison_shadow(ptr, size);
+	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
+		KASAN_PAGE_REDZONE);
+
+	return (void *)ptr;
+}
+
+void *kasan_krealloc(const void *object, size_t size, gfp_t flags)
+{
+	struct page *page;
+
+	if (unlikely(object == ZERO_SIZE_PTR))
+		return (void *)object;
+
+	page = virt_to_head_page(object);
+
+	if (unlikely(!PageSlab(page)))
+		return kasan_kmalloc_large(object, size, flags);
+	else
+		return kasan_kmalloc(page->slab_cache, object, size, flags);
+}
+
+void kasan_poison_kfree(void *ptr, unsigned long ip)
+{
+	struct page *page;
+
+	page = virt_to_head_page(ptr);
+
+	if (unlikely(!PageSlab(page))) {
+		if (ptr != page_address(page)) {
+			kasan_report_invalid_free(ptr, ip);
+			return;
+		}
+		kasan_poison_shadow(ptr, PAGE_SIZE << compound_order(page),
+				KASAN_FREE_PAGE);
+	} else {
+		__kasan_slab_free(page->slab_cache, ptr, ip, false);
+	}
+}
+
+void kasan_kfree_large(void *ptr, unsigned long ip)
+{
+	if (ptr != page_address(virt_to_head_page(ptr)))
+		kasan_report_invalid_free(ptr, ip);
+	/* The object will be poisoned by page_alloc. */
+}
+
+int kasan_module_alloc(void *addr, size_t size)
+{
+	void *ret;
+	size_t scaled_size;
+	size_t shadow_size;
+	unsigned long shadow_start;
+
+	shadow_start = (unsigned long)kasan_mem_to_shadow(addr);
+	scaled_size = (size + KASAN_SHADOW_MASK) >> KASAN_SHADOW_SCALE_SHIFT;
+	shadow_size = round_up(scaled_size, PAGE_SIZE);
+
+	if (WARN_ON(!PAGE_ALIGNED(shadow_start)))
+		return -EINVAL;
+
+	ret = __vmalloc_node_range(shadow_size, 1, shadow_start,
+			shadow_start + shadow_size,
+			GFP_KERNEL | __GFP_ZERO,
+			PAGE_KERNEL, VM_NO_GUARD, NUMA_NO_NODE,
+			__builtin_return_address(0));
+
+	if (ret) {
+		find_vm_area(addr)->flags |= VM_KASAN;
+		kmemleak_ignore(ret);
+		return 0;
+	}
+
+	return -ENOMEM;
+}
+
+void kasan_free_shadow(const struct vm_struct *vm)
+{
+	if (vm->flags & VM_KASAN)
+		vfree(kasan_mem_to_shadow(vm->addr));
+}
+
+#ifdef CONFIG_MEMORY_HOTPLUG
+static bool shadow_mapped(unsigned long addr)
+{
+	pgd_t *pgd = pgd_offset_k(addr);
+	p4d_t *p4d;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte;
+
+	if (pgd_none(*pgd))
+		return false;
+	p4d = p4d_offset(pgd, addr);
+	if (p4d_none(*p4d))
+		return false;
+	pud = pud_offset(p4d, addr);
+	if (pud_none(*pud))
+		return false;
+
+	/*
+	 * We can't use pud_large() or pud_huge(), the first one is
+	 * arch-specific, the last one depends on HUGETLB_PAGE.  So let's abuse
+	 * pud_bad(), if pud is bad then it's bad because it's huge.
+	 */
+	if (pud_bad(*pud))
+		return true;
+	pmd = pmd_offset(pud, addr);
+	if (pmd_none(*pmd))
+		return false;
+
+	if (pmd_bad(*pmd))
+		return true;
+	pte = pte_offset_kernel(pmd, addr);
+	return !pte_none(*pte);
+}
+
+static int __meminit kasan_mem_notifier(struct notifier_block *nb,
+			unsigned long action, void *data)
+{
+	struct memory_notify *mem_data = data;
+	unsigned long nr_shadow_pages, start_kaddr, shadow_start;
+	unsigned long shadow_end, shadow_size;
+
+	nr_shadow_pages = mem_data->nr_pages >> KASAN_SHADOW_SCALE_SHIFT;
+	start_kaddr = (unsigned long)pfn_to_kaddr(mem_data->start_pfn);
+	shadow_start = (unsigned long)kasan_mem_to_shadow((void *)start_kaddr);
+	shadow_size = nr_shadow_pages << PAGE_SHIFT;
+	shadow_end = shadow_start + shadow_size;
+
+	if (WARN_ON(mem_data->nr_pages % KASAN_SHADOW_SCALE_SIZE) ||
+		WARN_ON(start_kaddr % (KASAN_SHADOW_SCALE_SIZE << PAGE_SHIFT)))
+		return NOTIFY_BAD;
+
+	switch (action) {
+	case MEM_GOING_ONLINE: {
+		void *ret;
+
+		/*
+		 * If shadow is mapped already than it must have been mapped
+		 * during the boot. This could happen if we onlining previously
+		 * offlined memory.
+		 */
+		if (shadow_mapped(shadow_start))
+			return NOTIFY_OK;
+
+		ret = __vmalloc_node_range(shadow_size, PAGE_SIZE, shadow_start,
+					shadow_end, GFP_KERNEL,
+					PAGE_KERNEL, VM_NO_GUARD,
+					pfn_to_nid(mem_data->start_pfn),
+					__builtin_return_address(0));
+		if (!ret)
+			return NOTIFY_BAD;
+
+		kmemleak_ignore(ret);
+		return NOTIFY_OK;
+	}
+	case MEM_CANCEL_ONLINE:
+	case MEM_OFFLINE: {
+		struct vm_struct *vm;
+
+		/*
+		 * shadow_start was either mapped during boot by kasan_init()
+		 * or during memory online by __vmalloc_node_range().
+		 * In the latter case we can use vfree() to free shadow.
+		 * Non-NULL result of the find_vm_area() will tell us if
+		 * that was the second case.
+		 *
+		 * Currently it's not possible to free shadow mapped
+		 * during boot by kasan_init(). It's because the code
+		 * to do that hasn't been written yet. So we'll just
+		 * leak the memory.
+		 */
+		vm = find_vm_area((void *)shadow_start);
+		if (vm)
+			vfree((void *)shadow_start);
+	}
+	}
+
+	return NOTIFY_OK;
+}
+
+static int __init kasan_memhotplug_init(void)
+{
+	hotplug_memory_notifier(kasan_mem_notifier, 0);
+
+	return 0;
+}
+
+core_initcall(kasan_memhotplug_init);
+#endif
diff --git a/mm/kasan/kasan.c b/mm/kasan/kasan.c
index 55deff17a4d9..44ec228de0a2 100644
--- a/mm/kasan/kasan.c
+++ b/mm/kasan/kasan.c
@@ -1,5 +1,5 @@
 /*
- * This file contains shadow memory manipulation code.
+ * This file contains core KASAN code.
  *
  * Copyright (c) 2014 Samsung Electronics Co., Ltd.
  * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
@@ -40,82 +40,6 @@
 #include "kasan.h"
 #include "../slab.h"
 
-void kasan_enable_current(void)
-{
-	current->kasan_depth++;
-}
-
-void kasan_disable_current(void)
-{
-	current->kasan_depth--;
-}
-
-/*
- * Poisons the shadow memory for 'size' bytes starting from 'addr'.
- * Memory addresses should be aligned to KASAN_SHADOW_SCALE_SIZE.
- */
-static void kasan_poison_shadow(const void *address, size_t size, u8 value)
-{
-	void *shadow_start, *shadow_end;
-
-	shadow_start = kasan_mem_to_shadow(address);
-	shadow_end = kasan_mem_to_shadow(address + size);
-
-	memset(shadow_start, value, shadow_end - shadow_start);
-}
-
-void kasan_unpoison_shadow(const void *address, size_t size)
-{
-	kasan_poison_shadow(address, size, 0);
-
-	if (size & KASAN_SHADOW_MASK) {
-		u8 *shadow = (u8 *)kasan_mem_to_shadow(address + size);
-		*shadow = size & KASAN_SHADOW_MASK;
-	}
-}
-
-static void __kasan_unpoison_stack(struct task_struct *task, const void *sp)
-{
-	void *base = task_stack_page(task);
-	size_t size = sp - base;
-
-	kasan_unpoison_shadow(base, size);
-}
-
-/* Unpoison the entire stack for a task. */
-void kasan_unpoison_task_stack(struct task_struct *task)
-{
-	__kasan_unpoison_stack(task, task_stack_page(task) + THREAD_SIZE);
-}
-
-/* Unpoison the stack for the current task beyond a watermark sp value. */
-asmlinkage void kasan_unpoison_task_stack_below(const void *watermark)
-{
-	/*
-	 * Calculate the task stack base address.  Avoid using 'current'
-	 * because this function is called by early resume code which hasn't
-	 * yet set up the percpu register (%gs).
-	 */
-	void *base = (void *)((unsigned long)watermark & ~(THREAD_SIZE - 1));
-
-	kasan_unpoison_shadow(base, watermark - base);
-}
-
-/*
- * Clear all poison for the region between the current SP and a provided
- * watermark value, as is sometimes required prior to hand-crafted asm function
- * returns in the middle of functions.
- */
-void kasan_unpoison_stack_above_sp_to(const void *watermark)
-{
-	const void *sp = __builtin_frame_address(0);
-	size_t size = watermark - sp;
-
-	if (WARN_ON(sp > watermark))
-		return;
-	kasan_unpoison_shadow(sp, size);
-}
-
 /*
  * All functions below always inlined so compiler could
  * perform better optimizations in each of __asan_loadX/__assn_storeX
@@ -260,121 +184,12 @@ static __always_inline void check_memory_region_inline(unsigned long addr,
 	kasan_report(addr, size, write, ret_ip);
 }
 
-static void check_memory_region(unsigned long addr,
-				size_t size, bool write,
+void check_memory_region(unsigned long addr, size_t size, bool write,
 				unsigned long ret_ip)
 {
 	check_memory_region_inline(addr, size, write, ret_ip);
 }
 
-void kasan_check_read(const volatile void *p, unsigned int size)
-{
-	check_memory_region((unsigned long)p, size, false, _RET_IP_);
-}
-EXPORT_SYMBOL(kasan_check_read);
-
-void kasan_check_write(const volatile void *p, unsigned int size)
-{
-	check_memory_region((unsigned long)p, size, true, _RET_IP_);
-}
-EXPORT_SYMBOL(kasan_check_write);
-
-#undef memset
-void *memset(void *addr, int c, size_t len)
-{
-	check_memory_region((unsigned long)addr, len, true, _RET_IP_);
-
-	return __memset(addr, c, len);
-}
-
-#undef memmove
-void *memmove(void *dest, const void *src, size_t len)
-{
-	check_memory_region((unsigned long)src, len, false, _RET_IP_);
-	check_memory_region((unsigned long)dest, len, true, _RET_IP_);
-
-	return __memmove(dest, src, len);
-}
-
-#undef memcpy
-void *memcpy(void *dest, const void *src, size_t len)
-{
-	check_memory_region((unsigned long)src, len, false, _RET_IP_);
-	check_memory_region((unsigned long)dest, len, true, _RET_IP_);
-
-	return __memcpy(dest, src, len);
-}
-
-void kasan_alloc_pages(struct page *page, unsigned int order)
-{
-	if (likely(!PageHighMem(page)))
-		kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
-}
-
-void kasan_free_pages(struct page *page, unsigned int order)
-{
-	if (likely(!PageHighMem(page)))
-		kasan_poison_shadow(page_address(page),
-				PAGE_SIZE << order,
-				KASAN_FREE_PAGE);
-}
-
-/*
- * Adaptive redzone policy taken from the userspace AddressSanitizer runtime.
- * For larger allocations larger redzones are used.
- */
-static unsigned int optimal_redzone(unsigned int object_size)
-{
-	return
-		object_size <= 64        - 16   ? 16 :
-		object_size <= 128       - 32   ? 32 :
-		object_size <= 512       - 64   ? 64 :
-		object_size <= 4096      - 128  ? 128 :
-		object_size <= (1 << 14) - 256  ? 256 :
-		object_size <= (1 << 15) - 512  ? 512 :
-		object_size <= (1 << 16) - 1024 ? 1024 : 2048;
-}
-
-void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
-			slab_flags_t *flags)
-{
-	unsigned int orig_size = *size;
-	int redzone_adjust;
-
-	/* Add alloc meta. */
-	cache->kasan_info.alloc_meta_offset = *size;
-	*size += sizeof(struct kasan_alloc_meta);
-
-	/* Add free meta. */
-	if (cache->flags & SLAB_TYPESAFE_BY_RCU || cache->ctor ||
-	    cache->object_size < sizeof(struct kasan_free_meta)) {
-		cache->kasan_info.free_meta_offset = *size;
-		*size += sizeof(struct kasan_free_meta);
-	}
-	redzone_adjust = optimal_redzone(cache->object_size) -
-		(*size - cache->object_size);
-
-	if (redzone_adjust > 0)
-		*size += redzone_adjust;
-
-	*size = min_t(unsigned int, KMALLOC_MAX_SIZE,
-			max(*size, cache->object_size +
-					optimal_redzone(cache->object_size)));
-
-	/*
-	 * If the metadata doesn't fit, don't enable KASAN at all.
-	 */
-	if (*size <= cache->kasan_info.alloc_meta_offset ||
-			*size <= cache->kasan_info.free_meta_offset) {
-		cache->kasan_info.alloc_meta_offset = 0;
-		cache->kasan_info.free_meta_offset = 0;
-		*size = orig_size;
-		return;
-	}
-
-	*flags |= SLAB_KASAN;
-}
-
 void kasan_cache_shrink(struct kmem_cache *cache)
 {
 	quarantine_remove_cache(cache);
@@ -386,277 +201,6 @@ void kasan_cache_shutdown(struct kmem_cache *cache)
 		quarantine_remove_cache(cache);
 }
 
-size_t kasan_metadata_size(struct kmem_cache *cache)
-{
-	return (cache->kasan_info.alloc_meta_offset ?
-		sizeof(struct kasan_alloc_meta) : 0) +
-		(cache->kasan_info.free_meta_offset ?
-		sizeof(struct kasan_free_meta) : 0);
-}
-
-void kasan_poison_slab(struct page *page)
-{
-	kasan_poison_shadow(page_address(page),
-			PAGE_SIZE << compound_order(page),
-			KASAN_KMALLOC_REDZONE);
-}
-
-void kasan_unpoison_object_data(struct kmem_cache *cache, void *object)
-{
-	kasan_unpoison_shadow(object, cache->object_size);
-}
-
-void kasan_poison_object_data(struct kmem_cache *cache, void *object)
-{
-	kasan_poison_shadow(object,
-			round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE),
-			KASAN_KMALLOC_REDZONE);
-}
-
-static inline int in_irqentry_text(unsigned long ptr)
-{
-	return (ptr >= (unsigned long)&__irqentry_text_start &&
-		ptr < (unsigned long)&__irqentry_text_end) ||
-		(ptr >= (unsigned long)&__softirqentry_text_start &&
-		 ptr < (unsigned long)&__softirqentry_text_end);
-}
-
-static inline void filter_irq_stacks(struct stack_trace *trace)
-{
-	int i;
-
-	if (!trace->nr_entries)
-		return;
-	for (i = 0; i < trace->nr_entries; i++)
-		if (in_irqentry_text(trace->entries[i])) {
-			/* Include the irqentry function into the stack. */
-			trace->nr_entries = i + 1;
-			break;
-		}
-}
-
-static inline depot_stack_handle_t save_stack(gfp_t flags)
-{
-	unsigned long entries[KASAN_STACK_DEPTH];
-	struct stack_trace trace = {
-		.nr_entries = 0,
-		.entries = entries,
-		.max_entries = KASAN_STACK_DEPTH,
-		.skip = 0
-	};
-
-	save_stack_trace(&trace);
-	filter_irq_stacks(&trace);
-	if (trace.nr_entries != 0 &&
-	    trace.entries[trace.nr_entries-1] == ULONG_MAX)
-		trace.nr_entries--;
-
-	return depot_save_stack(&trace, flags);
-}
-
-static inline void set_track(struct kasan_track *track, gfp_t flags)
-{
-	track->pid = current->pid;
-	track->stack = save_stack(flags);
-}
-
-struct kasan_alloc_meta *get_alloc_info(struct kmem_cache *cache,
-					const void *object)
-{
-	BUILD_BUG_ON(sizeof(struct kasan_alloc_meta) > 32);
-	return (void *)object + cache->kasan_info.alloc_meta_offset;
-}
-
-struct kasan_free_meta *get_free_info(struct kmem_cache *cache,
-				      const void *object)
-{
-	BUILD_BUG_ON(sizeof(struct kasan_free_meta) > 32);
-	return (void *)object + cache->kasan_info.free_meta_offset;
-}
-
-void *kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
-{
-	struct kasan_alloc_meta *alloc_info;
-
-	if (!(cache->flags & SLAB_KASAN))
-		return (void *)object;
-
-	alloc_info = get_alloc_info(cache, object);
-	__memset(alloc_info, 0, sizeof(*alloc_info));
-
-	return (void *)object;
-}
-
-void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t flags)
-{
-	return kasan_kmalloc(cache, object, cache->object_size, flags);
-}
-
-static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
-			      unsigned long ip, bool quarantine)
-{
-	s8 shadow_byte;
-	unsigned long rounded_up_size;
-
-	if (unlikely(nearest_obj(cache, virt_to_head_page(object), object) !=
-	    object)) {
-		kasan_report_invalid_free(object, ip);
-		return true;
-	}
-
-	/* RCU slabs could be legally used after free within the RCU period */
-	if (unlikely(cache->flags & SLAB_TYPESAFE_BY_RCU))
-		return false;
-
-	shadow_byte = READ_ONCE(*(s8 *)kasan_mem_to_shadow(object));
-	if (shadow_byte < 0 || shadow_byte >= KASAN_SHADOW_SCALE_SIZE) {
-		kasan_report_invalid_free(object, ip);
-		return true;
-	}
-
-	rounded_up_size = round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE);
-	kasan_poison_shadow(object, rounded_up_size, KASAN_KMALLOC_FREE);
-
-	if (!quarantine || unlikely(!(cache->flags & SLAB_KASAN)))
-		return false;
-
-	set_track(&get_alloc_info(cache, object)->free_track, GFP_NOWAIT);
-	quarantine_put(get_free_info(cache, object), cache);
-	return true;
-}
-
-bool kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
-{
-	return __kasan_slab_free(cache, object, ip, true);
-}
-
-void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
-		   gfp_t flags)
-{
-	unsigned long redzone_start;
-	unsigned long redzone_end;
-
-	if (gfpflags_allow_blocking(flags))
-		quarantine_reduce();
-
-	if (unlikely(object == NULL))
-		return NULL;
-
-	redzone_start = round_up((unsigned long)(object + size),
-				KASAN_SHADOW_SCALE_SIZE);
-	redzone_end = round_up((unsigned long)object + cache->object_size,
-				KASAN_SHADOW_SCALE_SIZE);
-
-	kasan_unpoison_shadow(object, size);
-	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
-		KASAN_KMALLOC_REDZONE);
-
-	if (cache->flags & SLAB_KASAN)
-		set_track(&get_alloc_info(cache, object)->alloc_track, flags);
-
-	return (void *)object;
-}
-EXPORT_SYMBOL(kasan_kmalloc);
-
-void *kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
-{
-	struct page *page;
-	unsigned long redzone_start;
-	unsigned long redzone_end;
-
-	if (gfpflags_allow_blocking(flags))
-		quarantine_reduce();
-
-	if (unlikely(ptr == NULL))
-		return NULL;
-
-	page = virt_to_page(ptr);
-	redzone_start = round_up((unsigned long)(ptr + size),
-				KASAN_SHADOW_SCALE_SIZE);
-	redzone_end = (unsigned long)ptr + (PAGE_SIZE << compound_order(page));
-
-	kasan_unpoison_shadow(ptr, size);
-	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
-		KASAN_PAGE_REDZONE);
-
-	return (void *)ptr;
-}
-
-void *kasan_krealloc(const void *object, size_t size, gfp_t flags)
-{
-	struct page *page;
-
-	if (unlikely(object == ZERO_SIZE_PTR))
-		return ZERO_SIZE_PTR;
-
-	page = virt_to_head_page(object);
-
-	if (unlikely(!PageSlab(page)))
-		return kasan_kmalloc_large(object, size, flags);
-	else
-		return kasan_kmalloc(page->slab_cache, object, size, flags);
-}
-
-void kasan_poison_kfree(void *ptr, unsigned long ip)
-{
-	struct page *page;
-
-	page = virt_to_head_page(ptr);
-
-	if (unlikely(!PageSlab(page))) {
-		if (ptr != page_address(page)) {
-			kasan_report_invalid_free(ptr, ip);
-			return;
-		}
-		kasan_poison_shadow(ptr, PAGE_SIZE << compound_order(page),
-				KASAN_FREE_PAGE);
-	} else {
-		__kasan_slab_free(page->slab_cache, ptr, ip, false);
-	}
-}
-
-void kasan_kfree_large(void *ptr, unsigned long ip)
-{
-	if (ptr != page_address(virt_to_head_page(ptr)))
-		kasan_report_invalid_free(ptr, ip);
-	/* The object will be poisoned by page_alloc. */
-}
-
-int kasan_module_alloc(void *addr, size_t size)
-{
-	void *ret;
-	size_t scaled_size;
-	size_t shadow_size;
-	unsigned long shadow_start;
-
-	shadow_start = (unsigned long)kasan_mem_to_shadow(addr);
-	scaled_size = (size + KASAN_SHADOW_MASK) >> KASAN_SHADOW_SCALE_SHIFT;
-	shadow_size = round_up(scaled_size, PAGE_SIZE);
-
-	if (WARN_ON(!PAGE_ALIGNED(shadow_start)))
-		return -EINVAL;
-
-	ret = __vmalloc_node_range(shadow_size, 1, shadow_start,
-			shadow_start + shadow_size,
-			GFP_KERNEL | __GFP_ZERO,
-			PAGE_KERNEL, VM_NO_GUARD, NUMA_NO_NODE,
-			__builtin_return_address(0));
-
-	if (ret) {
-		find_vm_area(addr)->flags |= VM_KASAN;
-		kmemleak_ignore(ret);
-		return 0;
-	}
-
-	return -ENOMEM;
-}
-
-void kasan_free_shadow(const struct vm_struct *vm)
-{
-	if (vm->flags & VM_KASAN)
-		vfree(kasan_mem_to_shadow(vm->addr));
-}
-
 static void register_global(struct kasan_global *global)
 {
 	size_t aligned_size = round_up(global->size, KASAN_SHADOW_SCALE_SIZE);
@@ -797,113 +341,3 @@ DEFINE_ASAN_SET_SHADOW(f2);
 DEFINE_ASAN_SET_SHADOW(f3);
 DEFINE_ASAN_SET_SHADOW(f5);
 DEFINE_ASAN_SET_SHADOW(f8);
-
-#ifdef CONFIG_MEMORY_HOTPLUG
-static bool shadow_mapped(unsigned long addr)
-{
-	pgd_t *pgd = pgd_offset_k(addr);
-	p4d_t *p4d;
-	pud_t *pud;
-	pmd_t *pmd;
-	pte_t *pte;
-
-	if (pgd_none(*pgd))
-		return false;
-	p4d = p4d_offset(pgd, addr);
-	if (p4d_none(*p4d))
-		return false;
-	pud = pud_offset(p4d, addr);
-	if (pud_none(*pud))
-		return false;
-
-	/*
-	 * We can't use pud_large() or pud_huge(), the first one is
-	 * arch-specific, the last one depends on HUGETLB_PAGE.  So let's abuse
-	 * pud_bad(), if pud is bad then it's bad because it's huge.
-	 */
-	if (pud_bad(*pud))
-		return true;
-	pmd = pmd_offset(pud, addr);
-	if (pmd_none(*pmd))
-		return false;
-
-	if (pmd_bad(*pmd))
-		return true;
-	pte = pte_offset_kernel(pmd, addr);
-	return !pte_none(*pte);
-}
-
-static int __meminit kasan_mem_notifier(struct notifier_block *nb,
-			unsigned long action, void *data)
-{
-	struct memory_notify *mem_data = data;
-	unsigned long nr_shadow_pages, start_kaddr, shadow_start;
-	unsigned long shadow_end, shadow_size;
-
-	nr_shadow_pages = mem_data->nr_pages >> KASAN_SHADOW_SCALE_SHIFT;
-	start_kaddr = (unsigned long)pfn_to_kaddr(mem_data->start_pfn);
-	shadow_start = (unsigned long)kasan_mem_to_shadow((void *)start_kaddr);
-	shadow_size = nr_shadow_pages << PAGE_SHIFT;
-	shadow_end = shadow_start + shadow_size;
-
-	if (WARN_ON(mem_data->nr_pages % KASAN_SHADOW_SCALE_SIZE) ||
-		WARN_ON(start_kaddr % (KASAN_SHADOW_SCALE_SIZE << PAGE_SHIFT)))
-		return NOTIFY_BAD;
-
-	switch (action) {
-	case MEM_GOING_ONLINE: {
-		void *ret;
-
-		/*
-		 * If shadow is mapped already than it must have been mapped
-		 * during the boot. This could happen if we onlining previously
-		 * offlined memory.
-		 */
-		if (shadow_mapped(shadow_start))
-			return NOTIFY_OK;
-
-		ret = __vmalloc_node_range(shadow_size, PAGE_SIZE, shadow_start,
-					shadow_end, GFP_KERNEL,
-					PAGE_KERNEL, VM_NO_GUARD,
-					pfn_to_nid(mem_data->start_pfn),
-					__builtin_return_address(0));
-		if (!ret)
-			return NOTIFY_BAD;
-
-		kmemleak_ignore(ret);
-		return NOTIFY_OK;
-	}
-	case MEM_CANCEL_ONLINE:
-	case MEM_OFFLINE: {
-		struct vm_struct *vm;
-
-		/*
-		 * shadow_start was either mapped during boot by kasan_init()
-		 * or during memory online by __vmalloc_node_range().
-		 * In the latter case we can use vfree() to free shadow.
-		 * Non-NULL result of the find_vm_area() will tell us if
-		 * that was the second case.
-		 *
-		 * Currently it's not possible to free shadow mapped
-		 * during boot by kasan_init(). It's because the code
-		 * to do that hasn't been written yet. So we'll just
-		 * leak the memory.
-		 */
-		vm = find_vm_area((void *)shadow_start);
-		if (vm)
-			vfree((void *)shadow_start);
-	}
-	}
-
-	return NOTIFY_OK;
-}
-
-static int __init kasan_memhotplug_init(void)
-{
-	hotplug_memory_notifier(kasan_mem_notifier, 0);
-
-	return 0;
-}
-
-core_initcall(kasan_memhotplug_init);
-#endif
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index c12dcfde2ebd..659463800f10 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -105,6 +105,11 @@ static inline const void *kasan_shadow_to_mem(const void *shadow_addr)
 		<< KASAN_SHADOW_SCALE_SHIFT);
 }
 
+void kasan_poison_shadow(const void *address, size_t size, u8 value);
+
+void check_memory_region(unsigned long addr, size_t size, bool write,
+				unsigned long ret_ip);
+
 void kasan_report(unsigned long addr, size_t size,
 		bool is_write, unsigned long ip);
 void kasan_report_invalid_free(void *object, unsigned long ip);
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 04/25] kasan: rename source files to reflect the new naming scheme ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 04/25] kasan: rename source files to reflect the new naming scheme
Date: Tue, 27 Nov 2018 16:55:22 +0000
Message-ID: <a6cd5acdc334d532754279a7d2d770e2be96419d.1543337629.git.andreyknvl () google ! com>
--------------------
We now have two KASAN modes: generic KASAN and tag-based KASAN. Rename
kasan.c to generic.c to reflect that. Also rename kasan_init.c to init.c
as it contains initialization code for both KASAN modes.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/Makefile                 | 8 ++++----
 mm/kasan/{kasan.c => generic.c}   | 0
 mm/kasan/{kasan_init.c => init.c} | 0
 3 files changed, 4 insertions(+), 4 deletions(-)
 rename mm/kasan/{kasan.c => generic.c} (100%)
 rename mm/kasan/{kasan_init.c => init.c} (100%)

diff --git a/mm/kasan/Makefile b/mm/kasan/Makefile
index a6df14bffb6b..d643530b24aa 100644
--- a/mm/kasan/Makefile
+++ b/mm/kasan/Makefile
@@ -1,14 +1,14 @@
 # SPDX-License-Identifier: GPL-2.0
 KASAN_SANITIZE := n
 UBSAN_SANITIZE_common.o := n
-UBSAN_SANITIZE_kasan.o := n
+UBSAN_SANITIZE_generic.o := n
 KCOV_INSTRUMENT := n
 
-CFLAGS_REMOVE_kasan.o = -pg
+CFLAGS_REMOVE_generic.o = -pg
 # Function splitter causes unnecessary splits in __asan_load1/__asan_store1
 # see: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=63533
 
 CFLAGS_common.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
-CFLAGS_kasan.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
+CFLAGS_generic.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 
-obj-y := common.o kasan.o report.o kasan_init.o quarantine.o
+obj-y := common.o generic.o report.o init.o quarantine.o
diff --git a/mm/kasan/kasan.c b/mm/kasan/generic.c
similarity index 100%
rename from mm/kasan/kasan.c
rename to mm/kasan/generic.c
diff --git a/mm/kasan/kasan_init.c b/mm/kasan/init.c
similarity index 100%
rename from mm/kasan/kasan_init.c
rename to mm/kasan/init.c
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 05/25] kasan: add CONFIG_KASAN_GENERIC and CONFIG_KASAN_SW_TAGS ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 05/25] kasan: add CONFIG_KASAN_GENERIC and CONFIG_KASAN_SW_TAGS
Date: Tue, 27 Nov 2018 16:55:23 +0000
Message-ID: <20728567aae93b5eb88a6636c94c1af73db7cdbc.1543337629.git.andreyknvl () google ! com>
--------------------
This commit splits the current CONFIG_KASAN config option into two:
1. CONFIG_KASAN_GENERIC, that enables the generic KASAN mode (the one
   that exists now);
2. CONFIG_KASAN_SW_TAGS, that enables the software tag-based KASAN mode.

The name CONFIG_KASAN_SW_TAGS is chosen as in the future we will have
another hardware tag-based KASAN mode, that will rely on hardware memory
tagging support in arm64.

With CONFIG_KASAN_SW_TAGS enabled, compiler options are changed to
instrument kernel files with -fsantize=kernel-hwaddress (except the ones
for which KASAN_SANITIZE := n is set).

Both CONFIG_KASAN_GENERIC and CONFIG_KASAN_SW_TAGS support both
CONFIG_KASAN_INLINE and CONFIG_KASAN_OUTLINE instrumentation modes.

This commit also adds empty placeholder (for now) implementation of
tag-based KASAN specific hooks inserted by the compiler and adjusts
common hooks implementation.

While this commit adds the CONFIG_KASAN_SW_TAGS config option, this option
is not selectable, as it depends on HAVE_ARCH_KASAN_SW_TAGS, which we will
enable once all the infrastracture code has been added.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 include/linux/compiler-clang.h      |  6 +-
 include/linux/compiler-gcc.h        |  6 ++
 include/linux/compiler_attributes.h | 13 ----
 include/linux/kasan.h               | 16 +++--
 lib/Kconfig.kasan                   | 96 +++++++++++++++++++++++------
 mm/kasan/Makefile                   |  6 +-
 mm/kasan/generic.c                  |  2 +-
 mm/kasan/kasan.h                    |  3 +-
 mm/kasan/tags.c                     | 75 ++++++++++++++++++++++
 mm/slub.c                           |  2 +-
 scripts/Makefile.kasan              | 53 +++++++++-------
 11 files changed, 216 insertions(+), 62 deletions(-)
 create mode 100644 mm/kasan/tags.c

diff --git a/include/linux/compiler-clang.h b/include/linux/compiler-clang.h
index 3e7dafb3ea80..39f668d5066b 100644
--- a/include/linux/compiler-clang.h
+++ b/include/linux/compiler-clang.h
@@ -16,9 +16,13 @@
 /* all clang versions usable with the kernel support KASAN ABI version 5 */
 #define KASAN_ABI_VERSION 5
 
+#if __has_feature(address_sanitizer) || __has_feature(hwaddress_sanitizer)
 /* emulate gcc's __SANITIZE_ADDRESS__ flag */
-#if __has_feature(address_sanitizer)
 #define __SANITIZE_ADDRESS__
+#define __no_sanitize_address \
+		__attribute__((no_sanitize("address", "hwaddress")))
+#else
+#define __no_sanitize_address
 #endif
 
 /*
diff --git a/include/linux/compiler-gcc.h b/include/linux/compiler-gcc.h
index 2010493e1040..5776da43da97 100644
--- a/include/linux/compiler-gcc.h
+++ b/include/linux/compiler-gcc.h
@@ -143,6 +143,12 @@
 #define KASAN_ABI_VERSION 3
 #endif
 
+#if __has_attribute(__no_sanitize_address__)
+#define __no_sanitize_address __attribute__((no_sanitize_address))
+#else
+#define __no_sanitize_address
+#endif
+
 #if GCC_VERSION >= 50100
 #define COMPILER_HAS_GENERIC_BUILTIN_OVERFLOW 1
 #endif
diff --git a/include/linux/compiler_attributes.h b/include/linux/compiler_attributes.h
index f8c400ba1929..7bceb9469197 100644
--- a/include/linux/compiler_attributes.h
+++ b/include/linux/compiler_attributes.h
@@ -206,19 +206,6 @@
  */
 #define __noreturn                      __attribute__((__noreturn__))
 
-/*
- * Optional: only supported since gcc >= 4.8
- * Optional: not supported by icc
- *
- *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-no_005fsanitize_005faddress-function-attribute
- * clang: https://clang.llvm.org/docs/AttributeReference.html#no-sanitize-address-no-address-safety-analysis
- */
-#if __has_attribute(__no_sanitize_address__)
-# define __no_sanitize_address          __attribute__((__no_sanitize_address__))
-#else
-# define __no_sanitize_address
-#endif
-
 /*
  *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Type-Attributes.html#index-packed-type-attribute
  * clang: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-packed-variable-attribute
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index 52c86a568a4e..b66fdf5ea7ab 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -45,8 +45,6 @@ void kasan_free_pages(struct page *page, unsigned int order);
 
 void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 			slab_flags_t *flags);
-void kasan_cache_shrink(struct kmem_cache *cache);
-void kasan_cache_shutdown(struct kmem_cache *cache);
 
 void kasan_poison_slab(struct page *page);
 void kasan_unpoison_object_data(struct kmem_cache *cache, void *object);
@@ -97,8 +95,6 @@ static inline void kasan_free_pages(struct page *page, unsigned int order) {}
 static inline void kasan_cache_create(struct kmem_cache *cache,
 				      unsigned int *size,
 				      slab_flags_t *flags) {}
-static inline void kasan_cache_shrink(struct kmem_cache *cache) {}
-static inline void kasan_cache_shutdown(struct kmem_cache *cache) {}
 
 static inline void kasan_poison_slab(struct page *page) {}
 static inline void kasan_unpoison_object_data(struct kmem_cache *cache,
@@ -155,4 +151,16 @@ static inline size_t kasan_metadata_size(struct kmem_cache *cache) { return 0; }
 
 #endif /* CONFIG_KASAN */
 
+#ifdef CONFIG_KASAN_GENERIC
+
+void kasan_cache_shrink(struct kmem_cache *cache);
+void kasan_cache_shutdown(struct kmem_cache *cache);
+
+#else /* CONFIG_KASAN_GENERIC */
+
+static inline void kasan_cache_shrink(struct kmem_cache *cache) {}
+static inline void kasan_cache_shutdown(struct kmem_cache *cache) {}
+
+#endif /* CONFIG_KASAN_GENERIC */
+
 #endif /* LINUX_KASAN_H */
diff --git a/lib/Kconfig.kasan b/lib/Kconfig.kasan
index d0bad1bd9a2b..b5d0cbfce4a1 100644
--- a/lib/Kconfig.kasan
+++ b/lib/Kconfig.kasan
@@ -1,35 +1,95 @@
+# This config refers to the generic KASAN mode.
 config HAVE_ARCH_KASAN
 	bool
 
+config HAVE_ARCH_KASAN_SW_TAGS
+	bool
+
+config CC_HAS_KASAN_GENERIC
+	def_bool $(cc-option, -fsanitize=kernel-address)
+
+config CC_HAS_KASAN_SW_TAGS
+	def_bool $(cc-option, -fsanitize=kernel-hwaddress)
+
 if HAVE_ARCH_KASAN
 
 config KASAN
-	bool "KASan: runtime memory debugger"
+	bool "KASAN: runtime memory debugger"
+	help
+	  Enables KASAN (KernelAddressSANitizer) - runtime memory debugger,
+	  designed to find out-of-bounds accesses and use-after-free bugs.
+	  See Documentation/dev-tools/kasan.rst for details.
+
+choice
+	prompt "KASAN mode"
+	depends on KASAN
+	default KASAN_GENERIC
+	help
+	  KASAN has two modes: generic KASAN (similar to userspace ASan,
+	  x86_64/arm64/xtensa, enabled with CONFIG_KASAN_GENERIC) and
+	  software tag-based KASAN (a version based on software memory
+	  tagging, arm64 only, similar to userspace HWASan, enabled with
+	  CONFIG_KASAN_SW_TAGS).
+	  Both generic and tag-based KASAN are strictly debugging features.
+
+config KASAN_GENERIC
+	bool "Generic mode"
+	depends on CC_HAS_KASAN_GENERIC
 	depends on (SLUB && SYSFS) || (SLAB && !DEBUG_SLAB)
 	select SLUB_DEBUG if SLUB
 	select CONSTRUCTORS
 	select STACKDEPOT
 	help
-	  Enables kernel address sanitizer - runtime memory debugger,
-	  designed to find out-of-bounds accesses and use-after-free bugs.
-	  This is strictly a debugging feature and it requires a gcc version
-	  of 4.9.2 or later. Detection of out of bounds accesses to stack or
-	  global variables requires gcc 5.0 or later.
-	  This feature consumes about 1/8 of available memory and brings about
-	  ~x3 performance slowdown.
+	  Enables generic KASAN mode.
+	  Supported in both GCC and Clang. With GCC it requires version 4.9.2
+	  or later for basic support and version 5.0 or later for detection of
+	  out-of-bounds accesses for stack and global variables and for inline
+	  instrumentation mode (CONFIG_KASAN_INLINE). With Clang it requires
+	  version 3.7.0 or later and it doesn't support detection of
+	  out-of-bounds accesses for global variables yet.
+	  This mode consumes about 1/8th of available memory at kernel start
+	  and introduces an overhead of ~x1.5 for the rest of the allocations.
+	  The performance slowdown is ~x3.
 	  For better error detection enable CONFIG_STACKTRACE.
-	  Currently CONFIG_KASAN doesn't work with CONFIG_DEBUG_SLAB
+	  Currently CONFIG_KASAN_GENERIC doesn't work with CONFIG_DEBUG_SLAB
 	  (the resulting kernel does not boot).
 
+if HAVE_ARCH_KASAN_SW_TAGS
+
+config KASAN_SW_TAGS
+	bool "Software tag-based mode"
+	depends on CC_HAS_KASAN_SW_TAGS
+	depends on (SLUB && SYSFS) || (SLAB && !DEBUG_SLAB)
+	select SLUB_DEBUG if SLUB
+	select CONSTRUCTORS
+	select STACKDEPOT
+	help
+	  Enables software tag-based KASAN mode.
+	  This mode requires Top Byte Ignore support by the CPU and therefore
+	  is only supported for arm64.
+	  This mode requires Clang version 7.0.0 or later.
+	  This mode consumes about 1/16th of available memory at kernel start
+	  and introduces an overhead of ~20% for the rest of the allocations.
+	  This mode may potentially introduce problems relating to pointer
+	  casting and comparison, as it embeds tags into the top byte of each
+	  pointer.
+	  For better error detection enable CONFIG_STACKTRACE.
+	  Currently CONFIG_KASAN_SW_TAGS doesn't work with CONFIG_DEBUG_SLAB
+	  (the resulting kernel does not boot).
+
+endif
+
+endchoice
+
 config KASAN_EXTRA
-	bool "KAsan: extra checks"
-	depends on KASAN && DEBUG_KERNEL && !COMPILE_TEST
+	bool "KASAN: extra checks"
+	depends on KASAN_GENERIC && DEBUG_KERNEL && !COMPILE_TEST
 	help
-	  This enables further checks in the kernel address sanitizer, for now
-	  it only includes the address-use-after-scope check that can lead
-	  to excessive kernel stack usage, frame size warnings and longer
+	  This enables further checks in generic KASAN, for now it only
+	  includes the address-use-after-scope check that can lead to
+	  excessive kernel stack usage, frame size warnings and longer
 	  compile time.
-	  https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81715 has more
+	  See https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81715
 
 
 choice
@@ -53,7 +113,7 @@ config KASAN_INLINE
 	  memory accesses. This is faster than outline (in some workloads
 	  it gives about x2 boost over outline instrumentation), but
 	  make kernel's .text size much bigger.
-	  This requires a gcc version of 5.0 or later.
+	  For CONFIG_KASAN_GENERIC this requires GCC 5.0 or later.
 
 endchoice
 
@@ -67,11 +127,11 @@ config KASAN_S390_4_LEVEL_PAGING
 	  4-level paging instead.
 
 config TEST_KASAN
-	tristate "Module for testing kasan for bug detection"
+	tristate "Module for testing KASAN for bug detection"
 	depends on m && KASAN
 	help
 	  This is a test module doing various nasty things like
 	  out of bounds accesses, use after free. It is useful for testing
-	  kernel debugging features like kernel address sanitizer.
+	  kernel debugging features like KASAN.
 
 endif
diff --git a/mm/kasan/Makefile b/mm/kasan/Makefile
index d643530b24aa..68ba1822f003 100644
--- a/mm/kasan/Makefile
+++ b/mm/kasan/Makefile
@@ -2,6 +2,7 @@
 KASAN_SANITIZE := n
 UBSAN_SANITIZE_common.o := n
 UBSAN_SANITIZE_generic.o := n
+UBSAN_SANITIZE_tags.o := n
 KCOV_INSTRUMENT := n
 
 CFLAGS_REMOVE_generic.o = -pg
@@ -10,5 +11,8 @@ CFLAGS_REMOVE_generic.o = -pg
 
 CFLAGS_common.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 CFLAGS_generic.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
+CFLAGS_tags.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 
-obj-y := common.o generic.o report.o init.o quarantine.o
+obj-$(CONFIG_KASAN) := common.o init.o report.o
+obj-$(CONFIG_KASAN_GENERIC) += generic.o quarantine.o
+obj-$(CONFIG_KASAN_SW_TAGS) += tags.o
diff --git a/mm/kasan/generic.c b/mm/kasan/generic.c
index 44ec228de0a2..b8de6d33c55c 100644
--- a/mm/kasan/generic.c
+++ b/mm/kasan/generic.c
@@ -1,5 +1,5 @@
 /*
- * This file contains core KASAN code.
+ * This file contains core generic KASAN code.
  *
  * Copyright (c) 2014 Samsung Electronics Co., Ltd.
  * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index 659463800f10..19b950eaccff 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -114,7 +114,8 @@ void kasan_report(unsigned long addr, size_t size,
 		bool is_write, unsigned long ip);
 void kasan_report_invalid_free(void *object, unsigned long ip);
 
-#if defined(CONFIG_SLAB) || defined(CONFIG_SLUB)
+#if defined(CONFIG_KASAN_GENERIC) && \
+	(defined(CONFIG_SLAB) || defined(CONFIG_SLUB))
 void quarantine_put(struct kasan_free_meta *info, struct kmem_cache *cache);
 void quarantine_reduce(void);
 void quarantine_remove_cache(struct kmem_cache *cache);
diff --git a/mm/kasan/tags.c b/mm/kasan/tags.c
new file mode 100644
index 000000000000..04194923c543
--- /dev/null
+++ b/mm/kasan/tags.c
@@ -0,0 +1,75 @@
+/*
+ * This file contains core tag-based KASAN code.
+ *
+ * Copyright (c) 2018 Google, Inc.
+ * Author: Andrey Konovalov <andreyknvl@google.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+#define DISABLE_BRANCH_PROFILING
+
+#include <linux/export.h>
+#include <linux/interrupt.h>
+#include <linux/init.h>
+#include <linux/kasan.h>
+#include <linux/kernel.h>
+#include <linux/kmemleak.h>
+#include <linux/linkage.h>
+#include <linux/memblock.h>
+#include <linux/memory.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/printk.h>
+#include <linux/random.h>
+#include <linux/sched.h>
+#include <linux/sched/task_stack.h>
+#include <linux/slab.h>
+#include <linux/stacktrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/vmalloc.h>
+#include <linux/bug.h>
+
+#include "kasan.h"
+#include "../slab.h"
+
+void check_memory_region(unsigned long addr, size_t size, bool write,
+				unsigned long ret_ip)
+{
+}
+
+#define DEFINE_HWASAN_LOAD_STORE(size)					\
+	void __hwasan_load##size##_noabort(unsigned long addr)		\
+	{								\
+	}								\
+	EXPORT_SYMBOL(__hwasan_load##size##_noabort);			\
+	void __hwasan_store##size##_noabort(unsigned long addr)		\
+	{								\
+	}								\
+	EXPORT_SYMBOL(__hwasan_store##size##_noabort)
+
+DEFINE_HWASAN_LOAD_STORE(1);
+DEFINE_HWASAN_LOAD_STORE(2);
+DEFINE_HWASAN_LOAD_STORE(4);
+DEFINE_HWASAN_LOAD_STORE(8);
+DEFINE_HWASAN_LOAD_STORE(16);
+
+void __hwasan_loadN_noabort(unsigned long addr, unsigned long size)
+{
+}
+EXPORT_SYMBOL(__hwasan_loadN_noabort);
+
+void __hwasan_storeN_noabort(unsigned long addr, unsigned long size)
+{
+}
+EXPORT_SYMBOL(__hwasan_storeN_noabort);
+
+void __hwasan_tag_memory(unsigned long addr, u8 tag, unsigned long size)
+{
+}
+EXPORT_SYMBOL(__hwasan_tag_memory);
diff --git a/mm/slub.c b/mm/slub.c
index 8561a32910dd..e739d46600b9 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -2992,7 +2992,7 @@ static __always_inline void slab_free(struct kmem_cache *s, struct page *page,
 		do_slab_free(s, page, head, tail, cnt, addr);
 }
 
-#ifdef CONFIG_KASAN
+#ifdef CONFIG_KASAN_GENERIC
 void ___cache_free(struct kmem_cache *cache, void *x, unsigned long addr)
 {
 	do_slab_free(cache, virt_to_head_page(x), x, NULL, 1, addr);
diff --git a/scripts/Makefile.kasan b/scripts/Makefile.kasan
index 69552a39951d..25c259df8ffa 100644
--- a/scripts/Makefile.kasan
+++ b/scripts/Makefile.kasan
@@ -1,5 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0
-ifdef CONFIG_KASAN
+ifdef CONFIG_KASAN_GENERIC
+
 ifdef CONFIG_KASAN_INLINE
 	call_threshold := 10000
 else
@@ -12,36 +13,44 @@ CFLAGS_KASAN_MINIMAL := -fsanitize=kernel-address
 
 cc-param = $(call cc-option, -mllvm -$(1), $(call cc-option, --param $(1)))
 
-ifeq ($(call cc-option, $(CFLAGS_KASAN_MINIMAL) -Werror),)
-   ifneq ($(CONFIG_COMPILE_TEST),y)
-        $(warning Cannot use CONFIG_KASAN: \
-            -fsanitize=kernel-address is not supported by compiler)
-   endif
-else
-   # -fasan-shadow-offset fails without -fsanitize
-   CFLAGS_KASAN_SHADOW := $(call cc-option, -fsanitize=kernel-address \
+# -fasan-shadow-offset fails without -fsanitize
+CFLAGS_KASAN_SHADOW := $(call cc-option, -fsanitize=kernel-address \
 			-fasan-shadow-offset=$(KASAN_SHADOW_OFFSET), \
 			$(call cc-option, -fsanitize=kernel-address \
 			-mllvm -asan-mapping-offset=$(KASAN_SHADOW_OFFSET)))
 
-   ifeq ($(strip $(CFLAGS_KASAN_SHADOW)),)
-      CFLAGS_KASAN := $(CFLAGS_KASAN_MINIMAL)
-   else
-      # Now add all the compiler specific options that are valid standalone
-      CFLAGS_KASAN := $(CFLAGS_KASAN_SHADOW) \
-	$(call cc-param,asan-globals=1) \
-	$(call cc-param,asan-instrumentation-with-call-threshold=$(call_threshold)) \
-	$(call cc-param,asan-stack=1) \
-	$(call cc-param,asan-use-after-scope=1) \
-	$(call cc-param,asan-instrument-allocas=1)
-   endif
-
+ifeq ($(strip $(CFLAGS_KASAN_SHADOW)),)
+	CFLAGS_KASAN := $(CFLAGS_KASAN_MINIMAL)
+else
+	# Now add all the compiler specific options that are valid standalone
+	CFLAGS_KASAN := $(CFLAGS_KASAN_SHADOW) \
+	 $(call cc-param,asan-globals=1) \
+	 $(call cc-param,asan-instrumentation-with-call-threshold=$(call_threshold)) \
+	 $(call cc-param,asan-stack=1) \
+	 $(call cc-param,asan-use-after-scope=1) \
+	 $(call cc-param,asan-instrument-allocas=1)
 endif
 
 ifdef CONFIG_KASAN_EXTRA
 CFLAGS_KASAN += $(call cc-option, -fsanitize-address-use-after-scope)
 endif
 
-CFLAGS_KASAN_NOSANITIZE := -fno-builtin
+endif # CONFIG_KASAN_GENERIC
 
+ifdef CONFIG_KASAN_SW_TAGS
+
+ifdef CONFIG_KASAN_INLINE
+    instrumentation_flags := -mllvm -hwasan-mapping-offset=$(KASAN_SHADOW_OFFSET)
+else
+    instrumentation_flags := -mllvm -hwasan-instrument-with-calls=1
+endif
+
+CFLAGS_KASAN := -fsanitize=kernel-hwaddress \
+		-mllvm -hwasan-instrument-stack=0 \
+		$(instrumentation_flags)
+
+endif # CONFIG_KASAN_SW_TAGS
+
+ifdef CONFIG_KASAN
+CFLAGS_KASAN_NOSANITIZE := -fno-builtin
 endif
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 06/25] kasan, arm64: adjust shadow size for tag-based mode ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 06/25] kasan, arm64: adjust shadow size for tag-based mode
Date: Tue, 27 Nov 2018 16:55:24 +0000
Message-ID: <95fa472a03a8ff268e24b8730ebd108922824e74.1543337629.git.andreyknvl () google ! com>
--------------------
Tag-based KASAN uses 1 shadow byte for 16 bytes of kernel memory, so it
requires 1/16th of the kernel virtual address space for the shadow memory.

This commit sets KASAN_SHADOW_SCALE_SHIFT to 4 when the tag-based KASAN
mode is enabled.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/Makefile             | 11 ++++++++++-
 arch/arm64/include/asm/memory.h |  7 +++----
 2 files changed, 13 insertions(+), 5 deletions(-)

diff --git a/arch/arm64/Makefile b/arch/arm64/Makefile
index 6cb9fc7e9382..99e7d08c6083 100644
--- a/arch/arm64/Makefile
+++ b/arch/arm64/Makefile
@@ -91,10 +91,19 @@ else
 TEXT_OFFSET := 0x00080000
 endif
 
+ifeq ($(CONFIG_KASAN_SW_TAGS), y)
+KASAN_SHADOW_SCALE_SHIFT := 4
+else
+KASAN_SHADOW_SCALE_SHIFT := 3
+endif
+
+KBUILD_CFLAGS += -DKASAN_SHADOW_SCALE_SHIFT=$(KASAN_SHADOW_SCALE_SHIFT)
+KBUILD_CPPFLAGS += -DKASAN_SHADOW_SCALE_SHIFT=$(KASAN_SHADOW_SCALE_SHIFT)
+KBUILD_AFLAGS += -DKASAN_SHADOW_SCALE_SHIFT=$(KASAN_SHADOW_SCALE_SHIFT)
+
 # KASAN_SHADOW_OFFSET = VA_START + (1 << (VA_BITS - KASAN_SHADOW_SCALE_SHIFT))
 #				 - (1 << (64 - KASAN_SHADOW_SCALE_SHIFT))
 # in 32-bit arithmetic
-KASAN_SHADOW_SCALE_SHIFT := 3
 KASAN_SHADOW_OFFSET := $(shell printf "0x%08x00000000\n" $$(( \
 	(0xffffffff & (-1 << ($(CONFIG_ARM64_VA_BITS) - 32))) \
 	+ (1 << ($(CONFIG_ARM64_VA_BITS) - 32 - $(KASAN_SHADOW_SCALE_SHIFT))) \
diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index b96442960aea..05fbc7ffcd31 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -74,12 +74,11 @@
 #define KERNEL_END        _end
 
 /*
- * KASAN requires 1/8th of the kernel virtual address space for the shadow
- * region. KASAN can bloat the stack significantly, so double the (minimum)
- * stack size when KASAN is in use.
+ * Generic and tag-based KASAN require 1/8th and 1/16th of the kernel virtual
+ * address space for the shadow region respectively. They can bloat the stack
+ * significantly, so double the (minimum) stack size when they are in use.
  */
 #ifdef CONFIG_KASAN
-#define KASAN_SHADOW_SCALE_SHIFT 3
 #define KASAN_SHADOW_SIZE	(UL(1) << (VA_BITS - KASAN_SHADOW_SCALE_SHIFT))
 #define KASAN_THREAD_SHIFT	1
 #else
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 08/25] kasan: initialize shadow to 0xff for tag-based mode ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 08/25] kasan: initialize shadow to 0xff for tag-based mode
Date: Tue, 27 Nov 2018 16:55:26 +0000
Message-ID: <9004fd16d56d8772775cf671a8fa66e54ed138dd.1543337629.git.andreyknvl () google ! com>
--------------------
A tag-based KASAN shadow memory cell contains a memory tag, that
corresponds to the tag in the top byte of the pointer, that points to that
memory. The native top byte value of kernel pointers is 0xff, so with
tag-based KASAN we need to initialize shadow memory to 0xff.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/mm/kasan_init.c | 15 +++++++++++++--
 include/linux/kasan.h      |  8 ++++++++
 mm/kasan/common.c          |  3 ++-
 3 files changed, 23 insertions(+), 3 deletions(-)

diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c
index 4ebc19422931..7a4a0904cac8 100644
--- a/arch/arm64/mm/kasan_init.c
+++ b/arch/arm64/mm/kasan_init.c
@@ -43,6 +43,15 @@ static phys_addr_t __init kasan_alloc_zeroed_page(int node)
 	return __pa(p);
 }
 
+static phys_addr_t __init kasan_alloc_raw_page(int node)
+{
+	void *p = memblock_alloc_try_nid_raw(PAGE_SIZE, PAGE_SIZE,
+						__pa(MAX_DMA_ADDRESS),
+						MEMBLOCK_ALLOC_ACCESSIBLE,
+						node);
+	return __pa(p);
+}
+
 static pte_t *__init kasan_pte_offset(pmd_t *pmdp, unsigned long addr, int node,
 				      bool early)
 {
@@ -92,7 +101,9 @@ static void __init kasan_pte_populate(pmd_t *pmdp, unsigned long addr,
 	do {
 		phys_addr_t page_phys = early ?
 				__pa_symbol(kasan_early_shadow_page)
-					: kasan_alloc_zeroed_page(node);
+					: kasan_alloc_raw_page(node);
+		if (!early)
+			memset(__va(page_phys), KASAN_SHADOW_INIT, PAGE_SIZE);
 		next = addr + PAGE_SIZE;
 		set_pte(ptep, pfn_pte(__phys_to_pfn(page_phys), PAGE_KERNEL));
 	} while (ptep++, addr = next, addr != end && pte_none(READ_ONCE(*ptep)));
@@ -239,7 +250,7 @@ void __init kasan_init(void)
 			pfn_pte(sym_to_pfn(kasan_early_shadow_page),
 				PAGE_KERNEL_RO));
 
-	memset(kasan_early_shadow_page, 0, PAGE_SIZE);
+	memset(kasan_early_shadow_page, KASAN_SHADOW_INIT, PAGE_SIZE);
 	cpu_replace_ttbr1(lm_alias(swapper_pg_dir));
 
 	/* At this point kasan is fully initialized. Enable error messages */
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index ec22d548d0d7..c56af24bd3e7 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -153,6 +153,8 @@ static inline size_t kasan_metadata_size(struct kmem_cache *cache) { return 0; }
 
 #ifdef CONFIG_KASAN_GENERIC
 
+#define KASAN_SHADOW_INIT 0
+
 void kasan_cache_shrink(struct kmem_cache *cache);
 void kasan_cache_shutdown(struct kmem_cache *cache);
 
@@ -163,4 +165,10 @@ static inline void kasan_cache_shutdown(struct kmem_cache *cache) {}
 
 #endif /* CONFIG_KASAN_GENERIC */
 
+#ifdef CONFIG_KASAN_SW_TAGS
+
+#define KASAN_SHADOW_INIT 0xFF
+
+#endif /* CONFIG_KASAN_SW_TAGS */
+
 #endif /* LINUX_KASAN_H */
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 5f68c93734ba..7134e75447ff 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -473,11 +473,12 @@ int kasan_module_alloc(void *addr, size_t size)
 
 	ret = __vmalloc_node_range(shadow_size, 1, shadow_start,
 			shadow_start + shadow_size,
-			GFP_KERNEL | __GFP_ZERO,
+			GFP_KERNEL,
 			PAGE_KERNEL, VM_NO_GUARD, NUMA_NO_NODE,
 			__builtin_return_address(0));
 
 	if (ret) {
+		__memset(ret, KASAN_SHADOW_INIT, shadow_size);
 		find_vm_area(addr)->flags |= VM_KASAN;
 		kmemleak_ignore(ret);
 		return 0;
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 09/25] arm64: move untagged_addr macro from uaccess.h to memory.h ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 09/25] arm64: move untagged_addr macro from uaccess.h to memory.h
Date: Tue, 27 Nov 2018 16:55:27 +0000
Message-ID: <432ef6686a25b49244f54c4dfd86bc4b20381d8a.1543337629.git.andreyknvl () google ! com>
--------------------
Move the untagged_addr() macro from arch/arm64/include/asm/uaccess.h
to arch/arm64/include/asm/memory.h to be later reused by KASAN.

Also make the untagged_addr() macro accept all kinds of address types
(void *, unsigned long, etc.). This allows not to specify type casts in
each place where the macro is used. This is done by using __typeof__.

Acked-by: Mark Rutland <mark.rutland@arm.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/memory.h  | 8 ++++++++
 arch/arm64/include/asm/uaccess.h | 7 -------
 2 files changed, 8 insertions(+), 7 deletions(-)

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 05fbc7ffcd31..e2c9857157f2 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -211,6 +211,14 @@ static inline unsigned long kaslr_offset(void)
  */
 #define PHYS_PFN_OFFSET	(PHYS_OFFSET >> PAGE_SHIFT)
 
+/*
+ * When dealing with data aborts, watchpoints, or instruction traps we may end
+ * up with a tagged userland pointer. Clear the tag to get a sane pointer to
+ * pass on to access_ok(), for instance.
+ */
+#define untagged_addr(addr)	\
+	((__typeof__(addr))sign_extend64((u64)(addr), 55))
+
 /*
  * Physical vs virtual RAM address space conversion.  These are
  * private definitions which should NOT be used outside memory.h
diff --git a/arch/arm64/include/asm/uaccess.h b/arch/arm64/include/asm/uaccess.h
index 07c34087bd5e..281a1e47263d 100644
--- a/arch/arm64/include/asm/uaccess.h
+++ b/arch/arm64/include/asm/uaccess.h
@@ -96,13 +96,6 @@ static inline unsigned long __range_ok(const void __user *addr, unsigned long si
 	return ret;
 }
 
-/*
- * When dealing with data aborts, watchpoints, or instruction traps we may end
- * up with a tagged userland pointer. Clear the tag to get a sane pointer to
- * pass on to access_ok(), for instance.
- */
-#define untagged_addr(addr)		sign_extend64(addr, 55)
-
 #define access_ok(type, addr, size)	__range_ok(addr, size)
 #define user_addr_max			get_fs
 
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 10/25] kasan: add tag related helper functions ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 10/25] kasan: add tag related helper functions
Date: Tue, 27 Nov 2018 16:55:28 +0000
Message-ID: <643b46fbcd6433a4be18b3a19ce9f3e727618a8d.1543337629.git.andreyknvl () google ! com>
--------------------
This commit adds a few helper functions, that are meant to be used to
work with tags embedded in the top byte of kernel pointers: to set, to
get or to reset the top byte.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/kasan.h  |  8 +++++--
 arch/arm64/include/asm/memory.h | 12 +++++++++++
 arch/arm64/mm/kasan_init.c      |  2 ++
 include/linux/kasan.h           | 13 ++++++++++++
 mm/kasan/kasan.h                | 31 +++++++++++++++++++++++++++
 mm/kasan/tags.c                 | 37 +++++++++++++++++++++++++++++++++
 6 files changed, 101 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/kasan.h b/arch/arm64/include/asm/kasan.h
index 8758bb008436..b52aacd2c526 100644
--- a/arch/arm64/include/asm/kasan.h
+++ b/arch/arm64/include/asm/kasan.h
@@ -4,12 +4,16 @@
 
 #ifndef __ASSEMBLY__
 
-#ifdef CONFIG_KASAN
-
 #include <linux/linkage.h>
 #include <asm/memory.h>
 #include <asm/pgtable-types.h>
 
+#define arch_kasan_set_tag(addr, tag)	__tag_set(addr, tag)
+#define arch_kasan_reset_tag(addr)	__tag_reset(addr)
+#define arch_kasan_get_tag(addr)	__tag_get(addr)
+
+#ifdef CONFIG_KASAN
+
 /*
  * KASAN_SHADOW_START: beginning of the kernel virtual addresses.
  * KASAN_SHADOW_END: KASAN_SHADOW_START + 1/N of kernel virtual addresses,
diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index e2c9857157f2..83c1366a1233 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -219,6 +219,18 @@ static inline unsigned long kaslr_offset(void)
 #define untagged_addr(addr)	\
 	((__typeof__(addr))sign_extend64((u64)(addr), 55))
 
+#ifdef CONFIG_KASAN_SW_TAGS
+#define __tag_shifted(tag)	((u64)(tag) << 56)
+#define __tag_set(addr, tag)	(__typeof__(addr))( \
+		((u64)(addr) & ~__tag_shifted(0xff)) | __tag_shifted(tag))
+#define __tag_reset(addr)	untagged_addr(addr)
+#define __tag_get(addr)		(__u8)((u64)(addr) >> 56)
+#else
+#define __tag_set(addr, tag)	(addr)
+#define __tag_reset(addr)	(addr)
+#define __tag_get(addr)		0
+#endif
+
 /*
  * Physical vs virtual RAM address space conversion.  These are
  * private definitions which should NOT be used outside memory.h
diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c
index 7a4a0904cac8..1df536bdabcb 100644
--- a/arch/arm64/mm/kasan_init.c
+++ b/arch/arm64/mm/kasan_init.c
@@ -253,6 +253,8 @@ void __init kasan_init(void)
 	memset(kasan_early_shadow_page, KASAN_SHADOW_INIT, PAGE_SIZE);
 	cpu_replace_ttbr1(lm_alias(swapper_pg_dir));
 
+	kasan_init_tags();
+
 	/* At this point kasan is fully initialized. Enable error messages */
 	init_task.kasan_depth = 0;
 	pr_info("KernelAddressSanitizer initialized\n");
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index c56af24bd3e7..a477ce2abdc9 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -169,6 +169,19 @@ static inline void kasan_cache_shutdown(struct kmem_cache *cache) {}
 
 #define KASAN_SHADOW_INIT 0xFF
 
+void kasan_init_tags(void);
+
+void *kasan_reset_tag(const void *addr);
+
+#else /* CONFIG_KASAN_SW_TAGS */
+
+static inline void kasan_init_tags(void) { }
+
+static inline void *kasan_reset_tag(const void *addr)
+{
+	return (void *)addr;
+}
+
 #endif /* CONFIG_KASAN_SW_TAGS */
 
 #endif /* LINUX_KASAN_H */
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index 19b950eaccff..b080b8d92812 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -8,6 +8,10 @@
 #define KASAN_SHADOW_SCALE_SIZE (1UL << KASAN_SHADOW_SCALE_SHIFT)
 #define KASAN_SHADOW_MASK       (KASAN_SHADOW_SCALE_SIZE - 1)
 
+#define KASAN_TAG_KERNEL	0xFF /* native kernel pointers tag */
+#define KASAN_TAG_INVALID	0xFE /* inaccessible memory tag */
+#define KASAN_TAG_MAX		0xFD /* maximum value for random tags */
+
 #define KASAN_FREE_PAGE         0xFF  /* page was freed */
 #define KASAN_PAGE_REDZONE      0xFE  /* redzone for kmalloc_large allocations */
 #define KASAN_KMALLOC_REDZONE   0xFC  /* redzone inside slub object */
@@ -126,6 +130,33 @@ static inline void quarantine_reduce(void) { }
 static inline void quarantine_remove_cache(struct kmem_cache *cache) { }
 #endif
 
+#ifdef CONFIG_KASAN_SW_TAGS
+
+u8 random_tag(void);
+
+#else
+
+static inline u8 random_tag(void)
+{
+	return 0;
+}
+
+#endif
+
+#ifndef arch_kasan_set_tag
+#define arch_kasan_set_tag(addr, tag)	((void *)(addr))
+#endif
+#ifndef arch_kasan_reset_tag
+#define arch_kasan_reset_tag(addr)	((void *)(addr))
+#endif
+#ifndef arch_kasan_get_tag
+#define arch_kasan_get_tag(addr)	0
+#endif
+
+#define set_tag(addr, tag)	((void *)arch_kasan_set_tag((addr), (tag)))
+#define reset_tag(addr)		((void *)arch_kasan_reset_tag(addr))
+#define get_tag(addr)		arch_kasan_get_tag(addr)
+
 /*
  * Exported functions for interfaces called from assembly or from generated
  * code. Declarations here to avoid warning about missing declarations.
diff --git a/mm/kasan/tags.c b/mm/kasan/tags.c
index 04194923c543..1c4e7ce2e6fe 100644
--- a/mm/kasan/tags.c
+++ b/mm/kasan/tags.c
@@ -38,6 +38,43 @@
 #include "kasan.h"
 #include "../slab.h"
 
+static DEFINE_PER_CPU(u32, prng_state);
+
+void kasan_init_tags(void)
+{
+	int cpu;
+
+	for_each_possible_cpu(cpu)
+		per_cpu(prng_state, cpu) = get_random_u32();
+}
+
+/*
+ * If a preemption happens between this_cpu_read and this_cpu_write, the only
+ * side effect is that we'll give a few allocated in different contexts objects
+ * the same tag. Since tag-based KASAN is meant to be used a probabilistic
+ * bug-detection debug feature, this doesn't have significant negative impact.
+ *
+ * Ideally the tags use strong randomness to prevent any attempts to predict
+ * them during explicit exploit attempts. But strong randomness is expensive,
+ * and we did an intentional trade-off to use a PRNG. This non-atomic RMW
+ * sequence has in fact positive effect, since interrupts that randomly skew
+ * PRNG at unpredictable points do only good.
+ */
+u8 random_tag(void)
+{
+	u32 state = this_cpu_read(prng_state);
+
+	state = 1664525 * state + 1013904223;
+	this_cpu_write(prng_state, state);
+
+	return (u8)(state % (KASAN_TAG_MAX + 1));
+}
+
+void *kasan_reset_tag(const void *addr)
+{
+	return reset_tag(addr);
+}
+
 void check_memory_region(unsigned long addr, size_t size, bool write,
 				unsigned long ret_ip)
 {
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 12/25] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 12/25] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Tue, 27 Nov 2018 16:55:30 +0000
Message-ID: <b2c17b6674f1737f981ffa6dca7fdfc059a88435.1543337629.git.andreyknvl () google ! com>
--------------------
An object constructor can initialize pointers within this objects based on
the address of the object. Since the object address might be tagged, we
need to assign a tag before calling constructor.

The implemented approach is to assign tags to objects with constructors
when a slab is allocated and call constructors once as usual. The
downside is that such object would always have the same tag when it is
reallocated, so we won't catch use-after-frees on it.

Also pressign tags for objects from SLAB_TYPESAFE_BY_RCU caches, since
they can be validy accessed after having been freed.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/slab.c |  2 +-
 mm/slub.c | 24 ++++++++++++++----------
 2 files changed, 15 insertions(+), 11 deletions(-)

diff --git a/mm/slab.c b/mm/slab.c
index 26f60a22e5e0..27859fb39889 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -2574,7 +2574,7 @@ static void cache_init_objs(struct kmem_cache *cachep,
 
 	for (i = 0; i < cachep->num; i++) {
 		objp = index_to_obj(cachep, page, i);
-		kasan_init_slab_obj(cachep, objp);
+		objp = kasan_init_slab_obj(cachep, objp);
 
 		/* constructor could break poison info */
 		if (DEBUG == 0 && cachep->ctor) {
diff --git a/mm/slub.c b/mm/slub.c
index e739d46600b9..08740c3f3745 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1451,16 +1451,17 @@ static inline bool slab_free_freelist_hook(struct kmem_cache *s,
 #endif
 }
 
-static void setup_object(struct kmem_cache *s, struct page *page,
+static void *setup_object(struct kmem_cache *s, struct page *page,
 				void *object)
 {
 	setup_object_debug(s, page, object);
-	kasan_init_slab_obj(s, object);
+	object = kasan_init_slab_obj(s, object);
 	if (unlikely(s->ctor)) {
 		kasan_unpoison_object_data(s, object);
 		s->ctor(object);
 		kasan_poison_object_data(s, object);
 	}
+	return object;
 }
 
 /*
@@ -1568,16 +1569,16 @@ static bool shuffle_freelist(struct kmem_cache *s, struct page *page)
 	/* First entry is used as the base of the freelist */
 	cur = next_freelist_entry(s, page, &pos, start, page_limit,
 				freelist_count);
+	cur = setup_object(s, page, cur);
 	page->freelist = cur;
 
 	for (idx = 1; idx < page->objects; idx++) {
-		setup_object(s, page, cur);
 		next = next_freelist_entry(s, page, &pos, start, page_limit,
 			freelist_count);
+		next = setup_object(s, page, next);
 		set_freepointer(s, cur, next);
 		cur = next;
 	}
-	setup_object(s, page, cur);
 	set_freepointer(s, cur, NULL);
 
 	return true;
@@ -1599,7 +1600,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	struct page *page;
 	struct kmem_cache_order_objects oo = s->oo;
 	gfp_t alloc_gfp;
-	void *start, *p;
+	void *start, *p, *next;
 	int idx, order;
 	bool shuffle;
 
@@ -1651,13 +1652,16 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 
 	if (!shuffle) {
 		for_each_object_idx(p, idx, s, start, page->objects) {
-			setup_object(s, page, p);
-			if (likely(idx < page->objects))
-				set_freepointer(s, p, p + s->size);
-			else
+			if (likely(idx < page->objects)) {
+				next = p + s->size;
+				next = setup_object(s, page, next);
+				set_freepointer(s, p, next);
+			} else
 				set_freepointer(s, p, NULL);
 		}
-		page->freelist = fixup_red_left(s, start);
+		start = fixup_red_left(s, start);
+		start = setup_object(s, page, start);
+		page->freelist = start;
 	}
 
 	page->inuse = page->objects;
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 13/25] kasan, arm64: fix up fault handling logic ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 13/25] kasan, arm64: fix up fault handling logic
Date: Tue, 27 Nov 2018 16:55:31 +0000
Message-ID: <a54fe8c8c11948b0ac8c8b285fb36f845217c84a.1543337629.git.andreyknvl () google ! com>
--------------------
Right now arm64 fault handling code removes pointer tags from addresses
covered by TTBR0 in faults taken from both EL0 and EL1, but doesn't do
that for pointers covered by TTBR1.

This patch adds two helper functions is_ttbr0_addr() and is_ttbr1_addr(),
where the latter one accounts for the fact that TTBR1 pointers might be
tagged when tag-based KASAN is in use, and uses these helper functions to
perform pointer checks in arch/arm64/mm/fault.c.

Suggested-by: Mark Rutland <mark.rutland@arm.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/mm/fault.c | 31 ++++++++++++++++++++++---------
 1 file changed, 22 insertions(+), 9 deletions(-)

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 7d9571f4ae3d..c1d98f0a3086 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -40,6 +40,7 @@
 #include <asm/daifflags.h>
 #include <asm/debug-monitors.h>
 #include <asm/esr.h>
+#include <asm/kasan.h>
 #include <asm/sysreg.h>
 #include <asm/system_misc.h>
 #include <asm/pgtable.h>
@@ -132,6 +133,18 @@ static void mem_abort_decode(unsigned int esr)
 		data_abort_decode(esr);
 }
 
+static inline bool is_ttbr0_addr(unsigned long addr)
+{
+	/* entry assembly clears tags for TTBR0 addrs */
+	return addr < TASK_SIZE;
+}
+
+static inline bool is_ttbr1_addr(unsigned long addr)
+{
+	/* TTBR1 addresses may have a tag if KASAN_SW_TAGS is in use */
+	return arch_kasan_reset_tag(addr) >= VA_START;
+}
+
 /*
  * Dump out the page tables associated with 'addr' in the currently active mm.
  */
@@ -141,7 +154,7 @@ void show_pte(unsigned long addr)
 	pgd_t *pgdp;
 	pgd_t pgd;
 
-	if (addr < TASK_SIZE) {
+	if (is_ttbr0_addr(addr)) {
 		/* TTBR0 */
 		mm = current->active_mm;
 		if (mm == &init_mm) {
@@ -149,7 +162,7 @@ void show_pte(unsigned long addr)
 				 addr);
 			return;
 		}
-	} else if (addr >= VA_START) {
+	} else if (is_ttbr1_addr(addr)) {
 		/* TTBR1 */
 		mm = &init_mm;
 	} else {
@@ -254,7 +267,7 @@ static inline bool is_el1_permission_fault(unsigned long addr, unsigned int esr,
 	if (fsc_type == ESR_ELx_FSC_PERM)
 		return true;
 
-	if (addr < TASK_SIZE && system_uses_ttbr0_pan())
+	if (is_ttbr0_addr(addr) && system_uses_ttbr0_pan())
 		return fsc_type == ESR_ELx_FSC_FAULT &&
 			(regs->pstate & PSR_PAN_BIT);
 
@@ -319,7 +332,7 @@ static void set_thread_esr(unsigned long address, unsigned int esr)
 	 * type", so we ignore this wrinkle and just return the translation
 	 * fault.)
 	 */
-	if (current->thread.fault_address >= TASK_SIZE) {
+	if (!is_ttbr0_addr(current->thread.fault_address)) {
 		switch (ESR_ELx_EC(esr)) {
 		case ESR_ELx_EC_DABT_LOW:
 			/*
@@ -455,7 +468,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		mm_flags |= FAULT_FLAG_WRITE;
 	}
 
-	if (addr < TASK_SIZE && is_el1_permission_fault(addr, esr, regs)) {
+	if (is_ttbr0_addr(addr) && is_el1_permission_fault(addr, esr, regs)) {
 		/* regs->orig_addr_limit may be 0 if we entered from EL0 */
 		if (regs->orig_addr_limit == KERNEL_DS)
 			die_kernel_fault("access to user memory with fs=KERNEL_DS",
@@ -603,7 +616,7 @@ static int __kprobes do_translation_fault(unsigned long addr,
 					  unsigned int esr,
 					  struct pt_regs *regs)
 {
-	if (addr < TASK_SIZE)
+	if (is_ttbr0_addr(addr))
 		return do_page_fault(addr, esr, regs);
 
 	do_bad_area(addr, esr, regs);
@@ -758,7 +771,7 @@ asmlinkage void __exception do_el0_ia_bp_hardening(unsigned long addr,
 	 * re-enabled IRQs. If the address is a kernel address, apply
 	 * BP hardening prior to enabling IRQs and pre-emption.
 	 */
-	if (addr > TASK_SIZE)
+	if (!is_ttbr0_addr(addr))
 		arm64_apply_bp_hardening();
 
 	local_daif_restore(DAIF_PROCCTX);
@@ -771,7 +784,7 @@ asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
 					   struct pt_regs *regs)
 {
 	if (user_mode(regs)) {
-		if (instruction_pointer(regs) > TASK_SIZE)
+		if (!is_ttbr0_addr(instruction_pointer(regs)))
 			arm64_apply_bp_hardening();
 		local_daif_restore(DAIF_PROCCTX);
 	}
@@ -825,7 +838,7 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 	if (interrupts_enabled(regs))
 		trace_hardirqs_off();
 
-	if (user_mode(regs) && instruction_pointer(regs) > TASK_SIZE)
+	if (user_mode(regs) && !is_ttbr0_addr(instruction_pointer(regs)))
 		arm64_apply_bp_hardening();
 
 	if (!inf->fn(addr, esr, regs)) {
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 14/25] kasan, arm64: enable top byte ignore for the kernel ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 14/25] kasan, arm64: enable top byte ignore for the kernel
Date: Tue, 27 Nov 2018 16:55:32 +0000
Message-ID: <1ed03d53ee679cba52ba7118d2acbef948d21fcc.1543337629.git.andreyknvl () google ! com>
--------------------
Tag-based KASAN uses the Top Byte Ignore feature of arm64 CPUs to store a
pointer tag in the top byte of each pointer. This commit enables the
TCR_TBI1 bit, which enables Top Byte Ignore for the kernel, when tag-based
KASAN is used.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/pgtable-hwdef.h | 1 +
 arch/arm64/mm/proc.S                   | 8 +++++++-
 2 files changed, 8 insertions(+), 1 deletion(-)

diff --git a/arch/arm64/include/asm/pgtable-hwdef.h b/arch/arm64/include/asm/pgtable-hwdef.h
index 1d7d8da2ef9b..d43b870c39b3 100644
--- a/arch/arm64/include/asm/pgtable-hwdef.h
+++ b/arch/arm64/include/asm/pgtable-hwdef.h
@@ -291,6 +291,7 @@
 #define TCR_A1			(UL(1) << 22)
 #define TCR_ASID16		(UL(1) << 36)
 #define TCR_TBI0		(UL(1) << 37)
+#define TCR_TBI1		(UL(1) << 38)
 #define TCR_HA			(UL(1) << 39)
 #define TCR_HD			(UL(1) << 40)
 #define TCR_NFD1		(UL(1) << 54)
diff --git a/arch/arm64/mm/proc.S b/arch/arm64/mm/proc.S
index 2c75b0b903ae..d861f208eeb1 100644
--- a/arch/arm64/mm/proc.S
+++ b/arch/arm64/mm/proc.S
@@ -47,6 +47,12 @@
 /* PTWs cacheable, inner/outer WBWA */
 #define TCR_CACHE_FLAGS	TCR_IRGN_WBWA | TCR_ORGN_WBWA
 
+#ifdef CONFIG_KASAN_SW_TAGS
+#define TCR_KASAN_FLAGS TCR_TBI1
+#else
+#define TCR_KASAN_FLAGS 0
+#endif
+
 #define MAIR(attr, mt)	((attr) << ((mt) * 8))
 
 /*
@@ -445,7 +451,7 @@ ENTRY(__cpu_setup)
 	 */
 	ldr	x10, =TCR_TxSZ(VA_BITS) | TCR_CACHE_FLAGS | TCR_SMP_FLAGS | \
 			TCR_TG_FLAGS | TCR_KASLR_FLAGS | TCR_ASID16 | \
-			TCR_TBI0 | TCR_A1
+			TCR_TBI0 | TCR_A1 | TCR_KASAN_FLAGS
 	tcr_set_idmap_t0sz	x10, x9
 
 	/*
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 15/25] kasan, mm: perform untagged pointers comparison in krealloc ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 15/25] kasan, mm: perform untagged pointers comparison in krealloc
Date: Tue, 27 Nov 2018 16:55:33 +0000
Message-ID: <5045db8a8e249a1eda3199b952120035eacb3bd4.1543337629.git.andreyknvl () google ! com>
--------------------
The krealloc function checks where the same buffer was reused or a new one
allocated by comparing kernel pointers. Tag-based KASAN changes memory tag
on the krealloc'ed chunk of memory and therefore also changes the pointer
tag of the returned pointer. Therefore we need to perform comparison on
untagged (with tags reset) pointers to check whether it's the same memory
region or not.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/slab_common.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/mm/slab_common.c b/mm/slab_common.c
index 5f3504e26d4c..5aabcbd32d82 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -1534,7 +1534,7 @@ void *krealloc(const void *p, size_t new_size, gfp_t flags)
 	}
 
 	ret = __do_krealloc(p, new_size, flags);
-	if (ret && p != ret)
+	if (ret && kasan_reset_tag(p) != kasan_reset_tag(ret))
 		kfree(p);
 
 	return ret;
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 16/25] kasan: split out generic_report.c from report.c ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 16/25] kasan: split out generic_report.c from report.c
Date: Tue, 27 Nov 2018 16:55:34 +0000
Message-ID: <9030fe246a786be1348f8b08089f30e52be23ec4.1543337629.git.andreyknvl () google ! com>
--------------------
This patch moves generic KASAN specific error reporting routines to
generic_report.c without any functional changes, leaving common error
reporting code in report.c to be later reused by tag-based KASAN.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/Makefile         |   4 +-
 mm/kasan/generic_report.c | 158 +++++++++++++++++++++++++
 mm/kasan/kasan.h          |   7 ++
 mm/kasan/report.c         | 234 +++++++++-----------------------------
 mm/kasan/tags_report.c    |  39 +++++++
 5 files changed, 257 insertions(+), 185 deletions(-)
 create mode 100644 mm/kasan/generic_report.c
 create mode 100644 mm/kasan/tags_report.c

diff --git a/mm/kasan/Makefile b/mm/kasan/Makefile
index 68ba1822f003..0a14fcff70ed 100644
--- a/mm/kasan/Makefile
+++ b/mm/kasan/Makefile
@@ -14,5 +14,5 @@ CFLAGS_generic.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 CFLAGS_tags.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 
 obj-$(CONFIG_KASAN) := common.o init.o report.o
-obj-$(CONFIG_KASAN_GENERIC) += generic.o quarantine.o
-obj-$(CONFIG_KASAN_SW_TAGS) += tags.o
+obj-$(CONFIG_KASAN_GENERIC) += generic.o generic_report.o quarantine.o
+obj-$(CONFIG_KASAN_SW_TAGS) += tags.o tags_report.o
diff --git a/mm/kasan/generic_report.c b/mm/kasan/generic_report.c
new file mode 100644
index 000000000000..5201d1770700
--- /dev/null
+++ b/mm/kasan/generic_report.c
@@ -0,0 +1,158 @@
+/*
+ * This file contains generic KASAN specific error reporting code.
+ *
+ * Copyright (c) 2014 Samsung Electronics Co., Ltd.
+ * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
+ *
+ * Some code borrowed from https://github.com/xairy/kasan-prototype by
+ *        Andrey Konovalov <andreyknvl@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/bitops.h>
+#include <linux/ftrace.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/stackdepot.h>
+#include <linux/stacktrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/kasan.h>
+#include <linux/module.h>
+
+#include <asm/sections.h>
+
+#include "kasan.h"
+#include "../slab.h"
+
+static const void *find_first_bad_addr(const void *addr, size_t size)
+{
+	u8 shadow_val = *(u8 *)kasan_mem_to_shadow(addr);
+	const void *first_bad_addr = addr;
+
+	while (!shadow_val && first_bad_addr < addr + size) {
+		first_bad_addr += KASAN_SHADOW_SCALE_SIZE;
+		shadow_val = *(u8 *)kasan_mem_to_shadow(first_bad_addr);
+	}
+	return first_bad_addr;
+}
+
+static const char *get_shadow_bug_type(struct kasan_access_info *info)
+{
+	const char *bug_type = "unknown-crash";
+	u8 *shadow_addr;
+
+	info->first_bad_addr = find_first_bad_addr(info->access_addr,
+						info->access_size);
+
+	shadow_addr = (u8 *)kasan_mem_to_shadow(info->first_bad_addr);
+
+	/*
+	 * If shadow byte value is in [0, KASAN_SHADOW_SCALE_SIZE) we can look
+	 * at the next shadow byte to determine the type of the bad access.
+	 */
+	if (*shadow_addr > 0 && *shadow_addr <= KASAN_SHADOW_SCALE_SIZE - 1)
+		shadow_addr++;
+
+	switch (*shadow_addr) {
+	case 0 ... KASAN_SHADOW_SCALE_SIZE - 1:
+		/*
+		 * In theory it's still possible to see these shadow values
+		 * due to a data race in the kernel code.
+		 */
+		bug_type = "out-of-bounds";
+		break;
+	case KASAN_PAGE_REDZONE:
+	case KASAN_KMALLOC_REDZONE:
+		bug_type = "slab-out-of-bounds";
+		break;
+	case KASAN_GLOBAL_REDZONE:
+		bug_type = "global-out-of-bounds";
+		break;
+	case KASAN_STACK_LEFT:
+	case KASAN_STACK_MID:
+	case KASAN_STACK_RIGHT:
+	case KASAN_STACK_PARTIAL:
+		bug_type = "stack-out-of-bounds";
+		break;
+	case KASAN_FREE_PAGE:
+	case KASAN_KMALLOC_FREE:
+		bug_type = "use-after-free";
+		break;
+	case KASAN_USE_AFTER_SCOPE:
+		bug_type = "use-after-scope";
+		break;
+	case KASAN_ALLOCA_LEFT:
+	case KASAN_ALLOCA_RIGHT:
+		bug_type = "alloca-out-of-bounds";
+		break;
+	}
+
+	return bug_type;
+}
+
+static const char *get_wild_bug_type(struct kasan_access_info *info)
+{
+	const char *bug_type = "unknown-crash";
+
+	if ((unsigned long)info->access_addr < PAGE_SIZE)
+		bug_type = "null-ptr-deref";
+	else if ((unsigned long)info->access_addr < TASK_SIZE)
+		bug_type = "user-memory-access";
+	else
+		bug_type = "wild-memory-access";
+
+	return bug_type;
+}
+
+const char *get_bug_type(struct kasan_access_info *info)
+{
+	if (addr_has_shadow(info->access_addr))
+		return get_shadow_bug_type(info);
+	return get_wild_bug_type(info);
+}
+
+#define DEFINE_ASAN_REPORT_LOAD(size)                     \
+void __asan_report_load##size##_noabort(unsigned long addr) \
+{                                                         \
+	kasan_report(addr, size, false, _RET_IP_);	  \
+}                                                         \
+EXPORT_SYMBOL(__asan_report_load##size##_noabort)
+
+#define DEFINE_ASAN_REPORT_STORE(size)                     \
+void __asan_report_store##size##_noabort(unsigned long addr) \
+{                                                          \
+	kasan_report(addr, size, true, _RET_IP_);	   \
+}                                                          \
+EXPORT_SYMBOL(__asan_report_store##size##_noabort)
+
+DEFINE_ASAN_REPORT_LOAD(1);
+DEFINE_ASAN_REPORT_LOAD(2);
+DEFINE_ASAN_REPORT_LOAD(4);
+DEFINE_ASAN_REPORT_LOAD(8);
+DEFINE_ASAN_REPORT_LOAD(16);
+DEFINE_ASAN_REPORT_STORE(1);
+DEFINE_ASAN_REPORT_STORE(2);
+DEFINE_ASAN_REPORT_STORE(4);
+DEFINE_ASAN_REPORT_STORE(8);
+DEFINE_ASAN_REPORT_STORE(16);
+
+void __asan_report_load_n_noabort(unsigned long addr, size_t size)
+{
+	kasan_report(addr, size, false, _RET_IP_);
+}
+EXPORT_SYMBOL(__asan_report_load_n_noabort);
+
+void __asan_report_store_n_noabort(unsigned long addr, size_t size)
+{
+	kasan_report(addr, size, true, _RET_IP_);
+}
+EXPORT_SYMBOL(__asan_report_store_n_noabort);
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index b080b8d92812..33cc3b0e017e 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -109,11 +109,18 @@ static inline const void *kasan_shadow_to_mem(const void *shadow_addr)
 		<< KASAN_SHADOW_SCALE_SHIFT);
 }
 
+static inline bool addr_has_shadow(const void *addr)
+{
+	return (addr >= kasan_shadow_to_mem((void *)KASAN_SHADOW_START));
+}
+
 void kasan_poison_shadow(const void *address, size_t size, u8 value);
 
 void check_memory_region(unsigned long addr, size_t size, bool write,
 				unsigned long ret_ip);
 
+const char *get_bug_type(struct kasan_access_info *info);
+
 void kasan_report(unsigned long addr, size_t size,
 		bool is_write, unsigned long ip);
 void kasan_report_invalid_free(void *object, unsigned long ip);
diff --git a/mm/kasan/report.c b/mm/kasan/report.c
index 5c169aa688fd..64a74f334c45 100644
--- a/mm/kasan/report.c
+++ b/mm/kasan/report.c
@@ -1,5 +1,5 @@
 /*
- * This file contains error reporting code.
+ * This file contains common generic and tag-based KASAN error reporting code.
  *
  * Copyright (c) 2014 Samsung Electronics Co., Ltd.
  * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
@@ -39,103 +39,34 @@
 #define SHADOW_BYTES_PER_ROW (SHADOW_BLOCKS_PER_ROW * SHADOW_BYTES_PER_BLOCK)
 #define SHADOW_ROWS_AROUND_ADDR 2
 
-static const void *find_first_bad_addr(const void *addr, size_t size)
-{
-	u8 shadow_val = *(u8 *)kasan_mem_to_shadow(addr);
-	const void *first_bad_addr = addr;
-
-	while (!shadow_val && first_bad_addr < addr + size) {
-		first_bad_addr += KASAN_SHADOW_SCALE_SIZE;
-		shadow_val = *(u8 *)kasan_mem_to_shadow(first_bad_addr);
-	}
-	return first_bad_addr;
-}
+static unsigned long kasan_flags;
 
-static bool addr_has_shadow(struct kasan_access_info *info)
-{
-	return (info->access_addr >=
-		kasan_shadow_to_mem((void *)KASAN_SHADOW_START));
-}
+#define KASAN_BIT_REPORTED	0
+#define KASAN_BIT_MULTI_SHOT	1
 
-static const char *get_shadow_bug_type(struct kasan_access_info *info)
+bool kasan_save_enable_multi_shot(void)
 {
-	const char *bug_type = "unknown-crash";
-	u8 *shadow_addr;
-
-	info->first_bad_addr = find_first_bad_addr(info->access_addr,
-						info->access_size);
-
-	shadow_addr = (u8 *)kasan_mem_to_shadow(info->first_bad_addr);
-
-	/*
-	 * If shadow byte value is in [0, KASAN_SHADOW_SCALE_SIZE) we can look
-	 * at the next shadow byte to determine the type of the bad access.
-	 */
-	if (*shadow_addr > 0 && *shadow_addr <= KASAN_SHADOW_SCALE_SIZE - 1)
-		shadow_addr++;
-
-	switch (*shadow_addr) {
-	case 0 ... KASAN_SHADOW_SCALE_SIZE - 1:
-		/*
-		 * In theory it's still possible to see these shadow values
-		 * due to a data race in the kernel code.
-		 */
-		bug_type = "out-of-bounds";
-		break;
-	case KASAN_PAGE_REDZONE:
-	case KASAN_KMALLOC_REDZONE:
-		bug_type = "slab-out-of-bounds";
-		break;
-	case KASAN_GLOBAL_REDZONE:
-		bug_type = "global-out-of-bounds";
-		break;
-	case KASAN_STACK_LEFT:
-	case KASAN_STACK_MID:
-	case KASAN_STACK_RIGHT:
-	case KASAN_STACK_PARTIAL:
-		bug_type = "stack-out-of-bounds";
-		break;
-	case KASAN_FREE_PAGE:
-	case KASAN_KMALLOC_FREE:
-		bug_type = "use-after-free";
-		break;
-	case KASAN_USE_AFTER_SCOPE:
-		bug_type = "use-after-scope";
-		break;
-	case KASAN_ALLOCA_LEFT:
-	case KASAN_ALLOCA_RIGHT:
-		bug_type = "alloca-out-of-bounds";
-		break;
-	}
-
-	return bug_type;
+	return test_and_set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
 }
+EXPORT_SYMBOL_GPL(kasan_save_enable_multi_shot);
 
-static const char *get_wild_bug_type(struct kasan_access_info *info)
+void kasan_restore_multi_shot(bool enabled)
 {
-	const char *bug_type = "unknown-crash";
-
-	if ((unsigned long)info->access_addr < PAGE_SIZE)
-		bug_type = "null-ptr-deref";
-	else if ((unsigned long)info->access_addr < TASK_SIZE)
-		bug_type = "user-memory-access";
-	else
-		bug_type = "wild-memory-access";
-
-	return bug_type;
+	if (!enabled)
+		clear_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
 }
+EXPORT_SYMBOL_GPL(kasan_restore_multi_shot);
 
-static const char *get_bug_type(struct kasan_access_info *info)
+static int __init kasan_set_multi_shot(char *str)
 {
-	if (addr_has_shadow(info))
-		return get_shadow_bug_type(info);
-	return get_wild_bug_type(info);
+	set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
+	return 1;
 }
+__setup("kasan_multi_shot", kasan_set_multi_shot);
 
-static void print_error_description(struct kasan_access_info *info)
+static void print_error_description(struct kasan_access_info *info,
+					const char *bug_type)
 {
-	const char *bug_type = get_bug_type(info);
-
 	pr_err("BUG: KASAN: %s in %pS\n",
 		bug_type, (void *)info->ip);
 	pr_err("%s of size %zu at addr %px by task %s/%d\n",
@@ -143,25 +74,9 @@ static void print_error_description(struct kasan_access_info *info)
 		info->access_addr, current->comm, task_pid_nr(current));
 }
 
-static inline bool kernel_or_module_addr(const void *addr)
-{
-	if (addr >= (void *)_stext && addr < (void *)_end)
-		return true;
-	if (is_module_address((unsigned long)addr))
-		return true;
-	return false;
-}
-
-static inline bool init_task_stack_addr(const void *addr)
-{
-	return addr >= (void *)&init_thread_union.stack &&
-		(addr <= (void *)&init_thread_union.stack +
-			sizeof(init_thread_union.stack));
-}
-
 static DEFINE_SPINLOCK(report_lock);
 
-static void kasan_start_report(unsigned long *flags)
+static void start_report(unsigned long *flags)
 {
 	/*
 	 * Make sure we don't end up in loop.
@@ -171,7 +86,7 @@ static void kasan_start_report(unsigned long *flags)
 	pr_err("==================================================================\n");
 }
 
-static void kasan_end_report(unsigned long *flags)
+static void end_report(unsigned long *flags)
 {
 	pr_err("==================================================================\n");
 	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
@@ -249,6 +164,22 @@ static void describe_object(struct kmem_cache *cache, void *object,
 	describe_object_addr(cache, object, addr);
 }
 
+static inline bool kernel_or_module_addr(const void *addr)
+{
+	if (addr >= (void *)_stext && addr < (void *)_end)
+		return true;
+	if (is_module_address((unsigned long)addr))
+		return true;
+	return false;
+}
+
+static inline bool init_task_stack_addr(const void *addr)
+{
+	return addr >= (void *)&init_thread_union.stack &&
+		(addr <= (void *)&init_thread_union.stack +
+			sizeof(init_thread_union.stack));
+}
+
 static void print_address_description(void *addr)
 {
 	struct page *page = addr_to_page(addr);
@@ -326,29 +257,38 @@ static void print_shadow_for_address(const void *addr)
 	}
 }
 
+static bool report_enabled(void)
+{
+	if (current->kasan_depth)
+		return false;
+	if (test_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags))
+		return true;
+	return !test_and_set_bit(KASAN_BIT_REPORTED, &kasan_flags);
+}
+
 void kasan_report_invalid_free(void *object, unsigned long ip)
 {
 	unsigned long flags;
 
-	kasan_start_report(&flags);
+	start_report(&flags);
 	pr_err("BUG: KASAN: double-free or invalid-free in %pS\n", (void *)ip);
 	pr_err("\n");
 	print_address_description(object);
 	pr_err("\n");
 	print_shadow_for_address(object);
-	kasan_end_report(&flags);
+	end_report(&flags);
 }
 
 static void kasan_report_error(struct kasan_access_info *info)
 {
 	unsigned long flags;
 
-	kasan_start_report(&flags);
+	start_report(&flags);
 
-	print_error_description(info);
+	print_error_description(info, get_bug_type(info));
 	pr_err("\n");
 
-	if (!addr_has_shadow(info)) {
+	if (!addr_has_shadow(info->access_addr)) {
 		dump_stack();
 	} else {
 		print_address_description((void *)info->access_addr);
@@ -356,41 +296,7 @@ static void kasan_report_error(struct kasan_access_info *info)
 		print_shadow_for_address(info->first_bad_addr);
 	}
 
-	kasan_end_report(&flags);
-}
-
-static unsigned long kasan_flags;
-
-#define KASAN_BIT_REPORTED	0
-#define KASAN_BIT_MULTI_SHOT	1
-
-bool kasan_save_enable_multi_shot(void)
-{
-	return test_and_set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
-}
-EXPORT_SYMBOL_GPL(kasan_save_enable_multi_shot);
-
-void kasan_restore_multi_shot(bool enabled)
-{
-	if (!enabled)
-		clear_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
-}
-EXPORT_SYMBOL_GPL(kasan_restore_multi_shot);
-
-static int __init kasan_set_multi_shot(char *str)
-{
-	set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
-	return 1;
-}
-__setup("kasan_multi_shot", kasan_set_multi_shot);
-
-static inline bool kasan_report_enabled(void)
-{
-	if (current->kasan_depth)
-		return false;
-	if (test_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags))
-		return true;
-	return !test_and_set_bit(KASAN_BIT_REPORTED, &kasan_flags);
+	end_report(&flags);
 }
 
 void kasan_report(unsigned long addr, size_t size,
@@ -398,7 +304,7 @@ void kasan_report(unsigned long addr, size_t size,
 {
 	struct kasan_access_info info;
 
-	if (likely(!kasan_report_enabled()))
+	if (likely(!report_enabled()))
 		return;
 
 	disable_trace_on_warning();
@@ -411,41 +317,3 @@ void kasan_report(unsigned long addr, size_t size,
 
 	kasan_report_error(&info);
 }
-
-
-#define DEFINE_ASAN_REPORT_LOAD(size)                     \
-void __asan_report_load##size##_noabort(unsigned long addr) \
-{                                                         \
-	kasan_report(addr, size, false, _RET_IP_);	  \
-}                                                         \
-EXPORT_SYMBOL(__asan_report_load##size##_noabort)
-
-#define DEFINE_ASAN_REPORT_STORE(size)                     \
-void __asan_report_store##size##_noabort(unsigned long addr) \
-{                                                          \
-	kasan_report(addr, size, true, _RET_IP_);	   \
-}                                                          \
-EXPORT_SYMBOL(__asan_report_store##size##_noabort)
-
-DEFINE_ASAN_REPORT_LOAD(1);
-DEFINE_ASAN_REPORT_LOAD(2);
-DEFINE_ASAN_REPORT_LOAD(4);
-DEFINE_ASAN_REPORT_LOAD(8);
-DEFINE_ASAN_REPORT_LOAD(16);
-DEFINE_ASAN_REPORT_STORE(1);
-DEFINE_ASAN_REPORT_STORE(2);
-DEFINE_ASAN_REPORT_STORE(4);
-DEFINE_ASAN_REPORT_STORE(8);
-DEFINE_ASAN_REPORT_STORE(16);
-
-void __asan_report_load_n_noabort(unsigned long addr, size_t size)
-{
-	kasan_report(addr, size, false, _RET_IP_);
-}
-EXPORT_SYMBOL(__asan_report_load_n_noabort);
-
-void __asan_report_store_n_noabort(unsigned long addr, size_t size)
-{
-	kasan_report(addr, size, true, _RET_IP_);
-}
-EXPORT_SYMBOL(__asan_report_store_n_noabort);
diff --git a/mm/kasan/tags_report.c b/mm/kasan/tags_report.c
new file mode 100644
index 000000000000..8af15e87d3bc
--- /dev/null
+++ b/mm/kasan/tags_report.c
@@ -0,0 +1,39 @@
+/*
+ * This file contains tag-based KASAN specific error reporting code.
+ *
+ * Copyright (c) 2014 Samsung Electronics Co., Ltd.
+ * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
+ *
+ * Some code borrowed from https://github.com/xairy/kasan-prototype by
+ *        Andrey Konovalov <andreyknvl@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/bitops.h>
+#include <linux/ftrace.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/stackdepot.h>
+#include <linux/stacktrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/kasan.h>
+#include <linux/module.h>
+
+#include <asm/sections.h>
+
+#include "kasan.h"
+#include "../slab.h"
+
+const char *get_bug_type(struct kasan_access_info *info)
+{
+	return "invalid-access";
+}
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 18/25] mm: move obj_to_index to include/linux/slab_def.h ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 18/25] mm: move obj_to_index to include/linux/slab_def.h
Date: Tue, 27 Nov 2018 16:55:36 +0000
Message-ID: <b68796c554fba66d5285274ea6356e642e18a9e5.1543337629.git.andreyknvl () google ! com>
--------------------
While with SLUB we can actually preassign tags for caches with contructors
and store them in pointers in the freelist, SLAB doesn't allow that since
the freelist is stored as an array of indexes, so there are no pointers to
store the tags.

Instead we compute the tag twice, once when a slab is created before
calling the constructor and then again each time when an object is
allocated with kmalloc. Tag is computed simply by taking the lowest byte of
the index that corresponds to the object. However in kasan_kmalloc we only
have access to the objects pointer, so we need a way to find out which
index this object corresponds to.

This patch moves obj_to_index from slab.c to include/linux/slab_def.h to
be reused by KASAN.

Acked-by: Christoph Lameter <cl@linux.com>
Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 include/linux/slab_def.h | 13 +++++++++++++
 mm/slab.c                | 13 -------------
 2 files changed, 13 insertions(+), 13 deletions(-)

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 3485c58cfd1c..9a5eafb7145b 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -104,4 +104,17 @@ static inline void *nearest_obj(struct kmem_cache *cache, struct page *page,
 		return object;
 }
 
+/*
+ * We want to avoid an expensive divide : (offset / cache->size)
+ *   Using the fact that size is a constant for a particular cache,
+ *   we can replace (offset / cache->size) by
+ *   reciprocal_divide(offset, cache->reciprocal_buffer_size)
+ */
+static inline unsigned int obj_to_index(const struct kmem_cache *cache,
+					const struct page *page, void *obj)
+{
+	u32 offset = (obj - page->s_mem);
+	return reciprocal_divide(offset, cache->reciprocal_buffer_size);
+}
+
 #endif	/* _LINUX_SLAB_DEF_H */
diff --git a/mm/slab.c b/mm/slab.c
index 27859fb39889..d2f827316dfc 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -406,19 +406,6 @@ static inline void *index_to_obj(struct kmem_cache *cache, struct page *page,
 	return page->s_mem + cache->size * idx;
 }
 
-/*
- * We want to avoid an expensive divide : (offset / cache->size)
- *   Using the fact that size is a constant for a particular cache,
- *   we can replace (offset / cache->size) by
- *   reciprocal_divide(offset, cache->reciprocal_buffer_size)
- */
-static inline unsigned int obj_to_index(const struct kmem_cache *cache,
-					const struct page *page, void *obj)
-{
-	u32 offset = (obj - page->s_mem);
-	return reciprocal_divide(offset, cache->reciprocal_buffer_size);
-}
-
 #define BOOT_CPUCACHE_ENTRIES	1
 /* internal cache of cache description objs */
 static struct kmem_cache kmem_cache_boot = {
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 19/25] kasan: add hooks implementation for tag-based mode ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 19/25] kasan: add hooks implementation for tag-based mode
Date: Tue, 27 Nov 2018 16:55:37 +0000
Message-ID: <b10d44bace6a7e9279b9b5a5b4c2a9c4c58cbf4f.1543337629.git.andreyknvl () google ! com>
--------------------
This commit adds tag-based KASAN specific hooks implementation and
adjusts common generic and tag-based KASAN ones.

1. When a new slab cache is created, tag-based KASAN rounds up the size of
   the objects in this cache to KASAN_SHADOW_SCALE_SIZE (== 16).

2. On each kmalloc tag-based KASAN generates a random tag, sets the shadow
   memory, that corresponds to this object to this tag, and embeds this
   tag value into the top byte of the returned pointer.

3. On each kfree tag-based KASAN poisons the shadow memory with a random
   tag to allow detection of use-after-free bugs.

The rest of the logic of the hook implementation is very much similar to
the one provided by generic KASAN. Tag-based KASAN saves allocation and
free stack metadata to the slab object the same way generic KASAN does.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/common.c | 116 ++++++++++++++++++++++++++++++++++++++--------
 mm/kasan/kasan.h  |   8 ++++
 mm/kasan/tags.c   |  48 +++++++++++++++++++
 3 files changed, 153 insertions(+), 19 deletions(-)

diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 7134e75447ff..27f0cae336c9 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -140,6 +140,13 @@ void kasan_poison_shadow(const void *address, size_t size, u8 value)
 {
 	void *shadow_start, *shadow_end;
 
+	/*
+	 * Perform shadow offset calculation based on untagged address, as
+	 * some of the callers (e.g. kasan_poison_object_data) pass tagged
+	 * addresses to this function.
+	 */
+	address = reset_tag(address);
+
 	shadow_start = kasan_mem_to_shadow(address);
 	shadow_end = kasan_mem_to_shadow(address + size);
 
@@ -148,11 +155,24 @@ void kasan_poison_shadow(const void *address, size_t size, u8 value)
 
 void kasan_unpoison_shadow(const void *address, size_t size)
 {
-	kasan_poison_shadow(address, size, 0);
+	u8 tag = get_tag(address);
+
+	/*
+	 * Perform shadow offset calculation based on untagged address, as
+	 * some of the callers (e.g. kasan_unpoison_object_data) pass tagged
+	 * addresses to this function.
+	 */
+	address = reset_tag(address);
+
+	kasan_poison_shadow(address, size, tag);
 
 	if (size & KASAN_SHADOW_MASK) {
 		u8 *shadow = (u8 *)kasan_mem_to_shadow(address + size);
-		*shadow = size & KASAN_SHADOW_MASK;
+
+		if (IS_ENABLED(CONFIG_KASAN_SW_TAGS))
+			*shadow = tag;
+		else
+			*shadow = size & KASAN_SHADOW_MASK;
 	}
 }
 
@@ -200,8 +220,9 @@ void kasan_unpoison_stack_above_sp_to(const void *watermark)
 
 void kasan_alloc_pages(struct page *page, unsigned int order)
 {
-	if (likely(!PageHighMem(page)))
-		kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
+	if (unlikely(PageHighMem(page)))
+		return;
+	kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
 }
 
 void kasan_free_pages(struct page *page, unsigned int order)
@@ -218,6 +239,9 @@ void kasan_free_pages(struct page *page, unsigned int order)
  */
 static inline unsigned int optimal_redzone(unsigned int object_size)
 {
+	if (IS_ENABLED(CONFIG_KASAN_SW_TAGS))
+		return 0;
+
 	return
 		object_size <= 64        - 16   ? 16 :
 		object_size <= 128       - 32   ? 32 :
@@ -232,6 +256,7 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 			slab_flags_t *flags)
 {
 	unsigned int orig_size = *size;
+	unsigned int redzone_size;
 	int redzone_adjust;
 
 	/* Add alloc meta. */
@@ -239,20 +264,20 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 	*size += sizeof(struct kasan_alloc_meta);
 
 	/* Add free meta. */
-	if (cache->flags & SLAB_TYPESAFE_BY_RCU || cache->ctor ||
-	    cache->object_size < sizeof(struct kasan_free_meta)) {
+	if (IS_ENABLED(CONFIG_KASAN_GENERIC) &&
+	    (cache->flags & SLAB_TYPESAFE_BY_RCU || cache->ctor ||
+	     cache->object_size < sizeof(struct kasan_free_meta))) {
 		cache->kasan_info.free_meta_offset = *size;
 		*size += sizeof(struct kasan_free_meta);
 	}
-	redzone_adjust = optimal_redzone(cache->object_size) -
-		(*size - cache->object_size);
 
+	redzone_size = optimal_redzone(cache->object_size);
+	redzone_adjust = redzone_size -	(*size - cache->object_size);
 	if (redzone_adjust > 0)
 		*size += redzone_adjust;
 
 	*size = min_t(unsigned int, KMALLOC_MAX_SIZE,
-			max(*size, cache->object_size +
-					optimal_redzone(cache->object_size)));
+			max(*size, cache->object_size + redzone_size));
 
 	/*
 	 * If the metadata doesn't fit, don't enable KASAN at all.
@@ -265,6 +290,8 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 		return;
 	}
 
+	cache->align = round_up(cache->align, KASAN_SHADOW_SCALE_SIZE);
+
 	*flags |= SLAB_KASAN;
 }
 
@@ -309,6 +336,32 @@ void kasan_poison_object_data(struct kmem_cache *cache, void *object)
 			KASAN_KMALLOC_REDZONE);
 }
 
+/*
+ * Since it's desirable to only call object contructors once during slab
+ * allocation, we preassign tags to all such objects. Also preassign tags for
+ * SLAB_TYPESAFE_BY_RCU slabs to avoid use-after-free reports.
+ * For SLAB allocator we can't preassign tags randomly since the freelist is
+ * stored as an array of indexes instead of a linked list. Assign tags based
+ * on objects indexes, so that objects that are next to each other get
+ * different tags.
+ * After a tag is assigned, the object always gets allocated with the same tag.
+ * The reason is that we can't change tags for objects with constructors on
+ * reallocation (even for non-SLAB_TYPESAFE_BY_RCU), because the constructor
+ * code can save the pointer to the object somewhere (e.g. in the object
+ * itself). Then if we retag it, the old saved pointer will become invalid.
+ */
+static u8 assign_tag(struct kmem_cache *cache, const void *object, bool new)
+{
+	if (!cache->ctor && !(cache->flags & SLAB_TYPESAFE_BY_RCU))
+		return new ? KASAN_TAG_KERNEL : random_tag();
+
+#ifdef CONFIG_SLAB
+	return (u8)obj_to_index(cache, virt_to_page(object), (void *)object);
+#else
+	return new ? random_tag() : get_tag(object);
+#endif
+}
+
 void *kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
 {
 	struct kasan_alloc_meta *alloc_info;
@@ -319,6 +372,9 @@ void *kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
 	alloc_info = get_alloc_info(cache, object);
 	__memset(alloc_info, 0, sizeof(*alloc_info));
 
+	if (IS_ENABLED(CONFIG_KASAN_SW_TAGS))
+		object = set_tag(object, assign_tag(cache, object, true));
+
 	return (void *)object;
 }
 
@@ -327,15 +383,30 @@ void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t flags)
 	return kasan_kmalloc(cache, object, cache->object_size, flags);
 }
 
+static inline bool shadow_invalid(u8 tag, s8 shadow_byte)
+{
+	if (IS_ENABLED(CONFIG_KASAN_GENERIC))
+		return shadow_byte < 0 ||
+			shadow_byte >= KASAN_SHADOW_SCALE_SIZE;
+	else
+		return tag != (u8)shadow_byte;
+}
+
 static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
 			      unsigned long ip, bool quarantine)
 {
 	s8 shadow_byte;
+	u8 tag;
+	void *tagged_object;
 	unsigned long rounded_up_size;
 
+	tag = get_tag(object);
+	tagged_object = object;
+	object = reset_tag(object);
+
 	if (unlikely(nearest_obj(cache, virt_to_head_page(object), object) !=
 	    object)) {
-		kasan_report_invalid_free(object, ip);
+		kasan_report_invalid_free(tagged_object, ip);
 		return true;
 	}
 
@@ -344,20 +415,22 @@ static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
 		return false;
 
 	shadow_byte = READ_ONCE(*(s8 *)kasan_mem_to_shadow(object));
-	if (shadow_byte < 0 || shadow_byte >= KASAN_SHADOW_SCALE_SIZE) {
-		kasan_report_invalid_free(object, ip);
+	if (shadow_invalid(tag, shadow_byte)) {
+		kasan_report_invalid_free(tagged_object, ip);
 		return true;
 	}
 
 	rounded_up_size = round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE);
 	kasan_poison_shadow(object, rounded_up_size, KASAN_KMALLOC_FREE);
 
-	if (!quarantine || unlikely(!(cache->flags & SLAB_KASAN)))
+	if ((IS_ENABLED(CONFIG_KASAN_GENERIC) && !quarantine) ||
+			unlikely(!(cache->flags & SLAB_KASAN)))
 		return false;
 
 	set_track(&get_alloc_info(cache, object)->free_track, GFP_NOWAIT);
 	quarantine_put(get_free_info(cache, object), cache);
-	return true;
+
+	return IS_ENABLED(CONFIG_KASAN_GENERIC);
 }
 
 bool kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
@@ -370,6 +443,7 @@ void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 {
 	unsigned long redzone_start;
 	unsigned long redzone_end;
+	u8 tag;
 
 	if (gfpflags_allow_blocking(flags))
 		quarantine_reduce();
@@ -382,14 +456,18 @@ void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 	redzone_end = round_up((unsigned long)object + cache->object_size,
 				KASAN_SHADOW_SCALE_SIZE);
 
-	kasan_unpoison_shadow(object, size);
+	if (IS_ENABLED(CONFIG_KASAN_SW_TAGS))
+		tag = assign_tag(cache, object, false);
+
+	/* Tag is ignored in set_tag without CONFIG_KASAN_SW_TAGS */
+	kasan_unpoison_shadow(set_tag(object, tag), size);
 	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
 		KASAN_KMALLOC_REDZONE);
 
 	if (cache->flags & SLAB_KASAN)
 		set_track(&get_alloc_info(cache, object)->alloc_track, flags);
 
-	return (void *)object;
+	return set_tag(object, tag);
 }
 EXPORT_SYMBOL(kasan_kmalloc);
 
@@ -439,7 +517,7 @@ void kasan_poison_kfree(void *ptr, unsigned long ip)
 	page = virt_to_head_page(ptr);
 
 	if (unlikely(!PageSlab(page))) {
-		if (ptr != page_address(page)) {
+		if (reset_tag(ptr) != page_address(page)) {
 			kasan_report_invalid_free(ptr, ip);
 			return;
 		}
@@ -452,7 +530,7 @@ void kasan_poison_kfree(void *ptr, unsigned long ip)
 
 void kasan_kfree_large(void *ptr, unsigned long ip)
 {
-	if (ptr != page_address(virt_to_head_page(ptr)))
+	if (reset_tag(ptr) != page_address(virt_to_head_page(ptr)))
 		kasan_report_invalid_free(ptr, ip);
 	/* The object will be poisoned by page_alloc. */
 }
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index 82a23b23ff93..ea51b2d898ec 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -12,10 +12,18 @@
 #define KASAN_TAG_INVALID	0xFE /* inaccessible memory tag */
 #define KASAN_TAG_MAX		0xFD /* maximum value for random tags */
 
+#ifdef CONFIG_KASAN_GENERIC
 #define KASAN_FREE_PAGE         0xFF  /* page was freed */
 #define KASAN_PAGE_REDZONE      0xFE  /* redzone for kmalloc_large allocations */
 #define KASAN_KMALLOC_REDZONE   0xFC  /* redzone inside slub object */
 #define KASAN_KMALLOC_FREE      0xFB  /* object was freed (kmem_cache_free/kfree) */
+#else
+#define KASAN_FREE_PAGE         KASAN_TAG_INVALID
+#define KASAN_PAGE_REDZONE      KASAN_TAG_INVALID
+#define KASAN_KMALLOC_REDZONE   KASAN_TAG_INVALID
+#define KASAN_KMALLOC_FREE      KASAN_TAG_INVALID
+#endif
+
 #define KASAN_GLOBAL_REDZONE    0xFA  /* redzone for global variable */
 
 /*
diff --git a/mm/kasan/tags.c b/mm/kasan/tags.c
index 1c4e7ce2e6fe..1d1b79350e28 100644
--- a/mm/kasan/tags.c
+++ b/mm/kasan/tags.c
@@ -78,15 +78,60 @@ void *kasan_reset_tag(const void *addr)
 void check_memory_region(unsigned long addr, size_t size, bool write,
 				unsigned long ret_ip)
 {
+	u8 tag;
+	u8 *shadow_first, *shadow_last, *shadow;
+	void *untagged_addr;
+
+	if (unlikely(size == 0))
+		return;
+
+	tag = get_tag((const void *)addr);
+
+	/*
+	 * Ignore accesses for pointers tagged with 0xff (native kernel
+	 * pointer tag) to suppress false positives caused by kmap.
+	 *
+	 * Some kernel code was written to account for archs that don't keep
+	 * high memory mapped all the time, but rather map and unmap particular
+	 * pages when needed. Instead of storing a pointer to the kernel memory,
+	 * this code saves the address of the page structure and offset within
+	 * that page for later use. Those pages are then mapped and unmapped
+	 * with kmap/kunmap when necessary and virt_to_page is used to get the
+	 * virtual address of the page. For arm64 (that keeps the high memory
+	 * mapped all the time), kmap is turned into a page_address call.
+
+	 * The issue is that with use of the page_address + virt_to_page
+	 * sequence the top byte value of the original pointer gets lost (gets
+	 * set to KASAN_TAG_KERNEL (0xFF)).
+	 */
+	if (tag == KASAN_TAG_KERNEL)
+		return;
+
+	untagged_addr = reset_tag((const void *)addr);
+	if (unlikely(untagged_addr <
+			kasan_shadow_to_mem((void *)KASAN_SHADOW_START))) {
+		kasan_report(addr, size, write, ret_ip);
+		return;
+	}
+	shadow_first = kasan_mem_to_shadow(untagged_addr);
+	shadow_last = kasan_mem_to_shadow(untagged_addr + size - 1);
+	for (shadow = shadow_first; shadow <= shadow_last; shadow++) {
+		if (*shadow != tag) {
+			kasan_report(addr, size, write, ret_ip);
+			return;
+		}
+	}
 }
 
 #define DEFINE_HWASAN_LOAD_STORE(size)					\
 	void __hwasan_load##size##_noabort(unsigned long addr)		\
 	{								\
+		check_memory_region(addr, size, false, _RET_IP_);	\
 	}								\
 	EXPORT_SYMBOL(__hwasan_load##size##_noabort);			\
 	void __hwasan_store##size##_noabort(unsigned long addr)		\
 	{								\
+		check_memory_region(addr, size, true, _RET_IP_);	\
 	}								\
 	EXPORT_SYMBOL(__hwasan_store##size##_noabort)
 
@@ -98,15 +143,18 @@ DEFINE_HWASAN_LOAD_STORE(16);
 
 void __hwasan_loadN_noabort(unsigned long addr, unsigned long size)
 {
+	check_memory_region(addr, size, false, _RET_IP_);
 }
 EXPORT_SYMBOL(__hwasan_loadN_noabort);
 
 void __hwasan_storeN_noabort(unsigned long addr, unsigned long size)
 {
+	check_memory_region(addr, size, true, _RET_IP_);
 }
 EXPORT_SYMBOL(__hwasan_storeN_noabort);
 
 void __hwasan_tag_memory(unsigned long addr, u8 tag, unsigned long size)
 {
+	kasan_poison_shadow((void *)addr, size, tag);
 }
 EXPORT_SYMBOL(__hwasan_tag_memory);
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Tue, 27 Nov 2018 16:55:38 +0000
Message-ID: <e825441eda1dbbbb7f583f826a66c94e6f88316a.1543337629.git.andreyknvl () google ! com>
--------------------
Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
memory into the generated code, instead of inserting a callback) generates
a brk instruction when a tag mismatch is detected.

This commit adds a tag-based KASAN specific brk handler, that decodes the
immediate value passed to the brk instructions (to extract information
about the memory access that triggered the mismatch), reads the register
values (x0 contains the guilty address) and reports the bug.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/brk-imm.h |  2 +
 arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
 include/linux/kasan.h            |  3 ++
 3 files changed, 71 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
index ed693c5bcec0..2945fe6cd863 100644
--- a/arch/arm64/include/asm/brk-imm.h
+++ b/arch/arm64/include/asm/brk-imm.h
@@ -16,10 +16,12 @@
  * 0x400: for dynamic BRK instruction
  * 0x401: for compile time BRK instruction
  * 0x800: kernel-mode BUG() and WARN() traps
+ * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
  */
 #define FAULT_BRK_IMM			0x100
 #define KGDB_DYN_DBG_BRK_IMM		0x400
 #define KGDB_COMPILED_DBG_BRK_IMM	0x401
 #define BUG_BRK_IMM			0x800
+#define KASAN_BRK_IMM			0x900
 
 #endif
diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 5f4d9acb32f5..04bdc53716ef 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -35,6 +35,7 @@
 #include <linux/sizes.h>
 #include <linux/syscalls.h>
 #include <linux/mm_types.h>
+#include <linux/kasan.h>
 
 #include <asm/atomic.h>
 #include <asm/bug.h>
@@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
 	}
 }
 
-void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
 {
 	regs->pc += size;
+}
 
+void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+{
+	__arm64_skip_faulting_instruction(regs, size);
 	/*
 	 * If we were single stepping, we want to get the step exception after
 	 * we return from the trap.
@@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
 	}
 
 	/* If thread survives, skip over the BUG instruction and continue: */
-	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 	return DBG_HOOK_HANDLED;
 }
 
@@ -969,6 +974,58 @@ static struct break_hook bug_break_hook = {
 	.fn = bug_handler,
 };
 
+#ifdef CONFIG_KASAN_SW_TAGS
+
+#define KASAN_ESR_RECOVER	0x20
+#define KASAN_ESR_WRITE	0x10
+#define KASAN_ESR_SIZE_MASK	0x0f
+#define KASAN_ESR_SIZE(esr)	(1 << ((esr) & KASAN_ESR_SIZE_MASK))
+
+static int kasan_handler(struct pt_regs *regs, unsigned int esr)
+{
+	bool recover = esr & KASAN_ESR_RECOVER;
+	bool write = esr & KASAN_ESR_WRITE;
+	size_t size = KASAN_ESR_SIZE(esr);
+	u64 addr = regs->regs[0];
+	u64 pc = regs->pc;
+
+	if (user_mode(regs))
+		return DBG_HOOK_ERROR;
+
+	kasan_report(addr, size, write, pc);
+
+	/*
+	 * The instrumentation allows to control whether we can proceed after
+	 * a crash was detected. This is done by passing the -recover flag to
+	 * the compiler. Disabling recovery allows to generate more compact
+	 * code.
+	 *
+	 * Unfortunately disabling recovery doesn't work for the kernel right
+	 * now. KASAN reporting is disabled in some contexts (for example when
+	 * the allocator accesses slab object metadata; this is controlled by
+	 * current->kasan_depth). All these accesses are detected by the tool,
+	 * even though the reports for them are not printed.
+	 *
+	 * This is something that might be fixed at some point in the future.
+	 */
+	if (!recover)
+		die("Oops - KASAN", regs, 0);
+
+	/* If thread survives, skip over the brk instruction and continue: */
+	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	return DBG_HOOK_HANDLED;
+}
+
+#define KASAN_ESR_VAL (0xf2000000 | KASAN_BRK_IMM)
+#define KASAN_ESR_MASK 0xffffff00
+
+static struct break_hook kasan_break_hook = {
+	.esr_val = KASAN_ESR_VAL,
+	.esr_mask = KASAN_ESR_MASK,
+	.fn = kasan_handler,
+};
+#endif
+
 /*
  * Initial handler for AArch64 BRK exceptions
  * This handler only used until debug_traps_init().
@@ -976,6 +1033,10 @@ static struct break_hook bug_break_hook = {
 int __init early_brk64(unsigned long addr, unsigned int esr,
 		struct pt_regs *regs)
 {
+#ifdef CONFIG_KASAN_SW_TAGS
+	if ((esr & KASAN_ESR_MASK) == KASAN_ESR_VAL)
+		return kasan_handler(regs, esr) != DBG_HOOK_HANDLED;
+#endif
 	return bug_handler(regs, esr) != DBG_HOOK_HANDLED;
 }
 
@@ -983,4 +1044,7 @@ int __init early_brk64(unsigned long addr, unsigned int esr,
 void __init trap_init(void)
 {
 	register_break_hook(&bug_break_hook);
+#ifdef CONFIG_KASAN_SW_TAGS
+	register_break_hook(&kasan_break_hook);
+#endif
 }
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index a477ce2abdc9..8da7b7a4397a 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -173,6 +173,9 @@ void kasan_init_tags(void);
 
 void *kasan_reset_tag(const void *addr);
 
+void kasan_report(unsigned long addr, size_t size,
+		bool is_write, unsigned long ip);
+
 #else /* CONFIG_KASAN_SW_TAGS */
 
 static inline void kasan_init_tags(void) { }
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-mm
Subject: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Tue, 27 Nov 2018 16:55:38 +0000
Message-ID: <e825441eda1dbbbb7f583f826a66c94e6f88316a.1543337629.git.andreyknvl () google ! com>
--------------------
Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
memory into the generated code, instead of inserting a callback) generates
a brk instruction when a tag mismatch is detected.

This commit adds a tag-based KASAN specific brk handler, that decodes the
immediate value passed to the brk instructions (to extract information
about the memory access that triggered the mismatch), reads the register
values (x0 contains the guilty address) and reports the bug.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/brk-imm.h |  2 +
 arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
 include/linux/kasan.h            |  3 ++
 3 files changed, 71 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
index ed693c5bcec0..2945fe6cd863 100644
--- a/arch/arm64/include/asm/brk-imm.h
+++ b/arch/arm64/include/asm/brk-imm.h
@@ -16,10 +16,12 @@
  * 0x400: for dynamic BRK instruction
  * 0x401: for compile time BRK instruction
  * 0x800: kernel-mode BUG() and WARN() traps
+ * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
  */
 #define FAULT_BRK_IMM			0x100
 #define KGDB_DYN_DBG_BRK_IMM		0x400
 #define KGDB_COMPILED_DBG_BRK_IMM	0x401
 #define BUG_BRK_IMM			0x800
+#define KASAN_BRK_IMM			0x900
 
 #endif
diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 5f4d9acb32f5..04bdc53716ef 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -35,6 +35,7 @@
 #include <linux/sizes.h>
 #include <linux/syscalls.h>
 #include <linux/mm_types.h>
+#include <linux/kasan.h>
 
 #include <asm/atomic.h>
 #include <asm/bug.h>
@@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
 	}
 }
 
-void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
 {
 	regs->pc += size;
+}
 
+void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+{
+	__arm64_skip_faulting_instruction(regs, size);
 	/*
 	 * If we were single stepping, we want to get the step exception after
 	 * we return from the trap.
@@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
 	}
 
 	/* If thread survives, skip over the BUG instruction and continue: */
-	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 	return DBG_HOOK_HANDLED;
 }
 
@@ -969,6 +974,58 @@ static struct break_hook bug_break_hook = {
 	.fn = bug_handler,
 };
 
+#ifdef CONFIG_KASAN_SW_TAGS
+
+#define KASAN_ESR_RECOVER	0x20
+#define KASAN_ESR_WRITE	0x10
+#define KASAN_ESR_SIZE_MASK	0x0f
+#define KASAN_ESR_SIZE(esr)	(1 << ((esr) & KASAN_ESR_SIZE_MASK))
+
+static int kasan_handler(struct pt_regs *regs, unsigned int esr)
+{
+	bool recover = esr & KASAN_ESR_RECOVER;
+	bool write = esr & KASAN_ESR_WRITE;
+	size_t size = KASAN_ESR_SIZE(esr);
+	u64 addr = regs->regs[0];
+	u64 pc = regs->pc;
+
+	if (user_mode(regs))
+		return DBG_HOOK_ERROR;
+
+	kasan_report(addr, size, write, pc);
+
+	/*
+	 * The instrumentation allows to control whether we can proceed after
+	 * a crash was detected. This is done by passing the -recover flag to
+	 * the compiler. Disabling recovery allows to generate more compact
+	 * code.
+	 *
+	 * Unfortunately disabling recovery doesn't work for the kernel right
+	 * now. KASAN reporting is disabled in some contexts (for example when
+	 * the allocator accesses slab object metadata; this is controlled by
+	 * current->kasan_depth). All these accesses are detected by the tool,
+	 * even though the reports for them are not printed.
+	 *
+	 * This is something that might be fixed at some point in the future.
+	 */
+	if (!recover)
+		die("Oops - KASAN", regs, 0);
+
+	/* If thread survives, skip over the brk instruction and continue: */
+	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	return DBG_HOOK_HANDLED;
+}
+
+#define KASAN_ESR_VAL (0xf2000000 | KASAN_BRK_IMM)
+#define KASAN_ESR_MASK 0xffffff00
+
+static struct break_hook kasan_break_hook = {
+	.esr_val = KASAN_ESR_VAL,
+	.esr_mask = KASAN_ESR_MASK,
+	.fn = kasan_handler,
+};
+#endif
+
 /*
  * Initial handler for AArch64 BRK exceptions
  * This handler only used until debug_traps_init().
@@ -976,6 +1033,10 @@ static struct break_hook bug_break_hook = {
 int __init early_brk64(unsigned long addr, unsigned int esr,
 		struct pt_regs *regs)
 {
+#ifdef CONFIG_KASAN_SW_TAGS
+	if ((esr & KASAN_ESR_MASK) == KASAN_ESR_VAL)
+		return kasan_handler(regs, esr) != DBG_HOOK_HANDLED;
+#endif
 	return bug_handler(regs, esr) != DBG_HOOK_HANDLED;
 }
 
@@ -983,4 +1044,7 @@ int __init early_brk64(unsigned long addr, unsigned int esr,
 void __init trap_init(void)
 {
 	register_break_hook(&bug_break_hook);
+#ifdef CONFIG_KASAN_SW_TAGS
+	register_break_hook(&kasan_break_hook);
+#endif
 }
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index a477ce2abdc9..8da7b7a4397a 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -173,6 +173,9 @@ void kasan_init_tags(void);
 
 void *kasan_reset_tag(const void *addr);
 
+void kasan_report(unsigned long addr, size_t size,
+		bool is_write, unsigned long ip);
+
 #else /* CONFIG_KASAN_SW_TAGS */
 
 static inline void kasan_init_tags(void) { }
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-doc
Subject: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Tue, 27 Nov 2018 16:55:38 +0000
Message-ID: <e825441eda1dbbbb7f583f826a66c94e6f88316a.1543337629.git.andreyknvl () google ! com>
--------------------
Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
memory into the generated code, instead of inserting a callback) generates
a brk instruction when a tag mismatch is detected.

This commit adds a tag-based KASAN specific brk handler, that decodes the
immediate value passed to the brk instructions (to extract information
about the memory access that triggered the mismatch), reads the register
values (x0 contains the guilty address) and reports the bug.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/brk-imm.h |  2 +
 arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
 include/linux/kasan.h            |  3 ++
 3 files changed, 71 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
index ed693c5bcec0..2945fe6cd863 100644
--- a/arch/arm64/include/asm/brk-imm.h
+++ b/arch/arm64/include/asm/brk-imm.h
@@ -16,10 +16,12 @@
  * 0x400: for dynamic BRK instruction
  * 0x401: for compile time BRK instruction
  * 0x800: kernel-mode BUG() and WARN() traps
+ * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
  */
 #define FAULT_BRK_IMM			0x100
 #define KGDB_DYN_DBG_BRK_IMM		0x400
 #define KGDB_COMPILED_DBG_BRK_IMM	0x401
 #define BUG_BRK_IMM			0x800
+#define KASAN_BRK_IMM			0x900
 
 #endif
diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 5f4d9acb32f5..04bdc53716ef 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -35,6 +35,7 @@
 #include <linux/sizes.h>
 #include <linux/syscalls.h>
 #include <linux/mm_types.h>
+#include <linux/kasan.h>
 
 #include <asm/atomic.h>
 #include <asm/bug.h>
@@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
 	}
 }
 
-void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
 {
 	regs->pc += size;
+}
 
+void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+{
+	__arm64_skip_faulting_instruction(regs, size);
 	/*
 	 * If we were single stepping, we want to get the step exception after
 	 * we return from the trap.
@@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
 	}
 
 	/* If thread survives, skip over the BUG instruction and continue: */
-	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 	return DBG_HOOK_HANDLED;
 }
 
@@ -969,6 +974,58 @@ static struct break_hook bug_break_hook = {
 	.fn = bug_handler,
 };
 
+#ifdef CONFIG_KASAN_SW_TAGS
+
+#define KASAN_ESR_RECOVER	0x20
+#define KASAN_ESR_WRITE	0x10
+#define KASAN_ESR_SIZE_MASK	0x0f
+#define KASAN_ESR_SIZE(esr)	(1 << ((esr) & KASAN_ESR_SIZE_MASK))
+
+static int kasan_handler(struct pt_regs *regs, unsigned int esr)
+{
+	bool recover = esr & KASAN_ESR_RECOVER;
+	bool write = esr & KASAN_ESR_WRITE;
+	size_t size = KASAN_ESR_SIZE(esr);
+	u64 addr = regs->regs[0];
+	u64 pc = regs->pc;
+
+	if (user_mode(regs))
+		return DBG_HOOK_ERROR;
+
+	kasan_report(addr, size, write, pc);
+
+	/*
+	 * The instrumentation allows to control whether we can proceed after
+	 * a crash was detected. This is done by passing the -recover flag to
+	 * the compiler. Disabling recovery allows to generate more compact
+	 * code.
+	 *
+	 * Unfortunately disabling recovery doesn't work for the kernel right
+	 * now. KASAN reporting is disabled in some contexts (for example when
+	 * the allocator accesses slab object metadata; this is controlled by
+	 * current->kasan_depth). All these accesses are detected by the tool,
+	 * even though the reports for them are not printed.
+	 *
+	 * This is something that might be fixed at some point in the future.
+	 */
+	if (!recover)
+		die("Oops - KASAN", regs, 0);
+
+	/* If thread survives, skip over the brk instruction and continue: */
+	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	return DBG_HOOK_HANDLED;
+}
+
+#define KASAN_ESR_VAL (0xf2000000 | KASAN_BRK_IMM)
+#define KASAN_ESR_MASK 0xffffff00
+
+static struct break_hook kasan_break_hook = {
+	.esr_val = KASAN_ESR_VAL,
+	.esr_mask = KASAN_ESR_MASK,
+	.fn = kasan_handler,
+};
+#endif
+
 /*
  * Initial handler for AArch64 BRK exceptions
  * This handler only used until debug_traps_init().
@@ -976,6 +1033,10 @@ static struct break_hook bug_break_hook = {
 int __init early_brk64(unsigned long addr, unsigned int esr,
 		struct pt_regs *regs)
 {
+#ifdef CONFIG_KASAN_SW_TAGS
+	if ((esr & KASAN_ESR_MASK) == KASAN_ESR_VAL)
+		return kasan_handler(regs, esr) != DBG_HOOK_HANDLED;
+#endif
 	return bug_handler(regs, esr) != DBG_HOOK_HANDLED;
 }
 
@@ -983,4 +1044,7 @@ int __init early_brk64(unsigned long addr, unsigned int esr,
 void __init trap_init(void)
 {
 	register_break_hook(&bug_break_hook);
+#ifdef CONFIG_KASAN_SW_TAGS
+	register_break_hook(&kasan_break_hook);
+#endif
 }
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index a477ce2abdc9..8da7b7a4397a 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -173,6 +173,9 @@ void kasan_init_tags(void);
 
 void *kasan_reset_tag(const void *addr);
 
+void kasan_report(unsigned long addr, size_t size,
+		bool is_write, unsigned long ip);
+
 #else /* CONFIG_KASAN_SW_TAGS */
 
 static inline void kasan_init_tags(void) { }
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kernel
Subject: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Tue, 27 Nov 2018 16:55:38 +0000
Message-ID: <e825441eda1dbbbb7f583f826a66c94e6f88316a.1543337629.git.andreyknvl () google ! com>
--------------------
Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
memory into the generated code, instead of inserting a callback) generates
a brk instruction when a tag mismatch is detected.

This commit adds a tag-based KASAN specific brk handler, that decodes the
immediate value passed to the brk instructions (to extract information
about the memory access that triggered the mismatch), reads the register
values (x0 contains the guilty address) and reports the bug.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/brk-imm.h |  2 +
 arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
 include/linux/kasan.h            |  3 ++
 3 files changed, 71 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
index ed693c5bcec0..2945fe6cd863 100644
--- a/arch/arm64/include/asm/brk-imm.h
+++ b/arch/arm64/include/asm/brk-imm.h
@@ -16,10 +16,12 @@
  * 0x400: for dynamic BRK instruction
  * 0x401: for compile time BRK instruction
  * 0x800: kernel-mode BUG() and WARN() traps
+ * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
  */
 #define FAULT_BRK_IMM			0x100
 #define KGDB_DYN_DBG_BRK_IMM		0x400
 #define KGDB_COMPILED_DBG_BRK_IMM	0x401
 #define BUG_BRK_IMM			0x800
+#define KASAN_BRK_IMM			0x900
 
 #endif
diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 5f4d9acb32f5..04bdc53716ef 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -35,6 +35,7 @@
 #include <linux/sizes.h>
 #include <linux/syscalls.h>
 #include <linux/mm_types.h>
+#include <linux/kasan.h>
 
 #include <asm/atomic.h>
 #include <asm/bug.h>
@@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
 	}
 }
 
-void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
 {
 	regs->pc += size;
+}
 
+void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+{
+	__arm64_skip_faulting_instruction(regs, size);
 	/*
 	 * If we were single stepping, we want to get the step exception after
 	 * we return from the trap.
@@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
 	}
 
 	/* If thread survives, skip over the BUG instruction and continue: */
-	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 	return DBG_HOOK_HANDLED;
 }
 
@@ -969,6 +974,58 @@ static struct break_hook bug_break_hook = {
 	.fn = bug_handler,
 };
 
+#ifdef CONFIG_KASAN_SW_TAGS
+
+#define KASAN_ESR_RECOVER	0x20
+#define KASAN_ESR_WRITE	0x10
+#define KASAN_ESR_SIZE_MASK	0x0f
+#define KASAN_ESR_SIZE(esr)	(1 << ((esr) & KASAN_ESR_SIZE_MASK))
+
+static int kasan_handler(struct pt_regs *regs, unsigned int esr)
+{
+	bool recover = esr & KASAN_ESR_RECOVER;
+	bool write = esr & KASAN_ESR_WRITE;
+	size_t size = KASAN_ESR_SIZE(esr);
+	u64 addr = regs->regs[0];
+	u64 pc = regs->pc;
+
+	if (user_mode(regs))
+		return DBG_HOOK_ERROR;
+
+	kasan_report(addr, size, write, pc);
+
+	/*
+	 * The instrumentation allows to control whether we can proceed after
+	 * a crash was detected. This is done by passing the -recover flag to
+	 * the compiler. Disabling recovery allows to generate more compact
+	 * code.
+	 *
+	 * Unfortunately disabling recovery doesn't work for the kernel right
+	 * now. KASAN reporting is disabled in some contexts (for example when
+	 * the allocator accesses slab object metadata; this is controlled by
+	 * current->kasan_depth). All these accesses are detected by the tool,
+	 * even though the reports for them are not printed.
+	 *
+	 * This is something that might be fixed at some point in the future.
+	 */
+	if (!recover)
+		die("Oops - KASAN", regs, 0);
+
+	/* If thread survives, skip over the brk instruction and continue: */
+	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	return DBG_HOOK_HANDLED;
+}
+
+#define KASAN_ESR_VAL (0xf2000000 | KASAN_BRK_IMM)
+#define KASAN_ESR_MASK 0xffffff00
+
+static struct break_hook kasan_break_hook = {
+	.esr_val = KASAN_ESR_VAL,
+	.esr_mask = KASAN_ESR_MASK,
+	.fn = kasan_handler,
+};
+#endif
+
 /*
  * Initial handler for AArch64 BRK exceptions
  * This handler only used until debug_traps_init().
@@ -976,6 +1033,10 @@ static struct break_hook bug_break_hook = {
 int __init early_brk64(unsigned long addr, unsigned int esr,
 		struct pt_regs *regs)
 {
+#ifdef CONFIG_KASAN_SW_TAGS
+	if ((esr & KASAN_ESR_MASK) == KASAN_ESR_VAL)
+		return kasan_handler(regs, esr) != DBG_HOOK_HANDLED;
+#endif
 	return bug_handler(regs, esr) != DBG_HOOK_HANDLED;
 }
 
@@ -983,4 +1044,7 @@ int __init early_brk64(unsigned long addr, unsigned int esr,
 void __init trap_init(void)
 {
 	register_break_hook(&bug_break_hook);
+#ifdef CONFIG_KASAN_SW_TAGS
+	register_break_hook(&kasan_break_hook);
+#endif
 }
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index a477ce2abdc9..8da7b7a4397a 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -173,6 +173,9 @@ void kasan_init_tags(void);
 
 void *kasan_reset_tag(const void *addr);
 
+void kasan_report(unsigned long addr, size_t size,
+		bool is_write, unsigned long ip);
+
 #else /* CONFIG_KASAN_SW_TAGS */
 
 static inline void kasan_init_tags(void) { }
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-sparse
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 29 Nov 2018 18:01:38 +0000
Message-ID: <20181129180138.GB4318 () arm ! com>
--------------------
On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> memory into the generated code, instead of inserting a callback) generates
> a brk instruction when a tag mismatch is detected.
> 
> This commit adds a tag-based KASAN specific brk handler, that decodes the
> immediate value passed to the brk instructions (to extract information
> about the memory access that triggered the mismatch), reads the register
> values (x0 contains the guilty address) and reports the bug.
> 
> Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> ---
>  arch/arm64/include/asm/brk-imm.h |  2 +
>  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
>  include/linux/kasan.h            |  3 ++
>  3 files changed, 71 insertions(+), 2 deletions(-)
> 
> diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> index ed693c5bcec0..2945fe6cd863 100644
> --- a/arch/arm64/include/asm/brk-imm.h
> +++ b/arch/arm64/include/asm/brk-imm.h
> @@ -16,10 +16,12 @@
>   * 0x400: for dynamic BRK instruction
>   * 0x401: for compile time BRK instruction
>   * 0x800: kernel-mode BUG() and WARN() traps
> + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
>   */
>  #define FAULT_BRK_IMM			0x100
>  #define KGDB_DYN_DBG_BRK_IMM		0x400
>  #define KGDB_COMPILED_DBG_BRK_IMM	0x401
>  #define BUG_BRK_IMM			0x800
> +#define KASAN_BRK_IMM			0x900
>  
>  #endif
> diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> index 5f4d9acb32f5..04bdc53716ef 100644
> --- a/arch/arm64/kernel/traps.c
> +++ b/arch/arm64/kernel/traps.c
> @@ -35,6 +35,7 @@
>  #include <linux/sizes.h>
>  #include <linux/syscalls.h>
>  #include <linux/mm_types.h>
> +#include <linux/kasan.h>
>  
>  #include <asm/atomic.h>
>  #include <asm/bug.h>
> @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
>  	}
>  }
>  
> -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
>  {
>  	regs->pc += size;
> +}
>  
> +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> +{
> +	__arm64_skip_faulting_instruction(regs, size);
>  	/*
>  	 * If we were single stepping, we want to get the step exception after
>  	 * we return from the trap.
> @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
>  	}
>  
>  	/* If thread survives, skip over the BUG instruction and continue: */
> -	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> +	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);

Why do you want to avoid the single-step logic here? Given that we're
skipping over the brk instruction, why wouldn't you want that to trigger
a step exception if single-step is enabled?

Will
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-kbuild
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 29 Nov 2018 18:01:38 +0000
Message-ID: <20181129180138.GB4318 () arm ! com>
--------------------
On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> memory into the generated code, instead of inserting a callback) generates
> a brk instruction when a tag mismatch is detected.
> 
> This commit adds a tag-based KASAN specific brk handler, that decodes the
> immediate value passed to the brk instructions (to extract information
> about the memory access that triggered the mismatch), reads the register
> values (x0 contains the guilty address) and reports the bug.
> 
> Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> ---
>  arch/arm64/include/asm/brk-imm.h |  2 +
>  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
>  include/linux/kasan.h            |  3 ++
>  3 files changed, 71 insertions(+), 2 deletions(-)
> 
> diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> index ed693c5bcec0..2945fe6cd863 100644
> --- a/arch/arm64/include/asm/brk-imm.h
> +++ b/arch/arm64/include/asm/brk-imm.h
> @@ -16,10 +16,12 @@
>   * 0x400: for dynamic BRK instruction
>   * 0x401: for compile time BRK instruction
>   * 0x800: kernel-mode BUG() and WARN() traps
> + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
>   */
>  #define FAULT_BRK_IMM			0x100
>  #define KGDB_DYN_DBG_BRK_IMM		0x400
>  #define KGDB_COMPILED_DBG_BRK_IMM	0x401
>  #define BUG_BRK_IMM			0x800
> +#define KASAN_BRK_IMM			0x900
>  
>  #endif
> diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> index 5f4d9acb32f5..04bdc53716ef 100644
> --- a/arch/arm64/kernel/traps.c
> +++ b/arch/arm64/kernel/traps.c
> @@ -35,6 +35,7 @@
>  #include <linux/sizes.h>
>  #include <linux/syscalls.h>
>  #include <linux/mm_types.h>
> +#include <linux/kasan.h>
>  
>  #include <asm/atomic.h>
>  #include <asm/bug.h>
> @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
>  	}
>  }
>  
> -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
>  {
>  	regs->pc += size;
> +}
>  
> +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> +{
> +	__arm64_skip_faulting_instruction(regs, size);
>  	/*
>  	 * If we were single stepping, we want to get the step exception after
>  	 * we return from the trap.
> @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
>  	}
>  
>  	/* If thread survives, skip over the BUG instruction and continue: */
> -	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> +	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);

Why do you want to avoid the single-step logic here? Given that we're
skipping over the brk instruction, why wouldn't you want that to trigger
a step exception if single-step is enabled?

Will
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-mm
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 29 Nov 2018 18:01:38 +0000
Message-ID: <20181129180138.GB4318 () arm ! com>
--------------------
On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> memory into the generated code, instead of inserting a callback) generates
> a brk instruction when a tag mismatch is detected.
> 
> This commit adds a tag-based KASAN specific brk handler, that decodes the
> immediate value passed to the brk instructions (to extract information
> about the memory access that triggered the mismatch), reads the register
> values (x0 contains the guilty address) and reports the bug.
> 
> Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> ---
>  arch/arm64/include/asm/brk-imm.h |  2 +
>  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
>  include/linux/kasan.h            |  3 ++
>  3 files changed, 71 insertions(+), 2 deletions(-)
> 
> diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> index ed693c5bcec0..2945fe6cd863 100644
> --- a/arch/arm64/include/asm/brk-imm.h
> +++ b/arch/arm64/include/asm/brk-imm.h
> @@ -16,10 +16,12 @@
>   * 0x400: for dynamic BRK instruction
>   * 0x401: for compile time BRK instruction
>   * 0x800: kernel-mode BUG() and WARN() traps
> + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
>   */
>  #define FAULT_BRK_IMM			0x100
>  #define KGDB_DYN_DBG_BRK_IMM		0x400
>  #define KGDB_COMPILED_DBG_BRK_IMM	0x401
>  #define BUG_BRK_IMM			0x800
> +#define KASAN_BRK_IMM			0x900
>  
>  #endif
> diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> index 5f4d9acb32f5..04bdc53716ef 100644
> --- a/arch/arm64/kernel/traps.c
> +++ b/arch/arm64/kernel/traps.c
> @@ -35,6 +35,7 @@
>  #include <linux/sizes.h>
>  #include <linux/syscalls.h>
>  #include <linux/mm_types.h>
> +#include <linux/kasan.h>
>  
>  #include <asm/atomic.h>
>  #include <asm/bug.h>
> @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
>  	}
>  }
>  
> -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
>  {
>  	regs->pc += size;
> +}
>  
> +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> +{
> +	__arm64_skip_faulting_instruction(regs, size);
>  	/*
>  	 * If we were single stepping, we want to get the step exception after
>  	 * we return from the trap.
> @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
>  	}
>  
>  	/* If thread survives, skip over the BUG instruction and continue: */
> -	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> +	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);

Why do you want to avoid the single-step logic here? Given that we're
skipping over the brk instruction, why wouldn't you want that to trigger
a step exception if single-step is enabled?

Will

================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-kernel
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 29 Nov 2018 18:01:38 +0000
Message-ID: <20181129180138.GB4318 () arm ! com>
--------------------
On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> memory into the generated code, instead of inserting a callback) generates
> a brk instruction when a tag mismatch is detected.
> 
> This commit adds a tag-based KASAN specific brk handler, that decodes the
> immediate value passed to the brk instructions (to extract information
> about the memory access that triggered the mismatch), reads the register
> values (x0 contains the guilty address) and reports the bug.
> 
> Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> ---
>  arch/arm64/include/asm/brk-imm.h |  2 +
>  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
>  include/linux/kasan.h            |  3 ++
>  3 files changed, 71 insertions(+), 2 deletions(-)
> 
> diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> index ed693c5bcec0..2945fe6cd863 100644
> --- a/arch/arm64/include/asm/brk-imm.h
> +++ b/arch/arm64/include/asm/brk-imm.h
> @@ -16,10 +16,12 @@
>   * 0x400: for dynamic BRK instruction
>   * 0x401: for compile time BRK instruction
>   * 0x800: kernel-mode BUG() and WARN() traps
> + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
>   */
>  #define FAULT_BRK_IMM			0x100
>  #define KGDB_DYN_DBG_BRK_IMM		0x400
>  #define KGDB_COMPILED_DBG_BRK_IMM	0x401
>  #define BUG_BRK_IMM			0x800
> +#define KASAN_BRK_IMM			0x900
>  
>  #endif
> diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> index 5f4d9acb32f5..04bdc53716ef 100644
> --- a/arch/arm64/kernel/traps.c
> +++ b/arch/arm64/kernel/traps.c
> @@ -35,6 +35,7 @@
>  #include <linux/sizes.h>
>  #include <linux/syscalls.h>
>  #include <linux/mm_types.h>
> +#include <linux/kasan.h>
>  
>  #include <asm/atomic.h>
>  #include <asm/bug.h>
> @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
>  	}
>  }
>  
> -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
>  {
>  	regs->pc += size;
> +}
>  
> +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> +{
> +	__arm64_skip_faulting_instruction(regs, size);
>  	/*
>  	 * If we were single stepping, we want to get the step exception after
>  	 * we return from the trap.
> @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
>  	}
>  
>  	/* If thread survives, skip over the BUG instruction and continue: */
> -	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> +	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);

Why do you want to avoid the single-step logic here? Given that we're
skipping over the brk instruction, why wouldn't you want that to trigger
a step exception if single-step is enabled?

Will
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-doc
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 29 Nov 2018 18:01:38 +0000
Message-ID: <20181129180138.GB4318 () arm ! com>
--------------------
On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> memory into the generated code, instead of inserting a callback) generates
> a brk instruction when a tag mismatch is detected.
> 
> This commit adds a tag-based KASAN specific brk handler, that decodes the
> immediate value passed to the brk instructions (to extract information
> about the memory access that triggered the mismatch), reads the register
> values (x0 contains the guilty address) and reports the bug.
> 
> Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> ---
>  arch/arm64/include/asm/brk-imm.h |  2 +
>  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
>  include/linux/kasan.h            |  3 ++
>  3 files changed, 71 insertions(+), 2 deletions(-)
> 
> diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> index ed693c5bcec0..2945fe6cd863 100644
> --- a/arch/arm64/include/asm/brk-imm.h
> +++ b/arch/arm64/include/asm/brk-imm.h
> @@ -16,10 +16,12 @@
>   * 0x400: for dynamic BRK instruction
>   * 0x401: for compile time BRK instruction
>   * 0x800: kernel-mode BUG() and WARN() traps
> + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
>   */
>  #define FAULT_BRK_IMM			0x100
>  #define KGDB_DYN_DBG_BRK_IMM		0x400
>  #define KGDB_COMPILED_DBG_BRK_IMM	0x401
>  #define BUG_BRK_IMM			0x800
> +#define KASAN_BRK_IMM			0x900
>  
>  #endif
> diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> index 5f4d9acb32f5..04bdc53716ef 100644
> --- a/arch/arm64/kernel/traps.c
> +++ b/arch/arm64/kernel/traps.c
> @@ -35,6 +35,7 @@
>  #include <linux/sizes.h>
>  #include <linux/syscalls.h>
>  #include <linux/mm_types.h>
> +#include <linux/kasan.h>
>  
>  #include <asm/atomic.h>
>  #include <asm/bug.h>
> @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
>  	}
>  }
>  
> -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
>  {
>  	regs->pc += size;
> +}
>  
> +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> +{
> +	__arm64_skip_faulting_instruction(regs, size);
>  	/*
>  	 * If we were single stepping, we want to get the step exception after
>  	 * we return from the trap.
> @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
>  	}
>  
>  	/* If thread survives, skip over the BUG instruction and continue: */
> -	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> +	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);

Why do you want to avoid the single-step logic here? Given that we're
skipping over the brk instruction, why wouldn't you want that to trigger
a step exception if single-step is enabled?

Will
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 10:31:43 +0000
Message-ID: <CAAeHK+zVzWJ7RBsX88SOsebq0a40ypuawYFd4w4woFSHuximOw () mail ! gmail ! com>
--------------------
On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > memory into the generated code, instead of inserting a callback) generates
> > a brk instruction when a tag mismatch is detected.
> >
> > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > immediate value passed to the brk instructions (to extract information
> > about the memory access that triggered the mismatch), reads the register
> > values (x0 contains the guilty address) and reports the bug.
> >
> > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > ---
> >  arch/arm64/include/asm/brk-imm.h |  2 +
> >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> >  include/linux/kasan.h            |  3 ++
> >  3 files changed, 71 insertions(+), 2 deletions(-)
> >
> > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > index ed693c5bcec0..2945fe6cd863 100644
> > --- a/arch/arm64/include/asm/brk-imm.h
> > +++ b/arch/arm64/include/asm/brk-imm.h
> > @@ -16,10 +16,12 @@
> >   * 0x400: for dynamic BRK instruction
> >   * 0x401: for compile time BRK instruction
> >   * 0x800: kernel-mode BUG() and WARN() traps
> > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> >   */
> >  #define FAULT_BRK_IMM                        0x100
> >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> >  #define BUG_BRK_IMM                  0x800
> > +#define KASAN_BRK_IMM                        0x900
> >
> >  #endif
> > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > index 5f4d9acb32f5..04bdc53716ef 100644
> > --- a/arch/arm64/kernel/traps.c
> > +++ b/arch/arm64/kernel/traps.c
> > @@ -35,6 +35,7 @@
> >  #include <linux/sizes.h>
> >  #include <linux/syscalls.h>
> >  #include <linux/mm_types.h>
> > +#include <linux/kasan.h>
> >
> >  #include <asm/atomic.h>
> >  #include <asm/bug.h>
> > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> >       }
> >  }
> >
> > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> >  {
> >       regs->pc += size;
> > +}
> >
> > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > +{
> > +     __arm64_skip_faulting_instruction(regs, size);
> >       /*
> >        * If we were single stepping, we want to get the step exception after
> >        * we return from the trap.
> > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> >       }
> >
> >       /* If thread survives, skip over the BUG instruction and continue: */
> > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
>
> Why do you want to avoid the single-step logic here? Given that we're
> skipping over the brk instruction, why wouldn't you want that to trigger
> a step exception if single-step is enabled?

I was asked to do that, see the discussion here:

https://www.spinics.net/lists/linux-mm/msg146575.html
https://www.spinics.net/lists/linux-mm/msg148215.html
https://www.spinics.net/lists/linux-mm/msg148367.html
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-doc
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 10:31:43 +0000
Message-ID: <CAAeHK+zVzWJ7RBsX88SOsebq0a40ypuawYFd4w4woFSHuximOw () mail ! gmail ! com>
--------------------
On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > memory into the generated code, instead of inserting a callback) generates
> > a brk instruction when a tag mismatch is detected.
> >
> > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > immediate value passed to the brk instructions (to extract information
> > about the memory access that triggered the mismatch), reads the register
> > values (x0 contains the guilty address) and reports the bug.
> >
> > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > ---
> >  arch/arm64/include/asm/brk-imm.h |  2 +
> >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> >  include/linux/kasan.h            |  3 ++
> >  3 files changed, 71 insertions(+), 2 deletions(-)
> >
> > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > index ed693c5bcec0..2945fe6cd863 100644
> > --- a/arch/arm64/include/asm/brk-imm.h
> > +++ b/arch/arm64/include/asm/brk-imm.h
> > @@ -16,10 +16,12 @@
> >   * 0x400: for dynamic BRK instruction
> >   * 0x401: for compile time BRK instruction
> >   * 0x800: kernel-mode BUG() and WARN() traps
> > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> >   */
> >  #define FAULT_BRK_IMM                        0x100
> >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> >  #define BUG_BRK_IMM                  0x800
> > +#define KASAN_BRK_IMM                        0x900
> >
> >  #endif
> > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > index 5f4d9acb32f5..04bdc53716ef 100644
> > --- a/arch/arm64/kernel/traps.c
> > +++ b/arch/arm64/kernel/traps.c
> > @@ -35,6 +35,7 @@
> >  #include <linux/sizes.h>
> >  #include <linux/syscalls.h>
> >  #include <linux/mm_types.h>
> > +#include <linux/kasan.h>
> >
> >  #include <asm/atomic.h>
> >  #include <asm/bug.h>
> > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> >       }
> >  }
> >
> > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> >  {
> >       regs->pc += size;
> > +}
> >
> > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > +{
> > +     __arm64_skip_faulting_instruction(regs, size);
> >       /*
> >        * If we were single stepping, we want to get the step exception after
> >        * we return from the trap.
> > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> >       }
> >
> >       /* If thread survives, skip over the BUG instruction and continue: */
> > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
>
> Why do you want to avoid the single-step logic here? Given that we're
> skipping over the brk instruction, why wouldn't you want that to trigger
> a step exception if single-step is enabled?

I was asked to do that, see the discussion here:

https://www.spinics.net/lists/linux-mm/msg146575.html
https://www.spinics.net/lists/linux-mm/msg148215.html
https://www.spinics.net/lists/linux-mm/msg148367.html
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kbuild
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 10:31:43 +0000
Message-ID: <CAAeHK+zVzWJ7RBsX88SOsebq0a40ypuawYFd4w4woFSHuximOw () mail ! gmail ! com>
--------------------
On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > memory into the generated code, instead of inserting a callback) generates
> > a brk instruction when a tag mismatch is detected.
> >
> > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > immediate value passed to the brk instructions (to extract information
> > about the memory access that triggered the mismatch), reads the register
> > values (x0 contains the guilty address) and reports the bug.
> >
> > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > ---
> >  arch/arm64/include/asm/brk-imm.h |  2 +
> >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> >  include/linux/kasan.h            |  3 ++
> >  3 files changed, 71 insertions(+), 2 deletions(-)
> >
> > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > index ed693c5bcec0..2945fe6cd863 100644
> > --- a/arch/arm64/include/asm/brk-imm.h
> > +++ b/arch/arm64/include/asm/brk-imm.h
> > @@ -16,10 +16,12 @@
> >   * 0x400: for dynamic BRK instruction
> >   * 0x401: for compile time BRK instruction
> >   * 0x800: kernel-mode BUG() and WARN() traps
> > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> >   */
> >  #define FAULT_BRK_IMM                        0x100
> >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> >  #define BUG_BRK_IMM                  0x800
> > +#define KASAN_BRK_IMM                        0x900
> >
> >  #endif
> > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > index 5f4d9acb32f5..04bdc53716ef 100644
> > --- a/arch/arm64/kernel/traps.c
> > +++ b/arch/arm64/kernel/traps.c
> > @@ -35,6 +35,7 @@
> >  #include <linux/sizes.h>
> >  #include <linux/syscalls.h>
> >  #include <linux/mm_types.h>
> > +#include <linux/kasan.h>
> >
> >  #include <asm/atomic.h>
> >  #include <asm/bug.h>
> > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> >       }
> >  }
> >
> > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> >  {
> >       regs->pc += size;
> > +}
> >
> > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > +{
> > +     __arm64_skip_faulting_instruction(regs, size);
> >       /*
> >        * If we were single stepping, we want to get the step exception after
> >        * we return from the trap.
> > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> >       }
> >
> >       /* If thread survives, skip over the BUG instruction and continue: */
> > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
>
> Why do you want to avoid the single-step logic here? Given that we're
> skipping over the brk instruction, why wouldn't you want that to trigger
> a step exception if single-step is enabled?

I was asked to do that, see the discussion here:

https://www.spinics.net/lists/linux-mm/msg146575.html
https://www.spinics.net/lists/linux-mm/msg148215.html
https://www.spinics.net/lists/linux-mm/msg148367.html
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kernel
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 10:31:43 +0000
Message-ID: <CAAeHK+zVzWJ7RBsX88SOsebq0a40ypuawYFd4w4woFSHuximOw () mail ! gmail ! com>
--------------------
On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > memory into the generated code, instead of inserting a callback) generates
> > a brk instruction when a tag mismatch is detected.
> >
> > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > immediate value passed to the brk instructions (to extract information
> > about the memory access that triggered the mismatch), reads the register
> > values (x0 contains the guilty address) and reports the bug.
> >
> > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > ---
> >  arch/arm64/include/asm/brk-imm.h |  2 +
> >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> >  include/linux/kasan.h            |  3 ++
> >  3 files changed, 71 insertions(+), 2 deletions(-)
> >
> > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > index ed693c5bcec0..2945fe6cd863 100644
> > --- a/arch/arm64/include/asm/brk-imm.h
> > +++ b/arch/arm64/include/asm/brk-imm.h
> > @@ -16,10 +16,12 @@
> >   * 0x400: for dynamic BRK instruction
> >   * 0x401: for compile time BRK instruction
> >   * 0x800: kernel-mode BUG() and WARN() traps
> > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> >   */
> >  #define FAULT_BRK_IMM                        0x100
> >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> >  #define BUG_BRK_IMM                  0x800
> > +#define KASAN_BRK_IMM                        0x900
> >
> >  #endif
> > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > index 5f4d9acb32f5..04bdc53716ef 100644
> > --- a/arch/arm64/kernel/traps.c
> > +++ b/arch/arm64/kernel/traps.c
> > @@ -35,6 +35,7 @@
> >  #include <linux/sizes.h>
> >  #include <linux/syscalls.h>
> >  #include <linux/mm_types.h>
> > +#include <linux/kasan.h>
> >
> >  #include <asm/atomic.h>
> >  #include <asm/bug.h>
> > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> >       }
> >  }
> >
> > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> >  {
> >       regs->pc += size;
> > +}
> >
> > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > +{
> > +     __arm64_skip_faulting_instruction(regs, size);
> >       /*
> >        * If we were single stepping, we want to get the step exception after
> >        * we return from the trap.
> > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> >       }
> >
> >       /* If thread survives, skip over the BUG instruction and continue: */
> > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
>
> Why do you want to avoid the single-step logic here? Given that we're
> skipping over the brk instruction, why wouldn't you want that to trigger
> a step exception if single-step is enabled?

I was asked to do that, see the discussion here:

https://www.spinics.net/lists/linux-mm/msg146575.html
https://www.spinics.net/lists/linux-mm/msg148215.html
https://www.spinics.net/lists/linux-mm/msg148367.html
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-mm
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 10:31:43 +0000
Message-ID: <CAAeHK+zVzWJ7RBsX88SOsebq0a40ypuawYFd4w4woFSHuximOw () mail ! gmail ! com>
--------------------
On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > memory into the generated code, instead of inserting a callback) generates
> > a brk instruction when a tag mismatch is detected.
> >
> > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > immediate value passed to the brk instructions (to extract information
> > about the memory access that triggered the mismatch), reads the register
> > values (x0 contains the guilty address) and reports the bug.
> >
> > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > ---
> >  arch/arm64/include/asm/brk-imm.h |  2 +
> >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> >  include/linux/kasan.h            |  3 ++
> >  3 files changed, 71 insertions(+), 2 deletions(-)
> >
> > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > index ed693c5bcec0..2945fe6cd863 100644
> > --- a/arch/arm64/include/asm/brk-imm.h
> > +++ b/arch/arm64/include/asm/brk-imm.h
> > @@ -16,10 +16,12 @@
> >   * 0x400: for dynamic BRK instruction
> >   * 0x401: for compile time BRK instruction
> >   * 0x800: kernel-mode BUG() and WARN() traps
> > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> >   */
> >  #define FAULT_BRK_IMM                        0x100
> >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> >  #define BUG_BRK_IMM                  0x800
> > +#define KASAN_BRK_IMM                        0x900
> >
> >  #endif
> > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > index 5f4d9acb32f5..04bdc53716ef 100644
> > --- a/arch/arm64/kernel/traps.c
> > +++ b/arch/arm64/kernel/traps.c
> > @@ -35,6 +35,7 @@
> >  #include <linux/sizes.h>
> >  #include <linux/syscalls.h>
> >  #include <linux/mm_types.h>
> > +#include <linux/kasan.h>
> >
> >  #include <asm/atomic.h>
> >  #include <asm/bug.h>
> > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> >       }
> >  }
> >
> > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> >  {
> >       regs->pc += size;
> > +}
> >
> > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > +{
> > +     __arm64_skip_faulting_instruction(regs, size);
> >       /*
> >        * If we were single stepping, we want to get the step exception after
> >        * we return from the trap.
> > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> >       }
> >
> >       /* If thread survives, skip over the BUG instruction and continue: */
> > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
>
> Why do you want to avoid the single-step logic here? Given that we're
> skipping over the brk instruction, why wouldn't you want that to trigger
> a step exception if single-step is enabled?

I was asked to do that, see the discussion here:

https://www.spinics.net/lists/linux-mm/msg146575.html
https://www.spinics.net/lists/linux-mm/msg148215.html
https://www.spinics.net/lists/linux-mm/msg148367.html

================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 11:11:07 +0000
Message-ID: <20181206111107.GE23697 () arm ! com>
--------------------
On Thu, Dec 06, 2018 at 11:31:43AM +0100, Andrey Konovalov wrote:
> On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
> >
> > On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > > memory into the generated code, instead of inserting a callback) generates
> > > a brk instruction when a tag mismatch is detected.
> > >
> > > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > > immediate value passed to the brk instructions (to extract information
> > > about the memory access that triggered the mismatch), reads the register
> > > values (x0 contains the guilty address) and reports the bug.
> > >
> > > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > > ---
> > >  arch/arm64/include/asm/brk-imm.h |  2 +
> > >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> > >  include/linux/kasan.h            |  3 ++
> > >  3 files changed, 71 insertions(+), 2 deletions(-)
> > >
> > > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > > index ed693c5bcec0..2945fe6cd863 100644
> > > --- a/arch/arm64/include/asm/brk-imm.h
> > > +++ b/arch/arm64/include/asm/brk-imm.h
> > > @@ -16,10 +16,12 @@
> > >   * 0x400: for dynamic BRK instruction
> > >   * 0x401: for compile time BRK instruction
> > >   * 0x800: kernel-mode BUG() and WARN() traps
> > > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> > >   */
> > >  #define FAULT_BRK_IMM                        0x100
> > >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> > >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> > >  #define BUG_BRK_IMM                  0x800
> > > +#define KASAN_BRK_IMM                        0x900
> > >
> > >  #endif
> > > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > > index 5f4d9acb32f5..04bdc53716ef 100644
> > > --- a/arch/arm64/kernel/traps.c
> > > +++ b/arch/arm64/kernel/traps.c
> > > @@ -35,6 +35,7 @@
> > >  #include <linux/sizes.h>
> > >  #include <linux/syscalls.h>
> > >  #include <linux/mm_types.h>
> > > +#include <linux/kasan.h>
> > >
> > >  #include <asm/atomic.h>
> > >  #include <asm/bug.h>
> > > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> > >       }
> > >  }
> > >
> > > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > >  {
> > >       regs->pc += size;
> > > +}
> > >
> > > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > +{
> > > +     __arm64_skip_faulting_instruction(regs, size);
> > >       /*
> > >        * If we were single stepping, we want to get the step exception after
> > >        * we return from the trap.
> > > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> > >       }
> > >
> > >       /* If thread survives, skip over the BUG instruction and continue: */
> > > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> >
> > Why do you want to avoid the single-step logic here? Given that we're
> > skipping over the brk instruction, why wouldn't you want that to trigger
> > a step exception if single-step is enabled?
> 
> I was asked to do that, see the discussion here:
> 
> https://www.spinics.net/lists/linux-mm/msg146575.html
> https://www.spinics.net/lists/linux-mm/msg148215.html
> https://www.spinics.net/lists/linux-mm/msg148367.html

Aha, but we subsequently fixed the underlying problem in commit
9478f1927e6e ("arm64: only advance singlestep for user instruction traps").
You were on cc, but I appreciate it's not clear that it was related to this.

Anyway, you can just call arm64_skip_faulting_instruction() as you were
doing and there's no need for this refactoring.

Please could you spin a new version so that akpm can replace the one which
he has queued?

Thanks,

Will

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-sparse
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 11:11:07 +0000
Message-ID: <20181206111107.GE23697 () arm ! com>
--------------------
On Thu, Dec 06, 2018 at 11:31:43AM +0100, Andrey Konovalov wrote:
> On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
> >
> > On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > > memory into the generated code, instead of inserting a callback) generates
> > > a brk instruction when a tag mismatch is detected.
> > >
> > > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > > immediate value passed to the brk instructions (to extract information
> > > about the memory access that triggered the mismatch), reads the register
> > > values (x0 contains the guilty address) and reports the bug.
> > >
> > > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > > ---
> > >  arch/arm64/include/asm/brk-imm.h |  2 +
> > >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> > >  include/linux/kasan.h            |  3 ++
> > >  3 files changed, 71 insertions(+), 2 deletions(-)
> > >
> > > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > > index ed693c5bcec0..2945fe6cd863 100644
> > > --- a/arch/arm64/include/asm/brk-imm.h
> > > +++ b/arch/arm64/include/asm/brk-imm.h
> > > @@ -16,10 +16,12 @@
> > >   * 0x400: for dynamic BRK instruction
> > >   * 0x401: for compile time BRK instruction
> > >   * 0x800: kernel-mode BUG() and WARN() traps
> > > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> > >   */
> > >  #define FAULT_BRK_IMM                        0x100
> > >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> > >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> > >  #define BUG_BRK_IMM                  0x800
> > > +#define KASAN_BRK_IMM                        0x900
> > >
> > >  #endif
> > > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > > index 5f4d9acb32f5..04bdc53716ef 100644
> > > --- a/arch/arm64/kernel/traps.c
> > > +++ b/arch/arm64/kernel/traps.c
> > > @@ -35,6 +35,7 @@
> > >  #include <linux/sizes.h>
> > >  #include <linux/syscalls.h>
> > >  #include <linux/mm_types.h>
> > > +#include <linux/kasan.h>
> > >
> > >  #include <asm/atomic.h>
> > >  #include <asm/bug.h>
> > > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> > >       }
> > >  }
> > >
> > > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > >  {
> > >       regs->pc += size;
> > > +}
> > >
> > > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > +{
> > > +     __arm64_skip_faulting_instruction(regs, size);
> > >       /*
> > >        * If we were single stepping, we want to get the step exception after
> > >        * we return from the trap.
> > > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> > >       }
> > >
> > >       /* If thread survives, skip over the BUG instruction and continue: */
> > > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> >
> > Why do you want to avoid the single-step logic here? Given that we're
> > skipping over the brk instruction, why wouldn't you want that to trigger
> > a step exception if single-step is enabled?
> 
> I was asked to do that, see the discussion here:
> 
> https://www.spinics.net/lists/linux-mm/msg146575.html
> https://www.spinics.net/lists/linux-mm/msg148215.html
> https://www.spinics.net/lists/linux-mm/msg148367.html

Aha, but we subsequently fixed the underlying problem in commit
9478f1927e6e ("arm64: only advance singlestep for user instruction traps").
You were on cc, but I appreciate it's not clear that it was related to this.

Anyway, you can just call arm64_skip_faulting_instruction() as you were
doing and there's no need for this refactoring.

Please could you spin a new version so that akpm can replace the one which
he has queued?

Thanks,

Will
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-kernel
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 11:11:07 +0000
Message-ID: <20181206111107.GE23697 () arm ! com>
--------------------
On Thu, Dec 06, 2018 at 11:31:43AM +0100, Andrey Konovalov wrote:
> On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
> >
> > On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > > memory into the generated code, instead of inserting a callback) generates
> > > a brk instruction when a tag mismatch is detected.
> > >
> > > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > > immediate value passed to the brk instructions (to extract information
> > > about the memory access that triggered the mismatch), reads the register
> > > values (x0 contains the guilty address) and reports the bug.
> > >
> > > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > > ---
> > >  arch/arm64/include/asm/brk-imm.h |  2 +
> > >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> > >  include/linux/kasan.h            |  3 ++
> > >  3 files changed, 71 insertions(+), 2 deletions(-)
> > >
> > > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > > index ed693c5bcec0..2945fe6cd863 100644
> > > --- a/arch/arm64/include/asm/brk-imm.h
> > > +++ b/arch/arm64/include/asm/brk-imm.h
> > > @@ -16,10 +16,12 @@
> > >   * 0x400: for dynamic BRK instruction
> > >   * 0x401: for compile time BRK instruction
> > >   * 0x800: kernel-mode BUG() and WARN() traps
> > > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> > >   */
> > >  #define FAULT_BRK_IMM                        0x100
> > >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> > >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> > >  #define BUG_BRK_IMM                  0x800
> > > +#define KASAN_BRK_IMM                        0x900
> > >
> > >  #endif
> > > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > > index 5f4d9acb32f5..04bdc53716ef 100644
> > > --- a/arch/arm64/kernel/traps.c
> > > +++ b/arch/arm64/kernel/traps.c
> > > @@ -35,6 +35,7 @@
> > >  #include <linux/sizes.h>
> > >  #include <linux/syscalls.h>
> > >  #include <linux/mm_types.h>
> > > +#include <linux/kasan.h>
> > >
> > >  #include <asm/atomic.h>
> > >  #include <asm/bug.h>
> > > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> > >       }
> > >  }
> > >
> > > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > >  {
> > >       regs->pc += size;
> > > +}
> > >
> > > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > +{
> > > +     __arm64_skip_faulting_instruction(regs, size);
> > >       /*
> > >        * If we were single stepping, we want to get the step exception after
> > >        * we return from the trap.
> > > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> > >       }
> > >
> > >       /* If thread survives, skip over the BUG instruction and continue: */
> > > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> >
> > Why do you want to avoid the single-step logic here? Given that we're
> > skipping over the brk instruction, why wouldn't you want that to trigger
> > a step exception if single-step is enabled?
> 
> I was asked to do that, see the discussion here:
> 
> https://www.spinics.net/lists/linux-mm/msg146575.html
> https://www.spinics.net/lists/linux-mm/msg148215.html
> https://www.spinics.net/lists/linux-mm/msg148367.html

Aha, but we subsequently fixed the underlying problem in commit
9478f1927e6e ("arm64: only advance singlestep for user instruction traps").
You were on cc, but I appreciate it's not clear that it was related to this.

Anyway, you can just call arm64_skip_faulting_instruction() as you were
doing and there's no need for this refactoring.

Please could you spin a new version so that akpm can replace the one which
he has queued?

Thanks,

Will
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-doc
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 11:11:07 +0000
Message-ID: <20181206111107.GE23697 () arm ! com>
--------------------
On Thu, Dec 06, 2018 at 11:31:43AM +0100, Andrey Konovalov wrote:
> On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
> >
> > On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > > memory into the generated code, instead of inserting a callback) generates
> > > a brk instruction when a tag mismatch is detected.
> > >
> > > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > > immediate value passed to the brk instructions (to extract information
> > > about the memory access that triggered the mismatch), reads the register
> > > values (x0 contains the guilty address) and reports the bug.
> > >
> > > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > > ---
> > >  arch/arm64/include/asm/brk-imm.h |  2 +
> > >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> > >  include/linux/kasan.h            |  3 ++
> > >  3 files changed, 71 insertions(+), 2 deletions(-)
> > >
> > > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > > index ed693c5bcec0..2945fe6cd863 100644
> > > --- a/arch/arm64/include/asm/brk-imm.h
> > > +++ b/arch/arm64/include/asm/brk-imm.h
> > > @@ -16,10 +16,12 @@
> > >   * 0x400: for dynamic BRK instruction
> > >   * 0x401: for compile time BRK instruction
> > >   * 0x800: kernel-mode BUG() and WARN() traps
> > > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> > >   */
> > >  #define FAULT_BRK_IMM                        0x100
> > >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> > >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> > >  #define BUG_BRK_IMM                  0x800
> > > +#define KASAN_BRK_IMM                        0x900
> > >
> > >  #endif
> > > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > > index 5f4d9acb32f5..04bdc53716ef 100644
> > > --- a/arch/arm64/kernel/traps.c
> > > +++ b/arch/arm64/kernel/traps.c
> > > @@ -35,6 +35,7 @@
> > >  #include <linux/sizes.h>
> > >  #include <linux/syscalls.h>
> > >  #include <linux/mm_types.h>
> > > +#include <linux/kasan.h>
> > >
> > >  #include <asm/atomic.h>
> > >  #include <asm/bug.h>
> > > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> > >       }
> > >  }
> > >
> > > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > >  {
> > >       regs->pc += size;
> > > +}
> > >
> > > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > +{
> > > +     __arm64_skip_faulting_instruction(regs, size);
> > >       /*
> > >        * If we were single stepping, we want to get the step exception after
> > >        * we return from the trap.
> > > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> > >       }
> > >
> > >       /* If thread survives, skip over the BUG instruction and continue: */
> > > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> >
> > Why do you want to avoid the single-step logic here? Given that we're
> > skipping over the brk instruction, why wouldn't you want that to trigger
> > a step exception if single-step is enabled?
> 
> I was asked to do that, see the discussion here:
> 
> https://www.spinics.net/lists/linux-mm/msg146575.html
> https://www.spinics.net/lists/linux-mm/msg148215.html
> https://www.spinics.net/lists/linux-mm/msg148367.html

Aha, but we subsequently fixed the underlying problem in commit
9478f1927e6e ("arm64: only advance singlestep for user instruction traps").
You were on cc, but I appreciate it's not clear that it was related to this.

Anyway, you can just call arm64_skip_faulting_instruction() as you were
doing and there's no need for this refactoring.

Please could you spin a new version so that akpm can replace the one which
he has queued?

Thanks,

Will
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-mm
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 11:11:07 +0000
Message-ID: <20181206111107.GE23697 () arm ! com>
--------------------
On Thu, Dec 06, 2018 at 11:31:43AM +0100, Andrey Konovalov wrote:
> On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
> >
> > On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > > memory into the generated code, instead of inserting a callback) generates
> > > a brk instruction when a tag mismatch is detected.
> > >
> > > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > > immediate value passed to the brk instructions (to extract information
> > > about the memory access that triggered the mismatch), reads the register
> > > values (x0 contains the guilty address) and reports the bug.
> > >
> > > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > > ---
> > >  arch/arm64/include/asm/brk-imm.h |  2 +
> > >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> > >  include/linux/kasan.h            |  3 ++
> > >  3 files changed, 71 insertions(+), 2 deletions(-)
> > >
> > > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > > index ed693c5bcec0..2945fe6cd863 100644
> > > --- a/arch/arm64/include/asm/brk-imm.h
> > > +++ b/arch/arm64/include/asm/brk-imm.h
> > > @@ -16,10 +16,12 @@
> > >   * 0x400: for dynamic BRK instruction
> > >   * 0x401: for compile time BRK instruction
> > >   * 0x800: kernel-mode BUG() and WARN() traps
> > > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> > >   */
> > >  #define FAULT_BRK_IMM                        0x100
> > >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> > >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> > >  #define BUG_BRK_IMM                  0x800
> > > +#define KASAN_BRK_IMM                        0x900
> > >
> > >  #endif
> > > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > > index 5f4d9acb32f5..04bdc53716ef 100644
> > > --- a/arch/arm64/kernel/traps.c
> > > +++ b/arch/arm64/kernel/traps.c
> > > @@ -35,6 +35,7 @@
> > >  #include <linux/sizes.h>
> > >  #include <linux/syscalls.h>
> > >  #include <linux/mm_types.h>
> > > +#include <linux/kasan.h>
> > >
> > >  #include <asm/atomic.h>
> > >  #include <asm/bug.h>
> > > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> > >       }
> > >  }
> > >
> > > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > >  {
> > >       regs->pc += size;
> > > +}
> > >
> > > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > +{
> > > +     __arm64_skip_faulting_instruction(regs, size);
> > >       /*
> > >        * If we were single stepping, we want to get the step exception after
> > >        * we return from the trap.
> > > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> > >       }
> > >
> > >       /* If thread survives, skip over the BUG instruction and continue: */
> > > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> >
> > Why do you want to avoid the single-step logic here? Given that we're
> > skipping over the brk instruction, why wouldn't you want that to trigger
> > a step exception if single-step is enabled?
> 
> I was asked to do that, see the discussion here:
> 
> https://www.spinics.net/lists/linux-mm/msg146575.html
> https://www.spinics.net/lists/linux-mm/msg148215.html
> https://www.spinics.net/lists/linux-mm/msg148367.html

Aha, but we subsequently fixed the underlying problem in commit
9478f1927e6e ("arm64: only advance singlestep for user instruction traps").
You were on cc, but I appreciate it's not clear that it was related to this.

Anyway, you can just call arm64_skip_faulting_instruction() as you were
doing and there's no need for this refactoring.

Please could you spin a new version so that akpm can replace the one which
he has queued?

Thanks,

Will

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 12:25:54 +0000
Message-ID: <CAAeHK+w9NQYRHuNw-fAFDKVjF1L7pxTVRfe=DX1aC1iq5hYt1w () mail ! gmail ! com>
--------------------
On Thu, Dec 6, 2018 at 12:10 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Thu, Dec 06, 2018 at 11:31:43AM +0100, Andrey Konovalov wrote:
> > On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
> > >
> > > On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > > > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > > > memory into the generated code, instead of inserting a callback) generates
> > > > a brk instruction when a tag mismatch is detected.
> > > >
> > > > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > > > immediate value passed to the brk instructions (to extract information
> > > > about the memory access that triggered the mismatch), reads the register
> > > > values (x0 contains the guilty address) and reports the bug.
> > > >
> > > > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > > > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > > > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > > > ---
> > > >  arch/arm64/include/asm/brk-imm.h |  2 +
> > > >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> > > >  include/linux/kasan.h            |  3 ++
> > > >  3 files changed, 71 insertions(+), 2 deletions(-)
> > > >
> > > > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > > > index ed693c5bcec0..2945fe6cd863 100644
> > > > --- a/arch/arm64/include/asm/brk-imm.h
> > > > +++ b/arch/arm64/include/asm/brk-imm.h
> > > > @@ -16,10 +16,12 @@
> > > >   * 0x400: for dynamic BRK instruction
> > > >   * 0x401: for compile time BRK instruction
> > > >   * 0x800: kernel-mode BUG() and WARN() traps
> > > > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> > > >   */
> > > >  #define FAULT_BRK_IMM                        0x100
> > > >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> > > >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> > > >  #define BUG_BRK_IMM                  0x800
> > > > +#define KASAN_BRK_IMM                        0x900
> > > >
> > > >  #endif
> > > > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > > > index 5f4d9acb32f5..04bdc53716ef 100644
> > > > --- a/arch/arm64/kernel/traps.c
> > > > +++ b/arch/arm64/kernel/traps.c
> > > > @@ -35,6 +35,7 @@
> > > >  #include <linux/sizes.h>
> > > >  #include <linux/syscalls.h>
> > > >  #include <linux/mm_types.h>
> > > > +#include <linux/kasan.h>
> > > >
> > > >  #include <asm/atomic.h>
> > > >  #include <asm/bug.h>
> > > > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> > > >       }
> > > >  }
> > > >
> > > > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > >  {
> > > >       regs->pc += size;
> > > > +}
> > > >
> > > > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > > +{
> > > > +     __arm64_skip_faulting_instruction(regs, size);
> > > >       /*
> > > >        * If we were single stepping, we want to get the step exception after
> > > >        * we return from the trap.
> > > > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> > > >       }
> > > >
> > > >       /* If thread survives, skip over the BUG instruction and continue: */
> > > > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > > > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > >
> > > Why do you want to avoid the single-step logic here? Given that we're
> > > skipping over the brk instruction, why wouldn't you want that to trigger
> > > a step exception if single-step is enabled?
> >
> > I was asked to do that, see the discussion here:
> >
> > https://www.spinics.net/lists/linux-mm/msg146575.html
> > https://www.spinics.net/lists/linux-mm/msg148215.html
> > https://www.spinics.net/lists/linux-mm/msg148367.html
>
> Aha, but we subsequently fixed the underlying problem in commit
> 9478f1927e6e ("arm64: only advance singlestep for user instruction traps").
> You were on cc, but I appreciate it's not clear that it was related to this.

Sorry, missed that patch.

> Anyway, you can just call arm64_skip_faulting_instruction() as you were
> doing and there's no need for this refactoring.
>
> Please could you spin a new version so that akpm can replace the one which
> he has queued?

Done. Thanks!

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-mm
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 12:25:54 +0000
Message-ID: <CAAeHK+w9NQYRHuNw-fAFDKVjF1L7pxTVRfe=DX1aC1iq5hYt1w () mail ! gmail ! com>
--------------------
On Thu, Dec 6, 2018 at 12:10 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Thu, Dec 06, 2018 at 11:31:43AM +0100, Andrey Konovalov wrote:
> > On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
> > >
> > > On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > > > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > > > memory into the generated code, instead of inserting a callback) generates
> > > > a brk instruction when a tag mismatch is detected.
> > > >
> > > > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > > > immediate value passed to the brk instructions (to extract information
> > > > about the memory access that triggered the mismatch), reads the register
> > > > values (x0 contains the guilty address) and reports the bug.
> > > >
> > > > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > > > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > > > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > > > ---
> > > >  arch/arm64/include/asm/brk-imm.h |  2 +
> > > >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> > > >  include/linux/kasan.h            |  3 ++
> > > >  3 files changed, 71 insertions(+), 2 deletions(-)
> > > >
> > > > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > > > index ed693c5bcec0..2945fe6cd863 100644
> > > > --- a/arch/arm64/include/asm/brk-imm.h
> > > > +++ b/arch/arm64/include/asm/brk-imm.h
> > > > @@ -16,10 +16,12 @@
> > > >   * 0x400: for dynamic BRK instruction
> > > >   * 0x401: for compile time BRK instruction
> > > >   * 0x800: kernel-mode BUG() and WARN() traps
> > > > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> > > >   */
> > > >  #define FAULT_BRK_IMM                        0x100
> > > >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> > > >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> > > >  #define BUG_BRK_IMM                  0x800
> > > > +#define KASAN_BRK_IMM                        0x900
> > > >
> > > >  #endif
> > > > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > > > index 5f4d9acb32f5..04bdc53716ef 100644
> > > > --- a/arch/arm64/kernel/traps.c
> > > > +++ b/arch/arm64/kernel/traps.c
> > > > @@ -35,6 +35,7 @@
> > > >  #include <linux/sizes.h>
> > > >  #include <linux/syscalls.h>
> > > >  #include <linux/mm_types.h>
> > > > +#include <linux/kasan.h>
> > > >
> > > >  #include <asm/atomic.h>
> > > >  #include <asm/bug.h>
> > > > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> > > >       }
> > > >  }
> > > >
> > > > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > >  {
> > > >       regs->pc += size;
> > > > +}
> > > >
> > > > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > > +{
> > > > +     __arm64_skip_faulting_instruction(regs, size);
> > > >       /*
> > > >        * If we were single stepping, we want to get the step exception after
> > > >        * we return from the trap.
> > > > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> > > >       }
> > > >
> > > >       /* If thread survives, skip over the BUG instruction and continue: */
> > > > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > > > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > >
> > > Why do you want to avoid the single-step logic here? Given that we're
> > > skipping over the brk instruction, why wouldn't you want that to trigger
> > > a step exception if single-step is enabled?
> >
> > I was asked to do that, see the discussion here:
> >
> > https://www.spinics.net/lists/linux-mm/msg146575.html
> > https://www.spinics.net/lists/linux-mm/msg148215.html
> > https://www.spinics.net/lists/linux-mm/msg148367.html
>
> Aha, but we subsequently fixed the underlying problem in commit
> 9478f1927e6e ("arm64: only advance singlestep for user instruction traps").
> You were on cc, but I appreciate it's not clear that it was related to this.

Sorry, missed that patch.

> Anyway, you can just call arm64_skip_faulting_instruction() as you were
> doing and there's no need for this refactoring.
>
> Please could you spin a new version so that akpm can replace the one which
> he has queued?

Done. Thanks!

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-doc
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 12:25:54 +0000
Message-ID: <CAAeHK+w9NQYRHuNw-fAFDKVjF1L7pxTVRfe=DX1aC1iq5hYt1w () mail ! gmail ! com>
--------------------
On Thu, Dec 6, 2018 at 12:10 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Thu, Dec 06, 2018 at 11:31:43AM +0100, Andrey Konovalov wrote:
> > On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
> > >
> > > On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > > > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > > > memory into the generated code, instead of inserting a callback) generates
> > > > a brk instruction when a tag mismatch is detected.
> > > >
> > > > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > > > immediate value passed to the brk instructions (to extract information
> > > > about the memory access that triggered the mismatch), reads the register
> > > > values (x0 contains the guilty address) and reports the bug.
> > > >
> > > > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > > > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > > > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > > > ---
> > > >  arch/arm64/include/asm/brk-imm.h |  2 +
> > > >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> > > >  include/linux/kasan.h            |  3 ++
> > > >  3 files changed, 71 insertions(+), 2 deletions(-)
> > > >
> > > > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > > > index ed693c5bcec0..2945fe6cd863 100644
> > > > --- a/arch/arm64/include/asm/brk-imm.h
> > > > +++ b/arch/arm64/include/asm/brk-imm.h
> > > > @@ -16,10 +16,12 @@
> > > >   * 0x400: for dynamic BRK instruction
> > > >   * 0x401: for compile time BRK instruction
> > > >   * 0x800: kernel-mode BUG() and WARN() traps
> > > > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> > > >   */
> > > >  #define FAULT_BRK_IMM                        0x100
> > > >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> > > >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> > > >  #define BUG_BRK_IMM                  0x800
> > > > +#define KASAN_BRK_IMM                        0x900
> > > >
> > > >  #endif
> > > > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > > > index 5f4d9acb32f5..04bdc53716ef 100644
> > > > --- a/arch/arm64/kernel/traps.c
> > > > +++ b/arch/arm64/kernel/traps.c
> > > > @@ -35,6 +35,7 @@
> > > >  #include <linux/sizes.h>
> > > >  #include <linux/syscalls.h>
> > > >  #include <linux/mm_types.h>
> > > > +#include <linux/kasan.h>
> > > >
> > > >  #include <asm/atomic.h>
> > > >  #include <asm/bug.h>
> > > > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> > > >       }
> > > >  }
> > > >
> > > > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > >  {
> > > >       regs->pc += size;
> > > > +}
> > > >
> > > > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > > +{
> > > > +     __arm64_skip_faulting_instruction(regs, size);
> > > >       /*
> > > >        * If we were single stepping, we want to get the step exception after
> > > >        * we return from the trap.
> > > > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> > > >       }
> > > >
> > > >       /* If thread survives, skip over the BUG instruction and continue: */
> > > > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > > > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > >
> > > Why do you want to avoid the single-step logic here? Given that we're
> > > skipping over the brk instruction, why wouldn't you want that to trigger
> > > a step exception if single-step is enabled?
> >
> > I was asked to do that, see the discussion here:
> >
> > https://www.spinics.net/lists/linux-mm/msg146575.html
> > https://www.spinics.net/lists/linux-mm/msg148215.html
> > https://www.spinics.net/lists/linux-mm/msg148367.html
>
> Aha, but we subsequently fixed the underlying problem in commit
> 9478f1927e6e ("arm64: only advance singlestep for user instruction traps").
> You were on cc, but I appreciate it's not clear that it was related to this.

Sorry, missed that patch.

> Anyway, you can just call arm64_skip_faulting_instruction() as you were
> doing and there's no need for this refactoring.
>
> Please could you spin a new version so that akpm can replace the one which
> he has queued?

Done. Thanks!
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kernel
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 12:25:54 +0000
Message-ID: <CAAeHK+w9NQYRHuNw-fAFDKVjF1L7pxTVRfe=DX1aC1iq5hYt1w () mail ! gmail ! com>
--------------------
On Thu, Dec 6, 2018 at 12:10 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Thu, Dec 06, 2018 at 11:31:43AM +0100, Andrey Konovalov wrote:
> > On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
> > >
> > > On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > > > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > > > memory into the generated code, instead of inserting a callback) generates
> > > > a brk instruction when a tag mismatch is detected.
> > > >
> > > > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > > > immediate value passed to the brk instructions (to extract information
> > > > about the memory access that triggered the mismatch), reads the register
> > > > values (x0 contains the guilty address) and reports the bug.
> > > >
> > > > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > > > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > > > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > > > ---
> > > >  arch/arm64/include/asm/brk-imm.h |  2 +
> > > >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> > > >  include/linux/kasan.h            |  3 ++
> > > >  3 files changed, 71 insertions(+), 2 deletions(-)
> > > >
> > > > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > > > index ed693c5bcec0..2945fe6cd863 100644
> > > > --- a/arch/arm64/include/asm/brk-imm.h
> > > > +++ b/arch/arm64/include/asm/brk-imm.h
> > > > @@ -16,10 +16,12 @@
> > > >   * 0x400: for dynamic BRK instruction
> > > >   * 0x401: for compile time BRK instruction
> > > >   * 0x800: kernel-mode BUG() and WARN() traps
> > > > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> > > >   */
> > > >  #define FAULT_BRK_IMM                        0x100
> > > >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> > > >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> > > >  #define BUG_BRK_IMM                  0x800
> > > > +#define KASAN_BRK_IMM                        0x900
> > > >
> > > >  #endif
> > > > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > > > index 5f4d9acb32f5..04bdc53716ef 100644
> > > > --- a/arch/arm64/kernel/traps.c
> > > > +++ b/arch/arm64/kernel/traps.c
> > > > @@ -35,6 +35,7 @@
> > > >  #include <linux/sizes.h>
> > > >  #include <linux/syscalls.h>
> > > >  #include <linux/mm_types.h>
> > > > +#include <linux/kasan.h>
> > > >
> > > >  #include <asm/atomic.h>
> > > >  #include <asm/bug.h>
> > > > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> > > >       }
> > > >  }
> > > >
> > > > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > >  {
> > > >       regs->pc += size;
> > > > +}
> > > >
> > > > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > > +{
> > > > +     __arm64_skip_faulting_instruction(regs, size);
> > > >       /*
> > > >        * If we were single stepping, we want to get the step exception after
> > > >        * we return from the trap.
> > > > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> > > >       }
> > > >
> > > >       /* If thread survives, skip over the BUG instruction and continue: */
> > > > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > > > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > >
> > > Why do you want to avoid the single-step logic here? Given that we're
> > > skipping over the brk instruction, why wouldn't you want that to trigger
> > > a step exception if single-step is enabled?
> >
> > I was asked to do that, see the discussion here:
> >
> > https://www.spinics.net/lists/linux-mm/msg146575.html
> > https://www.spinics.net/lists/linux-mm/msg148215.html
> > https://www.spinics.net/lists/linux-mm/msg148367.html
>
> Aha, but we subsequently fixed the underlying problem in commit
> 9478f1927e6e ("arm64: only advance singlestep for user instruction traps").
> You were on cc, but I appreciate it's not clear that it was related to this.

Sorry, missed that patch.

> Anyway, you can just call arm64_skip_faulting_instruction() as you were
> doing and there's no need for this refactoring.
>
> Please could you spin a new version so that akpm can replace the one which
> he has queued?

Done. Thanks!
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kbuild
Subject: Re: [PATCH v12 20/25] kasan, arm64: add brk handler for inline instrumentation
Date: Thu, 06 Dec 2018 12:25:54 +0000
Message-ID: <CAAeHK+w9NQYRHuNw-fAFDKVjF1L7pxTVRfe=DX1aC1iq5hYt1w () mail ! gmail ! com>
--------------------
On Thu, Dec 6, 2018 at 12:10 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Thu, Dec 06, 2018 at 11:31:43AM +0100, Andrey Konovalov wrote:
> > On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
> > >
> > > On Tue, Nov 27, 2018 at 05:55:38PM +0100, Andrey Konovalov wrote:
> > > > Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
> > > > memory into the generated code, instead of inserting a callback) generates
> > > > a brk instruction when a tag mismatch is detected.
> > > >
> > > > This commit adds a tag-based KASAN specific brk handler, that decodes the
> > > > immediate value passed to the brk instructions (to extract information
> > > > about the memory access that triggered the mismatch), reads the register
> > > > values (x0 contains the guilty address) and reports the bug.
> > > >
> > > > Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
> > > > Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
> > > > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > > > ---
> > > >  arch/arm64/include/asm/brk-imm.h |  2 +
> > > >  arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
> > > >  include/linux/kasan.h            |  3 ++
> > > >  3 files changed, 71 insertions(+), 2 deletions(-)
> > > >
> > > > diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
> > > > index ed693c5bcec0..2945fe6cd863 100644
> > > > --- a/arch/arm64/include/asm/brk-imm.h
> > > > +++ b/arch/arm64/include/asm/brk-imm.h
> > > > @@ -16,10 +16,12 @@
> > > >   * 0x400: for dynamic BRK instruction
> > > >   * 0x401: for compile time BRK instruction
> > > >   * 0x800: kernel-mode BUG() and WARN() traps
> > > > + * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
> > > >   */
> > > >  #define FAULT_BRK_IMM                        0x100
> > > >  #define KGDB_DYN_DBG_BRK_IMM         0x400
> > > >  #define KGDB_COMPILED_DBG_BRK_IMM    0x401
> > > >  #define BUG_BRK_IMM                  0x800
> > > > +#define KASAN_BRK_IMM                        0x900
> > > >
> > > >  #endif
> > > > diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
> > > > index 5f4d9acb32f5..04bdc53716ef 100644
> > > > --- a/arch/arm64/kernel/traps.c
> > > > +++ b/arch/arm64/kernel/traps.c
> > > > @@ -35,6 +35,7 @@
> > > >  #include <linux/sizes.h>
> > > >  #include <linux/syscalls.h>
> > > >  #include <linux/mm_types.h>
> > > > +#include <linux/kasan.h>
> > > >
> > > >  #include <asm/atomic.h>
> > > >  #include <asm/bug.h>
> > > > @@ -284,10 +285,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
> > > >       }
> > > >  }
> > > >
> > > > -void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > > +void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > >  {
> > > >       regs->pc += size;
> > > > +}
> > > >
> > > > +void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
> > > > +{
> > > > +     __arm64_skip_faulting_instruction(regs, size);
> > > >       /*
> > > >        * If we were single stepping, we want to get the step exception after
> > > >        * we return from the trap.
> > > > @@ -959,7 +964,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
> > > >       }
> > > >
> > > >       /* If thread survives, skip over the BUG instruction and continue: */
> > > > -     arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > > > +     __arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
> > >
> > > Why do you want to avoid the single-step logic here? Given that we're
> > > skipping over the brk instruction, why wouldn't you want that to trigger
> > > a step exception if single-step is enabled?
> >
> > I was asked to do that, see the discussion here:
> >
> > https://www.spinics.net/lists/linux-mm/msg146575.html
> > https://www.spinics.net/lists/linux-mm/msg148215.html
> > https://www.spinics.net/lists/linux-mm/msg148367.html
>
> Aha, but we subsequently fixed the underlying problem in commit
> 9478f1927e6e ("arm64: only advance singlestep for user instruction traps").
> You were on cc, but I appreciate it's not clear that it was related to this.

Sorry, missed that patch.

> Anyway, you can just call arm64_skip_faulting_instruction() as you were
> doing and there's no need for this refactoring.
>
> Please could you spin a new version so that akpm can replace the one which
> he has queued?

Done. Thanks!
================================================================================


################################################################################

=== Thread: [PATCH v12 21/25] kasan, mm, arm64: tag non slab memory allocated via pagealloc ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 21/25] kasan, mm, arm64: tag non slab memory allocated via pagealloc
Date: Tue, 27 Nov 2018 16:55:39 +0000
Message-ID: <6c1004acf28880f6a5cc7d2f974ba08adb2853ea.1543337629.git.andreyknvl () google ! com>
--------------------
Tag-based KASAN doesn't check memory accesses through pointers tagged with
0xff. When page_address is used to get pointer to memory that corresponds
to some page, the tag of the resulting pointer gets set to 0xff, even
though the allocated memory might have been tagged differently.

For slab pages it's impossible to recover the correct tag to return from
page_address, since the page might contain multiple slab objects tagged
with different values, and we can't know in advance which one of them is
going to get accessed. For non slab pages however, we can recover the tag
in page_address, since the whole page was marked with the same tag.

This patch adds tagging to non slab memory allocated with pagealloc. To
set the tag of the pointer returned from page_address, the tag gets stored
to page->flags when the memory gets allocated.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/memory.h   |  8 +++++++-
 include/linux/mm.h                | 29 +++++++++++++++++++++++++++++
 include/linux/page-flags-layout.h | 10 ++++++++++
 mm/cma.c                          | 11 +++++++++++
 mm/kasan/common.c                 | 15 +++++++++++++--
 mm/page_alloc.c                   |  1 +
 mm/slab.c                         |  2 +-
 7 files changed, 72 insertions(+), 4 deletions(-)

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 5fe2353f111b..7db28404609b 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -314,7 +314,13 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define __virt_to_pgoff(kaddr)	(((u64)(kaddr) & ~PAGE_OFFSET) / PAGE_SIZE * sizeof(struct page))
 #define __page_to_voff(kaddr)	(((u64)(kaddr) & ~VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
 
-#define page_to_virt(page)	((void *)((__page_to_voff(page)) | PAGE_OFFSET))
+#define page_to_virt(page)	({					\
+	unsigned long __addr =						\
+		((__page_to_voff(page)) | PAGE_OFFSET);			\
+	__addr = __tag_set(__addr, page_kasan_tag(page));		\
+	((void *)__addr);						\
+})
+
 #define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) | VMEMMAP_START))
 
 #define _virt_addr_valid(kaddr)	pfn_valid((((u64)(kaddr) & ~PAGE_OFFSET) \
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 5411de93a363..b4d01969e700 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -804,6 +804,7 @@ vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf);
 #define NODES_PGOFF		(SECTIONS_PGOFF - NODES_WIDTH)
 #define ZONES_PGOFF		(NODES_PGOFF - ZONES_WIDTH)
 #define LAST_CPUPID_PGOFF	(ZONES_PGOFF - LAST_CPUPID_WIDTH)
+#define KASAN_TAG_PGOFF		(LAST_CPUPID_PGOFF - KASAN_TAG_WIDTH)
 
 /*
  * Define the bit shifts to access each section.  For non-existent
@@ -814,6 +815,7 @@ vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf);
 #define NODES_PGSHIFT		(NODES_PGOFF * (NODES_WIDTH != 0))
 #define ZONES_PGSHIFT		(ZONES_PGOFF * (ZONES_WIDTH != 0))
 #define LAST_CPUPID_PGSHIFT	(LAST_CPUPID_PGOFF * (LAST_CPUPID_WIDTH != 0))
+#define KASAN_TAG_PGSHIFT	(KASAN_TAG_PGOFF * (KASAN_TAG_WIDTH != 0))
 
 /* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */
 #ifdef NODE_NOT_IN_PAGE_FLAGS
@@ -836,6 +838,7 @@ vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf);
 #define NODES_MASK		((1UL << NODES_WIDTH) - 1)
 #define SECTIONS_MASK		((1UL << SECTIONS_WIDTH) - 1)
 #define LAST_CPUPID_MASK	((1UL << LAST_CPUPID_SHIFT) - 1)
+#define KASAN_TAG_MASK		((1UL << KASAN_TAG_WIDTH) - 1)
 #define ZONEID_MASK		((1UL << ZONEID_SHIFT) - 1)
 
 static inline enum zone_type page_zonenum(const struct page *page)
@@ -1101,6 +1104,32 @@ static inline bool cpupid_match_pid(struct task_struct *task, int cpupid)
 }
 #endif /* CONFIG_NUMA_BALANCING */
 
+#ifdef CONFIG_KASAN_SW_TAGS
+static inline u8 page_kasan_tag(const struct page *page)
+{
+	return (page->flags >> KASAN_TAG_PGSHIFT) & KASAN_TAG_MASK;
+}
+
+static inline void page_kasan_tag_set(struct page *page, u8 tag)
+{
+	page->flags &= ~(KASAN_TAG_MASK << KASAN_TAG_PGSHIFT);
+	page->flags |= (tag & KASAN_TAG_MASK) << KASAN_TAG_PGSHIFT;
+}
+
+static inline void page_kasan_tag_reset(struct page *page)
+{
+	page_kasan_tag_set(page, 0xff);
+}
+#else
+static inline u8 page_kasan_tag(const struct page *page)
+{
+	return 0xff;
+}
+
+static inline void page_kasan_tag_set(struct page *page, u8 tag) { }
+static inline void page_kasan_tag_reset(struct page *page) { }
+#endif
+
 static inline struct zone *page_zone(const struct page *page)
 {
 	return &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)];
diff --git a/include/linux/page-flags-layout.h b/include/linux/page-flags-layout.h
index 7ec86bf31ce4..1dda31825ec4 100644
--- a/include/linux/page-flags-layout.h
+++ b/include/linux/page-flags-layout.h
@@ -82,6 +82,16 @@
 #define LAST_CPUPID_WIDTH 0
 #endif
 
+#ifdef CONFIG_KASAN_SW_TAGS
+#define KASAN_TAG_WIDTH 8
+#if SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH+LAST_CPUPID_WIDTH+KASAN_TAG_WIDTH \
+	> BITS_PER_LONG - NR_PAGEFLAGS
+#error "KASAN: not enough bits in page flags for tag"
+#endif
+#else
+#define KASAN_TAG_WIDTH 0
+#endif
+
 /*
  * We are going to use the flags for the page to node mapping if its in
  * there.  This includes the case where there is no node, so it is implicit.
diff --git a/mm/cma.c b/mm/cma.c
index 4cb76121a3ab..c7b39dd3b4f6 100644
--- a/mm/cma.c
+++ b/mm/cma.c
@@ -407,6 +407,7 @@ struct page *cma_alloc(struct cma *cma, size_t count, unsigned int align,
 	unsigned long pfn = -1;
 	unsigned long start = 0;
 	unsigned long bitmap_maxno, bitmap_no, bitmap_count;
+	size_t i;
 	struct page *page = NULL;
 	int ret = -ENOMEM;
 
@@ -466,6 +467,16 @@ struct page *cma_alloc(struct cma *cma, size_t count, unsigned int align,
 
 	trace_cma_alloc(pfn, page, count, align);
 
+	/*
+	 * CMA can allocate multiple page blocks, which results in different
+	 * blocks being marked with different tags. Reset the tags to ignore
+	 * those page blocks.
+	 */
+	if (page) {
+		for (i = 0; i < count; i++)
+			page_kasan_tag_reset(page + i);
+	}
+
 	if (ret && !no_warn) {
 		pr_err("%s: alloc failed, req-size: %zu pages, ret: %d\n",
 			__func__, count, ret);
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 27f0cae336c9..195ca385cf7a 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -220,8 +220,15 @@ void kasan_unpoison_stack_above_sp_to(const void *watermark)
 
 void kasan_alloc_pages(struct page *page, unsigned int order)
 {
+	u8 tag;
+	unsigned long i;
+
 	if (unlikely(PageHighMem(page)))
 		return;
+
+	tag = random_tag();
+	for (i = 0; i < (1 << order); i++)
+		page_kasan_tag_set(page + i, tag);
 	kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
 }
 
@@ -319,6 +326,10 @@ struct kasan_free_meta *get_free_info(struct kmem_cache *cache,
 
 void kasan_poison_slab(struct page *page)
 {
+	unsigned long i;
+
+	for (i = 0; i < (1 << compound_order(page)); i++)
+		page_kasan_tag_reset(page + i);
 	kasan_poison_shadow(page_address(page),
 			PAGE_SIZE << compound_order(page),
 			KASAN_KMALLOC_REDZONE);
@@ -517,7 +528,7 @@ void kasan_poison_kfree(void *ptr, unsigned long ip)
 	page = virt_to_head_page(ptr);
 
 	if (unlikely(!PageSlab(page))) {
-		if (reset_tag(ptr) != page_address(page)) {
+		if (ptr != page_address(page)) {
 			kasan_report_invalid_free(ptr, ip);
 			return;
 		}
@@ -530,7 +541,7 @@ void kasan_poison_kfree(void *ptr, unsigned long ip)
 
 void kasan_kfree_large(void *ptr, unsigned long ip)
 {
-	if (reset_tag(ptr) != page_address(virt_to_head_page(ptr)))
+	if (ptr != page_address(virt_to_head_page(ptr)))
 		kasan_report_invalid_free(ptr, ip);
 	/* The object will be poisoned by page_alloc. */
 }
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 6847177dc4a1..5b2a1b6ef1be 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1183,6 +1183,7 @@ static void __meminit __init_single_page(struct page *page, unsigned long pfn,
 	init_page_count(page);
 	page_mapcount_reset(page);
 	page_cpupid_reset_last(page);
+	page_kasan_tag_reset(page);
 
 	INIT_LIST_HEAD(&page->lru);
 #ifdef WANT_PAGE_VIRTUAL
diff --git a/mm/slab.c b/mm/slab.c
index d2f827316dfc..d747433ecdbb 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -2357,7 +2357,7 @@ static void *alloc_slabmgmt(struct kmem_cache *cachep,
 	void *freelist;
 	void *addr = page_address(page);
 
-	page->s_mem = addr + colour_off;
+	page->s_mem = kasan_reset_tag(addr) + colour_off;
 	page->active = 0;
 
 	if (OBJFREELIST_SLAB(cachep))
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 22/25] kasan: add __must_check annotations to kasan hooks ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 22/25] kasan: add __must_check annotations to kasan hooks
Date: Tue, 27 Nov 2018 16:55:40 +0000
Message-ID: <6d8c6f59c5b5a3dde569f893ecf3b56e58030ba1.1543337629.git.andreyknvl () google ! com>
--------------------
This patch adds __must_check annotations to kasan hooks that return a
pointer to make sure that a tagged pointer always gets propagated.

Suggested-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 include/linux/kasan.h | 16 ++++++++++------
 mm/kasan/common.c     | 15 +++++++++------
 2 files changed, 19 insertions(+), 12 deletions(-)

diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index 8da7b7a4397a..b40ea104dd36 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -49,16 +49,20 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 void kasan_poison_slab(struct page *page);
 void kasan_unpoison_object_data(struct kmem_cache *cache, void *object);
 void kasan_poison_object_data(struct kmem_cache *cache, void *object);
-void *kasan_init_slab_obj(struct kmem_cache *cache, const void *object);
+void * __must_check kasan_init_slab_obj(struct kmem_cache *cache,
+					const void *object);
 
-void *kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags);
+void * __must_check kasan_kmalloc_large(const void *ptr, size_t size,
+						gfp_t flags);
 void kasan_kfree_large(void *ptr, unsigned long ip);
 void kasan_poison_kfree(void *ptr, unsigned long ip);
-void *kasan_kmalloc(struct kmem_cache *s, const void *object, size_t size,
-		  gfp_t flags);
-void *kasan_krealloc(const void *object, size_t new_size, gfp_t flags);
+void * __must_check kasan_kmalloc(struct kmem_cache *s, const void *object,
+					size_t size, gfp_t flags);
+void * __must_check kasan_krealloc(const void *object, size_t new_size,
+					gfp_t flags);
 
-void *kasan_slab_alloc(struct kmem_cache *s, void *object, gfp_t flags);
+void * __must_check kasan_slab_alloc(struct kmem_cache *s, void *object,
+					gfp_t flags);
 bool kasan_slab_free(struct kmem_cache *s, void *object, unsigned long ip);
 
 struct kasan_cache {
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 195ca385cf7a..1144e741feb6 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -373,7 +373,8 @@ static u8 assign_tag(struct kmem_cache *cache, const void *object, bool new)
 #endif
 }
 
-void *kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
+void * __must_check kasan_init_slab_obj(struct kmem_cache *cache,
+						const void *object)
 {
 	struct kasan_alloc_meta *alloc_info;
 
@@ -389,7 +390,8 @@ void *kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
 	return (void *)object;
 }
 
-void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t flags)
+void * __must_check kasan_slab_alloc(struct kmem_cache *cache, void *object,
+					gfp_t flags)
 {
 	return kasan_kmalloc(cache, object, cache->object_size, flags);
 }
@@ -449,8 +451,8 @@ bool kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
 	return __kasan_slab_free(cache, object, ip, true);
 }
 
-void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
-		   gfp_t flags)
+void * __must_check kasan_kmalloc(struct kmem_cache *cache, const void *object,
+					size_t size, gfp_t flags)
 {
 	unsigned long redzone_start;
 	unsigned long redzone_end;
@@ -482,7 +484,8 @@ void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 }
 EXPORT_SYMBOL(kasan_kmalloc);
 
-void *kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
+void * __must_check kasan_kmalloc_large(const void *ptr, size_t size,
+						gfp_t flags)
 {
 	struct page *page;
 	unsigned long redzone_start;
@@ -506,7 +509,7 @@ void *kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
 	return (void *)ptr;
 }
 
-void *kasan_krealloc(const void *object, size_t size, gfp_t flags)
+void * __must_check kasan_krealloc(const void *object, size_t size, gfp_t flags)
 {
 	struct page *page;
 
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kbuild
Subject: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
Date: Tue, 27 Nov 2018 16:55:41 +0000
Message-ID: <996c9b3898bb3c5de977d00215ddc4bf8cf154c1.1543337629.git.andreyknvl () google ! com>
--------------------
Now, that all the necessary infrastructure code has been introduced,
select HAVE_ARCH_KASAN_SW_TAGS for arm64 to enable software tag-based
KASAN mode.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/Kconfig | 1 +
 1 file changed, 1 insertion(+)

diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index 787d7850e064..8b331dcfb48e 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -111,6 +111,7 @@ config ARM64
 	select HAVE_ARCH_JUMP_LABEL
 	select HAVE_ARCH_JUMP_LABEL_RELATIVE
 	select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
+	select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
 	select HAVE_ARCH_KGDB
 	select HAVE_ARCH_MMAP_RND_BITS
 	select HAVE_ARCH_MMAP_RND_COMPAT_BITS if COMPAT
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
Date: Tue, 27 Nov 2018 16:55:41 +0000
Message-ID: <996c9b3898bb3c5de977d00215ddc4bf8cf154c1.1543337629.git.andreyknvl () google ! com>
--------------------
Now, that all the necessary infrastructure code has been introduced,
select HAVE_ARCH_KASAN_SW_TAGS for arm64 to enable software tag-based
KASAN mode.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/Kconfig | 1 +
 1 file changed, 1 insertion(+)

diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index 787d7850e064..8b331dcfb48e 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -111,6 +111,7 @@ config ARM64
 	select HAVE_ARCH_JUMP_LABEL
 	select HAVE_ARCH_JUMP_LABEL_RELATIVE
 	select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
+	select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
 	select HAVE_ARCH_KGDB
 	select HAVE_ARCH_MMAP_RND_BITS
 	select HAVE_ARCH_MMAP_RND_COMPAT_BITS if COMPAT
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-arm-kernel
Subject: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
Date: Tue, 27 Nov 2018 16:55:41 +0000
Message-ID: <996c9b3898bb3c5de977d00215ddc4bf8cf154c1.1543337629.git.andreyknvl () google ! com>
--------------------
Now, that all the necessary infrastructure code has been introduced,
select HAVE_ARCH_KASAN_SW_TAGS for arm64 to enable software tag-based
KASAN mode.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/Kconfig | 1 +
 1 file changed, 1 insertion(+)

diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index 787d7850e064..8b331dcfb48e 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -111,6 +111,7 @@ config ARM64
 	select HAVE_ARCH_JUMP_LABEL
 	select HAVE_ARCH_JUMP_LABEL_RELATIVE
 	select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
+	select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
 	select HAVE_ARCH_KGDB
 	select HAVE_ARCH_MMAP_RND_BITS
 	select HAVE_ARCH_MMAP_RND_COMPAT_BITS if COMPAT
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-doc
Subject: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
Date: Tue, 27 Nov 2018 16:55:41 +0000
Message-ID: <996c9b3898bb3c5de977d00215ddc4bf8cf154c1.1543337629.git.andreyknvl () google ! com>
--------------------
Now, that all the necessary infrastructure code has been introduced,
select HAVE_ARCH_KASAN_SW_TAGS for arm64 to enable software tag-based
KASAN mode.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/Kconfig | 1 +
 1 file changed, 1 insertion(+)

diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index 787d7850e064..8b331dcfb48e 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -111,6 +111,7 @@ config ARM64
 	select HAVE_ARCH_JUMP_LABEL
 	select HAVE_ARCH_JUMP_LABEL_RELATIVE
 	select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
+	select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
 	select HAVE_ARCH_KGDB
 	select HAVE_ARCH_MMAP_RND_BITS
 	select HAVE_ARCH_MMAP_RND_COMPAT_BITS if COMPAT
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kernel
Subject: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
Date: Tue, 27 Nov 2018 16:55:41 +0000
Message-ID: <996c9b3898bb3c5de977d00215ddc4bf8cf154c1.1543337629.git.andreyknvl () google ! com>
--------------------
Now, that all the necessary infrastructure code has been introduced,
select HAVE_ARCH_KASAN_SW_TAGS for arm64 to enable software tag-based
KASAN mode.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/Kconfig | 1 +
 1 file changed, 1 insertion(+)

diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index 787d7850e064..8b331dcfb48e 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -111,6 +111,7 @@ config ARM64
 	select HAVE_ARCH_JUMP_LABEL
 	select HAVE_ARCH_JUMP_LABEL_RELATIVE
 	select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
+	select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
 	select HAVE_ARCH_KGDB
 	select HAVE_ARCH_MMAP_RND_BITS
 	select HAVE_ARCH_MMAP_RND_COMPAT_BITS if COMPAT
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-doc
Subject: Re: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
Date: Thu, 29 Nov 2018 18:01:35 +0000
Message-ID: <20181129180134.GA4318 () arm ! com>
--------------------
On Tue, Nov 27, 2018 at 05:55:41PM +0100, Andrey Konovalov wrote:
> Now, that all the necessary infrastructure code has been introduced,
> select HAVE_ARCH_KASAN_SW_TAGS for arm64 to enable software tag-based
> KASAN mode.
> 
> Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> ---
>  arch/arm64/Kconfig | 1 +
>  1 file changed, 1 insertion(+)
> 
> diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
> index 787d7850e064..8b331dcfb48e 100644
> --- a/arch/arm64/Kconfig
> +++ b/arch/arm64/Kconfig
> @@ -111,6 +111,7 @@ config ARM64
>  	select HAVE_ARCH_JUMP_LABEL
>  	select HAVE_ARCH_JUMP_LABEL_RELATIVE
>  	select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
> +	select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)

Can you do if HAVE_ARCH_KASAN instead?

Will
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-mm
Subject: Re: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
Date: Thu, 29 Nov 2018 18:01:35 +0000
Message-ID: <20181129180134.GA4318 () arm ! com>
--------------------
On Tue, Nov 27, 2018 at 05:55:41PM +0100, Andrey Konovalov wrote:
> Now, that all the necessary infrastructure code has been introduced,
> select HAVE_ARCH_KASAN_SW_TAGS for arm64 to enable software tag-based
> KASAN mode.
> 
> Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> ---
>  arch/arm64/Kconfig | 1 +
>  1 file changed, 1 insertion(+)
> 
> diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
> index 787d7850e064..8b331dcfb48e 100644
> --- a/arch/arm64/Kconfig
> +++ b/arch/arm64/Kconfig
> @@ -111,6 +111,7 @@ config ARM64
>  	select HAVE_ARCH_JUMP_LABEL
>  	select HAVE_ARCH_JUMP_LABEL_RELATIVE
>  	select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
> +	select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)

Can you do if HAVE_ARCH_KASAN instead?

Will

================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-kbuild
Subject: Re: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
Date: Thu, 29 Nov 2018 18:01:35 +0000
Message-ID: <20181129180134.GA4318 () arm ! com>
--------------------
On Tue, Nov 27, 2018 at 05:55:41PM +0100, Andrey Konovalov wrote:
> Now, that all the necessary infrastructure code has been introduced,
> select HAVE_ARCH_KASAN_SW_TAGS for arm64 to enable software tag-based
> KASAN mode.
> 
> Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> ---
>  arch/arm64/Kconfig | 1 +
>  1 file changed, 1 insertion(+)
> 
> diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
> index 787d7850e064..8b331dcfb48e 100644
> --- a/arch/arm64/Kconfig
> +++ b/arch/arm64/Kconfig
> @@ -111,6 +111,7 @@ config ARM64
>  	select HAVE_ARCH_JUMP_LABEL
>  	select HAVE_ARCH_JUMP_LABEL_RELATIVE
>  	select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
> +	select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)

Can you do if HAVE_ARCH_KASAN instead?

Will
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-sparse
Subject: Re: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
Date: Thu, 29 Nov 2018 18:01:35 +0000
Message-ID: <20181129180134.GA4318 () arm ! com>
--------------------
On Tue, Nov 27, 2018 at 05:55:41PM +0100, Andrey Konovalov wrote:
> Now, that all the necessary infrastructure code has been introduced,
> select HAVE_ARCH_KASAN_SW_TAGS for arm64 to enable software tag-based
> KASAN mode.
> 
> Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> ---
>  arch/arm64/Kconfig | 1 +
>  1 file changed, 1 insertion(+)
> 
> diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
> index 787d7850e064..8b331dcfb48e 100644
> --- a/arch/arm64/Kconfig
> +++ b/arch/arm64/Kconfig
> @@ -111,6 +111,7 @@ config ARM64
>  	select HAVE_ARCH_JUMP_LABEL
>  	select HAVE_ARCH_JUMP_LABEL_RELATIVE
>  	select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
> +	select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)

Can you do if HAVE_ARCH_KASAN instead?

Will
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
Date: Thu, 06 Dec 2018 10:19:57 +0000
Message-ID: <CAAeHK+yN6Jrk6G6OjbkMHwCxkuQHfrz8PXtPTUdrfsHaru_eKA () mail ! gmail ! com>
--------------------
On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Tue, Nov 27, 2018 at 05:55:41PM +0100, Andrey Konovalov wrote:
> > Now, that all the necessary infrastructure code has been introduced,
> > select HAVE_ARCH_KASAN_SW_TAGS for arm64 to enable software tag-based
> > KASAN mode.
> >
> > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > ---
> >  arch/arm64/Kconfig | 1 +
> >  1 file changed, 1 insertion(+)
> >
> > diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
> > index 787d7850e064..8b331dcfb48e 100644
> > --- a/arch/arm64/Kconfig
> > +++ b/arch/arm64/Kconfig
> > @@ -111,6 +111,7 @@ config ARM64
> >       select HAVE_ARCH_JUMP_LABEL
> >       select HAVE_ARCH_JUMP_LABEL_RELATIVE
> >       select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
> > +     select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
>
> Can you do if HAVE_ARCH_KASAN instead?

Will do in v13, thanks!

>
> Will

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kbuild
Subject: Re: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
Date: Thu, 06 Dec 2018 10:19:57 +0000
Message-ID: <CAAeHK+yN6Jrk6G6OjbkMHwCxkuQHfrz8PXtPTUdrfsHaru_eKA () mail ! gmail ! com>
--------------------
On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Tue, Nov 27, 2018 at 05:55:41PM +0100, Andrey Konovalov wrote:
> > Now, that all the necessary infrastructure code has been introduced,
> > select HAVE_ARCH_KASAN_SW_TAGS for arm64 to enable software tag-based
> > KASAN mode.
> >
> > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > ---
> >  arch/arm64/Kconfig | 1 +
> >  1 file changed, 1 insertion(+)
> >
> > diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
> > index 787d7850e064..8b331dcfb48e 100644
> > --- a/arch/arm64/Kconfig
> > +++ b/arch/arm64/Kconfig
> > @@ -111,6 +111,7 @@ config ARM64
> >       select HAVE_ARCH_JUMP_LABEL
> >       select HAVE_ARCH_JUMP_LABEL_RELATIVE
> >       select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
> > +     select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
>
> Can you do if HAVE_ARCH_KASAN instead?

Will do in v13, thanks!

>
> Will
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: Re: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
Date: Thu, 06 Dec 2018 10:19:57 +0000
Message-ID: <CAAeHK+yN6Jrk6G6OjbkMHwCxkuQHfrz8PXtPTUdrfsHaru_eKA () mail ! gmail ! com>
--------------------
On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Tue, Nov 27, 2018 at 05:55:41PM +0100, Andrey Konovalov wrote:
> > Now, that all the necessary infrastructure code has been introduced,
> > select HAVE_ARCH_KASAN_SW_TAGS for arm64 to enable software tag-based
> > KASAN mode.
> >
> > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > ---
> >  arch/arm64/Kconfig | 1 +
> >  1 file changed, 1 insertion(+)
> >
> > diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
> > index 787d7850e064..8b331dcfb48e 100644
> > --- a/arch/arm64/Kconfig
> > +++ b/arch/arm64/Kconfig
> > @@ -111,6 +111,7 @@ config ARM64
> >       select HAVE_ARCH_JUMP_LABEL
> >       select HAVE_ARCH_JUMP_LABEL_RELATIVE
> >       select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
> > +     select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
>
> Can you do if HAVE_ARCH_KASAN instead?

Will do in v13, thanks!

>
> Will
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-mm
Subject: Re: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
Date: Thu, 06 Dec 2018 10:19:57 +0000
Message-ID: <CAAeHK+yN6Jrk6G6OjbkMHwCxkuQHfrz8PXtPTUdrfsHaru_eKA () mail ! gmail ! com>
--------------------
On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Tue, Nov 27, 2018 at 05:55:41PM +0100, Andrey Konovalov wrote:
> > Now, that all the necessary infrastructure code has been introduced,
> > select HAVE_ARCH_KASAN_SW_TAGS for arm64 to enable software tag-based
> > KASAN mode.
> >
> > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > ---
> >  arch/arm64/Kconfig | 1 +
> >  1 file changed, 1 insertion(+)
> >
> > diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
> > index 787d7850e064..8b331dcfb48e 100644
> > --- a/arch/arm64/Kconfig
> > +++ b/arch/arm64/Kconfig
> > @@ -111,6 +111,7 @@ config ARM64
> >       select HAVE_ARCH_JUMP_LABEL
> >       select HAVE_ARCH_JUMP_LABEL_RELATIVE
> >       select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
> > +     select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
>
> Can you do if HAVE_ARCH_KASAN instead?

Will do in v13, thanks!

>
> Will

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-doc
Subject: Re: [PATCH v12 23/25] kasan, arm64: select HAVE_ARCH_KASAN_SW_TAGS
Date: Thu, 06 Dec 2018 10:19:57 +0000
Message-ID: <CAAeHK+yN6Jrk6G6OjbkMHwCxkuQHfrz8PXtPTUdrfsHaru_eKA () mail ! gmail ! com>
--------------------
On Thu, Nov 29, 2018 at 7:01 PM Will Deacon <will.deacon@arm.com> wrote:
>
> On Tue, Nov 27, 2018 at 05:55:41PM +0100, Andrey Konovalov wrote:
> > Now, that all the necessary infrastructure code has been introduced,
> > select HAVE_ARCH_KASAN_SW_TAGS for arm64 to enable software tag-based
> > KASAN mode.
> >
> > Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> > ---
> >  arch/arm64/Kconfig | 1 +
> >  1 file changed, 1 insertion(+)
> >
> > diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
> > index 787d7850e064..8b331dcfb48e 100644
> > --- a/arch/arm64/Kconfig
> > +++ b/arch/arm64/Kconfig
> > @@ -111,6 +111,7 @@ config ARM64
> >       select HAVE_ARCH_JUMP_LABEL
> >       select HAVE_ARCH_JUMP_LABEL_RELATIVE
> >       select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
> > +     select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
>
> Can you do if HAVE_ARCH_KASAN instead?

Will do in v13, thanks!

>
> Will
================================================================================


################################################################################

=== Thread: [PATCH v12 25/25] kasan: add SPDX-License-Identifier mark to source files ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v12 25/25] kasan: add SPDX-License-Identifier mark to source files
Date: Tue, 27 Nov 2018 16:55:43 +0000
Message-ID: <8e26a568b12ea02e11c35b681f3c36aff2fc1d77.1543337629.git.andreyknvl () google ! com>
--------------------
This patch adds a "SPDX-License-Identifier: GPL-2.0" mark to all source
files under mm/kasan.

Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/common.c         | 1 +
 mm/kasan/generic.c        | 1 +
 mm/kasan/generic_report.c | 1 +
 mm/kasan/init.c           | 1 +
 mm/kasan/quarantine.c     | 1 +
 mm/kasan/report.c         | 1 +
 mm/kasan/tags.c           | 1 +
 mm/kasan/tags_report.c    | 1 +
 8 files changed, 8 insertions(+)

diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 1144e741feb6..03d5d1374ca7 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains common generic and tag-based KASAN code.
  *
diff --git a/mm/kasan/generic.c b/mm/kasan/generic.c
index b8de6d33c55c..ccb6207276e3 100644
--- a/mm/kasan/generic.c
+++ b/mm/kasan/generic.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains core generic KASAN code.
  *
diff --git a/mm/kasan/generic_report.c b/mm/kasan/generic_report.c
index a4604cceae59..5e12035888f2 100644
--- a/mm/kasan/generic_report.c
+++ b/mm/kasan/generic_report.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains generic KASAN specific error reporting code.
  *
diff --git a/mm/kasan/init.c b/mm/kasan/init.c
index 2b21d3717d62..34afad56497b 100644
--- a/mm/kasan/init.c
+++ b/mm/kasan/init.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains some kasan initialization code.
  *
diff --git a/mm/kasan/quarantine.c b/mm/kasan/quarantine.c
index b209dbaefde8..57334ef2d7ef 100644
--- a/mm/kasan/quarantine.c
+++ b/mm/kasan/quarantine.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * KASAN quarantine.
  *
diff --git a/mm/kasan/report.c b/mm/kasan/report.c
index 214d85035f99..ca9418fe9232 100644
--- a/mm/kasan/report.c
+++ b/mm/kasan/report.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains common generic and tag-based KASAN error reporting code.
  *
diff --git a/mm/kasan/tags.c b/mm/kasan/tags.c
index 1d1b79350e28..0777649e07c4 100644
--- a/mm/kasan/tags.c
+++ b/mm/kasan/tags.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains core tag-based KASAN code.
  *
diff --git a/mm/kasan/tags_report.c b/mm/kasan/tags_report.c
index 573c51d20d09..8eaf5f722271 100644
--- a/mm/kasan/tags_report.c
+++ b/mm/kasan/tags_report.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains tag-based KASAN specific error reporting code.
  *
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

================================================================================


################################################################################

=== Thread: [PATCH v13 19/25] kasan: add hooks implementation for tag-based mode ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: Re: [PATCH v13 19/25] kasan: add hooks implementation for tag-based mode
Date: Tue, 18 Dec 2018 13:31:37 +0000
Message-ID: <CAAeHK+yiGcu4u7rvugfY7wGUiHug7C1wvSuygXN6L5-wWMHtyw () mail ! gmail ! com>
--------------------
On Mon, Dec 17, 2018 at 9:38 PM Andrew Morton <akpm@linux-foundation.org> wrote:
>
> On Mon, 17 Dec 2018 20:33:42 +0100 Andrey Konovalov <andreyknvl@google.com> wrote:
>
> > > Curiosity, did you try your patches with SLUB red zoning enabled?
> > > Since the area used for the Redzone is just after the payload, aligning the
> > > object_size independently from the allocator could have side effects, at least
> > > if I understand well how the mechanism works.
> > >
> > > Setting ARCH_SLAB_MINALIGN should avoid this as well.
> > >
> > > What do you think?
> >
> > Sounds good to me.
> >
> > Andrew, how should proceed with this? Send another fixup patch or
> > resend the whole series?
>
> It depends on how extensive the changes are.  I prefer a fixup, but at
> some point it's time to drop it all and start again.

The fixup in only a few lines, so I just sent it as a separate patch
named "[PATCH mm] kasan, arm64: use ARCH_SLAB_MINALIGN instead of
manual aligning".

Thanks!
================================================================================


################################################################################

=== Thread: [PATCH v2 03/13] autodoc: extract doc from the C files ===

From: Randy Dunlap <rdunlap () infradead ! org>
To: linux-sparse
Subject: Re: [PATCH v2 03/13] autodoc: extract doc from the C files
Date: Sat, 19 May 2018 22:30:12 +0000
Message-ID: <e14bff84-7a7a-e724-bb0d-a6fba767083a () infradead ! org>
--------------------
On 05/19/18 06:05, Luc Van Oostenryck wrote:
> Add a tool which parse comment from source files and
> extract kerneldoc-like documentation from them.
> 
> Note: this is rather quite crude, support only generic 'blocs'
>       and doc for functions; it has no support for anything else.
> 
> Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> ---
>  .gitignore                   |   1 +
>  Documentation/sphinx/cdoc.py | 200 +++++++++++++++++++++++++++++++++++
>  2 files changed, 201 insertions(+)
>  create mode 100755 Documentation/sphinx/cdoc.py
> 
> diff --git a/Documentation/sphinx/cdoc.py b/Documentation/sphinx/cdoc.py
> new file mode 100755
> index 000000000..4eae75e89
> --- /dev/null
> +++ b/Documentation/sphinx/cdoc.py
> @@ -0,0 +1,200 @@
> +#!/usr/bin/env python
> +# SPDX_License-Identifier: MIT
> +#
> +# Copyright (C) 2018 Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> +#
> +
> +"""
> +///
> +// Sparse source files may contain documentation inside block-comments
> +// specifically formatted::
> +//
> +// 	///
> +// 	// Here is some doc
> +// 	// and here is some more.
> +//
> +// More precisely, a doc-block begins with a line containting only ``///``

                                                     containing

> +// and continues with lines beginning by ``//`` followed by either a space,
> +// a tab or nothing, the first space after ``//`` is ignored.
> +//
> +// For functions, some additional syntax must be respected inside the
> +// block-comment::
> +//
> +// 	///
> +// 	// <mandatory short one-line description>
> +// 	// <optional blanck line>

                     blank

> +// 	// @<1st paramater's name>: <description>

                 parameter's

> +// 	// @<2nd parameter's name>: ...
> +// 	// @return: <description> (absent for void functions)
> +// 	// <optional blank line>
> +// 	// <optional long multi-line description>
> +// 	int somefunction(void *ptr, int count);
> +//
> +// Inside the description fields, parameter's names can be referenced
> +// by using ``@<parameter name>``. A function doc-block must directly

                                                           must directly precede

> +// the function it documents. This function can span multiple lines and
> +// can either be a function prototype (ending with ``;``) or a
> +// function definition.
> +//
> +// Some future versions will also allow to document structures, unions,
> +// enums, typedefs and variables.
> +//



-- 
~Randy
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2 07/13] autodoc: add markup to argument's references ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v2 07/13] autodoc: add markup to argument's references
Date: Sat, 19 May 2018 13:05:56 +0000
Message-ID: <20180519130602.90096-8-luc.vanoostenryck () gmail ! com>
--------------------
The syntax for a parameter is '@<name>'. Let also use this when
referencing a parameter name in another section (other parameters
or long description) and mark these references in bold (the same
as the parameters themseelves are presented).

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/sphinx/cdoc.py | 7 +++++++
 1 file changed, 7 insertions(+)

diff --git a/Documentation/sphinx/cdoc.py b/Documentation/sphinx/cdoc.py
index 7137b2a67..410a55ea2 100755
--- a/Documentation/sphinx/cdoc.py
+++ b/Documentation/sphinx/cdoc.py
@@ -183,6 +183,11 @@ def process_file(f):
 
 	return docs
 
+def decorate(l):
+	# type: (str) -> str
+	l = re.sub(r"@(\w+)", "**\\1**", l)
+	return l
+
 def convert_to_rst(info):
 	# type: (Dict[str, Any]) -> List[Tuple[int, str]]
 	lst = []
@@ -217,6 +222,7 @@ def convert_to_rst(info):
 			for (n, name, l) in info.get('tags', []):
 				if name != 'return':
 					name = 'param ' + name
+				l = decorate(l)
 				l = '\t:%s: %s' % (name, l)
 				lst.append((n, l))
 			lst.append((n+1, ''))
@@ -225,6 +231,7 @@ def convert_to_rst(info):
 			n = desc[0]
 			r = ''
 			for l in desc[1:]:
+				l = decorate(l)
 				r += '\t' + l + '\n'
 			lst.append((n, r))
 	return lst
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2 08/13] autodoc: doc the doc ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v2 08/13] autodoc: doc the doc
Date: Sat, 19 May 2018 13:05:57 +0000
Message-ID: <20180519130602.90096-9-luc.vanoostenryck () gmail ! com>
--------------------
Add some documentation about the autodoc support and the
syntax to use for API documentation embedded into the code.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/doc-guide.rst | 32 ++++++++++++++++++++++++++++++++
 Documentation/index.rst     |  1 +
 2 files changed, 33 insertions(+)
 create mode 100644 Documentation/doc-guide.rst

diff --git a/Documentation/doc-guide.rst b/Documentation/doc-guide.rst
new file mode 100644
index 000000000..cbc3f842f
--- /dev/null
+++ b/Documentation/doc-guide.rst
@@ -0,0 +1,32 @@
+How to write sparse documentation
+=================================
+
+Introduction
+------------
+
+
+The documentation for Sparse is written in plain text augmented with
+either `reStructuredText`_ (.rst) or `MarkDown`_ (.md) markup. These
+files can be oarganized hiearchically, allowing a good structuration
+of the documentation.
+Sparse uses `Sphinx`_ to format this documentation in several formats,
+like HTML of PDF.
+
+All documentation related files are in the ``Documentation/`` directory.
+In this directory you can use ``make html`` or ``make man`` to generate
+the documentation in HTML or manpage format. The generated files can then
+be found in the ``build/`` sub-directory.
+
+The root of the documentation is the file ``index.rst`` which mainly
+list the name of the files with real documentation.
+
+.. _Sphinx: http://www.sphinx-doc.org/
+.. _reStructuredText: http://docutils.sourceforge.net/rst.html
+.. _MarkDown: https://en.wikipedia.org/wiki/Markdown
+
+Autodoc
+-------
+
+.. highlight:: none
+.. c:autodoc:: Documentation/sphinx/cdoc.py
+
diff --git a/Documentation/index.rst b/Documentation/index.rst
index 919cf4821..da006710e 100644
--- a/Documentation/index.rst
+++ b/Documentation/index.rst
@@ -22,6 +22,7 @@ Developer documentation
    dev-options
    IR
    api
+   doc-guide
 
 How to contribute
 -----------------
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Randy Dunlap <rdunlap () infradead ! org>
To: linux-sparse
Subject: Re: [PATCH v2 08/13] autodoc: doc the doc
Date: Sat, 19 May 2018 22:39:01 +0000
Message-ID: <d0b7890a-1b48-b46c-c6c5-60a36e92703e () infradead ! org>
--------------------
On 05/19/18 06:05, Luc Van Oostenryck wrote:
> Add some documentation about the autodoc support and the
> syntax to use for API documentation embedded into the code.
> 
> Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> ---
>  Documentation/doc-guide.rst | 32 ++++++++++++++++++++++++++++++++
>  Documentation/index.rst     |  1 +
>  2 files changed, 33 insertions(+)
>  create mode 100644 Documentation/doc-guide.rst
> 
> diff --git a/Documentation/doc-guide.rst b/Documentation/doc-guide.rst
> new file mode 100644
> index 000000000..cbc3f842f
> --- /dev/null
> +++ b/Documentation/doc-guide.rst
> @@ -0,0 +1,32 @@
> +How to write sparse documentation
> +=================================
> +
> +Introduction
> +------------
> +
> +
> +The documentation for Sparse is written in plain text augmented with
> +either `reStructuredText`_ (.rst) or `MarkDown`_ (.md) markup. These
> +files can be oarganized hiearchically, allowing a good structuration

                organized hierarchically,                 structuring (?)

> +of the documentation.
> +Sparse uses `Sphinx`_ to format this documentation in several formats,
> +like HTML of PDF.

             or

> +
> +All documentation related files are in the ``Documentation/`` directory.
> +In this directory you can use ``make html`` or ``make man`` to generate
> +the documentation in HTML or manpage format. The generated files can then
> +be found in the ``build/`` sub-directory.
> +
> +The root of the documentation is the file ``index.rst`` which mainly
> +list the name of the files with real documentation.

   lists the names

> +
> +.. _Sphinx: http://www.sphinx-doc.org/
> +.. _reStructuredText: http://docutils.sourceforge.net/rst.html
> +.. _MarkDown: https://en.wikipedia.org/wiki/Markdown
> +
> +Autodoc
> +-------
> +
> +.. highlight:: none
> +.. c:autodoc:: Documentation/sphinx/cdoc.py
> +


-- 
~Randy
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v2 08/13] autodoc: doc the doc
Date: Sun, 20 May 2018 10:47:03 +0000
Message-ID: <20180520104702.npyegslsx6s5nhc7 () ltop ! local>
--------------------
On Sat, May 19, 2018 at 03:39:01PM -0700, Randy Dunlap wrote:
> On 05/19/18 06:05, Luc Van Oostenryck wrote:
> > +
> > +The documentation for Sparse is written in plain text augmented with
> > +either `reStructuredText`_ (.rst) or `MarkDown`_ (.md) markup. These
> > +files can be oarganized hiearchically, allowing a good structuration
> 
>                 organized hierarchically,                 structuring (?)


Thanks a lot for spotting the typos here and in the other patches.

-- Luc 
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2 1/2] doc: options.md is for development ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v2 1/2] doc: options.md is for development
Date: Mon, 19 Mar 2018 17:25:33 +0000
Message-ID: <20180319172534.44730-2-luc.vanoostenryck () gmail ! com>
--------------------
The purpose of the file 'options.md' was rather vague.
Make it now explicit that it's to document options
useful for development.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/{options.md => dev-options.md} | 7 +++----
 1 file changed, 3 insertions(+), 4 deletions(-)
 rename Documentation/{options.md => dev-options.md} (74%)

diff --git a/Documentation/options.md b/Documentation/dev-options.md
similarity index 74%
rename from Documentation/options.md
rename to Documentation/dev-options.md
index 14698a981..23ea21330 100644
--- a/Documentation/options.md
+++ b/Documentation/dev-options.md
@@ -1,8 +1,7 @@
 # Options
 
-This file is a complement of man page for sparse but meant
-for options not to be used by sparse itself but by the other
-tools.
+This file is a complement of sparse's man page meant to
+document options only useful for development on sparse itself.
 
 ## Developer options:
 
@@ -17,7 +16,7 @@ tools.
   * 'mem2reg'
   * 'optim'
 
-### Internal Representation
+### Debugging
 
 * '-fdump-ir[=\<pass\>[,\<pass\>...]]'
 
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Randy Dunlap <rdunlap () infradead ! org>
To: linux-sparse
Subject: Re: [PATCH v2 1/2] doc: options.md is for development
Date: Mon, 19 Mar 2018 18:49:19 +0000
Message-ID: <cecd3ec2-a46e-c24c-ec6b-18673fc3593f () infradead ! org>
--------------------
On 03/19/2018 10:25 AM, Luc Van Oostenryck wrote:
> The purpose of the file 'options.md' was rather vague.
> Make it now explicit that it's to document options
> useful for development.
> 
> Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>

also
Reviewed-by: Randy Dunlap <rdunlap@infradead.org>

thanks.

> ---
>  Documentation/{options.md => dev-options.md} | 7 +++----
>  1 file changed, 3 insertions(+), 4 deletions(-)
>  rename Documentation/{options.md => dev-options.md} (74%)
> 
> diff --git a/Documentation/options.md b/Documentation/dev-options.md
> similarity index 74%
> rename from Documentation/options.md
> rename to Documentation/dev-options.md
> index 14698a981..23ea21330 100644
> --- a/Documentation/options.md
> +++ b/Documentation/dev-options.md
> @@ -1,8 +1,7 @@
>  # Options
>  
> -This file is a complement of man page for sparse but meant
> -for options not to be used by sparse itself but by the other
> -tools.
> +This file is a complement of sparse's man page meant to
> +document options only useful for development on sparse itself.
>  
>  ## Developer options:
>  
> @@ -17,7 +16,7 @@ tools.
>    * 'mem2reg'
>    * 'optim'
>  
> -### Internal Representation
> +### Debugging
>  
>  * '-fdump-ir[=\<pass\>[,\<pass\>...]]'
>  
> 


-- 
~Randy
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2 1/4] tpm: Add explicit endianness cast ===

From: Thiebaud Weksteen <tweek () google ! com>
To: linux-integrity
Subject: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Thu, 12 Apr 2018 10:13:47 +0000
Message-ID: <20180412101350.210547-2-tweek () google ! com>
--------------------
Signed-off-by: Thiebaud Weksteen <tweek@google.com>
---
 drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/drivers/char/tpm/tpm_eventlog_of.c b/drivers/char/tpm/tpm_eventlog_of.c
index 96fd5646f866..d74568d58a66 100644
--- a/drivers/char/tpm/tpm_eventlog_of.c
+++ b/drivers/char/tpm/tpm_eventlog_of.c
@@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
 	 * but physical tpm needs the conversion.
 	 */
 	if (of_property_match_string(np, "compatible", "IBM,vtpm") < 0) {
-		size = be32_to_cpup(sizep);
-		base = be64_to_cpup(basep);
+		size = be32_to_cpup((__be32 *)sizep);
+		base = be64_to_cpup((__be64 *)basep);
 	} else {
 		size = *sizep;
 		base = *basep;
-- 
2.17.0.484.g0c8726318c-goog

================================================================================

From: Thiebaud Weksteen <tweek () google ! com>
To: linux-kernel
Subject: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Thu, 12 Apr 2018 10:13:47 +0000
Message-ID: <20180412101350.210547-2-tweek () google ! com>
--------------------
Signed-off-by: Thiebaud Weksteen <tweek@google.com>
---
 drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/drivers/char/tpm/tpm_eventlog_of.c b/drivers/char/tpm/tpm_eventlog_of.c
index 96fd5646f866..d74568d58a66 100644
--- a/drivers/char/tpm/tpm_eventlog_of.c
+++ b/drivers/char/tpm/tpm_eventlog_of.c
@@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
 	 * but physical tpm needs the conversion.
 	 */
 	if (of_property_match_string(np, "compatible", "IBM,vtpm") < 0) {
-		size = be32_to_cpup(sizep);
-		base = be64_to_cpup(basep);
+		size = be32_to_cpup((__be32 *)sizep);
+		base = be64_to_cpup((__be64 *)basep);
 	} else {
 		size = *sizep;
 		base = *basep;
-- 
2.17.0.484.g0c8726318c-goog

================================================================================

From: Thiebaud Weksteen <tweek () google ! com>
To: linux-kernel
Subject: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Thu, 12 Apr 2018 10:13:47 +0000
Message-ID: <20180412101350.210547-2-tweek () google ! com>
--------------------
Signed-off-by: Thiebaud Weksteen <tweek@google.com>
---
 drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/drivers/char/tpm/tpm_eventlog_of.c b/drivers/char/tpm/tpm_eventlog_of.c
index 96fd5646f866..d74568d58a66 100644
--- a/drivers/char/tpm/tpm_eventlog_of.c
+++ b/drivers/char/tpm/tpm_eventlog_of.c
@@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
 	 * but physical tpm needs the conversion.
 	 */
 	if (of_property_match_string(np, "compatible", "IBM,vtpm") < 0) {
-		size = be32_to_cpup(sizep);
-		base = be64_to_cpup(basep);
+		size = be32_to_cpup((__be32 *)sizep);
+		base = be64_to_cpup((__be64 *)basep);
 	} else {
 		size = *sizep;
 		base = *basep;
-- 
2.17.0.484.g0c8726318c-goog

================================================================================

From: Jason Gunthorpe <jgg () ziepe ! ca>
To: linux-kernel
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Tue, 17 Apr 2018 03:02:02 +0000
Message-ID: <20180417030202.GA30624 () ziepe ! ca>
--------------------
On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen wrote:
> Signed-off-by: Thiebaud Weksteen <tweek@google.com>
>  drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
>  1 file changed, 2 insertions(+), 2 deletions(-)
> 
> diff --git a/drivers/char/tpm/tpm_eventlog_of.c b/drivers/char/tpm/tpm_eventlog_of.c
> index 96fd5646f866..d74568d58a66 100644
> +++ b/drivers/char/tpm/tpm_eventlog_of.c
> @@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
>  	 * but physical tpm needs the conversion.
>  	 */
>  	if (of_property_match_string(np, "compatible", "IBM,vtpm") < 0) {
> -		size = be32_to_cpup(sizep);
> -		base = be64_to_cpup(basep);
> +		size = be32_to_cpup((__be32 *)sizep);
> +		base = be64_to_cpup((__be64 *)basep);

Er, no.. change the definitions of sizep and basep to be __be

Jason
================================================================================

From: Thiebaud Weksteen <tweek () google ! com>
To: linux-integrity
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Tue, 17 Apr 2018 08:32:33 +0000
Message-ID: <CA+zpnLdU=t6eOY9GOLAyiXkZBw=NueckiejHSexd8P7MKYuRww () mail ! gmail ! com>
--------------------
On Tue, Apr 17, 2018 at 5:02 AM Jason Gunthorpe <jgg@ziepe.ca> wrote:

> On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen wrote:
> > Signed-off-by: Thiebaud Weksteen <tweek@google.com>
> >  drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
> >  1 file changed, 2 insertions(+), 2 deletions(-)
> >
> > diff --git a/drivers/char/tpm/tpm_eventlog_of.c
b/drivers/char/tpm/tpm_eventlog_of.c
> > index 96fd5646f866..d74568d58a66 100644
> > +++ b/drivers/char/tpm/tpm_eventlog_of.c
> > @@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
> >        * but physical tpm needs the conversion.
> >        */
> >       if (of_property_match_string(np, "compatible", "IBM,vtpm") < 0) {
> > -             size = be32_to_cpup(sizep);
> > -             base = be64_to_cpup(basep);
> > +             size = be32_to_cpup((__be32 *)sizep);
> > +             base = be64_to_cpup((__be64 *)basep);

> Er, no.. change the definitions of sizep and basep to be __be

> Jason

Please read the comment before the condition. sizep and
basep may contain either little endian or big endian and this block is used
to adjust that. Let me know if there is a better way for handling this.
================================================================================

From: Thiebaud Weksteen <tweek () google ! com>
To: linux-kernel
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Tue, 17 Apr 2018 08:32:33 +0000
Message-ID: <CA+zpnLdU=t6eOY9GOLAyiXkZBw=NueckiejHSexd8P7MKYuRww () mail ! gmail ! com>
--------------------
On Tue, Apr 17, 2018 at 5:02 AM Jason Gunthorpe <jgg@ziepe.ca> wrote:

> On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen wrote:
> > Signed-off-by: Thiebaud Weksteen <tweek@google.com>
> >  drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
> >  1 file changed, 2 insertions(+), 2 deletions(-)
> >
> > diff --git a/drivers/char/tpm/tpm_eventlog_of.c
b/drivers/char/tpm/tpm_eventlog_of.c
> > index 96fd5646f866..d74568d58a66 100644
> > +++ b/drivers/char/tpm/tpm_eventlog_of.c
> > @@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
> >        * but physical tpm needs the conversion.
> >        */
> >       if (of_property_match_string(np, "compatible", "IBM,vtpm") < 0) {
> > -             size = be32_to_cpup(sizep);
> > -             base = be64_to_cpup(basep);
> > +             size = be32_to_cpup((__be32 *)sizep);
> > +             base = be64_to_cpup((__be64 *)basep);

> Er, no.. change the definitions of sizep and basep to be __be

> Jason

Please read the comment before the condition. sizep and
basep may contain either little endian or big endian and this block is used
to adjust that. Let me know if there is a better way for handling this.
================================================================================

From: Jason Gunthorpe <jgg () ziepe ! ca>
To: linux-kernel
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Tue, 17 Apr 2018 14:00:13 +0000
Message-ID: <20180417140013.GA2029 () ziepe ! ca>
--------------------
On Tue, Apr 17, 2018 at 08:32:33AM +0000, Thiebaud Weksteen wrote:
> On Tue, Apr 17, 2018 at 5:02 AM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> 
> > On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen wrote:
> > > Signed-off-by: Thiebaud Weksteen <tweek@google.com>
> > >  drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
> > >  1 file changed, 2 insertions(+), 2 deletions(-)
> > >
> > > diff --git a/drivers/char/tpm/tpm_eventlog_of.c
> b/drivers/char/tpm/tpm_eventlog_of.c
> > > index 96fd5646f866..d74568d58a66 100644
> > > +++ b/drivers/char/tpm/tpm_eventlog_of.c
> > > @@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
> > >        * but physical tpm needs the conversion.
> > >        */
> > >       if (of_property_match_string(np, "compatible", "IBM,vtpm") < 0) {
> > > -             size = be32_to_cpup(sizep);
> > > -             base = be64_to_cpup(basep);
> > > +             size = be32_to_cpup((__be32 *)sizep);
> > > +             base = be64_to_cpup((__be64 *)basep);
> 
> > Er, no.. change the definitions of sizep and basep to be __be
> 
> > Jason
> 
> Please read the comment before the condition. sizep and
> basep may contain either little endian or big endian and this block is used
> to adjust that. Let me know if there is a better way for handling this.

Well a cast like that will throw sparse warnings, you need __force at
least

Jason
================================================================================

From: Thiebaud Weksteen <tweek () google ! com>
To: linux-integrity
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Thu, 19 Apr 2018 13:09:12 +0000
Message-ID: <CA+zpnLePcFJQURBNZEt=mRSRb9GUYR=1t9EQn82W0+311xKZ7w () mail ! gmail ! com>
--------------------
On Tue, Apr 17, 2018 at 4:00 PM Jason Gunthorpe <jgg@ziepe.ca> wrote:

> On Tue, Apr 17, 2018 at 08:32:33AM +0000, Thiebaud Weksteen wrote:
> > On Tue, Apr 17, 2018 at 5:02 AM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> >
> > > On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen wrote:
> > > > Signed-off-by: Thiebaud Weksteen <tweek@google.com>
> > > >  drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
> > > >  1 file changed, 2 insertions(+), 2 deletions(-)
> > > >
> > > > diff --git a/drivers/char/tpm/tpm_eventlog_of.c
> > b/drivers/char/tpm/tpm_eventlog_of.c
> > > > index 96fd5646f866..d74568d58a66 100644
> > > > +++ b/drivers/char/tpm/tpm_eventlog_of.c
> > > > @@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
> > > >        * but physical tpm needs the conversion.
> > > >        */
> > > >       if (of_property_match_string(np, "compatible", "IBM,vtpm") <
0) {
> > > > -             size = be32_to_cpup(sizep);
> > > > -             base = be64_to_cpup(basep);
> > > > +             size = be32_to_cpup((__be32 *)sizep);
> > > > +             base = be64_to_cpup((__be64 *)basep);
> >
> > > Er, no.. change the definitions of sizep and basep to be __be
> >
> > > Jason
> >
> > Please read the comment before the condition. sizep and
> > basep may contain either little endian or big endian and this block is
used
> > to adjust that. Let me know if there is a better way for handling this.

> Well a cast like that will throw sparse warnings, you need __force at
> least

> Jason

I don't think so. Since the variable is only defined as u32*, no specific
warning is generated. I've used `make C=2 drivers/char/tpm/` with this
patch applied and no new warning is being triggered.
================================================================================

From: Jarkko Sakkinen <jarkko.sakkinen () linux ! intel ! com>
To: linux-integrity
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Fri, 20 Apr 2018 05:39:05 +0000
Message-ID: <20180420053905.GA15364 () linux ! intel ! com>
--------------------
On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen wrote:
> Signed-off-by: Thiebaud Weksteen <tweek@google.com>

Reviewed-by: Jarkko Sakkinen <jarkko.sakkinen@linux.intel.com>
Tested-by: Jarkko Sakkinen <jarkko.sakkinen@linux.intel.com>

/Jarkko
================================================================================

From: Jason Gunthorpe <jgg () ziepe ! ca>
To: linux-kernel
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Fri, 20 Apr 2018 14:57:40 +0000
Message-ID: <20180420145740.GC30433 () ziepe ! ca>
--------------------
On Thu, Apr 19, 2018 at 01:09:12PM +0000, Thiebaud Weksteen wrote:
> On Tue, Apr 17, 2018 at 4:00 PM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> 
> > On Tue, Apr 17, 2018 at 08:32:33AM +0000, Thiebaud Weksteen wrote:
> > > On Tue, Apr 17, 2018 at 5:02 AM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> > >
> > > > On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen wrote:
> > > > > Signed-off-by: Thiebaud Weksteen <tweek@google.com>
> > > > >  drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
> > > > >  1 file changed, 2 insertions(+), 2 deletions(-)
> > > > >
> > > > > diff --git a/drivers/char/tpm/tpm_eventlog_of.c
> > > b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > index 96fd5646f866..d74568d58a66 100644
> > > > > +++ b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > @@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
> > > > >        * but physical tpm needs the conversion.
> > > > >        */
> > > > >       if (of_property_match_string(np, "compatible", "IBM,vtpm") <
> 0) {
> > > > > -             size = be32_to_cpup(sizep);
> > > > > -             base = be64_to_cpup(basep);
> > > > > +             size = be32_to_cpup((__be32 *)sizep);
> > > > > +             base = be64_to_cpup((__be64 *)basep);
> > >
> > > > Er, no.. change the definitions of sizep and basep to be __be
> > >
> > > > Jason
> > >
> > > Please read the comment before the condition. sizep and
> > > basep may contain either little endian or big endian and this block is
> used
> > > to adjust that. Let me know if there is a better way for handling this.
> 
> > Well a cast like that will throw sparse warnings, you need __force at
> > least
> 
> I don't think so. Since the variable is only defined as u32*, no specific
> warning is generated. I've used `make C=2 drivers/char/tpm/` with this
> patch applied and no new warning is being triggered.

I'm surprised to hear you say that..

Sparse is supposed to require force on all cast that change the
annotation, and there are many examples in the kernel that have force
in that case.

Jason
================================================================================

From: Jason Gunthorpe <jgg () ziepe ! ca>
To: linux-integrity
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Fri, 20 Apr 2018 14:57:40 +0000
Message-ID: <20180420145740.GC30433 () ziepe ! ca>
--------------------
On Thu, Apr 19, 2018 at 01:09:12PM +0000, Thiebaud Weksteen wrote:
> On Tue, Apr 17, 2018 at 4:00 PM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> 
> > On Tue, Apr 17, 2018 at 08:32:33AM +0000, Thiebaud Weksteen wrote:
> > > On Tue, Apr 17, 2018 at 5:02 AM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> > >
> > > > On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen wrote:
> > > > > Signed-off-by: Thiebaud Weksteen <tweek@google.com>
> > > > >  drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
> > > > >  1 file changed, 2 insertions(+), 2 deletions(-)
> > > > >
> > > > > diff --git a/drivers/char/tpm/tpm_eventlog_of.c
> > > b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > index 96fd5646f866..d74568d58a66 100644
> > > > > +++ b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > @@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
> > > > >        * but physical tpm needs the conversion.
> > > > >        */
> > > > >       if (of_property_match_string(np, "compatible", "IBM,vtpm") <
> 0) {
> > > > > -             size = be32_to_cpup(sizep);
> > > > > -             base = be64_to_cpup(basep);
> > > > > +             size = be32_to_cpup((__be32 *)sizep);
> > > > > +             base = be64_to_cpup((__be64 *)basep);
> > >
> > > > Er, no.. change the definitions of sizep and basep to be __be
> > >
> > > > Jason
> > >
> > > Please read the comment before the condition. sizep and
> > > basep may contain either little endian or big endian and this block is
> used
> > > to adjust that. Let me know if there is a better way for handling this.
> 
> > Well a cast like that will throw sparse warnings, you need __force at
> > least
> 
> I don't think so. Since the variable is only defined as u32*, no specific
> warning is generated. I've used `make C=2 drivers/char/tpm/` with this
> patch applied and no new warning is being triggered.

I'm surprised to hear you say that..

Sparse is supposed to require force on all cast that change the
annotation, and there are many examples in the kernel that have force
in that case.

Jason
================================================================================

From: Thiebaud Weksteen <tweek () google ! com>
To: linux-integrity
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Mon, 23 Apr 2018 09:22:06 +0000
Message-ID: <CA+zpnLcSDTz99Z4fe4CGxH6RzTGvQCoGqQc4p_EMOj9=cuFQgg () mail ! gmail ! com>
--------------------
On Fri, Apr 20, 2018 at 4:57 PM Jason Gunthorpe <jgg@ziepe.ca> wrote:

> On Thu, Apr 19, 2018 at 01:09:12PM +0000, Thiebaud Weksteen wrote:
> > On Tue, Apr 17, 2018 at 4:00 PM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> >
> > > On Tue, Apr 17, 2018 at 08:32:33AM +0000, Thiebaud Weksteen wrote:
> > > > On Tue, Apr 17, 2018 at 5:02 AM Jason Gunthorpe <jgg@ziepe.ca>
wrote:
> > > >
> > > > > On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen wrote:
> > > > > > Signed-off-by: Thiebaud Weksteen <tweek@google.com>
> > > > > >  drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
> > > > > >  1 file changed, 2 insertions(+), 2 deletions(-)
> > > > > >
> > > > > > diff --git a/drivers/char/tpm/tpm_eventlog_of.c
> > > > b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > index 96fd5646f866..d74568d58a66 100644
> > > > > > +++ b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > @@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
> > > > > >        * but physical tpm needs the conversion.
> > > > > >        */
> > > > > >       if (of_property_match_string(np, "compatible",
"IBM,vtpm") <
> > 0) {
> > > > > > -             size = be32_to_cpup(sizep);
> > > > > > -             base = be64_to_cpup(basep);
> > > > > > +             size = be32_to_cpup((__be32 *)sizep);
> > > > > > +             base = be64_to_cpup((__be64 *)basep);
> > > >
> > > > > Er, no.. change the definitions of sizep and basep to be __be
> > > >
> > > > > Jason
> > > >
> > > > Please read the comment before the condition. sizep and
> > > > basep may contain either little endian or big endian and this block
is
> > used
> > > > to adjust that. Let me know if there is a better way for handling
this.
> >
> > > Well a cast like that will throw sparse warnings, you need __force at
> > > least
> >
> > I don't think so. Since the variable is only defined as u32*, no
specific
> > warning is generated. I've used `make C=2 drivers/char/tpm/` with this
> > patch applied and no new warning is being triggered.

> I'm surprised to hear you say that..

> Sparse is supposed to require force on all cast that change the
> annotation, and there are many examples in the kernel that have force
> in that case.

> Jason

+linux-sparse@vger.kernel.org and sparse@chrisli.org for a sanity check.

If you look at the man page of sparse, under the bitwise option, it states:
"Sparse will warn on [...] any conversion of one restricted type into
another, except via a cast that includes __attribute__((force)).". In our
case, it is a conversion from unrestricted to restricted which does not
fall in this category.
================================================================================

From: Thiebaud Weksteen <tweek () google ! com>
To: linux-sparse
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Mon, 23 Apr 2018 09:22:06 +0000
Message-ID: <CA+zpnLcSDTz99Z4fe4CGxH6RzTGvQCoGqQc4p_EMOj9=cuFQgg () mail ! gmail ! com>
--------------------
On Fri, Apr 20, 2018 at 4:57 PM Jason Gunthorpe <jgg@ziepe.ca> wrote:

> On Thu, Apr 19, 2018 at 01:09:12PM +0000, Thiebaud Weksteen wrote:
> > On Tue, Apr 17, 2018 at 4:00 PM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> >
> > > On Tue, Apr 17, 2018 at 08:32:33AM +0000, Thiebaud Weksteen wrote:
> > > > On Tue, Apr 17, 2018 at 5:02 AM Jason Gunthorpe <jgg@ziepe.ca>
wrote:
> > > >
> > > > > On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen wrote:
> > > > > > Signed-off-by: Thiebaud Weksteen <tweek@google.com>
> > > > > >  drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
> > > > > >  1 file changed, 2 insertions(+), 2 deletions(-)
> > > > > >
> > > > > > diff --git a/drivers/char/tpm/tpm_eventlog_of.c
> > > > b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > index 96fd5646f866..d74568d58a66 100644
> > > > > > +++ b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > @@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
> > > > > >        * but physical tpm needs the conversion.
> > > > > >        */
> > > > > >       if (of_property_match_string(np, "compatible",
"IBM,vtpm") <
> > 0) {
> > > > > > -             size = be32_to_cpup(sizep);
> > > > > > -             base = be64_to_cpup(basep);
> > > > > > +             size = be32_to_cpup((__be32 *)sizep);
> > > > > > +             base = be64_to_cpup((__be64 *)basep);
> > > >
> > > > > Er, no.. change the definitions of sizep and basep to be __be
> > > >
> > > > > Jason
> > > >
> > > > Please read the comment before the condition. sizep and
> > > > basep may contain either little endian or big endian and this block
is
> > used
> > > > to adjust that. Let me know if there is a better way for handling
this.
> >
> > > Well a cast like that will throw sparse warnings, you need __force at
> > > least
> >
> > I don't think so. Since the variable is only defined as u32*, no
specific
> > warning is generated. I've used `make C=2 drivers/char/tpm/` with this
> > patch applied and no new warning is being triggered.

> I'm surprised to hear you say that..

> Sparse is supposed to require force on all cast that change the
> annotation, and there are many examples in the kernel that have force
> in that case.

> Jason

+linux-sparse@vger.kernel.org and sparse@chrisli.org for a sanity check.

If you look at the man page of sparse, under the bitwise option, it states:
"Sparse will warn on [...] any conversion of one restricted type into
another, except via a cast that includes __attribute__((force)).". In our
case, it is a conversion from unrestricted to restricted which does not
fall in this category.
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-integrity
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Mon, 23 Apr 2018 10:06:32 +0000
Message-ID: <20180423100629.effueb6i7q3hmhu3 () ltop ! local>
--------------------
On Mon, Apr 23, 2018 at 09:22:06AM +0000, Thiebaud Weksteen wrote:
> On Fri, Apr 20, 2018 at 4:57 PM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> 
> > On Thu, Apr 19, 2018 at 01:09:12PM +0000, Thiebaud Weksteen wrote:
> > > On Tue, Apr 17, 2018 at 4:00 PM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> > >
> > > > On Tue, Apr 17, 2018 at 08:32:33AM +0000, Thiebaud Weksteen wrote:
> > > > > On Tue, Apr 17, 2018 at 5:02 AM Jason Gunthorpe <jgg@ziepe.ca>
> wrote:
> > > > >
> > > > > > On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen wrote:
> > > > > > > Signed-off-by: Thiebaud Weksteen <tweek@google.com>
> > > > > > >  drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
> > > > > > >  1 file changed, 2 insertions(+), 2 deletions(-)
> > > > > > >
> > > > > > > diff --git a/drivers/char/tpm/tpm_eventlog_of.c
> > > > > b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > > index 96fd5646f866..d74568d58a66 100644
> > > > > > > +++ b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > > @@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
> > > > > > >        * but physical tpm needs the conversion.
> > > > > > >        */
> > > > > > >       if (of_property_match_string(np, "compatible",
> "IBM,vtpm") <
> > > 0) {
> > > > > > > -             size = be32_to_cpup(sizep);
> > > > > > > -             base = be64_to_cpup(basep);
> > > > > > > +             size = be32_to_cpup((__be32 *)sizep);
> > > > > > > +             base = be64_to_cpup((__be64 *)basep);
> > > > >
> > > > > > Er, no.. change the definitions of sizep and basep to be __be
> > > > >
> > > > > > Jason
> > > > >
> > > > > Please read the comment before the condition. sizep and
> > > > > basep may contain either little endian or big endian and this block
> is
> > > used
> > > > > to adjust that. Let me know if there is a better way for handling
> this.
> > >
> > > > Well a cast like that will throw sparse warnings, you need __force at
> > > > least
> > >
> > > I don't think so. Since the variable is only defined as u32*, no
> specific
> > > warning is generated. I've used `make C=2 drivers/char/tpm/` with this
> > > patch applied and no new warning is being triggered.

Interesting.

> > I'm surprised to hear you say that..

Same for me.

> > Sparse is supposed to require force on all cast that change the
> > annotation, and there are many examples in the kernel that have force
> > in that case.


Yes, sparse is supposed to warn in such cases and the __force is there
to quiets the warning when it is known that the cast is in fact correct.

 
> +linux-sparse@vger.kernel.org and sparse@chrisli.org for a sanity check.
> 
> If you look at the man page of sparse, under the bitwise option, it states:
> "Sparse will warn on [...] any conversion of one restricted type into
> another, except via a cast that includes __attribute__((force)).". In our
> case, it is a conversion from unrestricted to restricted which does not
> fall in this category.

The man page is not very clear here. It must be understood in the context
where each use of '__bitwise' will create a new *distinct* type.
Given this and the normal type checking, sparse should warn
"on any conversion of one restricted type into another *or* between a
restricted and the corresponding plain/unrestricted type" (or consider
that an 'unrestricted type' is 'a restricted type with no restriction',
which is, I think, what was meant here).

The fact that sparse doesn't warn in your case is clearly a bug in
sparse's type checking.

Regards,
-- Luc Van Oostenryck
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-kernel
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Mon, 23 Apr 2018 10:06:32 +0000
Message-ID: <20180423100629.effueb6i7q3hmhu3 () ltop ! local>
--------------------
On Mon, Apr 23, 2018 at 09:22:06AM +0000, Thiebaud Weksteen wrote:
> On Fri, Apr 20, 2018 at 4:57 PM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> 
> > On Thu, Apr 19, 2018 at 01:09:12PM +0000, Thiebaud Weksteen wrote:
> > > On Tue, Apr 17, 2018 at 4:00 PM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> > >
> > > > On Tue, Apr 17, 2018 at 08:32:33AM +0000, Thiebaud Weksteen wrote:
> > > > > On Tue, Apr 17, 2018 at 5:02 AM Jason Gunthorpe <jgg@ziepe.ca>
> wrote:
> > > > >
> > > > > > On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen wrote:
> > > > > > > Signed-off-by: Thiebaud Weksteen <tweek@google.com>
> > > > > > >  drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
> > > > > > >  1 file changed, 2 insertions(+), 2 deletions(-)
> > > > > > >
> > > > > > > diff --git a/drivers/char/tpm/tpm_eventlog_of.c
> > > > > b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > > index 96fd5646f866..d74568d58a66 100644
> > > > > > > +++ b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > > @@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
> > > > > > >        * but physical tpm needs the conversion.
> > > > > > >        */
> > > > > > >       if (of_property_match_string(np, "compatible",
> "IBM,vtpm") <
> > > 0) {
> > > > > > > -             size = be32_to_cpup(sizep);
> > > > > > > -             base = be64_to_cpup(basep);
> > > > > > > +             size = be32_to_cpup((__be32 *)sizep);
> > > > > > > +             base = be64_to_cpup((__be64 *)basep);
> > > > >
> > > > > > Er, no.. change the definitions of sizep and basep to be __be
> > > > >
> > > > > > Jason
> > > > >
> > > > > Please read the comment before the condition. sizep and
> > > > > basep may contain either little endian or big endian and this block
> is
> > > used
> > > > > to adjust that. Let me know if there is a better way for handling
> this.
> > >
> > > > Well a cast like that will throw sparse warnings, you need __force at
> > > > least
> > >
> > > I don't think so. Since the variable is only defined as u32*, no
> specific
> > > warning is generated. I've used `make C=2 drivers/char/tpm/` with this
> > > patch applied and no new warning is being triggered.

Interesting.

> > I'm surprised to hear you say that..

Same for me.

> > Sparse is supposed to require force on all cast that change the
> > annotation, and there are many examples in the kernel that have force
> > in that case.


Yes, sparse is supposed to warn in such cases and the __force is there
to quiets the warning when it is known that the cast is in fact correct.

 
> +linux-sparse@vger.kernel.org and sparse@chrisli.org for a sanity check.
> 
> If you look at the man page of sparse, under the bitwise option, it states:
> "Sparse will warn on [...] any conversion of one restricted type into
> another, except via a cast that includes __attribute__((force)).". In our
> case, it is a conversion from unrestricted to restricted which does not
> fall in this category.

The man page is not very clear here. It must be understood in the context
where each use of '__bitwise' will create a new *distinct* type.
Given this and the normal type checking, sparse should warn
"on any conversion of one restricted type into another *or* between a
restricted and the corresponding plain/unrestricted type" (or consider
that an 'unrestricted type' is 'a restricted type with no restriction',
which is, I think, what was meant here).

The fact that sparse doesn't warn in your case is clearly a bug in
sparse's type checking.

Regards,
-- Luc Van Oostenryck
================================================================================

From: Thiebaud Weksteen <tweek () google ! com>
To: linux-kernel
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Tue, 24 Apr 2018 09:57:58 +0000
Message-ID: <CA+zpnLeJ_aKnmiQaOwRERj43uAFSaJQsWnSKJVzsJJB0VW-52w () mail ! gmail ! com>
--------------------
On Mon, Apr 23, 2018 at 12:06 PM Luc Van Oostenryck <
luc.vanoostenryck@gmail.com> wrote:

> On Mon, Apr 23, 2018 at 09:22:06AM +0000, Thiebaud Weksteen wrote:
> > On Fri, Apr 20, 2018 at 4:57 PM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> >
> > > On Thu, Apr 19, 2018 at 01:09:12PM +0000, Thiebaud Weksteen wrote:
> > > > On Tue, Apr 17, 2018 at 4:00 PM Jason Gunthorpe <jgg@ziepe.ca>
wrote:
> > > >
> > > > > On Tue, Apr 17, 2018 at 08:32:33AM +0000, Thiebaud Weksteen wrote:
> > > > > > On Tue, Apr 17, 2018 at 5:02 AM Jason Gunthorpe <jgg@ziepe.ca>
> > wrote:
> > > > > >
> > > > > > > On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen
wrote:
> > > > > > > > Signed-off-by: Thiebaud Weksteen <tweek@google.com>
> > > > > > > >  drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
> > > > > > > >  1 file changed, 2 insertions(+), 2 deletions(-)
> > > > > > > >
> > > > > > > > diff --git a/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > > > index 96fd5646f866..d74568d58a66 100644
> > > > > > > > +++ b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > > > @@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
> > > > > > > >        * but physical tpm needs the conversion.
> > > > > > > >        */
> > > > > > > >       if (of_property_match_string(np, "compatible",
> > "IBM,vtpm") <
> > > > 0) {
> > > > > > > > -             size = be32_to_cpup(sizep);
> > > > > > > > -             base = be64_to_cpup(basep);
> > > > > > > > +             size = be32_to_cpup((__be32 *)sizep);
> > > > > > > > +             base = be64_to_cpup((__be64 *)basep);
> > > > > >
> > > > > > > Er, no.. change the definitions of sizep and basep to be __be
> > > > > >
> > > > > > > Jason
> > > > > >
> > > > > > Please read the comment before the condition. sizep and
> > > > > > basep may contain either little endian or big endian and this
block
> > is
> > > > used
> > > > > > to adjust that. Let me know if there is a better way for
handling
> > this.
> > > >
> > > > > Well a cast like that will throw sparse warnings, you need
__force at
> > > > > least
> > > >
> > > > I don't think so. Since the variable is only defined as u32*, no
> > specific
> > > > warning is generated. I've used `make C=2 drivers/char/tpm/` with
this
> > > > patch applied and no new warning is being triggered.

> Interesting.

> > > I'm surprised to hear you say that..

> Same for me.

> > > Sparse is supposed to require force on all cast that change the
> > > annotation, and there are many examples in the kernel that have force
> > > in that case.


> Yes, sparse is supposed to warn in such cases and the __force is there
> to quiets the warning when it is known that the cast is in fact correct.


> > +linux-sparse@vger.kernel.org and sparse@chrisli.org for a sanity check.
> >
> > If you look at the man page of sparse, under the bitwise option, it
states:
> > "Sparse will warn on [...] any conversion of one restricted type into
> > another, except via a cast that includes __attribute__((force)).". In
our
> > case, it is a conversion from unrestricted to restricted which does not
> > fall in this category.

> The man page is not very clear here. It must be understood in the context
> where each use of '__bitwise' will create a new *distinct* type.
> Given this and the normal type checking, sparse should warn
> "on any conversion of one restricted type into another *or* between a
> restricted and the corresponding plain/unrestricted type" (or consider
> that an 'unrestricted type' is 'a restricted type with no restriction',
> which is, I think, what was meant here).


Thanks for the explanation, that make sense. I believe the issue happens
when dealing with restricted pointer types more than regular types. Also,
this is not new and has probably been going on for the last 13 years. For
instance, 81179bb6a54c2c626b4cbcc084ca974bb2d7f2a3 explicitly removed the
__force attribute. I'll send a validation patch for sparse and update this
patch.

> The fact that sparse doesn't warn in your case is clearly a bug in
> sparse's type checking.

> Regards,
> -- Luc Van Oostenryck
================================================================================

From: Thiebaud Weksteen <tweek () google ! com>
To: linux-integrity
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Tue, 24 Apr 2018 09:57:58 +0000
Message-ID: <CA+zpnLeJ_aKnmiQaOwRERj43uAFSaJQsWnSKJVzsJJB0VW-52w () mail ! gmail ! com>
--------------------
On Mon, Apr 23, 2018 at 12:06 PM Luc Van Oostenryck <
luc.vanoostenryck@gmail.com> wrote:

> On Mon, Apr 23, 2018 at 09:22:06AM +0000, Thiebaud Weksteen wrote:
> > On Fri, Apr 20, 2018 at 4:57 PM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> >
> > > On Thu, Apr 19, 2018 at 01:09:12PM +0000, Thiebaud Weksteen wrote:
> > > > On Tue, Apr 17, 2018 at 4:00 PM Jason Gunthorpe <jgg@ziepe.ca>
wrote:
> > > >
> > > > > On Tue, Apr 17, 2018 at 08:32:33AM +0000, Thiebaud Weksteen wrote:
> > > > > > On Tue, Apr 17, 2018 at 5:02 AM Jason Gunthorpe <jgg@ziepe.ca>
> > wrote:
> > > > > >
> > > > > > > On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen
wrote:
> > > > > > > > Signed-off-by: Thiebaud Weksteen <tweek@google.com>
> > > > > > > >  drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
> > > > > > > >  1 file changed, 2 insertions(+), 2 deletions(-)
> > > > > > > >
> > > > > > > > diff --git a/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > > > index 96fd5646f866..d74568d58a66 100644
> > > > > > > > +++ b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > > > @@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
> > > > > > > >        * but physical tpm needs the conversion.
> > > > > > > >        */
> > > > > > > >       if (of_property_match_string(np, "compatible",
> > "IBM,vtpm") <
> > > > 0) {
> > > > > > > > -             size = be32_to_cpup(sizep);
> > > > > > > > -             base = be64_to_cpup(basep);
> > > > > > > > +             size = be32_to_cpup((__be32 *)sizep);
> > > > > > > > +             base = be64_to_cpup((__be64 *)basep);
> > > > > >
> > > > > > > Er, no.. change the definitions of sizep and basep to be __be
> > > > > >
> > > > > > > Jason
> > > > > >
> > > > > > Please read the comment before the condition. sizep and
> > > > > > basep may contain either little endian or big endian and this
block
> > is
> > > > used
> > > > > > to adjust that. Let me know if there is a better way for
handling
> > this.
> > > >
> > > > > Well a cast like that will throw sparse warnings, you need
__force at
> > > > > least
> > > >
> > > > I don't think so. Since the variable is only defined as u32*, no
> > specific
> > > > warning is generated. I've used `make C=2 drivers/char/tpm/` with
this
> > > > patch applied and no new warning is being triggered.

> Interesting.

> > > I'm surprised to hear you say that..

> Same for me.

> > > Sparse is supposed to require force on all cast that change the
> > > annotation, and there are many examples in the kernel that have force
> > > in that case.


> Yes, sparse is supposed to warn in such cases and the __force is there
> to quiets the warning when it is known that the cast is in fact correct.


> > +linux-sparse@vger.kernel.org and sparse@chrisli.org for a sanity check.
> >
> > If you look at the man page of sparse, under the bitwise option, it
states:
> > "Sparse will warn on [...] any conversion of one restricted type into
> > another, except via a cast that includes __attribute__((force)).". In
our
> > case, it is a conversion from unrestricted to restricted which does not
> > fall in this category.

> The man page is not very clear here. It must be understood in the context
> where each use of '__bitwise' will create a new *distinct* type.
> Given this and the normal type checking, sparse should warn
> "on any conversion of one restricted type into another *or* between a
> restricted and the corresponding plain/unrestricted type" (or consider
> that an 'unrestricted type' is 'a restricted type with no restriction',
> which is, I think, what was meant here).


Thanks for the explanation, that make sense. I believe the issue happens
when dealing with restricted pointer types more than regular types. Also,
this is not new and has probably been going on for the last 13 years. For
instance, 81179bb6a54c2c626b4cbcc084ca974bb2d7f2a3 explicitly removed the
__force attribute. I'll send a validation patch for sparse and update this
patch.

> The fact that sparse doesn't warn in your case is clearly a bug in
> sparse's type checking.

> Regards,
> -- Luc Van Oostenryck
================================================================================

From: Thiebaud Weksteen <tweek () google ! com>
To: linux-sparse
Subject: Re: [PATCH v2 1/4] tpm: Add explicit endianness cast
Date: Tue, 24 Apr 2018 09:57:58 +0000
Message-ID: <CA+zpnLeJ_aKnmiQaOwRERj43uAFSaJQsWnSKJVzsJJB0VW-52w () mail ! gmail ! com>
--------------------
On Mon, Apr 23, 2018 at 12:06 PM Luc Van Oostenryck <
luc.vanoostenryck@gmail.com> wrote:

> On Mon, Apr 23, 2018 at 09:22:06AM +0000, Thiebaud Weksteen wrote:
> > On Fri, Apr 20, 2018 at 4:57 PM Jason Gunthorpe <jgg@ziepe.ca> wrote:
> >
> > > On Thu, Apr 19, 2018 at 01:09:12PM +0000, Thiebaud Weksteen wrote:
> > > > On Tue, Apr 17, 2018 at 4:00 PM Jason Gunthorpe <jgg@ziepe.ca>
wrote:
> > > >
> > > > > On Tue, Apr 17, 2018 at 08:32:33AM +0000, Thiebaud Weksteen wrote:
> > > > > > On Tue, Apr 17, 2018 at 5:02 AM Jason Gunthorpe <jgg@ziepe.ca>
> > wrote:
> > > > > >
> > > > > > > On Thu, Apr 12, 2018 at 12:13:47PM +0200, Thiebaud Weksteen
wrote:
> > > > > > > > Signed-off-by: Thiebaud Weksteen <tweek@google.com>
> > > > > > > >  drivers/char/tpm/tpm_eventlog_of.c | 4 ++--
> > > > > > > >  1 file changed, 2 insertions(+), 2 deletions(-)
> > > > > > > >
> > > > > > > > diff --git a/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > > > index 96fd5646f866..d74568d58a66 100644
> > > > > > > > +++ b/drivers/char/tpm/tpm_eventlog_of.c
> > > > > > > > @@ -56,8 +56,8 @@ int tpm_read_log_of(struct tpm_chip *chip)
> > > > > > > >        * but physical tpm needs the conversion.
> > > > > > > >        */
> > > > > > > >       if (of_property_match_string(np, "compatible",
> > "IBM,vtpm") <
> > > > 0) {
> > > > > > > > -             size = be32_to_cpup(sizep);
> > > > > > > > -             base = be64_to_cpup(basep);
> > > > > > > > +             size = be32_to_cpup((__be32 *)sizep);
> > > > > > > > +             base = be64_to_cpup((__be64 *)basep);
> > > > > >
> > > > > > > Er, no.. change the definitions of sizep and basep to be __be
> > > > > >
> > > > > > > Jason
> > > > > >
> > > > > > Please read the comment before the condition. sizep and
> > > > > > basep may contain either little endian or big endian and this
block
> > is
> > > > used
> > > > > > to adjust that. Let me know if there is a better way for
handling
> > this.
> > > >
> > > > > Well a cast like that will throw sparse warnings, you need
__force at
> > > > > least
> > > >
> > > > I don't think so. Since the variable is only defined as u32*, no
> > specific
> > > > warning is generated. I've used `make C=2 drivers/char/tpm/` with
this
> > > > patch applied and no new warning is being triggered.

> Interesting.

> > > I'm surprised to hear you say that..

> Same for me.

> > > Sparse is supposed to require force on all cast that change the
> > > annotation, and there are many examples in the kernel that have force
> > > in that case.


> Yes, sparse is supposed to warn in such cases and the __force is there
> to quiets the warning when it is known that the cast is in fact correct.


> > +linux-sparse@vger.kernel.org and sparse@chrisli.org for a sanity check.
> >
> > If you look at the man page of sparse, under the bitwise option, it
states:
> > "Sparse will warn on [...] any conversion of one restricted type into
> > another, except via a cast that includes __attribute__((force)).". In
our
> > case, it is a conversion from unrestricted to restricted which does not
> > fall in this category.

> The man page is not very clear here. It must be understood in the context
> where each use of '__bitwise' will create a new *distinct* type.
> Given this and the normal type checking, sparse should warn
> "on any conversion of one restricted type into another *or* between a
> restricted and the corresponding plain/unrestricted type" (or consider
> that an 'unrestricted type' is 'a restricted type with no restriction',
> which is, I think, what was meant here).


Thanks for the explanation, that make sense. I believe the issue happens
when dealing with restricted pointer types more than regular types. Also,
this is not new and has probably been going on for the last 13 years. For
instance, 81179bb6a54c2c626b4cbcc084ca974bb2d7f2a3 explicitly removed the
__force attribute. I'll send a validation patch for sparse and update this
patch.

> The fact that sparse doesn't warn in your case is clearly a bug in
> sparse's type checking.

> Regards,
> -- Luc Van Oostenryck
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2 1/4] x86/mm: surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif ===

From: Du Changbin <changbin.du () gmail ! com>
To: linux-kbuild
Subject: [PATCH v2 1/4] x86/mm: surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif
Date: Fri, 19 Oct 2018 12:49:18 +0000
Message-ID: <20181019124921.13780-2-changbin.du () gmail ! com>
--------------------
The level4_kernel_pgt is only defined when X86_5LEVEL is enabled. So
surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif to
make code correct.

Signed-off-by: Du Changbin <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
---
 arch/x86/include/asm/pgtable_64.h |  2 ++
 arch/x86/kernel/head64.c          | 13 ++++++-------
 2 files changed, 8 insertions(+), 7 deletions(-)

diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h
index 9c85b54bf03c..9333f7fa5bdb 100644
--- a/arch/x86/include/asm/pgtable_64.h
+++ b/arch/x86/include/asm/pgtable_64.h
@@ -16,7 +16,9 @@
 #include <linux/threads.h>
 #include <asm/fixmap.h>
 
+#ifdef CONFIG_X86_5LEVEL
 extern p4d_t level4_kernel_pgt[512];
+#endif
 extern p4d_t level4_ident_pgt[512];
 extern pud_t level3_kernel_pgt[512];
 extern pud_t level3_ident_pgt[512];
diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
index ddee1f0870c4..4a59ef93c258 100644
--- a/arch/x86/kernel/head64.c
+++ b/arch/x86/kernel/head64.c
@@ -151,16 +151,15 @@ unsigned long __head __startup_64(unsigned long physaddr,
 
 	pgd = fixup_pointer(&early_top_pgt, physaddr);
 	p = pgd + pgd_index(__START_KERNEL_map);
-	if (la57)
-		*p = (unsigned long)level4_kernel_pgt;
-	else
-		*p = (unsigned long)level3_kernel_pgt;
-	*p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
-
+#ifdef CONFIG_X86_5LEVEL
 	if (la57) {
+		*p = (unsigned long)level4_kernel_pgt;
 		p4d = fixup_pointer(&level4_kernel_pgt, physaddr);
 		p4d[511] += load_delta;
-	}
+	} else
+#endif
+		*p = (unsigned long)level3_kernel_pgt;
+	*p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
 
 	pud = fixup_pointer(&level3_kernel_pgt, physaddr);
 	pud[510] += load_delta;
-- 
2.17.1

================================================================================

From: Du Changbin <changbin.du () gmail ! com>
To: linux-kernel
Subject: [PATCH v2 1/4] x86/mm: surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif
Date: Fri, 19 Oct 2018 12:49:18 +0000
Message-ID: <20181019124921.13780-2-changbin.du () gmail ! com>
--------------------
The level4_kernel_pgt is only defined when X86_5LEVEL is enabled. So
surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif to
make code correct.

Signed-off-by: Du Changbin <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
---
 arch/x86/include/asm/pgtable_64.h |  2 ++
 arch/x86/kernel/head64.c          | 13 ++++++-------
 2 files changed, 8 insertions(+), 7 deletions(-)

diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h
index 9c85b54bf03c..9333f7fa5bdb 100644
--- a/arch/x86/include/asm/pgtable_64.h
+++ b/arch/x86/include/asm/pgtable_64.h
@@ -16,7 +16,9 @@
 #include <linux/threads.h>
 #include <asm/fixmap.h>
 
+#ifdef CONFIG_X86_5LEVEL
 extern p4d_t level4_kernel_pgt[512];
+#endif
 extern p4d_t level4_ident_pgt[512];
 extern pud_t level3_kernel_pgt[512];
 extern pud_t level3_ident_pgt[512];
diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
index ddee1f0870c4..4a59ef93c258 100644
--- a/arch/x86/kernel/head64.c
+++ b/arch/x86/kernel/head64.c
@@ -151,16 +151,15 @@ unsigned long __head __startup_64(unsigned long physaddr,
 
 	pgd = fixup_pointer(&early_top_pgt, physaddr);
 	p = pgd + pgd_index(__START_KERNEL_map);
-	if (la57)
-		*p = (unsigned long)level4_kernel_pgt;
-	else
-		*p = (unsigned long)level3_kernel_pgt;
-	*p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
-
+#ifdef CONFIG_X86_5LEVEL
 	if (la57) {
+		*p = (unsigned long)level4_kernel_pgt;
 		p4d = fixup_pointer(&level4_kernel_pgt, physaddr);
 		p4d[511] += load_delta;
-	}
+	} else
+#endif
+		*p = (unsigned long)level3_kernel_pgt;
+	*p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
 
 	pud = fixup_pointer(&level3_kernel_pgt, physaddr);
 	pud[510] += load_delta;
-- 
2.17.1

================================================================================

From: Du Changbin <changbin.du () gmail ! com>
To: linux-sparse
Subject: [PATCH v2 1/4] x86/mm: surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif
Date: Fri, 19 Oct 2018 12:49:18 +0000
Message-ID: <20181019124921.13780-2-changbin.du () gmail ! com>
--------------------
The level4_kernel_pgt is only defined when X86_5LEVEL is enabled. So
surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif to
make code correct.

Signed-off-by: Du Changbin <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
---
 arch/x86/include/asm/pgtable_64.h |  2 ++
 arch/x86/kernel/head64.c          | 13 ++++++-------
 2 files changed, 8 insertions(+), 7 deletions(-)

diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h
index 9c85b54bf03c..9333f7fa5bdb 100644
--- a/arch/x86/include/asm/pgtable_64.h
+++ b/arch/x86/include/asm/pgtable_64.h
@@ -16,7 +16,9 @@
 #include <linux/threads.h>
 #include <asm/fixmap.h>
 
+#ifdef CONFIG_X86_5LEVEL
 extern p4d_t level4_kernel_pgt[512];
+#endif
 extern p4d_t level4_ident_pgt[512];
 extern pud_t level3_kernel_pgt[512];
 extern pud_t level3_ident_pgt[512];
diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
index ddee1f0870c4..4a59ef93c258 100644
--- a/arch/x86/kernel/head64.c
+++ b/arch/x86/kernel/head64.c
@@ -151,16 +151,15 @@ unsigned long __head __startup_64(unsigned long physaddr,
 
 	pgd = fixup_pointer(&early_top_pgt, physaddr);
 	p = pgd + pgd_index(__START_KERNEL_map);
-	if (la57)
-		*p = (unsigned long)level4_kernel_pgt;
-	else
-		*p = (unsigned long)level3_kernel_pgt;
-	*p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
-
+#ifdef CONFIG_X86_5LEVEL
 	if (la57) {
+		*p = (unsigned long)level4_kernel_pgt;
 		p4d = fixup_pointer(&level4_kernel_pgt, physaddr);
 		p4d[511] += load_delta;
-	}
+	} else
+#endif
+		*p = (unsigned long)level3_kernel_pgt;
+	*p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
 
 	pud = fixup_pointer(&level3_kernel_pgt, physaddr);
 	pud[510] += load_delta;
-- 
2.17.1

================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: [PATCH v2 1/4] x86/mm: surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif
Date: Sun, 28 Oct 2018 01:44:46 +0000
Message-ID: <CAK7LNAQJxVDWU5GPY4O6x+z_Pkmh4KuHiTpkY_LDJwNYO-eCBw () mail ! gmail ! com>
--------------------
On Fri, Oct 19, 2018 at 9:50 PM Du Changbin <changbin.du@gmail.com> wrote:
>
> The level4_kernel_pgt is only defined when X86_5LEVEL is enabled. So
> surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif to
> make code correct.

For clarification, is it better to mention
that this is a preparation for CONFIG_CC_OPTIMIZE_FOR_DEBUGGING ?


> Signed-off-by: Du Changbin <changbin.du@gmail.com>
> Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
> ---
>  arch/x86/include/asm/pgtable_64.h |  2 ++
>  arch/x86/kernel/head64.c          | 13 ++++++-------
>  2 files changed, 8 insertions(+), 7 deletions(-)
>
> diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h
> index 9c85b54bf03c..9333f7fa5bdb 100644
> --- a/arch/x86/include/asm/pgtable_64.h
> +++ b/arch/x86/include/asm/pgtable_64.h
> @@ -16,7 +16,9 @@
>  #include <linux/threads.h>
>  #include <asm/fixmap.h>
>
> +#ifdef CONFIG_X86_5LEVEL
>  extern p4d_t level4_kernel_pgt[512];
> +#endif


Is this #ifdef  necessary?

It is harmless to declaring unused stuff.



>  extern p4d_t level4_ident_pgt[512];
>  extern pud_t level3_kernel_pgt[512];
>  extern pud_t level3_ident_pgt[512];
> diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
> index ddee1f0870c4..4a59ef93c258 100644
> --- a/arch/x86/kernel/head64.c
> +++ b/arch/x86/kernel/head64.c
> @@ -151,16 +151,15 @@ unsigned long __head __startup_64(unsigned long physaddr,
>
>         pgd = fixup_pointer(&early_top_pgt, physaddr);
>         p = pgd + pgd_index(__START_KERNEL_map);
> -       if (la57)
> -               *p = (unsigned long)level4_kernel_pgt;
> -       else
> -               *p = (unsigned long)level3_kernel_pgt;
> -       *p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
> -
> +#ifdef CONFIG_X86_5LEVEL
>         if (la57) {
> +               *p = (unsigned long)level4_kernel_pgt;
>                 p4d = fixup_pointer(&level4_kernel_pgt, physaddr);
>                 p4d[511] += load_delta;
> -       }
> +       } else
> +#endif
> +               *p = (unsigned long)level3_kernel_pgt;
> +       *p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
>
>         pud = fixup_pointer(&level3_kernel_pgt, physaddr);
>         pud[510] += load_delta;
> --
> 2.17.1
>


Hmm, this code looks a bit ugly...

Does the following one liner work with CONFIG_CC_OPTIMIZE_FOR_DEBUGGING ?


diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
index 8047379..579847f 100644
--- a/arch/x86/kernel/head64.c
+++ b/arch/x86/kernel/head64.c
@@ -97,7 +97,7 @@ static bool __head check_la57_support(unsigned long physaddr)
        return true;
 }
 #else
-static bool __head check_la57_support(unsigned long physaddr)
+static __always_inline bool __head check_la57_support(unsigned long physaddr)
 {
        return false;
 }




--
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v2 1/4] x86/mm: surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif
Date: Sun, 28 Oct 2018 01:44:46 +0000
Message-ID: <CAK7LNAQJxVDWU5GPY4O6x+z_Pkmh4KuHiTpkY_LDJwNYO-eCBw () mail ! gmail ! com>
--------------------
On Fri, Oct 19, 2018 at 9:50 PM Du Changbin <changbin.du@gmail.com> wrote:
>
> The level4_kernel_pgt is only defined when X86_5LEVEL is enabled. So
> surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif to
> make code correct.

For clarification, is it better to mention
that this is a preparation for CONFIG_CC_OPTIMIZE_FOR_DEBUGGING ?


> Signed-off-by: Du Changbin <changbin.du@gmail.com>
> Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
> ---
>  arch/x86/include/asm/pgtable_64.h |  2 ++
>  arch/x86/kernel/head64.c          | 13 ++++++-------
>  2 files changed, 8 insertions(+), 7 deletions(-)
>
> diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h
> index 9c85b54bf03c..9333f7fa5bdb 100644
> --- a/arch/x86/include/asm/pgtable_64.h
> +++ b/arch/x86/include/asm/pgtable_64.h
> @@ -16,7 +16,9 @@
>  #include <linux/threads.h>
>  #include <asm/fixmap.h>
>
> +#ifdef CONFIG_X86_5LEVEL
>  extern p4d_t level4_kernel_pgt[512];
> +#endif


Is this #ifdef  necessary?

It is harmless to declaring unused stuff.



>  extern p4d_t level4_ident_pgt[512];
>  extern pud_t level3_kernel_pgt[512];
>  extern pud_t level3_ident_pgt[512];
> diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
> index ddee1f0870c4..4a59ef93c258 100644
> --- a/arch/x86/kernel/head64.c
> +++ b/arch/x86/kernel/head64.c
> @@ -151,16 +151,15 @@ unsigned long __head __startup_64(unsigned long physaddr,
>
>         pgd = fixup_pointer(&early_top_pgt, physaddr);
>         p = pgd + pgd_index(__START_KERNEL_map);
> -       if (la57)
> -               *p = (unsigned long)level4_kernel_pgt;
> -       else
> -               *p = (unsigned long)level3_kernel_pgt;
> -       *p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
> -
> +#ifdef CONFIG_X86_5LEVEL
>         if (la57) {
> +               *p = (unsigned long)level4_kernel_pgt;
>                 p4d = fixup_pointer(&level4_kernel_pgt, physaddr);
>                 p4d[511] += load_delta;
> -       }
> +       } else
> +#endif
> +               *p = (unsigned long)level3_kernel_pgt;
> +       *p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
>
>         pud = fixup_pointer(&level3_kernel_pgt, physaddr);
>         pud[510] += load_delta;
> --
> 2.17.1
>


Hmm, this code looks a bit ugly...

Does the following one liner work with CONFIG_CC_OPTIMIZE_FOR_DEBUGGING ?


diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
index 8047379..579847f 100644
--- a/arch/x86/kernel/head64.c
+++ b/arch/x86/kernel/head64.c
@@ -97,7 +97,7 @@ static bool __head check_la57_support(unsigned long physaddr)
        return true;
 }
 #else
-static bool __head check_la57_support(unsigned long physaddr)
+static __always_inline bool __head check_la57_support(unsigned long physaddr)
 {
        return false;
 }




--
Best Regards
Masahiro Yamada

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kernel
Subject: Re: [PATCH v2 1/4] x86/mm: surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif
Date: Sun, 28 Oct 2018 01:44:46 +0000
Message-ID: <CAK7LNAQJxVDWU5GPY4O6x+z_Pkmh4KuHiTpkY_LDJwNYO-eCBw () mail ! gmail ! com>
--------------------
On Fri, Oct 19, 2018 at 9:50 PM Du Changbin <changbin.du@gmail.com> wrote:
>
> The level4_kernel_pgt is only defined when X86_5LEVEL is enabled. So
> surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif to
> make code correct.

For clarification, is it better to mention
that this is a preparation for CONFIG_CC_OPTIMIZE_FOR_DEBUGGING ?


> Signed-off-by: Du Changbin <changbin.du@gmail.com>
> Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
> ---
>  arch/x86/include/asm/pgtable_64.h |  2 ++
>  arch/x86/kernel/head64.c          | 13 ++++++-------
>  2 files changed, 8 insertions(+), 7 deletions(-)
>
> diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h
> index 9c85b54bf03c..9333f7fa5bdb 100644
> --- a/arch/x86/include/asm/pgtable_64.h
> +++ b/arch/x86/include/asm/pgtable_64.h
> @@ -16,7 +16,9 @@
>  #include <linux/threads.h>
>  #include <asm/fixmap.h>
>
> +#ifdef CONFIG_X86_5LEVEL
>  extern p4d_t level4_kernel_pgt[512];
> +#endif


Is this #ifdef  necessary?

It is harmless to declaring unused stuff.



>  extern p4d_t level4_ident_pgt[512];
>  extern pud_t level3_kernel_pgt[512];
>  extern pud_t level3_ident_pgt[512];
> diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
> index ddee1f0870c4..4a59ef93c258 100644
> --- a/arch/x86/kernel/head64.c
> +++ b/arch/x86/kernel/head64.c
> @@ -151,16 +151,15 @@ unsigned long __head __startup_64(unsigned long physaddr,
>
>         pgd = fixup_pointer(&early_top_pgt, physaddr);
>         p = pgd + pgd_index(__START_KERNEL_map);
> -       if (la57)
> -               *p = (unsigned long)level4_kernel_pgt;
> -       else
> -               *p = (unsigned long)level3_kernel_pgt;
> -       *p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
> -
> +#ifdef CONFIG_X86_5LEVEL
>         if (la57) {
> +               *p = (unsigned long)level4_kernel_pgt;
>                 p4d = fixup_pointer(&level4_kernel_pgt, physaddr);
>                 p4d[511] += load_delta;
> -       }
> +       } else
> +#endif
> +               *p = (unsigned long)level3_kernel_pgt;
> +       *p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
>
>         pud = fixup_pointer(&level3_kernel_pgt, physaddr);
>         pud[510] += load_delta;
> --
> 2.17.1
>


Hmm, this code looks a bit ugly...

Does the following one liner work with CONFIG_CC_OPTIMIZE_FOR_DEBUGGING ?


diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
index 8047379..579847f 100644
--- a/arch/x86/kernel/head64.c
+++ b/arch/x86/kernel/head64.c
@@ -97,7 +97,7 @@ static bool __head check_la57_support(unsigned long physaddr)
        return true;
 }
 #else
-static bool __head check_la57_support(unsigned long physaddr)
+static __always_inline bool __head check_la57_support(unsigned long physaddr)
 {
        return false;
 }




--
Best Regards
Masahiro Yamada
================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v2 1/4] x86/mm: surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif
Date: Sun, 28 Oct 2018 12:46:18 +0000
Message-ID: <20181028124507.4n2sfkm7iteyoud5 () mail ! google ! com>
--------------------
On Sun, Oct 28, 2018 at 10:44:46AM +0900, Masahiro Yamada wrote:
> On Fri, Oct 19, 2018 at 9:50 PM Du Changbin <changbin.du@gmail.com> wrote:
> >
> > The level4_kernel_pgt is only defined when X86_5LEVEL is enabled. So
> > surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif to
> > make code correct.
> 
> For clarification, is it better to mention
> that this is a preparation for CONFIG_CC_OPTIMIZE_FOR_DEBUGGING ?
>
yes, I will update the commit msg.

> 
> > Signed-off-by: Du Changbin <changbin.du@gmail.com>
> > Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
> > ---
> >  arch/x86/include/asm/pgtable_64.h |  2 ++
> >  arch/x86/kernel/head64.c          | 13 ++++++-------
> >  2 files changed, 8 insertions(+), 7 deletions(-)
> >
> > diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h
> > index 9c85b54bf03c..9333f7fa5bdb 100644
> > --- a/arch/x86/include/asm/pgtable_64.h
> > +++ b/arch/x86/include/asm/pgtable_64.h
> > @@ -16,7 +16,9 @@
> >  #include <linux/threads.h>
> >  #include <asm/fixmap.h>
> >
> > +#ifdef CONFIG_X86_5LEVEL
> >  extern p4d_t level4_kernel_pgt[512];
> > +#endif
> 
> 
> Is this #ifdef  necessary?
> 
> It is harmless to declaring unused stuff.
> 
> 
> 
> >  extern p4d_t level4_ident_pgt[512];
> >  extern pud_t level3_kernel_pgt[512];
> >  extern pud_t level3_ident_pgt[512];
> > diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
> > index ddee1f0870c4..4a59ef93c258 100644
> > --- a/arch/x86/kernel/head64.c
> > +++ b/arch/x86/kernel/head64.c
> > @@ -151,16 +151,15 @@ unsigned long __head __startup_64(unsigned long physaddr,
> >
> >         pgd = fixup_pointer(&early_top_pgt, physaddr);
> >         p = pgd + pgd_index(__START_KERNEL_map);
> > -       if (la57)
> > -               *p = (unsigned long)level4_kernel_pgt;
> > -       else
> > -               *p = (unsigned long)level3_kernel_pgt;
> > -       *p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
> > -
> > +#ifdef CONFIG_X86_5LEVEL
> >         if (la57) {
> > +               *p = (unsigned long)level4_kernel_pgt;
> >                 p4d = fixup_pointer(&level4_kernel_pgt, physaddr);
> >                 p4d[511] += load_delta;
> > -       }
> > +       } else
> > +#endif
> > +               *p = (unsigned long)level3_kernel_pgt;
> > +       *p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
> >
> >         pud = fixup_pointer(&level3_kernel_pgt, physaddr);
> >         pud[510] += load_delta;
> > --
> > 2.17.1
> >
> 
> 
> Hmm, this code looks a bit ugly...
> 
> Does the following one liner work with CONFIG_CC_OPTIMIZE_FOR_DEBUGGING ?
> 
> 
> diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
> index 8047379..579847f 100644
> --- a/arch/x86/kernel/head64.c
> +++ b/arch/x86/kernel/head64.c
> @@ -97,7 +97,7 @@ static bool __head check_la57_support(unsigned long physaddr)
>         return true;
>  }
>  #else
> -static bool __head check_la57_support(unsigned long physaddr)
> +static __always_inline bool __head check_la57_support(unsigned long physaddr)
>  {
>         return false;
>  }
> 
This is much better. I just declared it as 'inline'. Thanks for your suggestion.

> 
> 
> 
> --
> Best Regards
> Masahiro Yamada

-- 
Thanks,
Changbin Du
================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-kbuild
Subject: Re: [PATCH v2 1/4] x86/mm: surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif
Date: Sun, 28 Oct 2018 12:46:18 +0000
Message-ID: <20181028124507.4n2sfkm7iteyoud5 () mail ! google ! com>
--------------------
On Sun, Oct 28, 2018 at 10:44:46AM +0900, Masahiro Yamada wrote:
> On Fri, Oct 19, 2018 at 9:50 PM Du Changbin <changbin.du@gmail.com> wrote:
> >
> > The level4_kernel_pgt is only defined when X86_5LEVEL is enabled. So
> > surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif to
> > make code correct.
> 
> For clarification, is it better to mention
> that this is a preparation for CONFIG_CC_OPTIMIZE_FOR_DEBUGGING ?
>
yes, I will update the commit msg.

> 
> > Signed-off-by: Du Changbin <changbin.du@gmail.com>
> > Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
> > ---
> >  arch/x86/include/asm/pgtable_64.h |  2 ++
> >  arch/x86/kernel/head64.c          | 13 ++++++-------
> >  2 files changed, 8 insertions(+), 7 deletions(-)
> >
> > diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h
> > index 9c85b54bf03c..9333f7fa5bdb 100644
> > --- a/arch/x86/include/asm/pgtable_64.h
> > +++ b/arch/x86/include/asm/pgtable_64.h
> > @@ -16,7 +16,9 @@
> >  #include <linux/threads.h>
> >  #include <asm/fixmap.h>
> >
> > +#ifdef CONFIG_X86_5LEVEL
> >  extern p4d_t level4_kernel_pgt[512];
> > +#endif
> 
> 
> Is this #ifdef  necessary?
> 
> It is harmless to declaring unused stuff.
> 
> 
> 
> >  extern p4d_t level4_ident_pgt[512];
> >  extern pud_t level3_kernel_pgt[512];
> >  extern pud_t level3_ident_pgt[512];
> > diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
> > index ddee1f0870c4..4a59ef93c258 100644
> > --- a/arch/x86/kernel/head64.c
> > +++ b/arch/x86/kernel/head64.c
> > @@ -151,16 +151,15 @@ unsigned long __head __startup_64(unsigned long physaddr,
> >
> >         pgd = fixup_pointer(&early_top_pgt, physaddr);
> >         p = pgd + pgd_index(__START_KERNEL_map);
> > -       if (la57)
> > -               *p = (unsigned long)level4_kernel_pgt;
> > -       else
> > -               *p = (unsigned long)level3_kernel_pgt;
> > -       *p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
> > -
> > +#ifdef CONFIG_X86_5LEVEL
> >         if (la57) {
> > +               *p = (unsigned long)level4_kernel_pgt;
> >                 p4d = fixup_pointer(&level4_kernel_pgt, physaddr);
> >                 p4d[511] += load_delta;
> > -       }
> > +       } else
> > +#endif
> > +               *p = (unsigned long)level3_kernel_pgt;
> > +       *p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
> >
> >         pud = fixup_pointer(&level3_kernel_pgt, physaddr);
> >         pud[510] += load_delta;
> > --
> > 2.17.1
> >
> 
> 
> Hmm, this code looks a bit ugly...
> 
> Does the following one liner work with CONFIG_CC_OPTIMIZE_FOR_DEBUGGING ?
> 
> 
> diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
> index 8047379..579847f 100644
> --- a/arch/x86/kernel/head64.c
> +++ b/arch/x86/kernel/head64.c
> @@ -97,7 +97,7 @@ static bool __head check_la57_support(unsigned long physaddr)
>         return true;
>  }
>  #else
> -static bool __head check_la57_support(unsigned long physaddr)
> +static __always_inline bool __head check_la57_support(unsigned long physaddr)
>  {
>         return false;
>  }
> 
This is much better. I just declared it as 'inline'. Thanks for your suggestion.

> 
> 
> 
> --
> Best Regards
> Masahiro Yamada

-- 
Thanks,
Changbin Du
================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-kernel
Subject: Re: [PATCH v2 1/4] x86/mm: surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif
Date: Sun, 28 Oct 2018 12:46:18 +0000
Message-ID: <20181028124507.4n2sfkm7iteyoud5 () mail ! google ! com>
--------------------
On Sun, Oct 28, 2018 at 10:44:46AM +0900, Masahiro Yamada wrote:
> On Fri, Oct 19, 2018 at 9:50 PM Du Changbin <changbin.du@gmail.com> wrote:
> >
> > The level4_kernel_pgt is only defined when X86_5LEVEL is enabled. So
> > surround level4_kernel_pgt with #ifdef CONFIG_X86_5LEVEL...#endif to
> > make code correct.
> 
> For clarification, is it better to mention
> that this is a preparation for CONFIG_CC_OPTIMIZE_FOR_DEBUGGING ?
>
yes, I will update the commit msg.

> 
> > Signed-off-by: Du Changbin <changbin.du@gmail.com>
> > Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
> > ---
> >  arch/x86/include/asm/pgtable_64.h |  2 ++
> >  arch/x86/kernel/head64.c          | 13 ++++++-------
> >  2 files changed, 8 insertions(+), 7 deletions(-)
> >
> > diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h
> > index 9c85b54bf03c..9333f7fa5bdb 100644
> > --- a/arch/x86/include/asm/pgtable_64.h
> > +++ b/arch/x86/include/asm/pgtable_64.h
> > @@ -16,7 +16,9 @@
> >  #include <linux/threads.h>
> >  #include <asm/fixmap.h>
> >
> > +#ifdef CONFIG_X86_5LEVEL
> >  extern p4d_t level4_kernel_pgt[512];
> > +#endif
> 
> 
> Is this #ifdef  necessary?
> 
> It is harmless to declaring unused stuff.
> 
> 
> 
> >  extern p4d_t level4_ident_pgt[512];
> >  extern pud_t level3_kernel_pgt[512];
> >  extern pud_t level3_ident_pgt[512];
> > diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
> > index ddee1f0870c4..4a59ef93c258 100644
> > --- a/arch/x86/kernel/head64.c
> > +++ b/arch/x86/kernel/head64.c
> > @@ -151,16 +151,15 @@ unsigned long __head __startup_64(unsigned long physaddr,
> >
> >         pgd = fixup_pointer(&early_top_pgt, physaddr);
> >         p = pgd + pgd_index(__START_KERNEL_map);
> > -       if (la57)
> > -               *p = (unsigned long)level4_kernel_pgt;
> > -       else
> > -               *p = (unsigned long)level3_kernel_pgt;
> > -       *p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
> > -
> > +#ifdef CONFIG_X86_5LEVEL
> >         if (la57) {
> > +               *p = (unsigned long)level4_kernel_pgt;
> >                 p4d = fixup_pointer(&level4_kernel_pgt, physaddr);
> >                 p4d[511] += load_delta;
> > -       }
> > +       } else
> > +#endif
> > +               *p = (unsigned long)level3_kernel_pgt;
> > +       *p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
> >
> >         pud = fixup_pointer(&level3_kernel_pgt, physaddr);
> >         pud[510] += load_delta;
> > --
> > 2.17.1
> >
> 
> 
> Hmm, this code looks a bit ugly...
> 
> Does the following one liner work with CONFIG_CC_OPTIMIZE_FOR_DEBUGGING ?
> 
> 
> diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c
> index 8047379..579847f 100644
> --- a/arch/x86/kernel/head64.c
> +++ b/arch/x86/kernel/head64.c
> @@ -97,7 +97,7 @@ static bool __head check_la57_support(unsigned long physaddr)
>         return true;
>  }
>  #else
> -static bool __head check_la57_support(unsigned long physaddr)
> +static __always_inline bool __head check_la57_support(unsigned long physaddr)
>  {
>         return false;
>  }
> 
This is much better. I just declared it as 'inline'. Thanks for your suggestion.

> 
> 
> 
> --
> Best Regards
> Masahiro Yamada

-- 
Thanks,
Changbin Du
================================================================================


################################################################################

=== Thread: [PATCH v2 1/5] add copy_ptr_list() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v2 1/5] add copy_ptr_list()
Date: Wed, 25 Jul 2018 20:44:37 +0000
Message-ID: <20180725204441.91527-2-luc.vanoostenryck () gmail ! com>
--------------------
When an instruction can be replaced by a pseudo, the user list of
this pseudo and the instruction's target must be merged.

Currently this is done by concat_ptr_list() which copy the elements
of one list into the other using add_ptr_list(). This incurs quite
a bit overhead.

Add a new more efficient ptrlist function: copy_ptr_list() which
copy the element by block by looping over both list in parallel.

This gives a speedup up to 26% on some pathological workloads.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 flow.c    |  2 +-
 ptrlist.c | 49 +++++++++++++++++++++++++++++++++++++++++++++++++
 ptrlist.h |  1 +
 3 files changed, 51 insertions(+), 1 deletion(-)

diff --git a/flow.c b/flow.c
index f928c2684..9483938fb 100644
--- a/flow.c
+++ b/flow.c
@@ -278,7 +278,7 @@ int simplify_flow(struct entrypoint *ep)
 
 static inline void concat_user_list(struct pseudo_user_list *src, struct pseudo_user_list **dst)
 {
-	concat_ptr_list((struct ptr_list *)src, (struct ptr_list **)dst);
+	copy_ptr_list((struct ptr_list **)dst, (struct ptr_list *)src);
 }
 
 void convert_instruction_target(struct instruction *insn, pseudo_t src)
diff --git a/ptrlist.c b/ptrlist.c
index c7ebf5a3f..684aff8c5 100644
--- a/ptrlist.c
+++ b/ptrlist.c
@@ -340,6 +340,55 @@ void concat_ptr_list(struct ptr_list *a, struct ptr_list **b)
 	} END_FOR_EACH_PTR(entry);
 }
 
+///
+// copy the elements of a list at the end of another list.
+// @listp: a pointer to the destination list.
+// @src: the head of the source list.
+void copy_ptr_list(struct ptr_list **listp, struct ptr_list *src)
+{
+	struct ptr_list *head, *tail;
+	struct ptr_list *cur = src;
+	int idx;
+
+	if (!src)
+		return;
+	head = *listp;
+	if (!head) {
+		*listp = src;
+		return;
+	}
+
+	tail = head->prev;
+	idx = tail->nr;
+	do {
+		struct ptr_list *next;
+		int nr = cur->nr;
+		int i;
+		for (i = 0; i < nr;) {
+			void *ptr = cur->list[i++];
+			if (!ptr)
+				continue;
+			if (idx >= LIST_NODE_NR) {
+				struct ptr_list *prev = tail;
+				tail = __alloc_ptrlist(0);
+				prev->next = tail;
+				tail->prev = prev;
+				prev->nr = idx;
+				idx = 0;
+			}
+			tail->list[idx++] = ptr;
+		}
+
+		next = cur->next;
+		__free_ptrlist(cur);
+		cur = next;
+	} while (cur != src);
+
+	tail->nr = idx;
+	head->prev = tail;
+	tail->next = head;
+}
+
 ///
 // free a ptrlist
 // @listp: a pointer to the list
diff --git a/ptrlist.h b/ptrlist.h
index e97cdda31..46a9baee2 100644
--- a/ptrlist.h
+++ b/ptrlist.h
@@ -35,6 +35,7 @@ int replace_ptr_list_entry(struct ptr_list **, void *old, void *new, int);
 extern void sort_list(struct ptr_list **, int (*)(const void *, const void *));
 
 extern void concat_ptr_list(struct ptr_list *a, struct ptr_list **b);
+extern void copy_ptr_list(struct ptr_list **h, struct ptr_list *t);
 extern int ptr_list_size(struct ptr_list *);
 extern int linearize_ptr_list(struct ptr_list *, void **, int);
 extern void *first_ptr_list(struct ptr_list *);
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2 10/13] autodoc: add a small cheatsheet for reST markup ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v2 10/13] autodoc: add a small cheatsheet for reST markup
Date: Sat, 19 May 2018 13:05:59 +0000
Message-ID: <20180519130602.90096-11-luc.vanoostenryck () gmail ! com>
--------------------
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/doc-guide.rst | 119 ++++++++++++++++++++++++++++++++++++
 1 file changed, 119 insertions(+)

diff --git a/Documentation/doc-guide.rst b/Documentation/doc-guide.rst
index cbc3f842f..334502c0f 100644
--- a/Documentation/doc-guide.rst
+++ b/Documentation/doc-guide.rst
@@ -24,9 +24,128 @@ list the name of the files with real documentation.
 .. _reStructuredText: http://docutils.sourceforge.net/rst.html
 .. _MarkDown: https://en.wikipedia.org/wiki/Markdown
 
+
+.. _rest-markup:
+
+Mnimal reST cheatsheet
+----------------------
+
+Basic inline markup is:
+
+* italic: ``*italic*`` gives *italic*
+* bold: ``**bold**`` gives **bold**
+* monospace: ````monospace```` gives ``monospace``
+
+Headings are created by underlining the title with a punctuation
+character; it can also be optionally overlined::
+
+	#############
+	Major heading
+	#############
+
+	Minor heading
+	-------------
+
+Any punctuation character can be used and the levels are automatically
+determined from their nesting. However, the convention is to use:
+
+* ``#`` with overline for parts
+* ``*`` with overline for chapters
+* ``=`` for sections
+* ``-`` for subsections
+* ``^`` for subsubsections
+
+
+Lists can be created like this::
+
+	* this is a bulleted list
+	* with the second item
+	  on two lines
+	* nested lists are supported
+
+		* subitem
+		* another subitem
+
+	* and here is the fourth item
+
+	#. this is an auto-numbered
+	#. with two items
+
+	1. this is an explicitely numbered list
+	2. with two items
+
+
+Definition lists are created with a simple indentation, like::
+
+	term, concept, whatever
+		Definition, must be indented and
+		continue here.
+
+		It can also have several paragraphs.
+
+Literal blocks are introduced with ``::``, either at the end of the
+preceding paragraph or on its own line, and indented text::
+
+	This is a paragraph introducing a literal block::
+
+		This is the literal block.
+		It can span several lines.
+
+		It can also consist of several paragraphs.
+
+Code examples with syntax hightlighting use the *code-block* directive.
+For example::
+
+	.. code-block:: c
+
+		int foo(int a)
+		{
+			return a + 1;
+		}
+
+will give:
+
+.. code-block:: c
+
+	int foo(int a)
+	{
+		return a + 1;
+	}
+
+
 Autodoc
 -------
 
 .. highlight:: none
 .. c:autodoc:: Documentation/sphinx/cdoc.py
 
+For example, a doc-block like::
+
+	///
+	// increment a value
+	//
+	// @val: the value to increment
+	// @return: the incremented value
+	//
+	// This function is to be used to increment a
+	// value.
+	//
+	// It's strongly encouraged to use this
+	// function instead of open coding a simple
+	// ``++``.
+	int inc(int val)
+
+will be displayed like this:
+
+.. c:function:: int inc(int val)
+	:noindex:
+
+	:param val: the value to increment
+	:return: the incremented value
+
+	This function is to be used to increment a
+	value.
+
+	It's strongly encouraged to use this
+	function instead of open coding a simple
+	``++``.
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2 11/13] autodoc: support muti-line param & return descriptions ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v2 11/13] autodoc: support muti-line param & return descriptions
Date: Sat, 19 May 2018 13:06:00 +0000
Message-ID: <20180519130602.90096-12-luc.vanoostenryck () gmail ! com>
--------------------
Short descriptions are good but sometimes you can't describe
thinsg well enough with a single line.

So, add support for multi-line descriptions.
The additional lines need to be indented with a tab to be
recognized as such.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/sphinx/cdoc.py | 18 ++++++++++++++++--
 1 file changed, 16 insertions(+), 2 deletions(-)

diff --git a/Documentation/sphinx/cdoc.py b/Documentation/sphinx/cdoc.py
index 410a55ea2..20d53f63e 100755
--- a/Documentation/sphinx/cdoc.py
+++ b/Documentation/sphinx/cdoc.py
@@ -24,7 +24,8 @@
 // 	// <mandatory short one-line description>
 // 	// <optional blanck line>
 // 	// @<1st paramater's name>: <description>
-// 	// @<2nd parameter's name>: ...
+// 	// @<2nd parameter's name>: <long description
+// 	// <tab>which needs multiple lines>
 // 	// @return: <description> (absent for void functions)
 // 	// <optional blank line>
 // 	// <optional long multi-line description>
@@ -83,6 +84,18 @@ class Lines:
 		# type: () -> None
 		self.back = True
 
+def readline_multi(lines, line):
+	# type: (Lines, str) -> str
+	try:
+		while True:
+			(n, l) = next(lines)
+			if not l.startswith('//\t'):
+				raise StopIteration
+			line += '\n' + l[3:]
+	except:
+		lines.undo()
+	return line
+
 def readline_delim(lines, delim):
 	# type: (Lines, Tuple[str, str]) -> Tuple[int, str]
 	try:
@@ -139,7 +152,7 @@ def process_block(lines):
 				sep = m.group(2)
 				## FIXME/ warn if sep != ': '
 				l = m.group(3)
-				## FIXME: try multi-line ???
+				l = readline_multi(lines, l)
 				tags.append((n, tag, l))
 			else:
 				lines.undo()
@@ -224,6 +237,7 @@ def convert_to_rst(info):
 					name = 'param ' + name
 				l = decorate(l)
 				l = '\t:%s: %s' % (name, l)
+				l = '\n\t\t'.join(l.split('\n'))
 				lst.append((n, l))
 			lst.append((n+1, ''))
 		if 'desc' in info:
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2 12/13] autodoc: document a few more APIs to test multiline ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v2 12/13] autodoc: document a few more APIs to test multiline
Date: Sat, 19 May 2018 13:06:01 +0000
Message-ID: <20180519130602.90096-13-luc.vanoostenryck () gmail ! com>
--------------------
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/api.rst | 10 ++++++++++
 evaluate.h            | 15 +++++++++++++++
 expression.h          | 18 ++++++++++++++++--
 3 files changed, 41 insertions(+), 2 deletions(-)

diff --git a/Documentation/api.rst b/Documentation/api.rst
index 8b6f04011..d1a1d3ca4 100644
--- a/Documentation/api.rst
+++ b/Documentation/api.rst
@@ -9,3 +9,13 @@ Utilities
 ~~~~~~~~~
 
 .. c:autodoc:: ptrlist.c
+
+Parsing
+~~~~~~~
+
+.. c:autodoc:: expression.h
+
+Typing
+~~~~~~
+
+.. c:autodoc:: evaluate.h
diff --git a/evaluate.h b/evaluate.h
index 36de7d696..f68f7fb7c 100644
--- a/evaluate.h
+++ b/evaluate.h
@@ -6,8 +6,23 @@ struct statement;
 struct symbol;
 struct symbol_list;
 
+///
+// evaluate the type of an expression
+// @expr: the expression to be evaluated
+// @return: the type of the expression or ``NULL``
+//	if the expression can't be evaluated
 struct symbol *evaluate_expression(struct expression *expr);
+
+///
+// evaluate the type of a statement
+// @stmt: the statement to be evaluated
+// @return: the type of the statement or ``NULL``
+//	if it can't be evaluated
 struct symbol *evaluate_statement(struct statement *stmt);
+
+///
+// evaluate the type of a set of symbols
+// @list: the list of the symbol to be evaluated
 void evaluate_symbol_list(struct symbol_list *list);
 
 #endif
diff --git a/expression.h b/expression.h
index b5fa9bc14..ba4157fda 100644
--- a/expression.h
+++ b/expression.h
@@ -243,9 +243,23 @@ struct expression {
 	};
 };
 
-/* Constant expression values */
-int is_zero_constant(struct expression *);
+///
+// Constant expression values
+// --------------------------
+
+///
+// test if an expression evaluates to the constant ``0``.
+// @return: ``1`` if @expr evaluate to ``0``,
+//	``0`` otherwise.
+int is_zero_constant(struct expression *expr);
+
+///
+// test the compile time truth value of an expression
+// @return:
+//	* ``-1`` if @expr is not constant,
+//	* ``0`` or ``1`` depending on the truth value of @expr.
 int expr_truth_value(struct expression *expr);
+
 long long get_expression_value(struct expression *);
 long long const_expression_value(struct expression *);
 long long get_expression_value_silent(struct expression *expr);
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2 13/13] autodoc: add autodoc tests in the testsuite ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v2 13/13] autodoc: add autodoc tests in the testsuite
Date: Sat, 19 May 2018 13:06:02 +0000
Message-ID: <20180519130602.90096-14-luc.vanoostenryck () gmail ! com>
--------------------
It's certainly worth to have some tests but to not
slow down the testsuite and to not create a dependency
on python this test need to be run explicitely with:
	./test-suite doc/cdoc.cdoc

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/doc/cdoc.cdoc | 177 +++++++++++++++++++++++++++++++++++++++
 validation/test-suite    |   2 +-
 2 files changed, 178 insertions(+), 1 deletion(-)
 create mode 100644 validation/doc/cdoc.cdoc

diff --git a/validation/doc/cdoc.cdoc b/validation/doc/cdoc.cdoc
new file mode 100644
index 000000000..f2d99ab9e
--- /dev/null
+++ b/validation/doc/cdoc.cdoc
@@ -0,0 +1,177 @@
+///
+// Title
+// -----
+
+///
+// short description
+int a(int param, int arg);
+
+///
+// short description
+// longer description
+int b(int param, int arg);
+
+///
+// short description
+//
+// longer description with empty line
+int c(int param, int arg);
+
+///
+// short description
+// longer description
+// which needs two lines
+int d(int param, int arg);
+
+///
+// short description
+//
+// longer description with empty line
+// which needs two lines
+int e(int param, int arg);
+
+///
+// condensed format
+// @param: desc param
+// @arg: desc arg
+// @return: desc return
+// longer description
+int f(int param, int arg);
+
+///
+// more airy format
+//
+// @param: desc param
+// @arg: desc arg
+// @return: desc return
+//
+// longer description
+int g(int param, int arg);
+
+///
+// short description
+// @return: ``1`` if @param is zero,
+//	``0`` otherwise.
+int h(int param, int arg);
+
+///
+// short description
+// @return:
+//	* ``1`` if @param is zero,
+//	* ``0`` otherwise.
+int i(int param, int arg);
+
+///
+// short description
+int m(int param, int arg)
+{ return 0; }
+
+///
+// short description
+int n(int param,
+	int arg)
+{ return 0; }
+
+///
+// short description
+int o(int param, int arg);
+
+///
+// short description
+int p(int param,
+	int arg);
+
+
+/*
+ * check-name: cdoc
+ * check-command: Documentation/sphinx/cdoc.py < $file
+ *
+ * check-output-start
+   2: Title
+   3: -----
+   4: 
+   4: 
+   5: 
+   7: .. c:function:: int a(int param, int arg)
+   8: 
+   6: 	Short description.
+   7: 
+  12: .. c:function:: int b(int param, int arg)
+  13: 
+  10: 	Short description.
+  11: 
+  11: 	longer description
+  12: 
+  18: .. c:function:: int c(int param, int arg)
+  19: 
+  15: 	Short description.
+  16: 
+  17: 	longer description with empty line
+  18: 
+  24: .. c:function:: int d(int param, int arg)
+  25: 
+  21: 	Short description.
+  22: 
+  22: 	longer description
+  23: 	which needs two lines
+  24: 
+  31: .. c:function:: int e(int param, int arg)
+  32: 
+  27: 	Short description.
+  28: 
+  29: 	longer description with empty line
+  30: 	which needs two lines
+  31: 
+  39: .. c:function:: int f(int param, int arg)
+  40: 
+  34: 	Condensed format.
+  35: 
+  35: 	:param param: desc param
+  36: 	:param arg: desc arg
+  37: 	:return: desc return
+  38: 
+  38: 	longer description
+  39: 
+  49: .. c:function:: int g(int param, int arg)
+  50: 
+  42: 	More airy format.
+  43: 
+  44: 	:param param: desc param
+  45: 	:param arg: desc arg
+  46: 	:return: desc return
+  47: 
+  48: 	longer description
+  49: 
+  55: .. c:function:: int h(int param, int arg)
+  56: 
+  52: 	Short description.
+  53: 
+  53: 	:return: ``1`` if **param** is zero,
+  54: 		``0`` otherwise.
+  54: 
+  62: .. c:function:: int i(int param, int arg)
+  63: 
+  58: 	Short description.
+  59: 
+  59: 	:return: 
+  60: 		* ``1`` if **param** is zero,
+  61: 		* ``0`` otherwise.
+  60: 
+  66: .. c:function:: int m(int param, int arg)
+  67: 
+  65: 	Short description.
+  66: 
+  71: .. c:function:: int n(int param, int arg)
+  72: 
+  70: 	Short description.
+  71: 
+  77: .. c:function:: int o(int param, int arg)
+  78: 
+  76: 	Short description.
+  77: 
+  81: .. c:function:: int p(int param, int arg)
+  82: 
+  80: 	Short description.
+  81: 
+ * check-output-end
+ */
diff --git a/validation/test-suite b/validation/test-suite
index 4fdc9e9fa..930919de3 100755
--- a/validation/test-suite
+++ b/validation/test-suite
@@ -540,7 +540,7 @@ while [ "$#" -gt "0" ]; do
 		exit 1
 		;;
 
-	*.c)
+	*.c|*.cdoc)
 		tests_list="$tests_list $1"
 		;;
 	*)
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2 2/2] doc: document the debug flags ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v2 2/2] doc: document the debug flags
Date: Mon, 19 Mar 2018 17:25:34 +0000
Message-ID: <20180319172534.44730-3-luc.vanoostenryck () gmail ! com>
--------------------
The flags -ventry & -vdead were not documented.
Fix this now by adding a small explanation.

Reviewed-by: Randy Dunlap <rdunlap@infradead.org>
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Documentation/dev-options.md | 6 ++++++
 1 file changed, 6 insertions(+)

diff --git a/Documentation/dev-options.md b/Documentation/dev-options.md
index 23ea21330..7c278135a 100644
--- a/Documentation/dev-options.md
+++ b/Documentation/dev-options.md
@@ -26,3 +26,9 @@ document options only useful for development on sparse itself.
   * 'linearize'
   * 'mem2reg'
   * 'final'
+
+* '-v<debug-flag>'
+
+  Add or display some debug info. The flag can be one of:
+  * 'dead': annotate dead pseudos.
+  * 'entry': dump the IR after all optimization passes.
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimiz ===

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: [PATCH v2 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimiz
Date: Sun, 28 Oct 2018 13:11:27 +0000
Message-ID: <CAK7LNASAv3OcujS9RssGue-0Nd+zVh+O4jMOOmKQOqdtWWt9Jw () mail ! gmail ! com>
--------------------
On Fri, Oct 19, 2018 at 9:50 PM Du Changbin <changbin.du@gmail.com> wrote:
>
> This patch add a new kernel hacking option NO_AUTO_INLINE. Selecting
> this option will prevent the compiler from optimizing the kernel by
> auto-inlining functions not marked with the inline keyword.
>
> With this option, only functions explicitly marked with "inline" will
> be inlined. This will allow the function tracer to trace more functions
> because it only traces functions that the compiler has not inlined.
>
> Signed-off-by: Du Changbin <changbin.du@gmail.com>
> Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>


This subject
"kernel hacking: new config NO_AUTO_INLINE to disable compiler
auto-inline optimizations"
is also too long.

Could you please make it a bit shorter?


-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v2 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimiz
Date: Sun, 28 Oct 2018 13:11:27 +0000
Message-ID: <CAK7LNASAv3OcujS9RssGue-0Nd+zVh+O4jMOOmKQOqdtWWt9Jw () mail ! gmail ! com>
--------------------
On Fri, Oct 19, 2018 at 9:50 PM Du Changbin <changbin.du@gmail.com> wrote:
>
> This patch add a new kernel hacking option NO_AUTO_INLINE. Selecting
> this option will prevent the compiler from optimizing the kernel by
> auto-inlining functions not marked with the inline keyword.
>
> With this option, only functions explicitly marked with "inline" will
> be inlined. This will allow the function tracer to trace more functions
> because it only traces functions that the compiler has not inlined.
>
> Signed-off-by: Du Changbin <changbin.du@gmail.com>
> Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>


This subject
"kernel hacking: new config NO_AUTO_INLINE to disable compiler
auto-inline optimizations"
is also too long.

Could you please make it a bit shorter?


-- 
Best Regards
Masahiro Yamada

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kbuild
Subject: Re: [PATCH v2 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimiz
Date: Sun, 28 Oct 2018 13:11:27 +0000
Message-ID: <CAK7LNASAv3OcujS9RssGue-0Nd+zVh+O4jMOOmKQOqdtWWt9Jw () mail ! gmail ! com>
--------------------
On Fri, Oct 19, 2018 at 9:50 PM Du Changbin <changbin.du@gmail.com> wrote:
>
> This patch add a new kernel hacking option NO_AUTO_INLINE. Selecting
> this option will prevent the compiler from optimizing the kernel by
> auto-inlining functions not marked with the inline keyword.
>
> With this option, only functions explicitly marked with "inline" will
> be inlined. This will allow the function tracer to trace more functions
> because it only traces functions that the compiler has not inlined.
>
> Signed-off-by: Du Changbin <changbin.du@gmail.com>
> Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>


This subject
"kernel hacking: new config NO_AUTO_INLINE to disable compiler
auto-inline optimizations"
is also too long.

Could you please make it a bit shorter?


-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kernel
Subject: Re: [PATCH v2 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimiz
Date: Sun, 28 Oct 2018 13:11:27 +0000
Message-ID: <CAK7LNASAv3OcujS9RssGue-0Nd+zVh+O4jMOOmKQOqdtWWt9Jw () mail ! gmail ! com>
--------------------
On Fri, Oct 19, 2018 at 9:50 PM Du Changbin <changbin.du@gmail.com> wrote:
>
> This patch add a new kernel hacking option NO_AUTO_INLINE. Selecting
> this option will prevent the compiler from optimizing the kernel by
> auto-inlining functions not marked with the inline keyword.
>
> With this option, only functions explicitly marked with "inline" will
> be inlined. This will allow the function tracer to trace more functions
> because it only traces functions that the compiler has not inlined.
>
> Signed-off-by: Du Changbin <changbin.du@gmail.com>
> Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>


This subject
"kernel hacking: new config NO_AUTO_INLINE to disable compiler
auto-inline optimizations"
is also too long.

Could you please make it a bit shorter?


-- 
Best Regards
Masahiro Yamada
================================================================================


################################################################################

=== Thread: [PATCH v2 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimizatio ===

From: Du Changbin <changbin.du () gmail ! com>
To: linux-kbuild
Subject: [PATCH v2 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimizatio
Date: Fri, 19 Oct 2018 12:49:19 +0000
Message-ID: <20181019124921.13780-3-changbin.du () gmail ! com>
--------------------
This patch add a new kernel hacking option NO_AUTO_INLINE. Selecting
this option will prevent the compiler from optimizing the kernel by
auto-inlining functions not marked with the inline keyword.

With this option, only functions explicitly marked with "inline" will
be inlined. This will allow the function tracer to trace more functions
because it only traces functions that the compiler has not inlined.

Signed-off-by: Du Changbin <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
---
 Makefile          |  6 ++++++
 lib/Kconfig.debug | 17 +++++++++++++++++
 2 files changed, 23 insertions(+)

diff --git a/Makefile b/Makefile
index e8b599b4dcde..757d6507cb5c 100644
--- a/Makefile
+++ b/Makefile
@@ -749,6 +749,12 @@ KBUILD_CFLAGS 	+= $(call cc-option, -femit-struct-debug-baseonly) \
 		   $(call cc-option,-fno-var-tracking)
 endif
 
+ifdef CONFIG_NO_AUTO_INLINE
+KBUILD_CFLAGS   += $(call cc-option, -fno-inline-functions) \
+		   $(call cc-option, -fno-inline-small-functions) \
+		   $(call cc-option, -fno-inline-functions-called-once)
+endif
+
 ifdef CONFIG_FUNCTION_TRACER
 ifdef CONFIG_FTRACE_MCOUNT_RECORD
   # gcc 5 supports generating the mcount tables directly
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 4966c4fbe7f7..c7c28ee01dfc 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -211,6 +211,23 @@ config GDB_SCRIPTS
 	  instance. See Documentation/dev-tools/gdb-kernel-debugging.rst
 	  for further details.
 
+config NO_AUTO_INLINE
+	bool "Disable compiler auto-inline optimizations"
+	help
+	  This will prevent the compiler from optimizing the kernel by
+	  auto-inlining functions not marked with the inline keyword.
+	  With this option, only functions explicitly marked with
+	  "inline" will be inlined. This will allow the function tracer
+	  to trace more functions because it only traces functions that
+	  the compiler has not inlined.
+
+	  Enabling this function can help debugging a kernel if using
+	  the function tracer. But it can also change how the kernel
+	  works, because inlining functions may change the timing,
+	  which could make it difficult while debugging race conditions.
+
+	  If unsure, select N.
+
 config ENABLE_MUST_CHECK
 	bool "Enable __must_check logic"
 	default y
-- 
2.17.1

================================================================================

From: Du Changbin <changbin.du () gmail ! com>
To: linux-arm-kernel
Subject: [PATCH v2 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimizatio
Date: Fri, 19 Oct 2018 12:49:19 +0000
Message-ID: <20181019124921.13780-3-changbin.du () gmail ! com>
--------------------
This patch add a new kernel hacking option NO_AUTO_INLINE. Selecting
this option will prevent the compiler from optimizing the kernel by
auto-inlining functions not marked with the inline keyword.

With this option, only functions explicitly marked with "inline" will
be inlined. This will allow the function tracer to trace more functions
because it only traces functions that the compiler has not inlined.

Signed-off-by: Du Changbin <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
---
 Makefile          |  6 ++++++
 lib/Kconfig.debug | 17 +++++++++++++++++
 2 files changed, 23 insertions(+)

diff --git a/Makefile b/Makefile
index e8b599b4dcde..757d6507cb5c 100644
--- a/Makefile
+++ b/Makefile
@@ -749,6 +749,12 @@ KBUILD_CFLAGS 	+= $(call cc-option, -femit-struct-debug-baseonly) \
 		   $(call cc-option,-fno-var-tracking)
 endif
 
+ifdef CONFIG_NO_AUTO_INLINE
+KBUILD_CFLAGS   += $(call cc-option, -fno-inline-functions) \
+		   $(call cc-option, -fno-inline-small-functions) \
+		   $(call cc-option, -fno-inline-functions-called-once)
+endif
+
 ifdef CONFIG_FUNCTION_TRACER
 ifdef CONFIG_FTRACE_MCOUNT_RECORD
   # gcc 5 supports generating the mcount tables directly
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 4966c4fbe7f7..c7c28ee01dfc 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -211,6 +211,23 @@ config GDB_SCRIPTS
 	  instance. See Documentation/dev-tools/gdb-kernel-debugging.rst
 	  for further details.
 
+config NO_AUTO_INLINE
+	bool "Disable compiler auto-inline optimizations"
+	help
+	  This will prevent the compiler from optimizing the kernel by
+	  auto-inlining functions not marked with the inline keyword.
+	  With this option, only functions explicitly marked with
+	  "inline" will be inlined. This will allow the function tracer
+	  to trace more functions because it only traces functions that
+	  the compiler has not inlined.
+
+	  Enabling this function can help debugging a kernel if using
+	  the function tracer. But it can also change how the kernel
+	  works, because inlining functions may change the timing,
+	  which could make it difficult while debugging race conditions.
+
+	  If unsure, select N.
+
 config ENABLE_MUST_CHECK
 	bool "Enable __must_check logic"
 	default y
-- 
2.17.1


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Du Changbin <changbin.du () gmail ! com>
To: linux-kernel
Subject: [PATCH v2 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimizatio
Date: Fri, 19 Oct 2018 12:49:19 +0000
Message-ID: <20181019124921.13780-3-changbin.du () gmail ! com>
--------------------
This patch add a new kernel hacking option NO_AUTO_INLINE. Selecting
this option will prevent the compiler from optimizing the kernel by
auto-inlining functions not marked with the inline keyword.

With this option, only functions explicitly marked with "inline" will
be inlined. This will allow the function tracer to trace more functions
because it only traces functions that the compiler has not inlined.

Signed-off-by: Du Changbin <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
---
 Makefile          |  6 ++++++
 lib/Kconfig.debug | 17 +++++++++++++++++
 2 files changed, 23 insertions(+)

diff --git a/Makefile b/Makefile
index e8b599b4dcde..757d6507cb5c 100644
--- a/Makefile
+++ b/Makefile
@@ -749,6 +749,12 @@ KBUILD_CFLAGS 	+= $(call cc-option, -femit-struct-debug-baseonly) \
 		   $(call cc-option,-fno-var-tracking)
 endif
 
+ifdef CONFIG_NO_AUTO_INLINE
+KBUILD_CFLAGS   += $(call cc-option, -fno-inline-functions) \
+		   $(call cc-option, -fno-inline-small-functions) \
+		   $(call cc-option, -fno-inline-functions-called-once)
+endif
+
 ifdef CONFIG_FUNCTION_TRACER
 ifdef CONFIG_FTRACE_MCOUNT_RECORD
   # gcc 5 supports generating the mcount tables directly
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 4966c4fbe7f7..c7c28ee01dfc 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -211,6 +211,23 @@ config GDB_SCRIPTS
 	  instance. See Documentation/dev-tools/gdb-kernel-debugging.rst
 	  for further details.
 
+config NO_AUTO_INLINE
+	bool "Disable compiler auto-inline optimizations"
+	help
+	  This will prevent the compiler from optimizing the kernel by
+	  auto-inlining functions not marked with the inline keyword.
+	  With this option, only functions explicitly marked with
+	  "inline" will be inlined. This will allow the function tracer
+	  to trace more functions because it only traces functions that
+	  the compiler has not inlined.
+
+	  Enabling this function can help debugging a kernel if using
+	  the function tracer. But it can also change how the kernel
+	  works, because inlining functions may change the timing,
+	  which could make it difficult while debugging race conditions.
+
+	  If unsure, select N.
+
 config ENABLE_MUST_CHECK
 	bool "Enable __must_check logic"
 	default y
-- 
2.17.1

================================================================================


################################################################################

=== Thread: [PATCH v2 2/5] add ptr_list_empty() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v2 2/5] add ptr_list_empty()
Date: Wed, 25 Jul 2018 20:44:38 +0000
Message-ID: <20180725204441.91527-3-luc.vanoostenryck () gmail ! com>
--------------------
Sometimes we need to know if a list is empty, for example, in
order to determine if a pseudo has some users or not.

Currently, this is done using ptr_list_size(), which always
walks the whole list but the needed answer can be returned as
soon as it's known that the list contains at least one element.

Add the helper ptr_list_empty() and use it for has_users().

This gives a speedup up to 18% on some pathological workloads.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 linearize.h |  7 ++++++-
 ptrlist.c   | 19 +++++++++++++++++++
 ptrlist.h   |  2 ++
 simplify.c  |  2 +-
 4 files changed, 28 insertions(+), 2 deletions(-)

diff --git a/linearize.h b/linearize.h
index 092e1ac23..de42e718d 100644
--- a/linearize.h
+++ b/linearize.h
@@ -333,9 +333,14 @@ static inline int pseudo_user_list_size(struct pseudo_user_list *list)
 	return ptr_list_size((struct ptr_list *)list);
 }
 
+static inline bool pseudo_user_list_empty(struct pseudo_user_list *list)
+{
+	return ptr_list_empty((struct ptr_list *)list);
+}
+
 static inline int has_users(pseudo_t p)
 {
-	return pseudo_user_list_size(p->users) != 0;
+	return !pseudo_user_list_empty(p->users);
 }
 
 static inline struct pseudo_user *alloc_pseudo_user(struct instruction *insn, pseudo_t *pp)
diff --git a/ptrlist.c b/ptrlist.c
index 684aff8c5..356785dfc 100644
--- a/ptrlist.c
+++ b/ptrlist.c
@@ -36,6 +36,25 @@ int ptr_list_size(struct ptr_list *head)
 	return nr;
 }
 
+///
+// test if a list is empty
+// @head: the head of the list
+// @return: ``true`` if the list is empty, ``false`` otherwise.
+bool ptr_list_empty(const struct ptr_list *head)
+{
+	const struct ptr_list *list = head;
+
+	if (!head)
+		return true;
+
+	do {
+		if (list->nr - list->rm)
+			return false;
+	} while ((list = list->next) != head);
+
+	return true;
+}
+
 ///
 // get the first element of a ptrlist
 // @head: the head of the list
diff --git a/ptrlist.h b/ptrlist.h
index 46a9baee2..f145bc5f1 100644
--- a/ptrlist.h
+++ b/ptrlist.h
@@ -2,6 +2,7 @@
 #define PTR_LIST_H
 
 #include <stdlib.h>
+#include <stdbool.h>
 
 /*
  * Generic pointer list manipulation code. 
@@ -37,6 +38,7 @@ extern void sort_list(struct ptr_list **, int (*)(const void *, const void *));
 extern void concat_ptr_list(struct ptr_list *a, struct ptr_list **b);
 extern void copy_ptr_list(struct ptr_list **h, struct ptr_list *t);
 extern int ptr_list_size(struct ptr_list *);
+extern bool ptr_list_empty(const struct ptr_list *head);
 extern int linearize_ptr_list(struct ptr_list *, void **, int);
 extern void *first_ptr_list(struct ptr_list *);
 extern void *last_ptr_list(struct ptr_list *);
diff --git a/simplify.c b/simplify.c
index 741b1272c..4dc24a505 100644
--- a/simplify.c
+++ b/simplify.c
@@ -179,7 +179,7 @@ static int delete_pseudo_user_list_entry(struct pseudo_user_list **list, pseudo_
 	} END_FOR_EACH_PTR(pu);
 	assert(count <= 0);
 out:
-	if (pseudo_user_list_size(*list) == 0)
+	if (pseudo_user_list_empty(*list))
 		*list = NULL;
 	return count;
 }
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2 3/5] add ptr_list_multiple() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v2 3/5] add ptr_list_multiple()
Date: Wed, 25 Jul 2018 20:44:39 +0000
Message-ID: <20180725204441.91527-4-luc.vanoostenryck () gmail ! com>
--------------------
When doing IR simplification, to know if an instruction can be
destructively modified, it's needed to know if the pseudo it defines
is used by a single instruction or not. Currently this is done using
ptr_list_size() which needs to walk the whole list.

This walk is relatively costly when the list is long but knowing
if the list contains more than 1 element can often be answered
more cheaply since an answer can be returned as soon as it's
known that the list contains at least 2 elements.

Add the helpers ptr_list_multiple() and multi_users().

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 linearize.h |  5 +++++
 ptrlist.c   | 21 +++++++++++++++++++++
 ptrlist.h   |  1 +
 simplify.c  |  2 +-
 4 files changed, 28 insertions(+), 1 deletion(-)

diff --git a/linearize.h b/linearize.h
index de42e718d..b067b3e84 100644
--- a/linearize.h
+++ b/linearize.h
@@ -343,6 +343,11 @@ static inline int has_users(pseudo_t p)
 	return !pseudo_user_list_empty(p->users);
 }
 
+static inline bool multi_users(pseudo_t p)
+{
+	return ptr_list_multiple((struct ptr_list *)(p->users));
+}
+
 static inline struct pseudo_user *alloc_pseudo_user(struct instruction *insn, pseudo_t *pp)
 {
 	struct pseudo_user *user = __alloc_pseudo_user(0);
diff --git a/ptrlist.c b/ptrlist.c
index 356785dfc..ae00b5134 100644
--- a/ptrlist.c
+++ b/ptrlist.c
@@ -55,6 +55,27 @@ bool ptr_list_empty(const struct ptr_list *head)
 	return true;
 }
 
+///
+// test is a list contains more than one element
+// @head: the head of the list
+// @return: ``true`` if the list has more than 1 element, ``false`` otherwise.
+bool ptr_list_multiple(const struct ptr_list *head)
+{
+	const struct ptr_list *list = head;
+	int nr = 0;
+
+	if (!head)
+		return false;
+
+	do {
+		nr += list->nr - list->rm;
+		if (nr > 1)
+			return true;
+	} while ((list = list->next) != head);
+
+	return false;
+}
+
 ///
 // get the first element of a ptrlist
 // @head: the head of the list
diff --git a/ptrlist.h b/ptrlist.h
index f145bc5f1..176bb0712 100644
--- a/ptrlist.h
+++ b/ptrlist.h
@@ -39,6 +39,7 @@ extern void concat_ptr_list(struct ptr_list *a, struct ptr_list **b);
 extern void copy_ptr_list(struct ptr_list **h, struct ptr_list *t);
 extern int ptr_list_size(struct ptr_list *);
 extern bool ptr_list_empty(const struct ptr_list *head);
+extern bool ptr_list_multiple(const struct ptr_list *head);
 extern int linearize_ptr_list(struct ptr_list *, void **, int);
 extern void *first_ptr_list(struct ptr_list *);
 extern void *last_ptr_list(struct ptr_list *);
diff --git a/simplify.c b/simplify.c
index 4dc24a505..65f29de0a 100644
--- a/simplify.c
+++ b/simplify.c
@@ -824,7 +824,7 @@ static int simplify_associative_binop(struct instruction *insn)
 		return 0;
 	if (!simple_pseudo(def->src2))
 		return 0;
-	if (pseudo_user_list_size(def->target->users) != 1)
+	if (multi_users(def->target))
 		return 0;
 	switch_pseudo(def, &def->src1, insn, &insn->src2);
 	return REPEAT_CSE;
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimizatio ===

From: Changbin Du <changbin.du () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v2 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimizatio
Date: Mon, 29 Oct 2018 14:56:56 +0000
Message-ID: <20181029145655.vzlhz7dipzmpw52m () mail ! google ! com>
--------------------
On Mon, Oct 29, 2018 at 10:01:44PM +0900, Masahiro Yamada wrote:
> Hi.
> 
> 
> On Sun, Oct 28, 2018 at 10:14 PM Changbin Du <changbin.du@gmail.com> wrote:
> >
> > On Sun, Oct 28, 2018 at 10:09:21PM +0900, Masahiro Yamada wrote:
> > > Hi Changbin,
> > >
> > > On Sun, Oct 28, 2018 at 9:52 PM Changbin Du <changbin.du@gmail.com> wrote:
> > >
> > > > >
> > > > Thanks for pointing this out and kind suggestions. I have tested with your
> > > > modification. Will update patch serias soon.
> > >
> > >
> > > I have one more request.
> > >
> > > The patch subject
> > > "kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og
> > > optimization"
> > > is too long (83 characters)
> > >
> > >
> > > I'd like the subject to be within 70 columns or so.
> > >
> > oh, I just sent new version. Could me update the title if there are more
> > suggestions then?
> 
> 
> 
> How about this?
> 
> 2/4: kernel hacking: add a config option to disable compiler auto-inlining
> 3/4: ARM: mm: fix build error in fix_to_virt with -Og optimization level
> 4/4: kernel hacking: support building kernel with -Og optimization level
> 
> (The CONFIG name is so long that the subject
> would easily exceeds 80-columns with it.)
> 
> 
> I applied this series to linux-kbuild,
> and will send another pull request in this MW.
> 
> 
> If you come up with better patch titles,
> please let me know.
> I will fix them up locally.
>
I am just fine with these new titles. Thanks.

> 
> 
> -- 
> Best Regards
> Masahiro Yamada

-- 
Thanks,
Changbin Du
================================================================================


################################################################################

=== Thread: [PATCH v2 5/5] no VOID test in convert_instruction_target() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v2 5/5] no VOID test in convert_instruction_target()
Date: Wed, 25 Jul 2018 20:44:41 +0000
Message-ID: <20180725204441.91527-6-luc.vanoostenryck () gmail ! com>
--------------------
In convert_instruction_target(), when replacing the pseudo
in the target user list, it's first checked if the old pseudo
is not VOID and nothing is done otherwise. But this test is
not needed because:
1) the only case where VOID is stored in the user list is when
   a BB is killed and a killed instruction wouln't be converted
2) this test used to be needed when OP_PHIs were converted during
   CSE (meaning that the pseudo stored there have been removed
   from the list) but OP_PHIs are not CSEed anymore.

So, removed this unneeded test.

This gives a speedup up to 9% in some pathological workloads.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 flow.c | 5 +----
 1 file changed, 1 insertion(+), 4 deletions(-)

diff --git a/flow.c b/flow.c
index 9483938fb..0fdbdf44d 100644
--- a/flow.c
+++ b/flow.c
@@ -292,10 +292,7 @@ void convert_instruction_target(struct instruction *insn, pseudo_t src)
 	if (target == src)
 		return;
 	FOR_EACH_PTR(target->users, pu) {
-		if (*pu->userp != VOID) {
-			assert(*pu->userp == target);
-			*pu->userp = src;
-		}
+		*pu->userp = src;
 	} END_FOR_EACH_PTR(pu);
 	if (has_use_list(src))
 		concat_user_list(target->users, &src->users);
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2] compiler.h: give up __compiletime_assert_fallback() ===

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Tue, 28 Aug 2018 23:00:55 +0000
Message-ID: <CAKwvOdkD0cmF5373LRS92U--ptRvikSrG_oKyE-AiaybCQYMBg () mail ! gmail ! com>
--------------------
On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
>
> Hello Nick,
>
> On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
> >>> Let's give up __compiletime_assert_fallback().  This commit does not
> >>> change the current behavior since it just rips off the useless code.
> >> Clang is not the only target audience of
> >> __compiletime_assert_fallback().  Instead of ripping out something that
> >> may benefit builds with gcc 4.2 and earlier, why not override its
> > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
> > version to 4.6") that gcc < 4.6 is irrelevant.
>
> Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
> I guess I would be OK with its removal, but I still think it would be
> better if a similar mechanism to break the Clang build could be found.

I'm consulting with our best language lawyers to see what combinations
of _Static_assert and __builtin_constant_p would do the trick.
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Tue, 28 Aug 2018 23:00:55 +0000
Message-ID: <CAKwvOdkD0cmF5373LRS92U--ptRvikSrG_oKyE-AiaybCQYMBg () mail ! gmail ! com>
--------------------
On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
>
> Hello Nick,
>
> On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
> >>> Let's give up __compiletime_assert_fallback().  This commit does not
> >>> change the current behavior since it just rips off the useless code.
> >> Clang is not the only target audience of
> >> __compiletime_assert_fallback().  Instead of ripping out something that
> >> may benefit builds with gcc 4.2 and earlier, why not override its
> > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
> > version to 4.6") that gcc < 4.6 is irrelevant.
>
> Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
> I guess I would be OK with its removal, but I still think it would be
> better if a similar mechanism to break the Clang build could be found.

I'm consulting with our best language lawyers to see what combinations
of _Static_assert and __builtin_constant_p would do the trick.
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Fri, 31 Aug 2018 16:46:02 +0000
Message-ID: <CAKwvOdnyA=Y_+KnJrDc5mu7twTAEOMXvb8cAZhwzsXTi7a9sEA () mail ! gmail ! com>
--------------------
On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
<ndesaulniers@google.com> wrote:
>
> On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
> >
> > Hello Nick,
> >
> > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
> > >>> Let's give up __compiletime_assert_fallback().  This commit does not
> > >>> change the current behavior since it just rips off the useless code.
> > >> Clang is not the only target audience of
> > >> __compiletime_assert_fallback().  Instead of ripping out something that
> > >> may benefit builds with gcc 4.2 and earlier, why not override its
> > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
> > > version to 4.6") that gcc < 4.6 is irrelevant.
> >
> > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
> > I guess I would be OK with its removal, but I still think it would be
> > better if a similar mechanism to break the Clang build could be found.
>
> I'm consulting with our best language lawyers to see what combinations
> of _Static_assert and __builtin_constant_p would do the trick.

Linus,
Can this patch be merged in the meantime?

-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Fri, 31 Aug 2018 16:46:02 +0000
Message-ID: <CAKwvOdnyA=Y_+KnJrDc5mu7twTAEOMXvb8cAZhwzsXTi7a9sEA () mail ! gmail ! com>
--------------------
On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
<ndesaulniers@google.com> wrote:
>
> On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
> >
> > Hello Nick,
> >
> > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
> > >>> Let's give up __compiletime_assert_fallback().  This commit does not
> > >>> change the current behavior since it just rips off the useless code.
> > >> Clang is not the only target audience of
> > >> __compiletime_assert_fallback().  Instead of ripping out something that
> > >> may benefit builds with gcc 4.2 and earlier, why not override its
> > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
> > > version to 4.6") that gcc < 4.6 is irrelevant.
> >
> > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
> > I guess I would be OK with its removal, but I still think it would be
> > better if a similar mechanism to break the Clang build could be found.
>
> I'm consulting with our best language lawyers to see what combinations
> of _Static_assert and __builtin_constant_p would do the trick.

Linus,
Can this patch be merged in the meantime?

-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Fri, 31 Aug 2018 16:46:02 +0000
Message-ID: <CAKwvOdnyA=Y_+KnJrDc5mu7twTAEOMXvb8cAZhwzsXTi7a9sEA () mail ! gmail ! com>
--------------------
On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
<ndesaulniers@google.com> wrote:
>
> On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
> >
> > Hello Nick,
> >
> > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
> > >>> Let's give up __compiletime_assert_fallback().  This commit does not
> > >>> change the current behavior since it just rips off the useless code.
> > >> Clang is not the only target audience of
> > >> __compiletime_assert_fallback().  Instead of ripping out something that
> > >> may benefit builds with gcc 4.2 and earlier, why not override its
> > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
> > > version to 4.6") that gcc < 4.6 is irrelevant.
> >
> > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
> > I guess I would be OK with its removal, but I still think it would be
> > better if a similar mechanism to break the Clang build could be found.
>
> I'm consulting with our best language lawyers to see what combinations
> of _Static_assert and __builtin_constant_p would do the trick.

Linus,
Can this patch be merged in the meantime?

-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Matthias Kaehlcke <mka () chromium ! org>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:00:19 +0000
Message-ID: <20180926180019.GD22824 () google ! com>
--------------------
On Fri, Aug 31, 2018 at 09:46:02AM -0700, Nick Desaulniers wrote:
> On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
> <ndesaulniers@google.com> wrote:
> >
> > On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
> > >
> > > Hello Nick,
> > >
> > > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
> > > >>> Let's give up __compiletime_assert_fallback().  This commit does not
> > > >>> change the current behavior since it just rips off the useless code.
> > > >> Clang is not the only target audience of
> > > >> __compiletime_assert_fallback().  Instead of ripping out something that
> > > >> may benefit builds with gcc 4.2 and earlier, why not override its
> > > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
> > > > version to 4.6") that gcc < 4.6 is irrelevant.
> > >
> > > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
> > > I guess I would be OK with its removal, but I still think it would be
> > > better if a similar mechanism to break the Clang build could be found.
> >
> > I'm consulting with our best language lawyers to see what combinations
> > of _Static_assert and __builtin_constant_p would do the trick.
> 
> Linus,
> Can this patch be merged in the meantime?

friendly ping :)

With c5c2b11894f4 ("drm/i915: Warn against variable length arrays")
clang raises plenty of vla warnings about
__compiletime_error_fallback() in the i915 driver. Would be great to
get rid of those without having to revert that commit.
================================================================================

From: Matthias Kaehlcke <mka () chromium ! org>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:00:19 +0000
Message-ID: <20180926180019.GD22824 () google ! com>
--------------------
On Fri, Aug 31, 2018 at 09:46:02AM -0700, Nick Desaulniers wrote:
> On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
> <ndesaulniers@google.com> wrote:
> >
> > On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
> > >
> > > Hello Nick,
> > >
> > > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
> > > >>> Let's give up __compiletime_assert_fallback().  This commit does not
> > > >>> change the current behavior since it just rips off the useless code.
> > > >> Clang is not the only target audience of
> > > >> __compiletime_assert_fallback().  Instead of ripping out something that
> > > >> may benefit builds with gcc 4.2 and earlier, why not override its
> > > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
> > > > version to 4.6") that gcc < 4.6 is irrelevant.
> > >
> > > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
> > > I guess I would be OK with its removal, but I still think it would be
> > > better if a similar mechanism to break the Clang build could be found.
> >
> > I'm consulting with our best language lawyers to see what combinations
> > of _Static_assert and __builtin_constant_p would do the trick.
> 
> Linus,
> Can this patch be merged in the meantime?

friendly ping :)

With c5c2b11894f4 ("drm/i915: Warn against variable length arrays")
clang raises plenty of vla warnings about
__compiletime_error_fallback() in the i915 driver. Would be great to
get rid of those without having to revert that commit.
================================================================================

From: Matthias Kaehlcke <mka () chromium ! org>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:00:19 +0000
Message-ID: <20180926180019.GD22824 () google ! com>
--------------------
On Fri, Aug 31, 2018 at 09:46:02AM -0700, Nick Desaulniers wrote:
> On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
> <ndesaulniers@google.com> wrote:
> >
> > On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
> > >
> > > Hello Nick,
> > >
> > > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
> > > >>> Let's give up __compiletime_assert_fallback().  This commit does not
> > > >>> change the current behavior since it just rips off the useless code.
> > > >> Clang is not the only target audience of
> > > >> __compiletime_assert_fallback().  Instead of ripping out something that
> > > >> may benefit builds with gcc 4.2 and earlier, why not override its
> > > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
> > > > version to 4.6") that gcc < 4.6 is irrelevant.
> > >
> > > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
> > > I guess I would be OK with its removal, but I still think it would be
> > > better if a similar mechanism to break the Clang build could be found.
> >
> > I'm consulting with our best language lawyers to see what combinations
> > of _Static_assert and __builtin_constant_p would do the trick.
> 
> Linus,
> Can this patch be merged in the meantime?

friendly ping :)

With c5c2b11894f4 ("drm/i915: Warn against variable length arrays")
clang raises plenty of vla warnings about
__compiletime_error_fallback() in the i915 driver. Would be great to
get rid of those without having to revert that commit.
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:03:03 +0000
Message-ID: <CAKwvOd=xYG+r4KU+QurUTyaxkPB2AMane4YfWxuvKtONvVtLxQ () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 11:00 AM Matthias Kaehlcke <mka@chromium.org> wrote:
>
> On Fri, Aug 31, 2018 at 09:46:02AM -0700, Nick Desaulniers wrote:
> > On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
> > <ndesaulniers@google.com> wrote:
> > >
> > > On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
> > > >
> > > > Hello Nick,
> > > >
> > > > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
> > > > >>> Let's give up __compiletime_assert_fallback().  This commit does not
> > > > >>> change the current behavior since it just rips off the useless code.
> > > > >> Clang is not the only target audience of
> > > > >> __compiletime_assert_fallback().  Instead of ripping out something that
> > > > >> may benefit builds with gcc 4.2 and earlier, why not override its
> > > > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
> > > > > version to 4.6") that gcc < 4.6 is irrelevant.
> > > >
> > > > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
> > > > I guess I would be OK with its removal, but I still think it would be
> > > > better if a similar mechanism to break the Clang build could be found.
> > >
> > > I'm consulting with our best language lawyers to see what combinations
> > > of _Static_assert and __builtin_constant_p would do the trick.
> >
> > Linus,
> > Can this patch be merged in the meantime?
>
> friendly ping :)
>
> With c5c2b11894f4 ("drm/i915: Warn against variable length arrays")
> clang raises plenty of vla warnings about
> __compiletime_error_fallback() in the i915 driver. Would be great to
> get rid of those without having to revert that commit.

I've been meaning to follow up on this, thanks Matthias.  I too would
really like this patch.

-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:03:03 +0000
Message-ID: <CAKwvOd=xYG+r4KU+QurUTyaxkPB2AMane4YfWxuvKtONvVtLxQ () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 11:00 AM Matthias Kaehlcke <mka@chromium.org> wrote:
>
> On Fri, Aug 31, 2018 at 09:46:02AM -0700, Nick Desaulniers wrote:
> > On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
> > <ndesaulniers@google.com> wrote:
> > >
> > > On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
> > > >
> > > > Hello Nick,
> > > >
> > > > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
> > > > >>> Let's give up __compiletime_assert_fallback().  This commit does not
> > > > >>> change the current behavior since it just rips off the useless code.
> > > > >> Clang is not the only target audience of
> > > > >> __compiletime_assert_fallback().  Instead of ripping out something that
> > > > >> may benefit builds with gcc 4.2 and earlier, why not override its
> > > > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
> > > > > version to 4.6") that gcc < 4.6 is irrelevant.
> > > >
> > > > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
> > > > I guess I would be OK with its removal, but I still think it would be
> > > > better if a similar mechanism to break the Clang build could be found.
> > >
> > > I'm consulting with our best language lawyers to see what combinations
> > > of _Static_assert and __builtin_constant_p would do the trick.
> >
> > Linus,
> > Can this patch be merged in the meantime?
>
> friendly ping :)
>
> With c5c2b11894f4 ("drm/i915: Warn against variable length arrays")
> clang raises plenty of vla warnings about
> __compiletime_error_fallback() in the i915 driver. Would be great to
> get rid of those without having to revert that commit.

I've been meaning to follow up on this, thanks Matthias.  I too would
really like this patch.

-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:03:03 +0000
Message-ID: <CAKwvOd=xYG+r4KU+QurUTyaxkPB2AMane4YfWxuvKtONvVtLxQ () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 11:00 AM Matthias Kaehlcke <mka@chromium.org> wrote:
>
> On Fri, Aug 31, 2018 at 09:46:02AM -0700, Nick Desaulniers wrote:
> > On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
> > <ndesaulniers@google.com> wrote:
> > >
> > > On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
> > > >
> > > > Hello Nick,
> > > >
> > > > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
> > > > >>> Let's give up __compiletime_assert_fallback().  This commit does not
> > > > >>> change the current behavior since it just rips off the useless code.
> > > > >> Clang is not the only target audience of
> > > > >> __compiletime_assert_fallback().  Instead of ripping out something that
> > > > >> may benefit builds with gcc 4.2 and earlier, why not override its
> > > > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
> > > > > version to 4.6") that gcc < 4.6 is irrelevant.
> > > >
> > > > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
> > > > I guess I would be OK with its removal, but I still think it would be
> > > > better if a similar mechanism to break the Clang build could be found.
> > >
> > > I'm consulting with our best language lawyers to see what combinations
> > > of _Static_assert and __builtin_constant_p would do the trick.
> >
> > Linus,
> > Can this patch be merged in the meantime?
>
> friendly ping :)
>
> With c5c2b11894f4 ("drm/i915: Warn against variable length arrays")
> clang raises plenty of vla warnings about
> __compiletime_error_fallback() in the i915 driver. Would be great to
> get rid of those without having to revert that commit.

I've been meaning to follow up on this, thanks Matthias.  I too would
really like this patch.

-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:26:46 +0000
Message-ID: <CAGXu5jLBz1YanL5X4HFZjhb8NZe5QK=Q1ySfBkBDNWqEg4hs5g () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 11:03 AM, Nick Desaulniers
<ndesaulniers@google.com> wrote:
> On Wed, Sep 26, 2018 at 11:00 AM Matthias Kaehlcke <mka@chromium.org> wrote:
>>
>> On Fri, Aug 31, 2018 at 09:46:02AM -0700, Nick Desaulniers wrote:
>> > On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
>> > <ndesaulniers@google.com> wrote:
>> > >
>> > > On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
>> > > >
>> > > > Hello Nick,
>> > > >
>> > > > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
>> > > > >>> Let's give up __compiletime_assert_fallback().  This commit does not
>> > > > >>> change the current behavior since it just rips off the useless code.
>> > > > >> Clang is not the only target audience of
>> > > > >> __compiletime_assert_fallback().  Instead of ripping out something that
>> > > > >> may benefit builds with gcc 4.2 and earlier, why not override its
>> > > > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
>> > > > > version to 4.6") that gcc < 4.6 is irrelevant.
>> > > >
>> > > > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
>> > > > I guess I would be OK with its removal, but I still think it would be
>> > > > better if a similar mechanism to break the Clang build could be found.
>> > >
>> > > I'm consulting with our best language lawyers to see what combinations
>> > > of _Static_assert and __builtin_constant_p would do the trick.
>> >
>> > Linus,
>> > Can this patch be merged in the meantime?
>>
>> friendly ping :)
>>
>> With c5c2b11894f4 ("drm/i915: Warn against variable length arrays")
>> clang raises plenty of vla warnings about
>> __compiletime_error_fallback() in the i915 driver. Would be great to
>> get rid of those without having to revert that commit.
>
> I've been meaning to follow up on this, thanks Matthias.  I too would
> really like this patch.

Adding Greg to the thread. Between Masahiro's detailed commit log and
the Clang-familiar reviewers, I think this should land for 4.19 (as
part of the other Clang-sanity patches that are already in 4.19). This
has no impact on gcc now that we're requiring 4.6+.

https://lore.kernel.org/patchwork/patch/977668/

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:26:46 +0000
Message-ID: <CAGXu5jLBz1YanL5X4HFZjhb8NZe5QK=Q1ySfBkBDNWqEg4hs5g () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 11:03 AM, Nick Desaulniers
<ndesaulniers@google.com> wrote:
> On Wed, Sep 26, 2018 at 11:00 AM Matthias Kaehlcke <mka@chromium.org> wrote:
>>
>> On Fri, Aug 31, 2018 at 09:46:02AM -0700, Nick Desaulniers wrote:
>> > On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
>> > <ndesaulniers@google.com> wrote:
>> > >
>> > > On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
>> > > >
>> > > > Hello Nick,
>> > > >
>> > > > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
>> > > > >>> Let's give up __compiletime_assert_fallback().  This commit does not
>> > > > >>> change the current behavior since it just rips off the useless code.
>> > > > >> Clang is not the only target audience of
>> > > > >> __compiletime_assert_fallback().  Instead of ripping out something that
>> > > > >> may benefit builds with gcc 4.2 and earlier, why not override its
>> > > > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
>> > > > > version to 4.6") that gcc < 4.6 is irrelevant.
>> > > >
>> > > > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
>> > > > I guess I would be OK with its removal, but I still think it would be
>> > > > better if a similar mechanism to break the Clang build could be found.
>> > >
>> > > I'm consulting with our best language lawyers to see what combinations
>> > > of _Static_assert and __builtin_constant_p would do the trick.
>> >
>> > Linus,
>> > Can this patch be merged in the meantime?
>>
>> friendly ping :)
>>
>> With c5c2b11894f4 ("drm/i915: Warn against variable length arrays")
>> clang raises plenty of vla warnings about
>> __compiletime_error_fallback() in the i915 driver. Would be great to
>> get rid of those without having to revert that commit.
>
> I've been meaning to follow up on this, thanks Matthias.  I too would
> really like this patch.

Adding Greg to the thread. Between Masahiro's detailed commit log and
the Clang-familiar reviewers, I think this should land for 4.19 (as
part of the other Clang-sanity patches that are already in 4.19). This
has no impact on gcc now that we're requiring 4.6+.

https://lore.kernel.org/patchwork/patch/977668/

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:26:46 +0000
Message-ID: <CAGXu5jLBz1YanL5X4HFZjhb8NZe5QK=Q1ySfBkBDNWqEg4hs5g () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 11:03 AM, Nick Desaulniers
<ndesaulniers@google.com> wrote:
> On Wed, Sep 26, 2018 at 11:00 AM Matthias Kaehlcke <mka@chromium.org> wrote:
>>
>> On Fri, Aug 31, 2018 at 09:46:02AM -0700, Nick Desaulniers wrote:
>> > On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
>> > <ndesaulniers@google.com> wrote:
>> > >
>> > > On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
>> > > >
>> > > > Hello Nick,
>> > > >
>> > > > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
>> > > > >>> Let's give up __compiletime_assert_fallback().  This commit does not
>> > > > >>> change the current behavior since it just rips off the useless code.
>> > > > >> Clang is not the only target audience of
>> > > > >> __compiletime_assert_fallback().  Instead of ripping out something that
>> > > > >> may benefit builds with gcc 4.2 and earlier, why not override its
>> > > > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
>> > > > > version to 4.6") that gcc < 4.6 is irrelevant.
>> > > >
>> > > > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
>> > > > I guess I would be OK with its removal, but I still think it would be
>> > > > better if a similar mechanism to break the Clang build could be found.
>> > >
>> > > I'm consulting with our best language lawyers to see what combinations
>> > > of _Static_assert and __builtin_constant_p would do the trick.
>> >
>> > Linus,
>> > Can this patch be merged in the meantime?
>>
>> friendly ping :)
>>
>> With c5c2b11894f4 ("drm/i915: Warn against variable length arrays")
>> clang raises plenty of vla warnings about
>> __compiletime_error_fallback() in the i915 driver. Would be great to
>> get rid of those without having to revert that commit.
>
> I've been meaning to follow up on this, thanks Matthias.  I too would
> really like this patch.

Adding Greg to the thread. Between Masahiro's detailed commit log and
the Clang-familiar reviewers, I think this should land for 4.19 (as
part of the other Clang-sanity patches that are already in 4.19). This
has no impact on gcc now that we're requiring 4.6+.

https://lore.kernel.org/patchwork/patch/977668/

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Greg KH <gregkh () linuxfoundation ! org>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:42:59 +0000
Message-ID: <20180926184259.GB14797 () kroah ! com>
--------------------
On Wed, Sep 26, 2018 at 11:26:46AM -0700, Kees Cook wrote:
> On Wed, Sep 26, 2018 at 11:03 AM, Nick Desaulniers
> <ndesaulniers@google.com> wrote:
> > On Wed, Sep 26, 2018 at 11:00 AM Matthias Kaehlcke <mka@chromium.org> wrote:
> >>
> >> On Fri, Aug 31, 2018 at 09:46:02AM -0700, Nick Desaulniers wrote:
> >> > On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
> >> > <ndesaulniers@google.com> wrote:
> >> > >
> >> > > On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
> >> > > >
> >> > > > Hello Nick,
> >> > > >
> >> > > > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
> >> > > > >>> Let's give up __compiletime_assert_fallback().  This commit does not
> >> > > > >>> change the current behavior since it just rips off the useless code.
> >> > > > >> Clang is not the only target audience of
> >> > > > >> __compiletime_assert_fallback().  Instead of ripping out something that
> >> > > > >> may benefit builds with gcc 4.2 and earlier, why not override its
> >> > > > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
> >> > > > > version to 4.6") that gcc < 4.6 is irrelevant.
> >> > > >
> >> > > > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
> >> > > > I guess I would be OK with its removal, but I still think it would be
> >> > > > better if a similar mechanism to break the Clang build could be found.
> >> > >
> >> > > I'm consulting with our best language lawyers to see what combinations
> >> > > of _Static_assert and __builtin_constant_p would do the trick.
> >> >
> >> > Linus,
> >> > Can this patch be merged in the meantime?
> >>
> >> friendly ping :)
> >>
> >> With c5c2b11894f4 ("drm/i915: Warn against variable length arrays")
> >> clang raises plenty of vla warnings about
> >> __compiletime_error_fallback() in the i915 driver. Would be great to
> >> get rid of those without having to revert that commit.
> >
> > I've been meaning to follow up on this, thanks Matthias.  I too would
> > really like this patch.
> 
> Adding Greg to the thread. Between Masahiro's detailed commit log and
> the Clang-familiar reviewers, I think this should land for 4.19 (as
> part of the other Clang-sanity patches that are already in 4.19). This
> has no impact on gcc now that we're requiring 4.6+.
> 
> https://lore.kernel.org/patchwork/patch/977668/

I'm not digging up a compiler.h patch from a web site and adding it to
the tree this late in the release cycle.  Especially given that it
hasn't had any testing anywhere...

nice try though :)

greg k-h
================================================================================

From: Greg KH <gregkh () linuxfoundation ! org>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:42:59 +0000
Message-ID: <20180926184259.GB14797 () kroah ! com>
--------------------
On Wed, Sep 26, 2018 at 11:26:46AM -0700, Kees Cook wrote:
> On Wed, Sep 26, 2018 at 11:03 AM, Nick Desaulniers
> <ndesaulniers@google.com> wrote:
> > On Wed, Sep 26, 2018 at 11:00 AM Matthias Kaehlcke <mka@chromium.org> wrote:
> >>
> >> On Fri, Aug 31, 2018 at 09:46:02AM -0700, Nick Desaulniers wrote:
> >> > On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
> >> > <ndesaulniers@google.com> wrote:
> >> > >
> >> > > On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
> >> > > >
> >> > > > Hello Nick,
> >> > > >
> >> > > > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
> >> > > > >>> Let's give up __compiletime_assert_fallback().  This commit does not
> >> > > > >>> change the current behavior since it just rips off the useless code.
> >> > > > >> Clang is not the only target audience of
> >> > > > >> __compiletime_assert_fallback().  Instead of ripping out something that
> >> > > > >> may benefit builds with gcc 4.2 and earlier, why not override its
> >> > > > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
> >> > > > > version to 4.6") that gcc < 4.6 is irrelevant.
> >> > > >
> >> > > > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
> >> > > > I guess I would be OK with its removal, but I still think it would be
> >> > > > better if a similar mechanism to break the Clang build could be found.
> >> > >
> >> > > I'm consulting with our best language lawyers to see what combinations
> >> > > of _Static_assert and __builtin_constant_p would do the trick.
> >> >
> >> > Linus,
> >> > Can this patch be merged in the meantime?
> >>
> >> friendly ping :)
> >>
> >> With c5c2b11894f4 ("drm/i915: Warn against variable length arrays")
> >> clang raises plenty of vla warnings about
> >> __compiletime_error_fallback() in the i915 driver. Would be great to
> >> get rid of those without having to revert that commit.
> >
> > I've been meaning to follow up on this, thanks Matthias.  I too would
> > really like this patch.
> 
> Adding Greg to the thread. Between Masahiro's detailed commit log and
> the Clang-familiar reviewers, I think this should land for 4.19 (as
> part of the other Clang-sanity patches that are already in 4.19). This
> has no impact on gcc now that we're requiring 4.6+.
> 
> https://lore.kernel.org/patchwork/patch/977668/

I'm not digging up a compiler.h patch from a web site and adding it to
the tree this late in the release cycle.  Especially given that it
hasn't had any testing anywhere...

nice try though :)

greg k-h
================================================================================

From: Greg KH <gregkh () linuxfoundation ! org>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:42:59 +0000
Message-ID: <20180926184259.GB14797 () kroah ! com>
--------------------
On Wed, Sep 26, 2018 at 11:26:46AM -0700, Kees Cook wrote:
> On Wed, Sep 26, 2018 at 11:03 AM, Nick Desaulniers
> <ndesaulniers@google.com> wrote:
> > On Wed, Sep 26, 2018 at 11:00 AM Matthias Kaehlcke <mka@chromium.org> wrote:
> >>
> >> On Fri, Aug 31, 2018 at 09:46:02AM -0700, Nick Desaulniers wrote:
> >> > On Tue, Aug 28, 2018 at 4:00 PM Nick Desaulniers
> >> > <ndesaulniers@google.com> wrote:
> >> > >
> >> > > On Mon, Aug 27, 2018 at 1:42 PM Daniel Santos <daniel.santos@pobox.com> wrote:
> >> > > >
> >> > > > Hello Nick,
> >> > > >
> >> > > > On 08/27/2018 03:09 PM, Nick Desaulniers wrote:
> >> > > > >>> Let's give up __compiletime_assert_fallback().  This commit does not
> >> > > > >>> change the current behavior since it just rips off the useless code.
> >> > > > >> Clang is not the only target audience of
> >> > > > >> __compiletime_assert_fallback().  Instead of ripping out something that
> >> > > > >> may benefit builds with gcc 4.2 and earlier, why not override its
> >> > > > > Note that with commit cafa0010cd51 ("Raise the minimum required gcc
> >> > > > > version to 4.6") that gcc < 4.6 is irrelevant.
> >> > > >
> >> > > > Ah, I guess I'm not keeping up, that's wonderful news!  Considering that
> >> > > > I guess I would be OK with its removal, but I still think it would be
> >> > > > better if a similar mechanism to break the Clang build could be found.
> >> > >
> >> > > I'm consulting with our best language lawyers to see what combinations
> >> > > of _Static_assert and __builtin_constant_p would do the trick.
> >> >
> >> > Linus,
> >> > Can this patch be merged in the meantime?
> >>
> >> friendly ping :)
> >>
> >> With c5c2b11894f4 ("drm/i915: Warn against variable length arrays")
> >> clang raises plenty of vla warnings about
> >> __compiletime_error_fallback() in the i915 driver. Would be great to
> >> get rid of those without having to revert that commit.
> >
> > I've been meaning to follow up on this, thanks Matthias.  I too would
> > really like this patch.
> 
> Adding Greg to the thread. Between Masahiro's detailed commit log and
> the Clang-familiar reviewers, I think this should land for 4.19 (as
> part of the other Clang-sanity patches that are already in 4.19). This
> has no impact on gcc now that we're requiring 4.6+.
> 
> https://lore.kernel.org/patchwork/patch/977668/

I'm not digging up a compiler.h patch from a web site and adding it to
the tree this late in the release cycle.  Especially given that it
hasn't had any testing anywhere...

nice try though :)

greg k-h
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:45:19 +0000
Message-ID: <CAGXu5jJaJ+1Sg3uwaZ8hzw3wFbiLrftpZDO8vdjY9_tB+P-HPQ () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
> I'm not digging up a compiler.h patch from a web site and adding it to
> the tree this late in the release cycle.  Especially given that it
> hasn't had any testing anywhere...

Good point about it not living in -next.

Who should be carrying these sorts of patches? In the past it's been
Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:45:19 +0000
Message-ID: <CAGXu5jJaJ+1Sg3uwaZ8hzw3wFbiLrftpZDO8vdjY9_tB+P-HPQ () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
> I'm not digging up a compiler.h patch from a web site and adding it to
> the tree this late in the release cycle.  Especially given that it
> hasn't had any testing anywhere...

Good point about it not living in -next.

Who should be carrying these sorts of patches? In the past it's been
Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:45:19 +0000
Message-ID: <CAGXu5jJaJ+1Sg3uwaZ8hzw3wFbiLrftpZDO8vdjY9_tB+P-HPQ () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
> I'm not digging up a compiler.h patch from a web site and adding it to
> the tree this late in the release cycle.  Especially given that it
> hasn't had any testing anywhere...

Good point about it not living in -next.

Who should be carrying these sorts of patches? In the past it's been
Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 18:45:19 +0000
Message-ID: <CAGXu5jJaJ+1Sg3uwaZ8hzw3wFbiLrftpZDO8vdjY9_tB+P-HPQ () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
> I'm not digging up a compiler.h patch from a web site and adding it to
> the tree this late in the release cycle.  Especially given that it
> hasn't had any testing anywhere...

Good point about it not living in -next.

Who should be carrying these sorts of patches? In the past it's been
Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Greg KH <gregkh () linuxfoundation ! org>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 19:03:03 +0000
Message-ID: <20180926190303.GA18293 () kroah ! com>
--------------------
On Wed, Sep 26, 2018 at 11:45:19AM -0700, Kees Cook wrote:
> On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
> > I'm not digging up a compiler.h patch from a web site and adding it to
> > the tree this late in the release cycle.  Especially given that it
> > hasn't had any testing anywhere...
> 
> Good point about it not living in -next.
> 
> Who should be carrying these sorts of patches? In the past it's been
> Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?

Either is fine with me, as long as it isn't one of my trees :)

thanks,

greg k-h
================================================================================

From: Greg KH <gregkh () linuxfoundation ! org>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 19:03:03 +0000
Message-ID: <20180926190303.GA18293 () kroah ! com>
--------------------
On Wed, Sep 26, 2018 at 11:45:19AM -0700, Kees Cook wrote:
> On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
> > I'm not digging up a compiler.h patch from a web site and adding it to
> > the tree this late in the release cycle.  Especially given that it
> > hasn't had any testing anywhere...
> 
> Good point about it not living in -next.
> 
> Who should be carrying these sorts of patches? In the past it's been
> Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?

Either is fine with me, as long as it isn't one of my trees :)

thanks,

greg k-h
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 19:29:19 +0000
Message-ID: <CAKwvOdnx6D+_LeSZ-1v_s8dLErXq4B5Nx1UQozdr1+ZaGYWnrA () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 12:03 PM Greg KH <gregkh@linuxfoundation.org> wrote:
>
> On Wed, Sep 26, 2018 at 11:45:19AM -0700, Kees Cook wrote:
> > On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
> > > I'm not digging up a compiler.h patch from a web site and adding it to
> > > the tree this late in the release cycle.  Especially given that it
> > > hasn't had any testing anywhere...
> >
> > Good point about it not living in -next.
> >
> > Who should be carrying these sorts of patches? In the past it's been
> > Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?
>
> Either is fine with me, as long as it isn't one of my trees :)
>
> thanks,
>
> greg k-h

Besides, I think we want the v2: https://lkml.org/lkml/2018/8/25/103
-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 19:29:19 +0000
Message-ID: <CAKwvOdnx6D+_LeSZ-1v_s8dLErXq4B5Nx1UQozdr1+ZaGYWnrA () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 12:03 PM Greg KH <gregkh@linuxfoundation.org> wrote:
>
> On Wed, Sep 26, 2018 at 11:45:19AM -0700, Kees Cook wrote:
> > On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
> > > I'm not digging up a compiler.h patch from a web site and adding it to
> > > the tree this late in the release cycle.  Especially given that it
> > > hasn't had any testing anywhere...
> >
> > Good point about it not living in -next.
> >
> > Who should be carrying these sorts of patches? In the past it's been
> > Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?
>
> Either is fine with me, as long as it isn't one of my trees :)
>
> thanks,
>
> greg k-h

Besides, I think we want the v2: https://lkml.org/lkml/2018/8/25/103
-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 19:35:06 +0000
Message-ID: <CAGXu5jKRn2EsPwNMtVTPyKyDnt+vqHZF=jdr+DajN_iN1DWjSw () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 12:29 PM, Nick Desaulniers
<ndesaulniers@google.com> wrote:
> On Wed, Sep 26, 2018 at 12:03 PM Greg KH <gregkh@linuxfoundation.org> wrote:
>>
>> On Wed, Sep 26, 2018 at 11:45:19AM -0700, Kees Cook wrote:
>> > On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
>> > > I'm not digging up a compiler.h patch from a web site and adding it to
>> > > the tree this late in the release cycle.  Especially given that it
>> > > hasn't had any testing anywhere...
>> >
>> > Good point about it not living in -next.
>> >
>> > Who should be carrying these sorts of patches? In the past it's been
>> > Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?
>>
>> Either is fine with me, as long as it isn't one of my trees :)
>>
>> thanks,
>>
>> greg k-h
>
> Besides, I think we want the v2: https://lkml.org/lkml/2018/8/25/103

Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 19:35:06 +0000
Message-ID: <CAGXu5jKRn2EsPwNMtVTPyKyDnt+vqHZF=jdr+DajN_iN1DWjSw () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 12:29 PM, Nick Desaulniers
<ndesaulniers@google.com> wrote:
> On Wed, Sep 26, 2018 at 12:03 PM Greg KH <gregkh@linuxfoundation.org> wrote:
>>
>> On Wed, Sep 26, 2018 at 11:45:19AM -0700, Kees Cook wrote:
>> > On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
>> > > I'm not digging up a compiler.h patch from a web site and adding it to
>> > > the tree this late in the release cycle.  Especially given that it
>> > > hasn't had any testing anywhere...
>> >
>> > Good point about it not living in -next.
>> >
>> > Who should be carrying these sorts of patches? In the past it's been
>> > Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?
>>
>> Either is fine with me, as long as it isn't one of my trees :)
>>
>> thanks,
>>
>> greg k-h
>
> Besides, I think we want the v2: https://lkml.org/lkml/2018/8/25/103

Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 19:35:06 +0000
Message-ID: <CAGXu5jKRn2EsPwNMtVTPyKyDnt+vqHZF=jdr+DajN_iN1DWjSw () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 12:29 PM, Nick Desaulniers
<ndesaulniers@google.com> wrote:
> On Wed, Sep 26, 2018 at 12:03 PM Greg KH <gregkh@linuxfoundation.org> wrote:
>>
>> On Wed, Sep 26, 2018 at 11:45:19AM -0700, Kees Cook wrote:
>> > On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
>> > > I'm not digging up a compiler.h patch from a web site and adding it to
>> > > the tree this late in the release cycle.  Especially given that it
>> > > hasn't had any testing anywhere...
>> >
>> > Good point about it not living in -next.
>> >
>> > Who should be carrying these sorts of patches? In the past it's been
>> > Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?
>>
>> Either is fine with me, as long as it isn't one of my trees :)
>>
>> thanks,
>>
>> greg k-h
>
> Besides, I think we want the v2: https://lkml.org/lkml/2018/8/25/103

Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 26 Sep 2018 19:35:06 +0000
Message-ID: <CAGXu5jKRn2EsPwNMtVTPyKyDnt+vqHZF=jdr+DajN_iN1DWjSw () mail ! gmail ! com>
--------------------
On Wed, Sep 26, 2018 at 12:29 PM, Nick Desaulniers
<ndesaulniers@google.com> wrote:
> On Wed, Sep 26, 2018 at 12:03 PM Greg KH <gregkh@linuxfoundation.org> wrote:
>>
>> On Wed, Sep 26, 2018 at 11:45:19AM -0700, Kees Cook wrote:
>> > On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
>> > > I'm not digging up a compiler.h patch from a web site and adding it to
>> > > the tree this late in the release cycle.  Especially given that it
>> > > hasn't had any testing anywhere...
>> >
>> > Good point about it not living in -next.
>> >
>> > Who should be carrying these sorts of patches? In the past it's been
>> > Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?
>>
>> Either is fine with me, as long as it isn't one of my trees :)
>>
>> thanks,
>>
>> greg k-h
>
> Besides, I think we want the v2: https://lkml.org/lkml/2018/8/25/103

Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Joel Stanley <joel () jms ! id ! au>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 10 Oct 2018 06:22:24 +0000
Message-ID: <CACPK8XeAbiMk_yW-t+8HdW_WLqaFJFX-xnNnDSZJ5ZfhK63euA () mail ! gmail ! com>
--------------------
On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
>
> On Wed, Sep 26, 2018 at 12:29 PM, Nick Desaulniers
> <ndesaulniers@google.com> wrote:
> > On Wed, Sep 26, 2018 at 12:03 PM Greg KH <gregkh@linuxfoundation.org> wrote:
> >>
> >> On Wed, Sep 26, 2018 at 11:45:19AM -0700, Kees Cook wrote:
> >> > On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
> >> > > I'm not digging up a compiler.h patch from a web site and adding it to
> >> > > the tree this late in the release cycle.  Especially given that it
> >> > > hasn't had any testing anywhere...
> >> >
> >> > Good point about it not living in -next.
> >> >
> >> > Who should be carrying these sorts of patches? In the past it's been
> >> > Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?
> >>
> >> Either is fine with me, as long as it isn't one of my trees :)
> >>
> >> thanks,
> >>
> >> greg k-h
> >
> > Besides, I think we want the v2: https://lkml.org/lkml/2018/8/25/103
>
> Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?

clang built -next is blowing up now that Kees' -Wvla patch has been
included. This patch fixes it.

Kees, perhaps it should go in your tree along side of the -Wvla patch
if no one else wants to take it?

Cheers,

Joel
================================================================================

From: Joel Stanley <joel () jms ! id ! au>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 10 Oct 2018 06:22:24 +0000
Message-ID: <CACPK8XeAbiMk_yW-t+8HdW_WLqaFJFX-xnNnDSZJ5ZfhK63euA () mail ! gmail ! com>
--------------------
On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
>
> On Wed, Sep 26, 2018 at 12:29 PM, Nick Desaulniers
> <ndesaulniers@google.com> wrote:
> > On Wed, Sep 26, 2018 at 12:03 PM Greg KH <gregkh@linuxfoundation.org> wrote:
> >>
> >> On Wed, Sep 26, 2018 at 11:45:19AM -0700, Kees Cook wrote:
> >> > On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
> >> > > I'm not digging up a compiler.h patch from a web site and adding it to
> >> > > the tree this late in the release cycle.  Especially given that it
> >> > > hasn't had any testing anywhere...
> >> >
> >> > Good point about it not living in -next.
> >> >
> >> > Who should be carrying these sorts of patches? In the past it's been
> >> > Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?
> >>
> >> Either is fine with me, as long as it isn't one of my trees :)
> >>
> >> thanks,
> >>
> >> greg k-h
> >
> > Besides, I think we want the v2: https://lkml.org/lkml/2018/8/25/103
>
> Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?

clang built -next is blowing up now that Kees' -Wvla patch has been
included. This patch fixes it.

Kees, perhaps it should go in your tree along side of the -Wvla patch
if no one else wants to take it?

Cheers,

Joel
================================================================================

From: Joel Stanley <joel () jms ! id ! au>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 10 Oct 2018 06:22:24 +0000
Message-ID: <CACPK8XeAbiMk_yW-t+8HdW_WLqaFJFX-xnNnDSZJ5ZfhK63euA () mail ! gmail ! com>
--------------------
On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
>
> On Wed, Sep 26, 2018 at 12:29 PM, Nick Desaulniers
> <ndesaulniers@google.com> wrote:
> > On Wed, Sep 26, 2018 at 12:03 PM Greg KH <gregkh@linuxfoundation.org> wrote:
> >>
> >> On Wed, Sep 26, 2018 at 11:45:19AM -0700, Kees Cook wrote:
> >> > On Wed, Sep 26, 2018 at 11:42 AM, Greg KH <gregkh@linuxfoundation.org> wrote:
> >> > > I'm not digging up a compiler.h patch from a web site and adding it to
> >> > > the tree this late in the release cycle.  Especially given that it
> >> > > hasn't had any testing anywhere...
> >> >
> >> > Good point about it not living in -next.
> >> >
> >> > Who should be carrying these sorts of patches? In the past it's been
> >> > Andrew or Masahiro, yes? For linux-next, maybe it can go via -mm?
> >>
> >> Either is fine with me, as long as it isn't one of my trees :)
> >>
> >> thanks,
> >>
> >> greg k-h
> >
> > Besides, I think we want the v2: https://lkml.org/lkml/2018/8/25/103
>
> Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?

clang built -next is blowing up now that Kees' -Wvla patch has been
included. This patch fixes it.

Kees, perhaps it should go in your tree along side of the -Wvla patch
if no one else wants to take it?

Cheers,

Joel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 10 Oct 2018 07:03:43 +0000
Message-ID: <CANiq72nGrNMEqnPrxGXCp_iKf1_FuJLMvoV1+bbfYYCKnfTPVw () mail ! gmail ! com>
--------------------
On Wed, Oct 10, 2018 at 8:12 AM Joel Stanley <joel@jms.id.au> wrote:
>
> On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
> >
> > Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?
>
> clang built -next is blowing up now that Kees' -Wvla patch has been
> included. This patch fixes it.
>
> Kees, perhaps it should go in your tree along side of the -Wvla patch
> if no one else wants to take it?
>

I can take it along in the compiler attributes tree, since that
touches the compiler*.h stuff. Although that would make it
not-only-attributes, i.e. slightly lying :-)

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 10 Oct 2018 07:03:43 +0000
Message-ID: <CANiq72nGrNMEqnPrxGXCp_iKf1_FuJLMvoV1+bbfYYCKnfTPVw () mail ! gmail ! com>
--------------------
On Wed, Oct 10, 2018 at 8:12 AM Joel Stanley <joel@jms.id.au> wrote:
>
> On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
> >
> > Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?
>
> clang built -next is blowing up now that Kees' -Wvla patch has been
> included. This patch fixes it.
>
> Kees, perhaps it should go in your tree along side of the -Wvla patch
> if no one else wants to take it?
>

I can take it along in the compiler attributes tree, since that
touches the compiler*.h stuff. Although that would make it
not-only-attributes, i.e. slightly lying :-)

Cheers,
Miguel
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 10 Oct 2018 14:49:16 +0000
Message-ID: <CAGXu5jKShFRJcP3vroEcZUevcHBSi53Towgs-t0v1JyEqmHJCA () mail ! gmail ! com>
--------------------
On Wed, Oct 10, 2018 at 12:03 AM, Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
> On Wed, Oct 10, 2018 at 8:12 AM Joel Stanley <joel@jms.id.au> wrote:
>>
>> On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
>> >
>> > Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?
>>
>> clang built -next is blowing up now that Kees' -Wvla patch has been
>> included. This patch fixes it.
>>
>> Kees, perhaps it should go in your tree along side of the -Wvla patch
>> if no one else wants to take it?
>>
>
> I can take it along in the compiler attributes tree, since that
> touches the compiler*.h stuff. Although that would make it
> not-only-attributes, i.e. slightly lying :-)

Oh, I had assumed Masahiro was going to carry it. If that's not true,
sure I'll pick it up as part of my VLA "series".

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 10 Oct 2018 14:49:16 +0000
Message-ID: <CAGXu5jKShFRJcP3vroEcZUevcHBSi53Towgs-t0v1JyEqmHJCA () mail ! gmail ! com>
--------------------
On Wed, Oct 10, 2018 at 12:03 AM, Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
> On Wed, Oct 10, 2018 at 8:12 AM Joel Stanley <joel@jms.id.au> wrote:
>>
>> On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
>> >
>> > Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?
>>
>> clang built -next is blowing up now that Kees' -Wvla patch has been
>> included. This patch fixes it.
>>
>> Kees, perhaps it should go in your tree along side of the -Wvla patch
>> if no one else wants to take it?
>>
>
> I can take it along in the compiler attributes tree, since that
> touches the compiler*.h stuff. Although that would make it
> not-only-attributes, i.e. slightly lying :-)

Oh, I had assumed Masahiro was going to carry it. If that's not true,
sure I'll pick it up as part of my VLA "series".

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 10 Oct 2018 14:49:16 +0000
Message-ID: <CAGXu5jKShFRJcP3vroEcZUevcHBSi53Towgs-t0v1JyEqmHJCA () mail ! gmail ! com>
--------------------
On Wed, Oct 10, 2018 at 12:03 AM, Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
> On Wed, Oct 10, 2018 at 8:12 AM Joel Stanley <joel@jms.id.au> wrote:
>>
>> On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
>> >
>> > Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?
>>
>> clang built -next is blowing up now that Kees' -Wvla patch has been
>> included. This patch fixes it.
>>
>> Kees, perhaps it should go in your tree along side of the -Wvla patch
>> if no one else wants to take it?
>>
>
> I can take it along in the compiler attributes tree, since that
> touches the compiler*.h stuff. Although that would make it
> not-only-attributes, i.e. slightly lying :-)

Oh, I had assumed Masahiro was going to carry it. If that's not true,
sure I'll pick it up as part of my VLA "series".

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Wed, 10 Oct 2018 14:49:16 +0000
Message-ID: <CAGXu5jKShFRJcP3vroEcZUevcHBSi53Towgs-t0v1JyEqmHJCA () mail ! gmail ! com>
--------------------
On Wed, Oct 10, 2018 at 12:03 AM, Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
> On Wed, Oct 10, 2018 at 8:12 AM Joel Stanley <joel@jms.id.au> wrote:
>>
>> On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
>> >
>> > Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?
>>
>> clang built -next is blowing up now that Kees' -Wvla patch has been
>> included. This patch fixes it.
>>
>> Kees, perhaps it should go in your tree along side of the -Wvla patch
>> if no one else wants to take it?
>>
>
> I can take it along in the compiler attributes tree, since that
> touches the compiler*.h stuff. Although that would make it
> not-only-attributes, i.e. slightly lying :-)

Oh, I had assumed Masahiro was going to carry it. If that's not true,
sure I'll pick it up as part of my VLA "series".

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Thu, 11 Oct 2018 02:48:59 +0000
Message-ID: <CAK7LNATXiJkXa4bfrCMb-kuWrn2gg-7jJg3qT3UR6HVSpC3Y3w () mail ! gmail ! com>
--------------------
On Wed, Oct 10, 2018 at 11:51 PM Kees Cook <keescook@chromium.org> wrote:
>
> On Wed, Oct 10, 2018 at 12:03 AM, Miguel Ojeda
> <miguel.ojeda.sandonis@gmail.com> wrote:
> > On Wed, Oct 10, 2018 at 8:12 AM Joel Stanley <joel@jms.id.au> wrote:
> >>
> >> On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
> >> >
> >> > Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?
> >>
> >> clang built -next is blowing up now that Kees' -Wvla patch has been
> >> included. This patch fixes it.
> >>
> >> Kees, perhaps it should go in your tree along side of the -Wvla patch
> >> if no one else wants to take it?
> >>
> >
> > I can take it along in the compiler attributes tree, since that
> > touches the compiler*.h stuff. Although that would make it
> > not-only-attributes, i.e. slightly lying :-)
>
> Oh, I had assumed Masahiro was going to carry it.

No, I am not.

Putting all sort of things into kbuild basket
is painful for me.



> If that's not true,
> sure I'll pick it up as part of my VLA "series".


-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Thu, 11 Oct 2018 02:48:59 +0000
Message-ID: <CAK7LNATXiJkXa4bfrCMb-kuWrn2gg-7jJg3qT3UR6HVSpC3Y3w () mail ! gmail ! com>
--------------------
On Wed, Oct 10, 2018 at 11:51 PM Kees Cook <keescook@chromium.org> wrote:
>
> On Wed, Oct 10, 2018 at 12:03 AM, Miguel Ojeda
> <miguel.ojeda.sandonis@gmail.com> wrote:
> > On Wed, Oct 10, 2018 at 8:12 AM Joel Stanley <joel@jms.id.au> wrote:
> >>
> >> On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
> >> >
> >> > Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?
> >>
> >> clang built -next is blowing up now that Kees' -Wvla patch has been
> >> included. This patch fixes it.
> >>
> >> Kees, perhaps it should go in your tree along side of the -Wvla patch
> >> if no one else wants to take it?
> >>
> >
> > I can take it along in the compiler attributes tree, since that
> > touches the compiler*.h stuff. Although that would make it
> > not-only-attributes, i.e. slightly lying :-)
>
> Oh, I had assumed Masahiro was going to carry it.

No, I am not.

Putting all sort of things into kbuild basket
is painful for me.



> If that's not true,
> sure I'll pick it up as part of my VLA "series".


-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Thu, 11 Oct 2018 02:48:59 +0000
Message-ID: <CAK7LNATXiJkXa4bfrCMb-kuWrn2gg-7jJg3qT3UR6HVSpC3Y3w () mail ! gmail ! com>
--------------------
On Wed, Oct 10, 2018 at 11:51 PM Kees Cook <keescook@chromium.org> wrote:
>
> On Wed, Oct 10, 2018 at 12:03 AM, Miguel Ojeda
> <miguel.ojeda.sandonis@gmail.com> wrote:
> > On Wed, Oct 10, 2018 at 8:12 AM Joel Stanley <joel@jms.id.au> wrote:
> >>
> >> On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
> >> >
> >> > Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?
> >>
> >> clang built -next is blowing up now that Kees' -Wvla patch has been
> >> included. This patch fixes it.
> >>
> >> Kees, perhaps it should go in your tree along side of the -Wvla patch
> >> if no one else wants to take it?
> >>
> >
> > I can take it along in the compiler attributes tree, since that
> > touches the compiler*.h stuff. Although that would make it
> > not-only-attributes, i.e. slightly lying :-)
>
> Oh, I had assumed Masahiro was going to carry it.

No, I am not.

Putting all sort of things into kbuild basket
is painful for me.



> If that's not true,
> sure I'll pick it up as part of my VLA "series".


-- 
Best Regards
Masahiro Yamada
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Thu, 11 Oct 2018 15:15:44 +0000
Message-ID: <CAGXu5jKxF4AmA3GCuijqryca6fYgJYFZJUNf9vnvgJAYCUUiuw () mail ! gmail ! com>
--------------------
On Wed, Oct 10, 2018 at 7:48 PM, Masahiro Yamada
<yamada.masahiro@socionext.com> wrote:
> On Wed, Oct 10, 2018 at 11:51 PM Kees Cook <keescook@chromium.org> wrote:
>>
>> On Wed, Oct 10, 2018 at 12:03 AM, Miguel Ojeda
>> <miguel.ojeda.sandonis@gmail.com> wrote:
>> > On Wed, Oct 10, 2018 at 8:12 AM Joel Stanley <joel@jms.id.au> wrote:
>> >>
>> >> On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
>> >> >
>> >> > Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?
>> >>
>> >> clang built -next is blowing up now that Kees' -Wvla patch has been
>> >> included. This patch fixes it.
>> >>
>> >> Kees, perhaps it should go in your tree along side of the -Wvla patch
>> >> if no one else wants to take it?
>> >>
>> >
>> > I can take it along in the compiler attributes tree, since that
>> > touches the compiler*.h stuff. Although that would make it
>> > not-only-attributes, i.e. slightly lying :-)
>>
>> Oh, I had assumed Masahiro was going to carry it.
>
> No, I am not.
>
> Putting all sort of things into kbuild basket
> is painful for me.

Okay, I'll take it for the VLA series.

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-kernel
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Thu, 11 Oct 2018 15:15:44 +0000
Message-ID: <CAGXu5jKxF4AmA3GCuijqryca6fYgJYFZJUNf9vnvgJAYCUUiuw () mail ! gmail ! com>
--------------------
On Wed, Oct 10, 2018 at 7:48 PM, Masahiro Yamada
<yamada.masahiro@socionext.com> wrote:
> On Wed, Oct 10, 2018 at 11:51 PM Kees Cook <keescook@chromium.org> wrote:
>>
>> On Wed, Oct 10, 2018 at 12:03 AM, Miguel Ojeda
>> <miguel.ojeda.sandonis@gmail.com> wrote:
>> > On Wed, Oct 10, 2018 at 8:12 AM Joel Stanley <joel@jms.id.au> wrote:
>> >>
>> >> On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
>> >> >
>> >> > Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?
>> >>
>> >> clang built -next is blowing up now that Kees' -Wvla patch has been
>> >> included. This patch fixes it.
>> >>
>> >> Kees, perhaps it should go in your tree along side of the -Wvla patch
>> >> if no one else wants to take it?
>> >>
>> >
>> > I can take it along in the compiler attributes tree, since that
>> > touches the compiler*.h stuff. Although that would make it
>> > not-only-attributes, i.e. slightly lying :-)
>>
>> Oh, I had assumed Masahiro was going to carry it.
>
> No, I am not.
>
> Putting all sort of things into kbuild basket
> is painful for me.

Okay, I'll take it for the VLA series.

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-sparse
Subject: Re: [PATCH v2] compiler.h: give up __compiletime_assert_fallback()
Date: Thu, 11 Oct 2018 15:15:44 +0000
Message-ID: <CAGXu5jKxF4AmA3GCuijqryca6fYgJYFZJUNf9vnvgJAYCUUiuw () mail ! gmail ! com>
--------------------
On Wed, Oct 10, 2018 at 7:48 PM, Masahiro Yamada
<yamada.masahiro@socionext.com> wrote:
> On Wed, Oct 10, 2018 at 11:51 PM Kees Cook <keescook@chromium.org> wrote:
>>
>> On Wed, Oct 10, 2018 at 12:03 AM, Miguel Ojeda
>> <miguel.ojeda.sandonis@gmail.com> wrote:
>> > On Wed, Oct 10, 2018 at 8:12 AM Joel Stanley <joel@jms.id.au> wrote:
>> >>
>> >> On Thu, 27 Sep 2018 at 05:07, Kees Cook <keescook@chromium.org> wrote:
>> >> >
>> >> > Yeah, that's what I'd linked to in the patchwork URL. Andrew, can you take this?
>> >>
>> >> clang built -next is blowing up now that Kees' -Wvla patch has been
>> >> included. This patch fixes it.
>> >>
>> >> Kees, perhaps it should go in your tree along side of the -Wvla patch
>> >> if no one else wants to take it?
>> >>
>> >
>> > I can take it along in the compiler attributes tree, since that
>> > touches the compiler*.h stuff. Although that would make it
>> > not-only-attributes, i.e. slightly lying :-)
>>
>> Oh, I had assumed Masahiro was going to carry it.
>
> No, I am not.
>
> Putting all sort of things into kbuild basket
> is painful for me.

Okay, I'll take it for the VLA series.

-Kees

-- 
Kees Cook
Pixel Security
================================================================================


################################################################################

=== Thread: [PATCH v2] doc: the man page in reST ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v2] doc: the man page in reST
Date: Mon, 21 May 2018 01:40:53 +0000
Message-ID: <20180521014053.56901-1-luc.vanoostenryck () gmail ! com>
--------------------
Manpages are a good thing but as an input format it's a
nightmare. It would be nice to have a more adapted format
for it.

So convert the manpage to reST and add to the Makefile what
is needed to generate the man page from it.

NB. This is still an experiment, but it's my intention to
    make this the source file for the manpage.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---

Note: This is not yet merged with the main sphinx patches
      because I don't want the distros to be dependent on
      sphinx to generate sparse's manpage.
      I suppose that having the master version in .rst
      and regenerate the troff/groff version at each change
      would be an acceptable solution but still I don't like
      this much.

Changes since v1:
* use the '.. option::' directive instead of option lists

 Documentation/.gitignore      |   1 +
 Documentation/conf.py         |   1 +
 Documentation/dev-options.rst |   4 +
 Documentation/index.rst       |   1 +
 Documentation/sparse.rst      | 462 ++++++++++++++++++++++++++++++++++
 5 files changed, 469 insertions(+)
 create mode 100644 Documentation/sparse.rst

diff --git a/Documentation/.gitignore b/Documentation/.gitignore
index 6c08d03c8..299ecac4a 100644
--- a/Documentation/.gitignore
+++ b/Documentation/.gitignore
@@ -1,2 +1,3 @@
 build
 dev-options.1
+sparse.1
diff --git a/Documentation/conf.py b/Documentation/conf.py
index f7a680147..7f825715a 100644
--- a/Documentation/conf.py
+++ b/Documentation/conf.py
@@ -161,6 +161,7 @@ latex_documents = [
 # (source start file, name, description, authors, manual section).
 man_pages = [
 	('dev-options', 'dev-options', u'options for development', [author], 1),
+	('sparse', 'sparse', u'Semantic Parser for C', [u'Linus Torvalds', author], 1),
 ]
 
 
diff --git a/Documentation/dev-options.rst b/Documentation/dev-options.rst
index 1f213afee..d9e518e33 100644
--- a/Documentation/dev-options.rst
+++ b/Documentation/dev-options.rst
@@ -43,3 +43,7 @@ OPTIONS
 .. option:: -ventry
 
   Dump the IR after all optimization passes.
+
+SEE ALSO
+--------
+:manpage:`cgcc(1)`
diff --git a/Documentation/index.rst b/Documentation/index.rst
index da006710e..18232e6dc 100644
--- a/Documentation/index.rst
+++ b/Documentation/index.rst
@@ -11,6 +11,7 @@ User documentation
 .. toctree::
    :maxdepth: 1
 
+   sparse
    nocast-vs-bitwise
 
 Developer documentation
diff --git a/Documentation/sparse.rst b/Documentation/sparse.rst
new file mode 100644
index 000000000..92f6a9eeb
--- /dev/null
+++ b/Documentation/sparse.rst
@@ -0,0 +1,462 @@
+.. Sparse manpage by Josh Triplett
+.. highlight:: c
+
+sparse - Semantic Parser for C
+##############################
+
+SYNOPSIS
+========
+``sparse`` [*options*]... *file.c*
+
+DESCRIPTION
+===========
+Sparse parses C source and looks for errors, producing warnings on standard
+error.
+
+Sparse accepts options controlling the set of warnings to generate. To turn
+on warnings Sparse does not issue by default, use the corresponding warning
+option ``-Wsomething``. Sparse issues some warnings by default; to turn
+off those warnings, pass the negation of the associated warning option,
+``-Wno-something``.
+
+OPTIONS
+=======
+
+WARNING OPTIONS
+---------------
+
+.. option:: -Wsparse-all
+
+	Turn on all sparse warnings, except for those explicitly disabled via
+	``-Wno-something``.
+
+
+.. option:: -Wsparse-error
+
+	Turn all sparse warnings into errors.
+
+
+.. option:: -Waddress-space
+
+	Warn about code which mixing pointers to different address spaces.
+
+	Sparse allows on pointers an extended attribute
+	``__attribute((address_space(n)))`` which designates a pointer target
+	in address space *n* (a constant integer). With ``-Waddress-space``,
+	Sparse treats pointers with identical target types but different
+	address spaces as distinct types. To override this warning, such as
+	for functions which convert pointers between address spaces, use a
+	type that includes ``__attribute__((force))``.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-address-space``.
+
+
+.. option:: -Wbitwise
+
+	Warn about unsupported operations or type mismatches with restricted
+	integer types.
+
+	Sparse supports an extended attribute, ``__attribute__((bitwise))``,
+	which creates a new restricted integer type from a base integer type,
+	distinct from the base integer type and from any other restricted
+	integer type not declared in the same declaration or ``typedef``.
+	For example, this allows programs to create typedefs for integer
+	types with specific endianness. With ``-Wbitwise``, Sparse will warn
+	on any use of a restricted type in arithmetic operations other than
+	bitwise operations, and on any conversion of one restricted type into
+	another, except via a cast that includes ``__attribute__((force))``.
+
+	``__bitwise`` ends up being a "stronger integer separation", one that
+	doesn't allow you to mix with non-bitwise integers, so now it's much
+	harder to lose the type by mistake.
+
+	``__bitwise`` is for *unique types* that cannot be mixed with other
+	types, and that you'd never want to just use as a random integer (the
+	integer 0 is special, though, and gets silently accepted iirc - it's
+	kind of like "NULL" for pointers). So ``gfp_t`` or the safe endianness
+	types like ``le16`` would be ``__bitwise``. You can only operate on
+	them by doing specific operations that know about *that* particular
+	type.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-bitwise``.
+
+
+.. option:: -Wcast-to-as
+
+	Warn about casts which add an address space to a pointer type.
+
+	A cast that includes ``__attribute__((force))`` will suppress this
+	warning.
+
+	Sparse does not issue these warnings by default.
+
+
+.. option:: -Wcast-truncate
+
+	Warn about casts that truncate constant values.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-cast-truncate``.
+
+
+.. option:: -Wconstexpr-not-const
+
+	Warn if a non-constant expression is encountered when really
+	expecting a constant expression instead.
+	Currently, this warns when initializing an object of static storage
+	duration with an initializer which is not a constant expression.
+
+	Sparse does not issue these warnings by default.
+
+
+.. option:: -Wcontext
+
+	Warn about potential errors in synchronization or other delimited
+	contexts.
+
+	Sparse supports several means of designating functions or statements
+	that delimit contexts, such as synchronization. Functions with the
+	extended attribute ``__attribute__((context(expression, in, out))``
+	require the context *expression* (for instance, a lock) to have the
+	value *in* (a constant nonnegative integer) when called, and return
+	with the value *out* (a constant nonnegative integer). For APIs
+	defined via macros, use the statement form ``__context__(expression,
+	in, out)`` in the body of the macro.
+
+	With ``-Wcontext`` Sparse will warn when it sees a function change
+	the context without indicating this with a ``context`` attribute,
+	either by decreasing a context below zero (such as by releasing a
+	lock without acquiring it), or returning with a changed context
+	(such as by acquiring a lock without releasing it). Sparse will
+	also warn about blocks of code which may potentially execute with
+	different contexts.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-context``.
+
+
+.. option:: -Wdecl
+
+	Warn about any non-``static`` variable or function definition that
+	has no previous declaration.
+
+	Private symbols (functions and variables) internal to a given source
+	file should use ``static``, to allow additional compiler
+	optimizations, allow detection of unused symbols, and prevent other
+	code from relying on these internal symbols. Public symbols used by
+	other source files will need declarations visible to those other
+	source files, such as in a header file. All declarations should fall
+	into one of these two categories. Thus, with ``-Wdecl``, Sparse warns
+	about any symbol definition with neither ``static`` nor a declaration.
+	To fix this warning, declare private symbols ``static``, and ensure
+	that the files defining public symbols have the symbol declarations
+	available first (such as by including the appropriate header file).
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-decl``.
+
+
+.. option:: -Wdeclaration-after-statement
+
+	Warn about declarations that are not at the start of a block.
+
+	These declarations are permitted in C99 but not in C89.
+
+	Sparse issues these warnings by default only when the C dialect is
+	C89 (i.e. with ``-ansi`` or ``-std=c89``). To turn them off, use
+	``-Wno-declaration-after-statement``.
+
+
+.. option:: -Wdefault-bitfield-sign
+
+	Warn about any bitfield with no explicit signedness.
+
+	Bitfields have no standard-specified default signedness (C99 6.7.2).
+	A bitfield without an explicit ``signed`` or ``unsigned`` creates a
+	portability problem for software that relies on the available range
+	of values. To fix this, specify the bitfield type as ``signed`` or
+	``unsigned`` explicitly.
+
+	Sparse does not issue these warnings by default.
+
+
+.. option:: -Wdesignated-init
+
+	Warn about positional initialization of structs marked as requiring
+	designated initializers.
+
+	Sparse allows an attribute ``__attribute__((designated_init))``
+	which marks a struct as requiring designated initializers. Sparse
+	will warn about positional initialization of a struct variable or
+	struct literal of a type that has this attribute.
+
+	Requiring designated initializers for a particular struct type will
+	insulate code using that struct type from changes to the layout of
+	the type, avoiding the need to change initializers for that type
+	unless they initialize a removed or incompatibly changed field.
+
+	Common examples of this type of struct include collections of
+	function pointers for the implementations of a class of related
+	operations, for which the default ``NULL`` for an unmentioned field
+	in a designated initializer will correctly indicate the absence of
+	that operation.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-designated-init``.
+
+
+.. option:: -Wdo-while
+
+	Warn about do-while loops that do not delimit the loop body with
+	braces.
+
+	Sparse does not issue these warnings by default.
+
+
+.. option:: -Wenum-mismatch
+
+	Warn about the use of an expression of an incorrect ``enum`` type
+	when initializing another ``enum`` type, assigning to another
+	``enum`` type, or passing an argument to a function which expects
+	another ``enum`` type.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-enum-mismatch``.
+
+
+.. option:: -Winit-cstring
+
+	Warn about initialization of a char array with a too long constant
+	C string.
+
+	If the size of the char array and the length of the string are the
+	same, there is no space for the last nul char of the string in the
+	array::
+
+		char s[3] = "abc";
+
+	If the array is used as a byte array, not as C string, this
+	warning is just noise. However, if the array is passed to functions
+	dealing with C string like printf(%s) and strcmp, it may cause a
+	trouble.
+
+	Sparse does not issue these warnings by default.
+
+
+.. option:: -Wmemcpy-max-count
+
+	Warn about call of ``memcpy()``, ``memset()``, ``copy_from_user()``,
+	or ``copy_to_user()`` with a large compile-time byte count.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-memcpy-max-count``.
+
+	The limit can be changed with ``-fmemcpy-max-count=COUNT``,
+	the default being ``100000``.
+
+
+.. option:: -Wnon-pointer-null
+
+	Warn about the use of 0 as a NULL pointer.
+
+	0 has integer type. NULL has pointer type.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-non-pointer-null``.
+
+
+.. option:: -Wold-initializer
+
+	Warn about the use of the pre-C99 GCC syntax for designated
+	initializers.
+
+	C99 provides a standard syntax for designated fields in ``struct``
+	or ``union`` initializers::
+
+		struct structname var = { .field = value };
+
+	GCC also has an old, non-standard syntax for designated initializers
+	which predates C99::
+
+		struct structname var = { field: value };
+
+	Sparse will warn about the use of GCC's non-standard syntax for
+	designated initializers. To fix this warning, convert designated
+	initializers to use the standard C99 syntax.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-old-initializer``.
+
+
+.. option:: -Wone-bit-signed-bitfield
+
+	Warn about any one-bit ``signed`` bitfields.
+
+	A one-bit ``signed`` bitfield can only have the values 0 and -1, or
+	with some compilers only 0; this results in unexpected behavior for
+	programs which expected the ability to store 0 and 1.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-one-bit-signed-bitfield``.
+
+
+.. option:: -Wparen-string
+
+	Warn about the use of a parenthesized string to initialize an array.
+
+	Standard C syntax does not permit a parenthesized string as an array
+	initializer. GCC allows this syntax as an extension. With
+	``-Wparen-string``, Sparse will warn about this syntax.
+
+	Sparse does not issue these warnings by default.
+
+
+.. option:: -Wptr-subtraction-blows
+
+	Warn when subtracting two pointers to a type with a non-power-of-two
+	size.
+
+	Subtracting two pointers to a given type gives a difference in terms
+	of the number of items of that type. To generate this value, compilers
+	will usually need to divide the difference by the size of the type,
+	an potentially expensive operation for sizes other than powers of two.
+
+	Code written using pointer subtraction can often use another approach
+	instead, such as array indexing with an explicit array index variable,
+	which may allow compilers to generate more efficient code.
+
+	Sparse does not issue these warnings by default.
+
+
+.. option:: -Wreturn-void
+
+	Warn if a function with return type void returns a void expression.
+
+	C99 permits this, and in some cases this allows for more generic code
+	in macros that use typeof or take a type as a macro argument.
+	However, some programs consider this poor style, and those programs
+	can use ``-Wreturn-void`` to get warnings about it.
+
+	Sparse does not issue these warnings by default.
+
+
+.. option:: -Wshadow
+
+	Warn when declaring a symbol which shadows a declaration with the
+	same name in an outer scope.
+
+	Such declarations can lead to error-prone code.
+
+	Sparse does not issue these warnings by default.
+
+
+.. option:: -Wsizeof-bool
+
+	Warn when checking the sizeof a _Bool.
+
+	C99 does not specify the sizeof a _Bool. gcc uses 1.
+
+	Sparse does not issue these warnings by default.
+
+
+.. option:: -Wtransparent-union
+
+	Warn about any declaration using the GCC extension
+	``__attribute__((transparent_union))``.
+
+	Sparse issues these warnings by default. To turn them off, use
+	``-Wno-transparent-union``.
+
+
+.. option:: -Wtypesign
+
+	Warn when converting a pointer to an integer type into a pointer to
+	an integer type with different signedness.
+
+	Sparse does not issue these warnings by default.
+
+
+.. option:: -Wundef
+
+	Warn about preprocessor conditionals that use the value of an
+	undefined preprocessor symbol.
+
+	Standard C (C99 6.10.1) permits using the value of an undefined
+	preprocessor symbol in preprocessor conditionals, and specifies it
+	has a value of 0. However, this behavior can lead to subtle errors.
+
+	Sparse does not issue these warnings by default.
+
+
+MISC OPTIONS
+------------
+
+.. option:: -gcc-base-dir dir
+
+	Look for compiler-provided system headers in *dir*/include/ and
+	*dir*/include-fixed/.
+
+
+.. option:: -multiarch-dir dir
+
+	Look for system headers in the multiarch subdirectory *dir*.
+	The *dir* name would normally take the form of the target's
+	normalized GNU triplet. (e.g. i386-linux-gnu).
+
+
+DEBUG OPTIONS
+-------------
+
+.. option:: -fmem-report
+
+	Report some statistics about memory allocation used by the tool.
+
+
+OTHER OPTIONS
+-------------
+
+.. option:: -fmax-warnings=COUNT
+
+	Set the maximum number of displayed warnings
+	to *COUNT*, which should be a numerical value or ``unlimited``.
+	The default limit is 100.
+
+
+.. option:: -fmemcpy-max-count=COUNT
+
+	Set the limit for the warnings given by ``-Wmemcpy-max-count``.
+	A *COUNT* of ``unlimited`` or ``0`` will effectively disable the
+	warning. The default limit is 100000.
+
+
+.. option:: -ftabstop=WIDTH
+
+	Set the distance between tab stops. This helps sparse report correct
+	column numbers in warnings or errors. If the value is less than 1 or
+	greater than 100, the option is ignored. The default is 8.
+
+
+
+.. option:: -f[no-]unsigned-char, -f[no-]signed-char
+
+	Let plain ``char`` be unsigned or signed.
+	By default chars are signed.
+
+
+SEE ALSO
+========
+:manpage:`cgcc(1)`
+
+HOMEPAGE
+========
+`http://www.kernel.org/pub/software/devel/sparse/`
+
+MAILING LIST
+============
+linux-sparse@vger.kernel.org
+
+MAINTAINER
+==========
+Christopher Li <sparse@chrisli.org>
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v2] doc: the man page in reST
Date: Mon, 21 May 2018 02:28:04 +0000
Message-ID: <20180521022803.ksjnq6gvbsm4awfd () ltop ! local>
--------------------
On Sun, May 20, 2018 at 07:06:58PM -0700, Randy Dunlap wrote:
> On 05/20/2018 06:40 PM, Luc Van Oostenryck wrote:
> > Manpages are a good thing but as an input format it's a
> > nightmare. It would be nice to have a more adapted format
> > for it.
> > 
> > So convert the manpage to reST and add to the Makefile what
> > is needed to generate the man page from it.
> 
> and this patch doesn't do the Makefile part.  :)

Yes indeed, that was already done in the patch for dev-options.

> > NB. This is still an experiment, but it's my intention to
> >     make this the source file for the manpage.
> > 
> > Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> > ---
> > 
> > Note: This is not yet merged with the main sphinx patches
> >       because I don't want the distros to be dependent on
> >       sphinx to generate sparse's manpage.
> >       I suppose that having the master version in .rst
> >       and regenerate the troff/groff version at each change
> >       would be an acceptable solution but still I don't like
> >       this much.
> 
> I would be more concerned about $USER not having sphinx than the
> distros being dependent on it.

$USER not installing sparse via its distro?
But the problem is the same, anyway.

Just to be clear, by the 'solution' here above I mean:
- use the .rst as the master version, where all changes are made
- still ship a sparse.1 file so people and distros don't depend on sphinx
- the maintainer must care to regenerate sparse.1 when sparse.rst is
  modified.

> > Changes since v1:
> > * use the '.. option::' directive instead of option lists
> > 
> >  Documentation/.gitignore      |   1 +
> >  Documentation/conf.py         |   1 +
> >  Documentation/dev-options.rst |   4 +
> >  Documentation/index.rst       |   1 +
> >  Documentation/sparse.rst      | 462 ++++++++++++++++++++++++++++++++++
> >  5 files changed, 469 insertions(+)
> >  create mode 100644 Documentation/sparse.rst
> > 
> > diff --git a/Documentation/sparse.rst b/Documentation/sparse.rst
> > new file mode 100644
> > index 000000000..92f6a9eeb
> > --- /dev/null
> > +++ b/Documentation/sparse.rst
> > @@ -0,0 +1,462 @@
> > +.. Sparse manpage by Josh Triplett
> > +.. highlight:: c
> > +
> 
> [snip]
> 
> > +DEBUG OPTIONS
> > +-------------
> > +
> > +.. option:: -fmem-report
> > +
> > +	Report some statistics about memory allocation used by the tool.
> > +
> 
> Should we add the -v options here?

I hesitated myself the other day.
I almost moved this option to dev-options and then I forgot about it.
I really don't have a strong opinion about it.
On one side, most the dev-options (and this -fmem-report) have no
use for the normal sparse users, even the ones that uses it a lot.
On the other side, it's annoying to have two files/manpages for
these options.

> > +HOMEPAGE
> > +========
> > +`http://www.kernel.org/pub/software/devel/sparse/`
> 
> I know that this is just a conversion from sparse.1, but that's not much of
> a homepage.  The sparse wiki would be better IMO (sparse.wiki.kernel.org).

Yes, I totally agree.
 
-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Randy Dunlap <rdunlap () infradead ! org>
To: linux-sparse
Subject: Re: [PATCH v2] doc: the man page in reST
Date: Mon, 21 May 2018 02:32:30 +0000
Message-ID: <4802ef44-1fd9-55be-f3b1-71b8907039fc () infradead ! org>
--------------------
On 05/20/2018 07:28 PM, Luc Van Oostenryck wrote:
> On Sun, May 20, 2018 at 07:06:58PM -0700, Randy Dunlap wrote:
>> On 05/20/2018 06:40 PM, Luc Van Oostenryck wrote:
>>> Manpages are a good thing but as an input format it's a
>>> nightmare. It would be nice to have a more adapted format
>>> for it.
>>>
>>> So convert the manpage to reST and add to the Makefile what
>>> is needed to generate the man page from it.
>>
>> and this patch doesn't do the Makefile part.  :)
> 
> Yes indeed, that was already done in the patch for dev-options.

Yep, I noticed that after I sent this comment.

>>> NB. This is still an experiment, but it's my intention to
>>>     make this the source file for the manpage.
>>>
>>> Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
>>> ---
>>>
>>> Note: This is not yet merged with the main sphinx patches
>>>       because I don't want the distros to be dependent on
>>>       sphinx to generate sparse's manpage.
>>>       I suppose that having the master version in .rst
>>>       and regenerate the troff/groff version at each change
>>>       would be an acceptable solution but still I don't like
>>>       this much.
>>
>> I would be more concerned about $USER not having sphinx than the
>> distros being dependent on it.
> 
> $USER not installing sparse via its distro?

Right.

> But the problem is the same, anyway.
> 
> Just to be clear, by the 'solution' here above I mean:
> - use the .rst as the master version, where all changes are made
> - still ship a sparse.1 file so people and distros don't depend on sphinx
> - the maintainer must care to regenerate sparse.1 when sparse.rst is
>   modified.

Sure, I got that.
But IMO distros will not have a problem with using sphinx.
Users might.

>>> Changes since v1:
>>> * use the '.. option::' directive instead of option lists
>>>
>>>  Documentation/.gitignore      |   1 +
>>>  Documentation/conf.py         |   1 +
>>>  Documentation/dev-options.rst |   4 +
>>>  Documentation/index.rst       |   1 +
>>>  Documentation/sparse.rst      | 462 ++++++++++++++++++++++++++++++++++
>>>  5 files changed, 469 insertions(+)
>>>  create mode 100644 Documentation/sparse.rst
>>>
>>> diff --git a/Documentation/sparse.rst b/Documentation/sparse.rst
>>> new file mode 100644
>>> index 000000000..92f6a9eeb
>>> --- /dev/null
>>> +++ b/Documentation/sparse.rst
>>> @@ -0,0 +1,462 @@
>>> +.. Sparse manpage by Josh Triplett
>>> +.. highlight:: c
>>> +
>>
>> [snip]
>>
>>> +DEBUG OPTIONS
>>> +-------------
>>> +
>>> +.. option:: -fmem-report
>>> +
>>> +	Report some statistics about memory allocation used by the tool.
>>> +
>>
>> Should we add the -v options here?
> 
> I hesitated myself the other day.
> I almost moved this option to dev-options and then I forgot about it.
> I really don't have a strong opinion about it.
> On one side, most the dev-options (and this -fmem-report) have no
> use for the normal sparse users, even the ones that uses it a lot.
> On the other side, it's annoying to have two files/manpages for
> these options.

I think that it makes sense to have two files/manpages and move some of
the options from sparse.rst to dev-options.rst.

>>> +HOMEPAGE
>>> +========
>>> +`http://www.kernel.org/pub/software/devel/sparse/`
>>
>> I know that this is just a conversion from sparse.1, but that's not much of
>> a homepage.  The sparse wiki would be better IMO (sparse.wiki.kernel.org).
> 
> Yes, I totally agree.
>  
> -- Luc


cheers.
-- 
~Randy
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v2] doc: the man page in reST
Date: Mon, 21 May 2018 02:50:20 +0000
Message-ID: <20180521025018.htxvdag7ms6s63jd () ltop ! local>
--------------------
On Sun, May 20, 2018 at 07:32:30PM -0700, Randy Dunlap wrote:
> On 05/20/2018 07:28 PM, Luc Van Oostenryck wrote:
> > On Sun, May 20, 2018 at 07:06:58PM -0700, Randy Dunlap wrote:
> >> On 05/20/2018 06:40 PM, Luc Van Oostenryck wrote:
> >>>
> >>> Note: This is not yet merged with the main sphinx patches
> >>>       because I don't want the distros to be dependent on
> >>>       sphinx to generate sparse's manpage.
> >>>       I suppose that having the master version in .rst
> >>>       and regenerate the troff/groff version at each change
> >>>       would be an acceptable solution but still I don't like
> >>>       this much.
> >>
> >> I would be more concerned about $USER not having sphinx than the
> >> distros being dependent on it.
> > 
> > $USER not installing sparse via its distro?
> 
> Right.
> 
> > But the problem is the same, anyway.
> > 
> > Just to be clear, by the 'solution' here above I mean:
> > - use the .rst as the master version, where all changes are made
> > - still ship a sparse.1 file so people and distros don't depend on sphinx
> > - the maintainer must care to regenerate sparse.1 when sparse.rst is
> >   modified.
> 
> Sure, I got that.
> But IMO distros will not have a problem with using sphinx.
> Users might.

Yes, most probably.
My point of view is that only sparse developers who make changes to
the documentation need to have sphinx installed (and thanks, sphinx
is sooo much simpler to install than most other doc toolchains).

> >>> +DEBUG OPTIONS
> >>> +-------------
> >>> +
> >>> +.. option:: -fmem-report
> >>> +
> >>> +	Report some statistics about memory allocation used by the tool.
> >>> +
> >>
> >> Should we add the -v options here?
> > 
> > I hesitated myself the other day.
> > I almost moved this option to dev-options and then I forgot about it.
> > I really don't have a strong opinion about it.
> > On one side, most the dev-options (and this -fmem-report) have no
> > use for the normal sparse users, even the ones that uses it a lot.
> > On the other side, it's annoying to have two files/manpages for
> > these options.
> 
> I think that it makes sense to have two files/manpages and move some of
> the options from sparse.rst to dev-options.rst.

Perfect then.
 
Cheers,
-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Ramsay Jones <ramsay () ramsayjones ! plus ! com>
To: linux-sparse
Subject: Re: [PATCH v2] doc: the man page in reST
Date: Mon, 21 May 2018 16:47:41 +0000
Message-ID: <3a1c7821-bbfb-5906-e28a-f3196390214d () ramsayjones ! plus ! com>
--------------------


On 21/05/18 03:50, Luc Van Oostenryck wrote:
> On Sun, May 20, 2018 at 07:32:30PM -0700, Randy Dunlap wrote:
>> On 05/20/2018 07:28 PM, Luc Van Oostenryck wrote:
>>> On Sun, May 20, 2018 at 07:06:58PM -0700, Randy Dunlap wrote:
>>>> On 05/20/2018 06:40 PM, Luc Van Oostenryck wrote:
>>>>>
>>>>> Note: This is not yet merged with the main sphinx patches
>>>>>       because I don't want the distros to be dependent on
>>>>>       sphinx to generate sparse's manpage.
>>>>>       I suppose that having the master version in .rst
>>>>>       and regenerate the troff/groff version at each change
>>>>>       would be an acceptable solution but still I don't like
>>>>>       this much.
>>>>
>>>> I would be more concerned about $USER not having sphinx than the
>>>> distros being dependent on it.
>>>
>>> $USER not installing sparse via its distro?
>>
>> Right.
>>
>>> But the problem is the same, anyway.
>>>
>>> Just to be clear, by the 'solution' here above I mean:
>>> - use the .rst as the master version, where all changes are made
>>> - still ship a sparse.1 file so people and distros don't depend on sphinx
>>> - the maintainer must care to regenerate sparse.1 when sparse.rst is
>>>   modified.
>>
>> Sure, I got that.
>> But IMO distros will not have a problem with using sphinx.
>> Users might.
> 
> Yes, most probably.
> My point of view is that only sparse developers who make changes to
> the documentation need to have sphinx installed (and thanks, sphinx
> is sooo much simpler to install than most other doc toolchains).

Sorry for not looking for myself, but have you documented
anywhere (for other sparse developers) how to install sphinx?

[and any other software necessary to re-build the docs, if any.]

ATB,
Ramsay Jones


--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v2] doc: the man page in reST
Date: Mon, 21 May 2018 18:42:25 +0000
Message-ID: <20180521184223.ujogjz5td5i4vcj3 () ltop ! local>
--------------------
On Mon, May 21, 2018 at 05:47:41PM +0100, Ramsay Jones wrote:
> On 21/05/18 03:50, Luc Van Oostenryck wrote:
> > 
> > Yes, most probably.
> > My point of view is that only sparse developers who make changes to
> > the documentation need to have sphinx installed (and thanks, sphinx
> > is sooo much simpler to install than most other doc toolchains).
> 
> Sorry for not looking for myself, but have you documented
> anywhere (for other sparse developers) how to install sphinx?
> 
> [and any other software necessary to re-build the docs, if any.]

No. not directly at least.
But in Documentation/doc-guide.rst there is a link to the sphinx
main page and from there you can find, among other things, instructions
for installing it.

That said:
* it's as easy as it could be:
  either (for Debian or Ubuntu)
	$ apt install python-sphinx
  or
	$ pip install sphinx
  and there is no other dependencies
* there is no absolute needs to generate the doc as the .rst and .md
  are quite legible as-is (but yes the HTML doc is nice, have an index, ...)
* the current version of the doc is available at
	http://sparse-doc.readthedocs.io/en/latest/
  (which correspond to the master branch of my stable tree)

But yes, I can certainly also add a link to
	http://www.sphinx-doc.org/en/stable/install.html

Best regards,
-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v2] x86, kbuild: revert macrofying inline assembly code ===

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kbuild
Subject: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Sun, 16 Dec 2018 02:50:32 +0000
Message-ID: <1544928632-9717-1-git-send-email-yamada.masahiro () socionext ! com>
--------------------
Revert the following 9 commits:

[1] 5bdcd510c2ac ("x86/jump-labels: Macrofy inline assembly code to
    work around GCC inlining bugs")

    This was partially reverted because it made good cleanups
    irrespective of the inlining issue; the error message is still
    unneeded, and the conversion to STATIC_BRANCH_{NOP,JUMP} should
    be kept.

[2] d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to
    work around GCC inlining bugs")

[3] 0474d5d9d2f7 ("x86/extable: Macrofy inline assembly code to work
    around GCC inlining bugs")

[4] 494b5168f2de ("x86/paravirt: Work around GCC inlining bugs when
    compiling paravirt ops")

[5] f81f8ad56fd1 ("x86/bug: Macrofy the BUG table section handling,
    to work around GCC inlining bugs")

[6] 77f48ec28e4c ("x86/alternatives: Macrofy lock prefixes to work
   around GCC inlining bugs")

[7] 9e1725b41059 ("x86/refcount: Work around GCC inlining bug")

    Resolved conflicts in arch/x86/include/asm/refcount.h caused by
    288e4521f0f6 ("x86/asm: 'Simplify' GEN_*_RMWcc() macros").

[8] c06c4d809051 ("x86/objtool: Use asm macros to work around GCC
    inlining bugs")

[9] 77b0bf55bc67 ("kbuild/Makefile: Prepare for using macros in inline
    assembly code to work around asm() related GCC inlining bugs")

A few days after those commits applied, discussion started to solve
the issue more elegantly with the help of compiler:

  https://lkml.org/lkml/2018/10/7/92

The new syntax "asm inline" was implemented by Segher Boessenkool, and
now queued up for GCC 9. (People were positive even for back-porting it
to older compilers).

Since the in-kernel workarounds merged, some issues have been reported:
breakage of building with distcc/icecc, breakage of distro packages for
module building. (More fundamentally, we cannot build external modules
after 'make clean'.)

I do not want to mess up the build system any more.

Given that this issue will be solved in a cleaner way sooner or later,
let's revert the in-kernel workarounds, and wait for GCC 9.

Reported-by: Logan Gunthorpe <logang@deltatee.com> # distcc
Reported-by: Sedat Dilek <sedat.dilek@gmail.com> # deb/rpm package
Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
Cc: Nadav Amit <namit@vmware.com>
Cc: Segher Boessenkool <segher@kernel.crashing.org>
---

Please consider this for v4.20 release.
Currently, distro package build is broken.


Changes in v2:
  - Revive clean-ups made by 5bdcd510c2ac (per Peter Zijlstra)
  - Fix commit quoting style (per Peter Zijlstra)

 Makefile                               |  9 +---
 arch/x86/Makefile                      |  7 ---
 arch/x86/include/asm/alternative-asm.h | 20 +++----
 arch/x86/include/asm/alternative.h     | 11 +++-
 arch/x86/include/asm/asm.h             | 53 +++++++++++-------
 arch/x86/include/asm/bug.h             | 98 +++++++++++++++-------------------
 arch/x86/include/asm/cpufeature.h      | 82 ++++++++++++----------------
 arch/x86/include/asm/jump_label.h      | 22 ++++++--
 arch/x86/include/asm/paravirt_types.h  | 56 +++++++++----------
 arch/x86/include/asm/refcount.h        | 81 ++++++++++++----------------
 arch/x86/kernel/macros.S               | 16 ------
 include/asm-generic/bug.h              |  8 +--
 include/linux/compiler.h               | 56 +++++--------------
 scripts/Kbuild.include                 |  4 +-
 scripts/mod/Makefile                   |  2 -
 15 files changed, 224 insertions(+), 301 deletions(-)
 delete mode 100644 arch/x86/kernel/macros.S

diff --git a/Makefile b/Makefile
index f2c3423..4cf4c5b 100644
--- a/Makefile
+++ b/Makefile
@@ -1081,7 +1081,7 @@ scripts: scripts_basic scripts_dtc asm-generic gcc-plugins $(autoksyms_h)
 # version.h and scripts_basic is processed / created.
 
 # Listed in dependency order
-PHONY += prepare archprepare macroprepare prepare0 prepare1 prepare2 prepare3
+PHONY += prepare archprepare prepare0 prepare1 prepare2 prepare3
 
 # prepare3 is used to check if we are building in a separate output directory,
 # and if so do:
@@ -1104,9 +1104,7 @@ prepare2: prepare3 outputmakefile asm-generic
 prepare1: prepare2 $(version_h) $(autoksyms_h) include/generated/utsrelease.h
 	$(cmd_crmodverdir)
 
-macroprepare: prepare1 archmacros
-
-archprepare: archheaders archscripts macroprepare scripts_basic
+archprepare: archheaders archscripts prepare1 scripts_basic
 
 prepare0: archprepare gcc-plugins
 	$(Q)$(MAKE) $(build)=.
@@ -1174,9 +1172,6 @@ archheaders:
 PHONY += archscripts
 archscripts:
 
-PHONY += archmacros
-archmacros:
-
 PHONY += __headers
 __headers: $(version_h) scripts_basic uapi-asm-generic archheaders archscripts
 	$(Q)$(MAKE) $(build)=scripts build_unifdef
diff --git a/arch/x86/Makefile b/arch/x86/Makefile
index 75ef499..85a66c4 100644
--- a/arch/x86/Makefile
+++ b/arch/x86/Makefile
@@ -232,13 +232,6 @@ archscripts: scripts_basic
 archheaders:
 	$(Q)$(MAKE) $(build)=arch/x86/entry/syscalls all
 
-archmacros:
-	$(Q)$(MAKE) $(build)=arch/x86/kernel arch/x86/kernel/macros.s
-
-ASM_MACRO_FLAGS = -Wa,arch/x86/kernel/macros.s
-export ASM_MACRO_FLAGS
-KBUILD_CFLAGS += $(ASM_MACRO_FLAGS)
-
 ###
 # Kernel objects
 
diff --git a/arch/x86/include/asm/alternative-asm.h b/arch/x86/include/asm/alternative-asm.h
index 8e4ea39..31b627b 100644
--- a/arch/x86/include/asm/alternative-asm.h
+++ b/arch/x86/include/asm/alternative-asm.h
@@ -7,24 +7,16 @@
 #include <asm/asm.h>
 
 #ifdef CONFIG_SMP
-.macro LOCK_PREFIX_HERE
+	.macro LOCK_PREFIX
+672:	lock
 	.pushsection .smp_locks,"a"
 	.balign 4
-	.long 671f - .		# offset
+	.long 672b - .
 	.popsection
-671:
-.endm
-
-.macro LOCK_PREFIX insn:vararg
-	LOCK_PREFIX_HERE
-	lock \insn
-.endm
+	.endm
 #else
-.macro LOCK_PREFIX_HERE
-.endm
-
-.macro LOCK_PREFIX insn:vararg
-.endm
+	.macro LOCK_PREFIX
+	.endm
 #endif
 
 /*
diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h
index d7faa16..4cd6a3b 100644
--- a/arch/x86/include/asm/alternative.h
+++ b/arch/x86/include/asm/alternative.h
@@ -31,8 +31,15 @@
  */
 
 #ifdef CONFIG_SMP
-#define LOCK_PREFIX_HERE "LOCK_PREFIX_HERE\n\t"
-#define LOCK_PREFIX "LOCK_PREFIX "
+#define LOCK_PREFIX_HERE \
+		".pushsection .smp_locks,\"a\"\n"	\
+		".balign 4\n"				\
+		".long 671f - .\n" /* offset */		\
+		".popsection\n"				\
+		"671:"
+
+#define LOCK_PREFIX LOCK_PREFIX_HERE "\n\tlock; "
+
 #else /* ! CONFIG_SMP */
 #define LOCK_PREFIX_HERE ""
 #define LOCK_PREFIX ""
diff --git a/arch/x86/include/asm/asm.h b/arch/x86/include/asm/asm.h
index 21b0867..6467757b 100644
--- a/arch/x86/include/asm/asm.h
+++ b/arch/x86/include/asm/asm.h
@@ -120,25 +120,12 @@
 /* Exception table entry */
 #ifdef __ASSEMBLY__
 # define _ASM_EXTABLE_HANDLE(from, to, handler)			\
-	ASM_EXTABLE_HANDLE from to handler
-
-.macro ASM_EXTABLE_HANDLE from:req to:req handler:req
-	.pushsection "__ex_table","a"
-	.balign 4
-	.long (\from) - .
-	.long (\to) - .
-	.long (\handler) - .
+	.pushsection "__ex_table","a" ;				\
+	.balign 4 ;						\
+	.long (from) - . ;					\
+	.long (to) - . ;					\
+	.long (handler) - . ;					\
 	.popsection
-.endm
-#else /* __ASSEMBLY__ */
-
-# define _ASM_EXTABLE_HANDLE(from, to, handler)			\
-	"ASM_EXTABLE_HANDLE from=" #from " to=" #to		\
-	" handler=\"" #handler "\"\n\t"
-
-/* For C file, we already have NOKPROBE_SYMBOL macro */
-
-#endif /* __ASSEMBLY__ */
 
 # define _ASM_EXTABLE(from, to)					\
 	_ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
@@ -161,7 +148,6 @@
 	_ASM_PTR (entry);					\
 	.popsection
 
-#ifdef __ASSEMBLY__
 .macro ALIGN_DESTINATION
 	/* check for bad alignment of destination */
 	movl %edi,%ecx
@@ -185,7 +171,34 @@
 	_ASM_EXTABLE_UA(100b, 103b)
 	_ASM_EXTABLE_UA(101b, 103b)
 	.endm
-#endif /* __ASSEMBLY__ */
+
+#else
+# define _EXPAND_EXTABLE_HANDLE(x) #x
+# define _ASM_EXTABLE_HANDLE(from, to, handler)			\
+	" .pushsection \"__ex_table\",\"a\"\n"			\
+	" .balign 4\n"						\
+	" .long (" #from ") - .\n"				\
+	" .long (" #to ") - .\n"				\
+	" .long (" _EXPAND_EXTABLE_HANDLE(handler) ") - .\n"	\
+	" .popsection\n"
+
+# define _ASM_EXTABLE(from, to)					\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
+
+# define _ASM_EXTABLE_UA(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_uaccess)
+
+# define _ASM_EXTABLE_FAULT(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_fault)
+
+# define _ASM_EXTABLE_EX(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_ext)
+
+# define _ASM_EXTABLE_REFCOUNT(from, to)			\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_refcount)
+
+/* For C file, we already have NOKPROBE_SYMBOL macro */
+#endif
 
 #ifndef __ASSEMBLY__
 /*
diff --git a/arch/x86/include/asm/bug.h b/arch/x86/include/asm/bug.h
index 5090035..6804d66 100644
--- a/arch/x86/include/asm/bug.h
+++ b/arch/x86/include/asm/bug.h
@@ -4,8 +4,6 @@
 
 #include <linux/stringify.h>
 
-#ifndef __ASSEMBLY__
-
 /*
  * Despite that some emulators terminate on UD2, we use it for WARN().
  *
@@ -22,15 +20,53 @@
 
 #define LEN_UD2		2
 
+#ifdef CONFIG_GENERIC_BUG
+
+#ifdef CONFIG_X86_32
+# define __BUG_REL(val)	".long " __stringify(val)
+#else
+# define __BUG_REL(val)	".long " __stringify(val) " - 2b"
+#endif
+
+#ifdef CONFIG_DEBUG_BUGVERBOSE
+
+#define _BUG_FLAGS(ins, flags)						\
+do {									\
+	asm volatile("1:\t" ins "\n"					\
+		     ".pushsection __bug_table,\"aw\"\n"		\
+		     "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"	\
+		     "\t"  __BUG_REL(%c0) "\t# bug_entry::file\n"	\
+		     "\t.word %c1"        "\t# bug_entry::line\n"	\
+		     "\t.word %c2"        "\t# bug_entry::flags\n"	\
+		     "\t.org 2b+%c3\n"					\
+		     ".popsection"					\
+		     : : "i" (__FILE__), "i" (__LINE__),		\
+			 "i" (flags),					\
+			 "i" (sizeof(struct bug_entry)));		\
+} while (0)
+
+#else /* !CONFIG_DEBUG_BUGVERBOSE */
+
 #define _BUG_FLAGS(ins, flags)						\
 do {									\
-	asm volatile("ASM_BUG ins=\"" ins "\" file=%c0 line=%c1 "	\
-		     "flags=%c2 size=%c3"				\
-		     : : "i" (__FILE__), "i" (__LINE__),                \
-			 "i" (flags),                                   \
+	asm volatile("1:\t" ins "\n"					\
+		     ".pushsection __bug_table,\"aw\"\n"		\
+		     "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"	\
+		     "\t.word %c0"        "\t# bug_entry::flags\n"	\
+		     "\t.org 2b+%c1\n"					\
+		     ".popsection"					\
+		     : : "i" (flags),					\
 			 "i" (sizeof(struct bug_entry)));		\
 } while (0)
 
+#endif /* CONFIG_DEBUG_BUGVERBOSE */
+
+#else
+
+#define _BUG_FLAGS(ins, flags)  asm volatile(ins)
+
+#endif /* CONFIG_GENERIC_BUG */
+
 #define HAVE_ARCH_BUG
 #define BUG()							\
 do {								\
@@ -46,54 +82,4 @@ do {								\
 
 #include <asm-generic/bug.h>
 
-#else /* __ASSEMBLY__ */
-
-#ifdef CONFIG_GENERIC_BUG
-
-#ifdef CONFIG_X86_32
-.macro __BUG_REL val:req
-	.long \val
-.endm
-#else
-.macro __BUG_REL val:req
-	.long \val - 2b
-.endm
-#endif
-
-#ifdef CONFIG_DEBUG_BUGVERBOSE
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-1:	\ins
-	.pushsection __bug_table,"aw"
-2:	__BUG_REL val=1b	# bug_entry::bug_addr
-	__BUG_REL val=\file	# bug_entry::file
-	.word \line		# bug_entry::line
-	.word \flags		# bug_entry::flags
-	.org 2b+\size
-	.popsection
-.endm
-
-#else /* !CONFIG_DEBUG_BUGVERBOSE */
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-1:	\ins
-	.pushsection __bug_table,"aw"
-2:	__BUG_REL val=1b	# bug_entry::bug_addr
-	.word \flags		# bug_entry::flags
-	.org 2b+\size
-	.popsection
-.endm
-
-#endif /* CONFIG_DEBUG_BUGVERBOSE */
-
-#else /* CONFIG_GENERIC_BUG */
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-	\ins
-.endm
-
-#endif /* CONFIG_GENERIC_BUG */
-
-#endif /* __ASSEMBLY__ */
-
 #endif /* _ASM_X86_BUG_H */
diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h
index 7d44272..aced6c9 100644
--- a/arch/x86/include/asm/cpufeature.h
+++ b/arch/x86/include/asm/cpufeature.h
@@ -2,10 +2,10 @@
 #ifndef _ASM_X86_CPUFEATURE_H
 #define _ASM_X86_CPUFEATURE_H
 
-#ifdef __KERNEL__
-#ifndef __ASSEMBLY__
-
 #include <asm/processor.h>
+
+#if defined(__KERNEL__) && !defined(__ASSEMBLY__)
+
 #include <asm/asm.h>
 #include <linux/bitops.h>
 
@@ -161,10 +161,37 @@ extern void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int bit);
  */
 static __always_inline __pure bool _static_cpu_has(u16 bit)
 {
-	asm_volatile_goto("STATIC_CPU_HAS bitnum=%[bitnum] "
-			  "cap_byte=\"%[cap_byte]\" "
-			  "feature=%P[feature] t_yes=%l[t_yes] "
-			  "t_no=%l[t_no] always=%P[always]"
+	asm_volatile_goto("1: jmp 6f\n"
+		 "2:\n"
+		 ".skip -(((5f-4f) - (2b-1b)) > 0) * "
+			 "((5f-4f) - (2b-1b)),0x90\n"
+		 "3:\n"
+		 ".section .altinstructions,\"a\"\n"
+		 " .long 1b - .\n"		/* src offset */
+		 " .long 4f - .\n"		/* repl offset */
+		 " .word %P[always]\n"		/* always replace */
+		 " .byte 3b - 1b\n"		/* src len */
+		 " .byte 5f - 4f\n"		/* repl len */
+		 " .byte 3b - 2b\n"		/* pad len */
+		 ".previous\n"
+		 ".section .altinstr_replacement,\"ax\"\n"
+		 "4: jmp %l[t_no]\n"
+		 "5:\n"
+		 ".previous\n"
+		 ".section .altinstructions,\"a\"\n"
+		 " .long 1b - .\n"		/* src offset */
+		 " .long 0\n"			/* no replacement */
+		 " .word %P[feature]\n"		/* feature bit */
+		 " .byte 3b - 1b\n"		/* src len */
+		 " .byte 0\n"			/* repl len */
+		 " .byte 0\n"			/* pad len */
+		 ".previous\n"
+		 ".section .altinstr_aux,\"ax\"\n"
+		 "6:\n"
+		 " testb %[bitnum],%[cap_byte]\n"
+		 " jnz %l[t_yes]\n"
+		 " jmp %l[t_no]\n"
+		 ".previous\n"
 		 : : [feature]  "i" (bit),
 		     [always]   "i" (X86_FEATURE_ALWAYS),
 		     [bitnum]   "i" (1 << (bit & 7)),
@@ -199,44 +226,5 @@ static __always_inline __pure bool _static_cpu_has(u16 bit)
 #define CPU_FEATURE_TYPEVAL		boot_cpu_data.x86_vendor, boot_cpu_data.x86, \
 					boot_cpu_data.x86_model
 
-#else /* __ASSEMBLY__ */
-
-.macro STATIC_CPU_HAS bitnum:req cap_byte:req feature:req t_yes:req t_no:req always:req
-1:
-	jmp 6f
-2:
-	.skip -(((5f-4f) - (2b-1b)) > 0) * ((5f-4f) - (2b-1b)),0x90
-3:
-	.section .altinstructions,"a"
-	.long 1b - .		/* src offset */
-	.long 4f - .		/* repl offset */
-	.word \always		/* always replace */
-	.byte 3b - 1b		/* src len */
-	.byte 5f - 4f		/* repl len */
-	.byte 3b - 2b		/* pad len */
-	.previous
-	.section .altinstr_replacement,"ax"
-4:
-	jmp \t_no
-5:
-	.previous
-	.section .altinstructions,"a"
-	.long 1b - .		/* src offset */
-	.long 0			/* no replacement */
-	.word \feature		/* feature bit */
-	.byte 3b - 1b		/* src len */
-	.byte 0			/* repl len */
-	.byte 0			/* pad len */
-	.previous
-	.section .altinstr_aux,"ax"
-6:
-	testb \bitnum,\cap_byte
-	jnz \t_yes
-	jmp \t_no
-	.previous
-.endm
-
-#endif /* __ASSEMBLY__ */
-
-#endif /* __KERNEL__ */
+#endif /* defined(__KERNEL__) && !defined(__ASSEMBLY__) */
 #endif /* _ASM_X86_CPUFEATURE_H */
diff --git a/arch/x86/include/asm/jump_label.h b/arch/x86/include/asm/jump_label.h
index a5fb34f..cf88ebf 100644
--- a/arch/x86/include/asm/jump_label.h
+++ b/arch/x86/include/asm/jump_label.h
@@ -20,9 +20,15 @@
 
 static __always_inline bool arch_static_branch(struct static_key *key, bool branch)
 {
-	asm_volatile_goto("STATIC_BRANCH_NOP l_yes=\"%l[l_yes]\" key=\"%c0\" "
-			  "branch=\"%c1\""
-			: :  "i" (key), "i" (branch) : : l_yes);
+	asm_volatile_goto("1:"
+		".byte " __stringify(STATIC_KEY_INIT_NOP) "\n\t"
+		".pushsection __jump_table,  \"aw\" \n\t"
+		_ASM_ALIGN "\n\t"
+		".long 1b - ., %l[l_yes] - . \n\t"
+		_ASM_PTR "%c0 + %c1 - .\n\t"
+		".popsection \n\t"
+		: :  "i" (key), "i" (branch) : : l_yes);
+
 	return false;
 l_yes:
 	return true;
@@ -30,8 +36,14 @@ static __always_inline bool arch_static_branch(struct static_key *key, bool bran
 
 static __always_inline bool arch_static_branch_jump(struct static_key *key, bool branch)
 {
-	asm_volatile_goto("STATIC_BRANCH_JMP l_yes=\"%l[l_yes]\" key=\"%c0\" "
-			  "branch=\"%c1\""
+	asm_volatile_goto("1:"
+		".byte 0xe9\n\t .long %l[l_yes] - 2f\n\t"
+		"2:\n\t"
+		".pushsection __jump_table,  \"aw\" \n\t"
+		_ASM_ALIGN "\n\t"
+		".long 1b - ., %l[l_yes] - . \n\t"
+		_ASM_PTR "%c0 + %c1 - .\n\t"
+		".popsection \n\t"
 		: :  "i" (key), "i" (branch) : : l_yes);
 
 	return false;
diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h
index 26942ad..488c596 100644
--- a/arch/x86/include/asm/paravirt_types.h
+++ b/arch/x86/include/asm/paravirt_types.h
@@ -348,11 +348,23 @@ extern struct paravirt_patch_template pv_ops;
 #define paravirt_clobber(clobber)		\
 	[paravirt_clobber] "i" (clobber)
 
+/*
+ * Generate some code, and mark it as patchable by the
+ * apply_paravirt() alternate instruction patcher.
+ */
+#define _paravirt_alt(insn_string, type, clobber)	\
+	"771:\n\t" insn_string "\n" "772:\n"		\
+	".pushsection .parainstructions,\"a\"\n"	\
+	_ASM_ALIGN "\n"					\
+	_ASM_PTR " 771b\n"				\
+	"  .byte " type "\n"				\
+	"  .byte 772b-771b\n"				\
+	"  .short " clobber "\n"			\
+	".popsection\n"
+
 /* Generate patchable code, with the default asm parameters. */
-#define paravirt_call							\
-	"PARAVIRT_CALL type=\"%c[paravirt_typenum]\""			\
-	" clobber=\"%c[paravirt_clobber]\""				\
-	" pv_opptr=\"%c[paravirt_opptr]\";"
+#define paravirt_alt(insn_string)					\
+	_paravirt_alt(insn_string, "%c[paravirt_typenum]", "%c[paravirt_clobber]")
 
 /* Simple instruction patching code. */
 #define NATIVE_LABEL(a,x,b) "\n\t.globl " a #x "_" #b "\n" a #x "_" #b ":\n\t"
@@ -373,6 +385,16 @@ unsigned native_patch(u8 type, void *ibuf, unsigned long addr, unsigned len);
 int paravirt_disable_iospace(void);
 
 /*
+ * This generates an indirect call based on the operation type number.
+ * The type number, computed in PARAVIRT_PATCH, is derived from the
+ * offset into the paravirt_patch_template structure, and can therefore be
+ * freely converted back into a structure offset.
+ */
+#define PARAVIRT_CALL					\
+	ANNOTATE_RETPOLINE_SAFE				\
+	"call *%c[paravirt_opptr];"
+
+/*
  * These macros are intended to wrap calls through one of the paravirt
  * ops structs, so that they can be later identified and patched at
  * runtime.
@@ -509,7 +531,7 @@ int paravirt_disable_iospace(void);
 		/* since this condition will never hold */		\
 		if (sizeof(rettype) > sizeof(unsigned long)) {		\
 			asm volatile(pre				\
-				     paravirt_call			\
+				     paravirt_alt(PARAVIRT_CALL)	\
 				     post				\
 				     : call_clbr, ASM_CALL_CONSTRAINT	\
 				     : paravirt_type(op),		\
@@ -519,7 +541,7 @@ int paravirt_disable_iospace(void);
 			__ret = (rettype)((((u64)__edx) << 32) | __eax); \
 		} else {						\
 			asm volatile(pre				\
-				     paravirt_call			\
+				     paravirt_alt(PARAVIRT_CALL)	\
 				     post				\
 				     : call_clbr, ASM_CALL_CONSTRAINT	\
 				     : paravirt_type(op),		\
@@ -546,7 +568,7 @@ int paravirt_disable_iospace(void);
 		PVOP_VCALL_ARGS;					\
 		PVOP_TEST_NULL(op);					\
 		asm volatile(pre					\
-			     paravirt_call				\
+			     paravirt_alt(PARAVIRT_CALL)		\
 			     post					\
 			     : call_clbr, ASM_CALL_CONSTRAINT		\
 			     : paravirt_type(op),			\
@@ -664,26 +686,6 @@ struct paravirt_patch_site {
 extern struct paravirt_patch_site __parainstructions[],
 	__parainstructions_end[];
 
-#else	/* __ASSEMBLY__ */
-
-/*
- * This generates an indirect call based on the operation type number.
- * The type number, computed in PARAVIRT_PATCH, is derived from the
- * offset into the paravirt_patch_template structure, and can therefore be
- * freely converted back into a structure offset.
- */
-.macro PARAVIRT_CALL type:req clobber:req pv_opptr:req
-771:	ANNOTATE_RETPOLINE_SAFE
-	call *\pv_opptr
-772:	.pushsection .parainstructions,"a"
-	_ASM_ALIGN
-	_ASM_PTR 771b
-	.byte \type
-	.byte 772b-771b
-	.short \clobber
-	.popsection
-.endm
-
 #endif	/* __ASSEMBLY__ */
 
 #endif	/* _ASM_X86_PARAVIRT_TYPES_H */
diff --git a/arch/x86/include/asm/refcount.h b/arch/x86/include/asm/refcount.h
index a8b5e1e..dbaed55 100644
--- a/arch/x86/include/asm/refcount.h
+++ b/arch/x86/include/asm/refcount.h
@@ -4,41 +4,6 @@
  * x86-specific implementation of refcount_t. Based on PAX_REFCOUNT from
  * PaX/grsecurity.
  */
-
-#ifdef __ASSEMBLY__
-
-#include <asm/asm.h>
-#include <asm/bug.h>
-
-.macro REFCOUNT_EXCEPTION counter:req
-	.pushsection .text..refcount
-111:	lea \counter, %_ASM_CX
-112:	ud2
-	ASM_UNREACHABLE
-	.popsection
-113:	_ASM_EXTABLE_REFCOUNT(112b, 113b)
-.endm
-
-/* Trigger refcount exception if refcount result is negative. */
-.macro REFCOUNT_CHECK_LT_ZERO counter:req
-	js 111f
-	REFCOUNT_EXCEPTION counter="\counter"
-.endm
-
-/* Trigger refcount exception if refcount result is zero or negative. */
-.macro REFCOUNT_CHECK_LE_ZERO counter:req
-	jz 111f
-	REFCOUNT_CHECK_LT_ZERO counter="\counter"
-.endm
-
-/* Trigger refcount exception unconditionally. */
-.macro REFCOUNT_ERROR counter:req
-	jmp 111f
-	REFCOUNT_EXCEPTION counter="\counter"
-.endm
-
-#else /* __ASSEMBLY__ */
-
 #include <linux/refcount.h>
 #include <asm/bug.h>
 
@@ -50,12 +15,35 @@
  * central refcount exception. The fixup address for the exception points
  * back to the regular execution flow in .text.
  */
+#define _REFCOUNT_EXCEPTION				\
+	".pushsection .text..refcount\n"		\
+	"111:\tlea %[var], %%" _ASM_CX "\n"		\
+	"112:\t" ASM_UD2 "\n"				\
+	ASM_UNREACHABLE					\
+	".popsection\n"					\
+	"113:\n"					\
+	_ASM_EXTABLE_REFCOUNT(112b, 113b)
+
+/* Trigger refcount exception if refcount result is negative. */
+#define REFCOUNT_CHECK_LT_ZERO				\
+	"js 111f\n\t"					\
+	_REFCOUNT_EXCEPTION
+
+/* Trigger refcount exception if refcount result is zero or negative. */
+#define REFCOUNT_CHECK_LE_ZERO				\
+	"jz 111f\n\t"					\
+	REFCOUNT_CHECK_LT_ZERO
+
+/* Trigger refcount exception unconditionally. */
+#define REFCOUNT_ERROR					\
+	"jmp 111f\n\t"					\
+	_REFCOUNT_EXCEPTION
 
 static __always_inline void refcount_add(unsigned int i, refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "addl %1,%0\n\t"
-		"REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LT_ZERO
+		: [var] "+m" (r->refs.counter)
 		: "ir" (i)
 		: "cc", "cx");
 }
@@ -63,32 +51,31 @@ static __always_inline void refcount_add(unsigned int i, refcount_t *r)
 static __always_inline void refcount_inc(refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "incl %0\n\t"
-		"REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LT_ZERO
+		: [var] "+m" (r->refs.counter)
 		: : "cc", "cx");
 }
 
 static __always_inline void refcount_dec(refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "decl %0\n\t"
-		"REFCOUNT_CHECK_LE_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LE_ZERO
+		: [var] "+m" (r->refs.counter)
 		: : "cc", "cx");
 }
 
 static __always_inline __must_check
 bool refcount_sub_and_test(unsigned int i, refcount_t *r)
 {
-
 	return GEN_BINARY_SUFFIXED_RMWcc(LOCK_PREFIX "subl",
-					 "REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
+					 REFCOUNT_CHECK_LT_ZERO,
 					 r->refs.counter, e, "er", i, "cx");
 }
 
 static __always_inline __must_check bool refcount_dec_and_test(refcount_t *r)
 {
 	return GEN_UNARY_SUFFIXED_RMWcc(LOCK_PREFIX "decl",
-					"REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
+					REFCOUNT_CHECK_LT_ZERO,
 					r->refs.counter, e, "cx");
 }
 
@@ -106,8 +93,8 @@ bool refcount_add_not_zero(unsigned int i, refcount_t *r)
 
 		/* Did we try to increment from/to an undesirable state? */
 		if (unlikely(c < 0 || c == INT_MAX || result < c)) {
-			asm volatile("REFCOUNT_ERROR counter=\"%[counter]\""
-				     : : [counter] "m" (r->refs.counter)
+			asm volatile(REFCOUNT_ERROR
+				     : : [var] "m" (r->refs.counter)
 				     : "cc", "cx");
 			break;
 		}
@@ -122,6 +109,4 @@ static __always_inline __must_check bool refcount_inc_not_zero(refcount_t *r)
 	return refcount_add_not_zero(1, r);
 }
 
-#endif /* __ASSEMBLY__ */
-
 #endif
diff --git a/arch/x86/kernel/macros.S b/arch/x86/kernel/macros.S
deleted file mode 100644
index 161c950..0000000
--- a/arch/x86/kernel/macros.S
+++ /dev/null
@@ -1,16 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-
-/*
- * This file includes headers whose assembly part includes macros which are
- * commonly used. The macros are precompiled into assmebly file which is later
- * assembled together with each compiled file.
- */
-
-#include <linux/compiler.h>
-#include <asm/refcount.h>
-#include <asm/alternative-asm.h>
-#include <asm/bug.h>
-#include <asm/paravirt.h>
-#include <asm/asm.h>
-#include <asm/cpufeature.h>
-#include <asm/jump_label.h>
diff --git a/include/asm-generic/bug.h b/include/asm-generic/bug.h
index cdafa5e..20561a6 100644
--- a/include/asm-generic/bug.h
+++ b/include/asm-generic/bug.h
@@ -17,8 +17,10 @@
 #ifndef __ASSEMBLY__
 #include <linux/kernel.h>
 
-struct bug_entry {
+#ifdef CONFIG_BUG
+
 #ifdef CONFIG_GENERIC_BUG
+struct bug_entry {
 #ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS
 	unsigned long	bug_addr;
 #else
@@ -33,10 +35,8 @@ struct bug_entry {
 	unsigned short	line;
 #endif
 	unsigned short	flags;
-#endif	/* CONFIG_GENERIC_BUG */
 };
-
-#ifdef CONFIG_BUG
+#endif	/* CONFIG_GENERIC_BUG */
 
 /*
  * Don't use BUG() or BUG_ON() unless there's really no way out; one
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 06396c1..fc5004a 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -99,13 +99,22 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
  * unique, to convince GCC not to merge duplicate inline asm statements.
  */
 #define annotate_reachable() ({						\
-	asm volatile("ANNOTATE_REACHABLE counter=%c0"			\
-		     : : "i" (__COUNTER__));				\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.reachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
 #define annotate_unreachable() ({					\
-	asm volatile("ANNOTATE_UNREACHABLE counter=%c0"			\
-		     : : "i" (__COUNTER__));				\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.unreachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
+#define ASM_UNREACHABLE							\
+	"999:\n\t"							\
+	".pushsection .discard.unreachable\n\t"				\
+	".long 999b - .\n\t"						\
+	".popsection\n\t"
 #else
 #define annotate_reachable()
 #define annotate_unreachable()
@@ -293,45 +302,6 @@ static inline void *offset_to_ptr(const int *off)
 	return (void *)((unsigned long)off + *off);
 }
 
-#else /* __ASSEMBLY__ */
-
-#ifdef __KERNEL__
-#ifndef LINKER_SCRIPT
-
-#ifdef CONFIG_STACK_VALIDATION
-.macro ANNOTATE_UNREACHABLE counter:req
-\counter:
-	.pushsection .discard.unreachable
-	.long \counter\()b -.
-	.popsection
-.endm
-
-.macro ANNOTATE_REACHABLE counter:req
-\counter:
-	.pushsection .discard.reachable
-	.long \counter\()b -.
-	.popsection
-.endm
-
-.macro ASM_UNREACHABLE
-999:
-	.pushsection .discard.unreachable
-	.long 999b - .
-	.popsection
-.endm
-#else /* CONFIG_STACK_VALIDATION */
-.macro ANNOTATE_UNREACHABLE counter:req
-.endm
-
-.macro ANNOTATE_REACHABLE counter:req
-.endm
-
-.macro ASM_UNREACHABLE
-.endm
-#endif /* CONFIG_STACK_VALIDATION */
-
-#endif /* LINKER_SCRIPT */
-#endif /* __KERNEL__ */
 #endif /* __ASSEMBLY__ */
 
 /* Compile time object size, -1 for unknown */
diff --git a/scripts/Kbuild.include b/scripts/Kbuild.include
index bb01555..3d09844 100644
--- a/scripts/Kbuild.include
+++ b/scripts/Kbuild.include
@@ -115,9 +115,7 @@ __cc-option = $(call try-run,\
 
 # Do not attempt to build with gcc plugins during cc-option tests.
 # (And this uses delayed resolution so the flags will be up to date.)
-# In addition, do not include the asm macros which are built later.
-CC_OPTION_FILTERED = $(GCC_PLUGINS_CFLAGS) $(ASM_MACRO_FLAGS)
-CC_OPTION_CFLAGS = $(filter-out $(CC_OPTION_FILTERED),$(KBUILD_CFLAGS))
+CC_OPTION_CFLAGS = $(filter-out $(GCC_PLUGINS_CFLAGS),$(KBUILD_CFLAGS))
 
 # cc-option
 # Usage: cflags-y += $(call cc-option,-march=winchip-c6,-march=i586)
diff --git a/scripts/mod/Makefile b/scripts/mod/Makefile
index a5b4af4..42c5d50 100644
--- a/scripts/mod/Makefile
+++ b/scripts/mod/Makefile
@@ -4,8 +4,6 @@ OBJECT_FILES_NON_STANDARD := y
 hostprogs-y	:= modpost mk_elfconfig
 always		:= $(hostprogs-y) empty.o
 
-CFLAGS_REMOVE_empty.o := $(ASM_MACRO_FLAGS)
-
 modpost-objs	:= modpost.o file2alias.o sumversion.o
 
 devicetable-offsets-file := devicetable-offsets.h
-- 
2.7.4

================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-virtualization
Subject: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Sun, 16 Dec 2018 02:50:32 +0000
Message-ID: <1544928632-9717-1-git-send-email-yamada.masahiro () socionext ! com>
--------------------
Revert the following 9 commits:

[1] 5bdcd510c2ac ("x86/jump-labels: Macrofy inline assembly code to
    work around GCC inlining bugs")

    This was partially reverted because it made good cleanups
    irrespective of the inlining issue; the error message is still
    unneeded, and the conversion to STATIC_BRANCH_{NOP,JUMP} should
    be kept.

[2] d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to
    work around GCC inlining bugs")

[3] 0474d5d9d2f7 ("x86/extable: Macrofy inline assembly code to work
    around GCC inlining bugs")

[4] 494b5168f2de ("x86/paravirt: Work around GCC inlining bugs when
    compiling paravirt ops")

[5] f81f8ad56fd1 ("x86/bug: Macrofy the BUG table section handling,
    to work around GCC inlining bugs")

[6] 77f48ec28e4c ("x86/alternatives: Macrofy lock prefixes to work
   around GCC inlining bugs")

[7] 9e1725b41059 ("x86/refcount: Work around GCC inlining bug")

    Resolved conflicts in arch/x86/include/asm/refcount.h caused by
    288e4521f0f6 ("x86/asm: 'Simplify' GEN_*_RMWcc() macros").

[8] c06c4d809051 ("x86/objtool: Use asm macros to work around GCC
    inlining bugs")

[9] 77b0bf55bc67 ("kbuild/Makefile: Prepare for using macros in inline
    assembly code to work around asm() related GCC inlining bugs")

A few days after those commits applied, discussion started to solve
the issue more elegantly with the help of compiler:

  https://lkml.org/lkml/2018/10/7/92

The new syntax "asm inline" was implemented by Segher Boessenkool, and
now queued up for GCC 9. (People were positive even for back-porting it
to older compilers).

Since the in-kernel workarounds merged, some issues have been reported:
breakage of building with distcc/icecc, breakage of distro packages for
module building. (More fundamentally, we cannot build external modules
after 'make clean'.)

I do not want to mess up the build system any more.

Given that this issue will be solved in a cleaner way sooner or later,
let's revert the in-kernel workarounds, and wait for GCC 9.

Reported-by: Logan Gunthorpe <logang@deltatee.com> # distcc
Reported-by: Sedat Dilek <sedat.dilek@gmail.com> # deb/rpm package
Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
Cc: Nadav Amit <namit@vmware.com>
Cc: Segher Boessenkool <segher@kernel.crashing.org>
---

Please consider this for v4.20 release.
Currently, distro package build is broken.


Changes in v2:
  - Revive clean-ups made by 5bdcd510c2ac (per Peter Zijlstra)
  - Fix commit quoting style (per Peter Zijlstra)

 Makefile                               |  9 +---
 arch/x86/Makefile                      |  7 ---
 arch/x86/include/asm/alternative-asm.h | 20 +++----
 arch/x86/include/asm/alternative.h     | 11 +++-
 arch/x86/include/asm/asm.h             | 53 +++++++++++-------
 arch/x86/include/asm/bug.h             | 98 +++++++++++++++-------------------
 arch/x86/include/asm/cpufeature.h      | 82 ++++++++++++----------------
 arch/x86/include/asm/jump_label.h      | 22 ++++++--
 arch/x86/include/asm/paravirt_types.h  | 56 +++++++++----------
 arch/x86/include/asm/refcount.h        | 81 ++++++++++++----------------
 arch/x86/kernel/macros.S               | 16 ------
 include/asm-generic/bug.h              |  8 +--
 include/linux/compiler.h               | 56 +++++--------------
 scripts/Kbuild.include                 |  4 +-
 scripts/mod/Makefile                   |  2 -
 15 files changed, 224 insertions(+), 301 deletions(-)
 delete mode 100644 arch/x86/kernel/macros.S

diff --git a/Makefile b/Makefile
index f2c3423..4cf4c5b 100644
--- a/Makefile
+++ b/Makefile
@@ -1081,7 +1081,7 @@ scripts: scripts_basic scripts_dtc asm-generic gcc-plugins $(autoksyms_h)
 # version.h and scripts_basic is processed / created.
 
 # Listed in dependency order
-PHONY += prepare archprepare macroprepare prepare0 prepare1 prepare2 prepare3
+PHONY += prepare archprepare prepare0 prepare1 prepare2 prepare3
 
 # prepare3 is used to check if we are building in a separate output directory,
 # and if so do:
@@ -1104,9 +1104,7 @@ prepare2: prepare3 outputmakefile asm-generic
 prepare1: prepare2 $(version_h) $(autoksyms_h) include/generated/utsrelease.h
 	$(cmd_crmodverdir)
 
-macroprepare: prepare1 archmacros
-
-archprepare: archheaders archscripts macroprepare scripts_basic
+archprepare: archheaders archscripts prepare1 scripts_basic
 
 prepare0: archprepare gcc-plugins
 	$(Q)$(MAKE) $(build)=.
@@ -1174,9 +1172,6 @@ archheaders:
 PHONY += archscripts
 archscripts:
 
-PHONY += archmacros
-archmacros:
-
 PHONY += __headers
 __headers: $(version_h) scripts_basic uapi-asm-generic archheaders archscripts
 	$(Q)$(MAKE) $(build)=scripts build_unifdef
diff --git a/arch/x86/Makefile b/arch/x86/Makefile
index 75ef499..85a66c4 100644
--- a/arch/x86/Makefile
+++ b/arch/x86/Makefile
@@ -232,13 +232,6 @@ archscripts: scripts_basic
 archheaders:
 	$(Q)$(MAKE) $(build)=arch/x86/entry/syscalls all
 
-archmacros:
-	$(Q)$(MAKE) $(build)=arch/x86/kernel arch/x86/kernel/macros.s
-
-ASM_MACRO_FLAGS = -Wa,arch/x86/kernel/macros.s
-export ASM_MACRO_FLAGS
-KBUILD_CFLAGS += $(ASM_MACRO_FLAGS)
-
 ###
 # Kernel objects
 
diff --git a/arch/x86/include/asm/alternative-asm.h b/arch/x86/include/asm/alternative-asm.h
index 8e4ea39..31b627b 100644
--- a/arch/x86/include/asm/alternative-asm.h
+++ b/arch/x86/include/asm/alternative-asm.h
@@ -7,24 +7,16 @@
 #include <asm/asm.h>
 
 #ifdef CONFIG_SMP
-.macro LOCK_PREFIX_HERE
+	.macro LOCK_PREFIX
+672:	lock
 	.pushsection .smp_locks,"a"
 	.balign 4
-	.long 671f - .		# offset
+	.long 672b - .
 	.popsection
-671:
-.endm
-
-.macro LOCK_PREFIX insn:vararg
-	LOCK_PREFIX_HERE
-	lock \insn
-.endm
+	.endm
 #else
-.macro LOCK_PREFIX_HERE
-.endm
-
-.macro LOCK_PREFIX insn:vararg
-.endm
+	.macro LOCK_PREFIX
+	.endm
 #endif
 
 /*
diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h
index d7faa16..4cd6a3b 100644
--- a/arch/x86/include/asm/alternative.h
+++ b/arch/x86/include/asm/alternative.h
@@ -31,8 +31,15 @@
  */
 
 #ifdef CONFIG_SMP
-#define LOCK_PREFIX_HERE "LOCK_PREFIX_HERE\n\t"
-#define LOCK_PREFIX "LOCK_PREFIX "
+#define LOCK_PREFIX_HERE \
+		".pushsection .smp_locks,\"a\"\n"	\
+		".balign 4\n"				\
+		".long 671f - .\n" /* offset */		\
+		".popsection\n"				\
+		"671:"
+
+#define LOCK_PREFIX LOCK_PREFIX_HERE "\n\tlock; "
+
 #else /* ! CONFIG_SMP */
 #define LOCK_PREFIX_HERE ""
 #define LOCK_PREFIX ""
diff --git a/arch/x86/include/asm/asm.h b/arch/x86/include/asm/asm.h
index 21b0867..6467757b 100644
--- a/arch/x86/include/asm/asm.h
+++ b/arch/x86/include/asm/asm.h
@@ -120,25 +120,12 @@
 /* Exception table entry */
 #ifdef __ASSEMBLY__
 # define _ASM_EXTABLE_HANDLE(from, to, handler)			\
-	ASM_EXTABLE_HANDLE from to handler
-
-.macro ASM_EXTABLE_HANDLE from:req to:req handler:req
-	.pushsection "__ex_table","a"
-	.balign 4
-	.long (\from) - .
-	.long (\to) - .
-	.long (\handler) - .
+	.pushsection "__ex_table","a" ;				\
+	.balign 4 ;						\
+	.long (from) - . ;					\
+	.long (to) - . ;					\
+	.long (handler) - . ;					\
 	.popsection
-.endm
-#else /* __ASSEMBLY__ */
-
-# define _ASM_EXTABLE_HANDLE(from, to, handler)			\
-	"ASM_EXTABLE_HANDLE from=" #from " to=" #to		\
-	" handler=\"" #handler "\"\n\t"
-
-/* For C file, we already have NOKPROBE_SYMBOL macro */
-
-#endif /* __ASSEMBLY__ */
 
 # define _ASM_EXTABLE(from, to)					\
 	_ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
@@ -161,7 +148,6 @@
 	_ASM_PTR (entry);					\
 	.popsection
 
-#ifdef __ASSEMBLY__
 .macro ALIGN_DESTINATION
 	/* check for bad alignment of destination */
 	movl %edi,%ecx
@@ -185,7 +171,34 @@
 	_ASM_EXTABLE_UA(100b, 103b)
 	_ASM_EXTABLE_UA(101b, 103b)
 	.endm
-#endif /* __ASSEMBLY__ */
+
+#else
+# define _EXPAND_EXTABLE_HANDLE(x) #x
+# define _ASM_EXTABLE_HANDLE(from, to, handler)			\
+	" .pushsection \"__ex_table\",\"a\"\n"			\
+	" .balign 4\n"						\
+	" .long (" #from ") - .\n"				\
+	" .long (" #to ") - .\n"				\
+	" .long (" _EXPAND_EXTABLE_HANDLE(handler) ") - .\n"	\
+	" .popsection\n"
+
+# define _ASM_EXTABLE(from, to)					\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
+
+# define _ASM_EXTABLE_UA(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_uaccess)
+
+# define _ASM_EXTABLE_FAULT(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_fault)
+
+# define _ASM_EXTABLE_EX(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_ext)
+
+# define _ASM_EXTABLE_REFCOUNT(from, to)			\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_refcount)
+
+/* For C file, we already have NOKPROBE_SYMBOL macro */
+#endif
 
 #ifndef __ASSEMBLY__
 /*
diff --git a/arch/x86/include/asm/bug.h b/arch/x86/include/asm/bug.h
index 5090035..6804d66 100644
--- a/arch/x86/include/asm/bug.h
+++ b/arch/x86/include/asm/bug.h
@@ -4,8 +4,6 @@
 
 #include <linux/stringify.h>
 
-#ifndef __ASSEMBLY__
-
 /*
  * Despite that some emulators terminate on UD2, we use it for WARN().
  *
@@ -22,15 +20,53 @@
 
 #define LEN_UD2		2
 
+#ifdef CONFIG_GENERIC_BUG
+
+#ifdef CONFIG_X86_32
+# define __BUG_REL(val)	".long " __stringify(val)
+#else
+# define __BUG_REL(val)	".long " __stringify(val) " - 2b"
+#endif
+
+#ifdef CONFIG_DEBUG_BUGVERBOSE
+
+#define _BUG_FLAGS(ins, flags)						\
+do {									\
+	asm volatile("1:\t" ins "\n"					\
+		     ".pushsection __bug_table,\"aw\"\n"		\
+		     "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"	\
+		     "\t"  __BUG_REL(%c0) "\t# bug_entry::file\n"	\
+		     "\t.word %c1"        "\t# bug_entry::line\n"	\
+		     "\t.word %c2"        "\t# bug_entry::flags\n"	\
+		     "\t.org 2b+%c3\n"					\
+		     ".popsection"					\
+		     : : "i" (__FILE__), "i" (__LINE__),		\
+			 "i" (flags),					\
+			 "i" (sizeof(struct bug_entry)));		\
+} while (0)
+
+#else /* !CONFIG_DEBUG_BUGVERBOSE */
+
 #define _BUG_FLAGS(ins, flags)						\
 do {									\
-	asm volatile("ASM_BUG ins=\"" ins "\" file=%c0 line=%c1 "	\
-		     "flags=%c2 size=%c3"				\
-		     : : "i" (__FILE__), "i" (__LINE__),                \
-			 "i" (flags),                                   \
+	asm volatile("1:\t" ins "\n"					\
+		     ".pushsection __bug_table,\"aw\"\n"		\
+		     "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"	\
+		     "\t.word %c0"        "\t# bug_entry::flags\n"	\
+		     "\t.org 2b+%c1\n"					\
+		     ".popsection"					\
+		     : : "i" (flags),					\
 			 "i" (sizeof(struct bug_entry)));		\
 } while (0)
 
+#endif /* CONFIG_DEBUG_BUGVERBOSE */
+
+#else
+
+#define _BUG_FLAGS(ins, flags)  asm volatile(ins)
+
+#endif /* CONFIG_GENERIC_BUG */
+
 #define HAVE_ARCH_BUG
 #define BUG()							\
 do {								\
@@ -46,54 +82,4 @@ do {								\
 
 #include <asm-generic/bug.h>
 
-#else /* __ASSEMBLY__ */
-
-#ifdef CONFIG_GENERIC_BUG
-
-#ifdef CONFIG_X86_32
-.macro __BUG_REL val:req
-	.long \val
-.endm
-#else
-.macro __BUG_REL val:req
-	.long \val - 2b
-.endm
-#endif
-
-#ifdef CONFIG_DEBUG_BUGVERBOSE
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-1:	\ins
-	.pushsection __bug_table,"aw"
-2:	__BUG_REL val=1b	# bug_entry::bug_addr
-	__BUG_REL val=\file	# bug_entry::file
-	.word \line		# bug_entry::line
-	.word \flags		# bug_entry::flags
-	.org 2b+\size
-	.popsection
-.endm
-
-#else /* !CONFIG_DEBUG_BUGVERBOSE */
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-1:	\ins
-	.pushsection __bug_table,"aw"
-2:	__BUG_REL val=1b	# bug_entry::bug_addr
-	.word \flags		# bug_entry::flags
-	.org 2b+\size
-	.popsection
-.endm
-
-#endif /* CONFIG_DEBUG_BUGVERBOSE */
-
-#else /* CONFIG_GENERIC_BUG */
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-	\ins
-.endm
-
-#endif /* CONFIG_GENERIC_BUG */
-
-#endif /* __ASSEMBLY__ */
-
 #endif /* _ASM_X86_BUG_H */
diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h
index 7d44272..aced6c9 100644
--- a/arch/x86/include/asm/cpufeature.h
+++ b/arch/x86/include/asm/cpufeature.h
@@ -2,10 +2,10 @@
 #ifndef _ASM_X86_CPUFEATURE_H
 #define _ASM_X86_CPUFEATURE_H
 
-#ifdef __KERNEL__
-#ifndef __ASSEMBLY__
-
 #include <asm/processor.h>
+
+#if defined(__KERNEL__) && !defined(__ASSEMBLY__)
+
 #include <asm/asm.h>
 #include <linux/bitops.h>
 
@@ -161,10 +161,37 @@ extern void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int bit);
  */
 static __always_inline __pure bool _static_cpu_has(u16 bit)
 {
-	asm_volatile_goto("STATIC_CPU_HAS bitnum=%[bitnum] "
-			  "cap_byte=\"%[cap_byte]\" "
-			  "feature=%P[feature] t_yes=%l[t_yes] "
-			  "t_no=%l[t_no] always=%P[always]"
+	asm_volatile_goto("1: jmp 6f\n"
+		 "2:\n"
+		 ".skip -(((5f-4f) - (2b-1b)) > 0) * "
+			 "((5f-4f) - (2b-1b)),0x90\n"
+		 "3:\n"
+		 ".section .altinstructions,\"a\"\n"
+		 " .long 1b - .\n"		/* src offset */
+		 " .long 4f - .\n"		/* repl offset */
+		 " .word %P[always]\n"		/* always replace */
+		 " .byte 3b - 1b\n"		/* src len */
+		 " .byte 5f - 4f\n"		/* repl len */
+		 " .byte 3b - 2b\n"		/* pad len */
+		 ".previous\n"
+		 ".section .altinstr_replacement,\"ax\"\n"
+		 "4: jmp %l[t_no]\n"
+		 "5:\n"
+		 ".previous\n"
+		 ".section .altinstructions,\"a\"\n"
+		 " .long 1b - .\n"		/* src offset */
+		 " .long 0\n"			/* no replacement */
+		 " .word %P[feature]\n"		/* feature bit */
+		 " .byte 3b - 1b\n"		/* src len */
+		 " .byte 0\n"			/* repl len */
+		 " .byte 0\n"			/* pad len */
+		 ".previous\n"
+		 ".section .altinstr_aux,\"ax\"\n"
+		 "6:\n"
+		 " testb %[bitnum],%[cap_byte]\n"
+		 " jnz %l[t_yes]\n"
+		 " jmp %l[t_no]\n"
+		 ".previous\n"
 		 : : [feature]  "i" (bit),
 		     [always]   "i" (X86_FEATURE_ALWAYS),
 		     [bitnum]   "i" (1 << (bit & 7)),
@@ -199,44 +226,5 @@ static __always_inline __pure bool _static_cpu_has(u16 bit)
 #define CPU_FEATURE_TYPEVAL		boot_cpu_data.x86_vendor, boot_cpu_data.x86, \
 					boot_cpu_data.x86_model
 
-#else /* __ASSEMBLY__ */
-
-.macro STATIC_CPU_HAS bitnum:req cap_byte:req feature:req t_yes:req t_no:req always:req
-1:
-	jmp 6f
-2:
-	.skip -(((5f-4f) - (2b-1b)) > 0) * ((5f-4f) - (2b-1b)),0x90
-3:
-	.section .altinstructions,"a"
-	.long 1b - .		/* src offset */
-	.long 4f - .		/* repl offset */
-	.word \always		/* always replace */
-	.byte 3b - 1b		/* src len */
-	.byte 5f - 4f		/* repl len */
-	.byte 3b - 2b		/* pad len */
-	.previous
-	.section .altinstr_replacement,"ax"
-4:
-	jmp \t_no
-5:
-	.previous
-	.section .altinstructions,"a"
-	.long 1b - .		/* src offset */
-	.long 0			/* no replacement */
-	.word \feature		/* feature bit */
-	.byte 3b - 1b		/* src len */
-	.byte 0			/* repl len */
-	.byte 0			/* pad len */
-	.previous
-	.section .altinstr_aux,"ax"
-6:
-	testb \bitnum,\cap_byte
-	jnz \t_yes
-	jmp \t_no
-	.previous
-.endm
-
-#endif /* __ASSEMBLY__ */
-
-#endif /* __KERNEL__ */
+#endif /* defined(__KERNEL__) && !defined(__ASSEMBLY__) */
 #endif /* _ASM_X86_CPUFEATURE_H */
diff --git a/arch/x86/include/asm/jump_label.h b/arch/x86/include/asm/jump_label.h
index a5fb34f..cf88ebf 100644
--- a/arch/x86/include/asm/jump_label.h
+++ b/arch/x86/include/asm/jump_label.h
@@ -20,9 +20,15 @@
 
 static __always_inline bool arch_static_branch(struct static_key *key, bool branch)
 {
-	asm_volatile_goto("STATIC_BRANCH_NOP l_yes=\"%l[l_yes]\" key=\"%c0\" "
-			  "branch=\"%c1\""
-			: :  "i" (key), "i" (branch) : : l_yes);
+	asm_volatile_goto("1:"
+		".byte " __stringify(STATIC_KEY_INIT_NOP) "\n\t"
+		".pushsection __jump_table,  \"aw\" \n\t"
+		_ASM_ALIGN "\n\t"
+		".long 1b - ., %l[l_yes] - . \n\t"
+		_ASM_PTR "%c0 + %c1 - .\n\t"
+		".popsection \n\t"
+		: :  "i" (key), "i" (branch) : : l_yes);
+
 	return false;
 l_yes:
 	return true;
@@ -30,8 +36,14 @@ static __always_inline bool arch_static_branch(struct static_key *key, bool bran
 
 static __always_inline bool arch_static_branch_jump(struct static_key *key, bool branch)
 {
-	asm_volatile_goto("STATIC_BRANCH_JMP l_yes=\"%l[l_yes]\" key=\"%c0\" "
-			  "branch=\"%c1\""
+	asm_volatile_goto("1:"
+		".byte 0xe9\n\t .long %l[l_yes] - 2f\n\t"
+		"2:\n\t"
+		".pushsection __jump_table,  \"aw\" \n\t"
+		_ASM_ALIGN "\n\t"
+		".long 1b - ., %l[l_yes] - . \n\t"
+		_ASM_PTR "%c0 + %c1 - .\n\t"
+		".popsection \n\t"
 		: :  "i" (key), "i" (branch) : : l_yes);
 
 	return false;
diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h
index 26942ad..488c596 100644
--- a/arch/x86/include/asm/paravirt_types.h
+++ b/arch/x86/include/asm/paravirt_types.h
@@ -348,11 +348,23 @@ extern struct paravirt_patch_template pv_ops;
 #define paravirt_clobber(clobber)		\
 	[paravirt_clobber] "i" (clobber)
 
+/*
+ * Generate some code, and mark it as patchable by the
+ * apply_paravirt() alternate instruction patcher.
+ */
+#define _paravirt_alt(insn_string, type, clobber)	\
+	"771:\n\t" insn_string "\n" "772:\n"		\
+	".pushsection .parainstructions,\"a\"\n"	\
+	_ASM_ALIGN "\n"					\
+	_ASM_PTR " 771b\n"				\
+	"  .byte " type "\n"				\
+	"  .byte 772b-771b\n"				\
+	"  .short " clobber "\n"			\
+	".popsection\n"
+
 /* Generate patchable code, with the default asm parameters. */
-#define paravirt_call							\
-	"PARAVIRT_CALL type=\"%c[paravirt_typenum]\""			\
-	" clobber=\"%c[paravirt_clobber]\""				\
-	" pv_opptr=\"%c[paravirt_opptr]\";"
+#define paravirt_alt(insn_string)					\
+	_paravirt_alt(insn_string, "%c[paravirt_typenum]", "%c[paravirt_clobber]")
 
 /* Simple instruction patching code. */
 #define NATIVE_LABEL(a,x,b) "\n\t.globl " a #x "_" #b "\n" a #x "_" #b ":\n\t"
@@ -373,6 +385,16 @@ unsigned native_patch(u8 type, void *ibuf, unsigned long addr, unsigned len);
 int paravirt_disable_iospace(void);
 
 /*
+ * This generates an indirect call based on the operation type number.
+ * The type number, computed in PARAVIRT_PATCH, is derived from the
+ * offset into the paravirt_patch_template structure, and can therefore be
+ * freely converted back into a structure offset.
+ */
+#define PARAVIRT_CALL					\
+	ANNOTATE_RETPOLINE_SAFE				\
+	"call *%c[paravirt_opptr];"
+
+/*
  * These macros are intended to wrap calls through one of the paravirt
  * ops structs, so that they can be later identified and patched at
  * runtime.
@@ -509,7 +531,7 @@ int paravirt_disable_iospace(void);
 		/* since this condition will never hold */		\
 		if (sizeof(rettype) > sizeof(unsigned long)) {		\
 			asm volatile(pre				\
-				     paravirt_call			\
+				     paravirt_alt(PARAVIRT_CALL)	\
 				     post				\
 				     : call_clbr, ASM_CALL_CONSTRAINT	\
 				     : paravirt_type(op),		\
@@ -519,7 +541,7 @@ int paravirt_disable_iospace(void);
 			__ret = (rettype)((((u64)__edx) << 32) | __eax); \
 		} else {						\
 			asm volatile(pre				\
-				     paravirt_call			\
+				     paravirt_alt(PARAVIRT_CALL)	\
 				     post				\
 				     : call_clbr, ASM_CALL_CONSTRAINT	\
 				     : paravirt_type(op),		\
@@ -546,7 +568,7 @@ int paravirt_disable_iospace(void);
 		PVOP_VCALL_ARGS;					\
 		PVOP_TEST_NULL(op);					\
 		asm volatile(pre					\
-			     paravirt_call				\
+			     paravirt_alt(PARAVIRT_CALL)		\
 			     post					\
 			     : call_clbr, ASM_CALL_CONSTRAINT		\
 			     : paravirt_type(op),			\
@@ -664,26 +686,6 @@ struct paravirt_patch_site {
 extern struct paravirt_patch_site __parainstructions[],
 	__parainstructions_end[];
 
-#else	/* __ASSEMBLY__ */
-
-/*
- * This generates an indirect call based on the operation type number.
- * The type number, computed in PARAVIRT_PATCH, is derived from the
- * offset into the paravirt_patch_template structure, and can therefore be
- * freely converted back into a structure offset.
- */
-.macro PARAVIRT_CALL type:req clobber:req pv_opptr:req
-771:	ANNOTATE_RETPOLINE_SAFE
-	call *\pv_opptr
-772:	.pushsection .parainstructions,"a"
-	_ASM_ALIGN
-	_ASM_PTR 771b
-	.byte \type
-	.byte 772b-771b
-	.short \clobber
-	.popsection
-.endm
-
 #endif	/* __ASSEMBLY__ */
 
 #endif	/* _ASM_X86_PARAVIRT_TYPES_H */
diff --git a/arch/x86/include/asm/refcount.h b/arch/x86/include/asm/refcount.h
index a8b5e1e..dbaed55 100644
--- a/arch/x86/include/asm/refcount.h
+++ b/arch/x86/include/asm/refcount.h
@@ -4,41 +4,6 @@
  * x86-specific implementation of refcount_t. Based on PAX_REFCOUNT from
  * PaX/grsecurity.
  */
-
-#ifdef __ASSEMBLY__
-
-#include <asm/asm.h>
-#include <asm/bug.h>
-
-.macro REFCOUNT_EXCEPTION counter:req
-	.pushsection .text..refcount
-111:	lea \counter, %_ASM_CX
-112:	ud2
-	ASM_UNREACHABLE
-	.popsection
-113:	_ASM_EXTABLE_REFCOUNT(112b, 113b)
-.endm
-
-/* Trigger refcount exception if refcount result is negative. */
-.macro REFCOUNT_CHECK_LT_ZERO counter:req
-	js 111f
-	REFCOUNT_EXCEPTION counter="\counter"
-.endm
-
-/* Trigger refcount exception if refcount result is zero or negative. */
-.macro REFCOUNT_CHECK_LE_ZERO counter:req
-	jz 111f
-	REFCOUNT_CHECK_LT_ZERO counter="\counter"
-.endm
-
-/* Trigger refcount exception unconditionally. */
-.macro REFCOUNT_ERROR counter:req
-	jmp 111f
-	REFCOUNT_EXCEPTION counter="\counter"
-.endm
-
-#else /* __ASSEMBLY__ */
-
 #include <linux/refcount.h>
 #include <asm/bug.h>
 
@@ -50,12 +15,35 @@
  * central refcount exception. The fixup address for the exception points
  * back to the regular execution flow in .text.
  */
+#define _REFCOUNT_EXCEPTION				\
+	".pushsection .text..refcount\n"		\
+	"111:\tlea %[var], %%" _ASM_CX "\n"		\
+	"112:\t" ASM_UD2 "\n"				\
+	ASM_UNREACHABLE					\
+	".popsection\n"					\
+	"113:\n"					\
+	_ASM_EXTABLE_REFCOUNT(112b, 113b)
+
+/* Trigger refcount exception if refcount result is negative. */
+#define REFCOUNT_CHECK_LT_ZERO				\
+	"js 111f\n\t"					\
+	_REFCOUNT_EXCEPTION
+
+/* Trigger refcount exception if refcount result is zero or negative. */
+#define REFCOUNT_CHECK_LE_ZERO				\
+	"jz 111f\n\t"					\
+	REFCOUNT_CHECK_LT_ZERO
+
+/* Trigger refcount exception unconditionally. */
+#define REFCOUNT_ERROR					\
+	"jmp 111f\n\t"					\
+	_REFCOUNT_EXCEPTION
 
 static __always_inline void refcount_add(unsigned int i, refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "addl %1,%0\n\t"
-		"REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LT_ZERO
+		: [var] "+m" (r->refs.counter)
 		: "ir" (i)
 		: "cc", "cx");
 }
@@ -63,32 +51,31 @@ static __always_inline void refcount_add(unsigned int i, refcount_t *r)
 static __always_inline void refcount_inc(refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "incl %0\n\t"
-		"REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LT_ZERO
+		: [var] "+m" (r->refs.counter)
 		: : "cc", "cx");
 }
 
 static __always_inline void refcount_dec(refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "decl %0\n\t"
-		"REFCOUNT_CHECK_LE_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LE_ZERO
+		: [var] "+m" (r->refs.counter)
 		: : "cc", "cx");
 }
 
 static __always_inline __must_check
 bool refcount_sub_and_test(unsigned int i, refcount_t *r)
 {
-
 	return GEN_BINARY_SUFFIXED_RMWcc(LOCK_PREFIX "subl",
-					 "REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
+					 REFCOUNT_CHECK_LT_ZERO,
 					 r->refs.counter, e, "er", i, "cx");
 }
 
 static __always_inline __must_check bool refcount_dec_and_test(refcount_t *r)
 {
 	return GEN_UNARY_SUFFIXED_RMWcc(LOCK_PREFIX "decl",
-					"REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
+					REFCOUNT_CHECK_LT_ZERO,
 					r->refs.counter, e, "cx");
 }
 
@@ -106,8 +93,8 @@ bool refcount_add_not_zero(unsigned int i, refcount_t *r)
 
 		/* Did we try to increment from/to an undesirable state? */
 		if (unlikely(c < 0 || c == INT_MAX || result < c)) {
-			asm volatile("REFCOUNT_ERROR counter=\"%[counter]\""
-				     : : [counter] "m" (r->refs.counter)
+			asm volatile(REFCOUNT_ERROR
+				     : : [var] "m" (r->refs.counter)
 				     : "cc", "cx");
 			break;
 		}
@@ -122,6 +109,4 @@ static __always_inline __must_check bool refcount_inc_not_zero(refcount_t *r)
 	return refcount_add_not_zero(1, r);
 }
 
-#endif /* __ASSEMBLY__ */
-
 #endif
diff --git a/arch/x86/kernel/macros.S b/arch/x86/kernel/macros.S
deleted file mode 100644
index 161c950..0000000
--- a/arch/x86/kernel/macros.S
+++ /dev/null
@@ -1,16 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-
-/*
- * This file includes headers whose assembly part includes macros which are
- * commonly used. The macros are precompiled into assmebly file which is later
- * assembled together with each compiled file.
- */
-
-#include <linux/compiler.h>
-#include <asm/refcount.h>
-#include <asm/alternative-asm.h>
-#include <asm/bug.h>
-#include <asm/paravirt.h>
-#include <asm/asm.h>
-#include <asm/cpufeature.h>
-#include <asm/jump_label.h>
diff --git a/include/asm-generic/bug.h b/include/asm-generic/bug.h
index cdafa5e..20561a6 100644
--- a/include/asm-generic/bug.h
+++ b/include/asm-generic/bug.h
@@ -17,8 +17,10 @@
 #ifndef __ASSEMBLY__
 #include <linux/kernel.h>
 
-struct bug_entry {
+#ifdef CONFIG_BUG
+
 #ifdef CONFIG_GENERIC_BUG
+struct bug_entry {
 #ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS
 	unsigned long	bug_addr;
 #else
@@ -33,10 +35,8 @@ struct bug_entry {
 	unsigned short	line;
 #endif
 	unsigned short	flags;
-#endif	/* CONFIG_GENERIC_BUG */
 };
-
-#ifdef CONFIG_BUG
+#endif	/* CONFIG_GENERIC_BUG */
 
 /*
  * Don't use BUG() or BUG_ON() unless there's really no way out; one
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 06396c1..fc5004a 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -99,13 +99,22 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
  * unique, to convince GCC not to merge duplicate inline asm statements.
  */
 #define annotate_reachable() ({						\
-	asm volatile("ANNOTATE_REACHABLE counter=%c0"			\
-		     : : "i" (__COUNTER__));				\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.reachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
 #define annotate_unreachable() ({					\
-	asm volatile("ANNOTATE_UNREACHABLE counter=%c0"			\
-		     : : "i" (__COUNTER__));				\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.unreachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
+#define ASM_UNREACHABLE							\
+	"999:\n\t"							\
+	".pushsection .discard.unreachable\n\t"				\
+	".long 999b - .\n\t"						\
+	".popsection\n\t"
 #else
 #define annotate_reachable()
 #define annotate_unreachable()
@@ -293,45 +302,6 @@ static inline void *offset_to_ptr(const int *off)
 	return (void *)((unsigned long)off + *off);
 }
 
-#else /* __ASSEMBLY__ */
-
-#ifdef __KERNEL__
-#ifndef LINKER_SCRIPT
-
-#ifdef CONFIG_STACK_VALIDATION
-.macro ANNOTATE_UNREACHABLE counter:req
-\counter:
-	.pushsection .discard.unreachable
-	.long \counter\()b -.
-	.popsection
-.endm
-
-.macro ANNOTATE_REACHABLE counter:req
-\counter:
-	.pushsection .discard.reachable
-	.long \counter\()b -.
-	.popsection
-.endm
-
-.macro ASM_UNREACHABLE
-999:
-	.pushsection .discard.unreachable
-	.long 999b - .
-	.popsection
-.endm
-#else /* CONFIG_STACK_VALIDATION */
-.macro ANNOTATE_UNREACHABLE counter:req
-.endm
-
-.macro ANNOTATE_REACHABLE counter:req
-.endm
-
-.macro ASM_UNREACHABLE
-.endm
-#endif /* CONFIG_STACK_VALIDATION */
-
-#endif /* LINKER_SCRIPT */
-#endif /* __KERNEL__ */
 #endif /* __ASSEMBLY__ */
 
 /* Compile time object size, -1 for unknown */
diff --git a/scripts/Kbuild.include b/scripts/Kbuild.include
index bb01555..3d09844 100644
--- a/scripts/Kbuild.include
+++ b/scripts/Kbuild.include
@@ -115,9 +115,7 @@ __cc-option = $(call try-run,\
 
 # Do not attempt to build with gcc plugins during cc-option tests.
 # (And this uses delayed resolution so the flags will be up to date.)
-# In addition, do not include the asm macros which are built later.
-CC_OPTION_FILTERED = $(GCC_PLUGINS_CFLAGS) $(ASM_MACRO_FLAGS)
-CC_OPTION_CFLAGS = $(filter-out $(CC_OPTION_FILTERED),$(KBUILD_CFLAGS))
+CC_OPTION_CFLAGS = $(filter-out $(GCC_PLUGINS_CFLAGS),$(KBUILD_CFLAGS))
 
 # cc-option
 # Usage: cflags-y += $(call cc-option,-march=winchip-c6,-march=i586)
diff --git a/scripts/mod/Makefile b/scripts/mod/Makefile
index a5b4af4..42c5d50 100644
--- a/scripts/mod/Makefile
+++ b/scripts/mod/Makefile
@@ -4,8 +4,6 @@ OBJECT_FILES_NON_STANDARD := y
 hostprogs-y	:= modpost mk_elfconfig
 always		:= $(hostprogs-y) empty.o
 
-CFLAGS_REMOVE_empty.o := $(ASM_MACRO_FLAGS)
-
 modpost-objs	:= modpost.o file2alias.o sumversion.o
 
 devicetable-offsets-file := devicetable-offsets.h
-- 
2.7.4

_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Sun, 16 Dec 2018 02:50:32 +0000
Message-ID: <1544928632-9717-1-git-send-email-yamada.masahiro () socionext ! com>
--------------------
Revert the following 9 commits:

[1] 5bdcd510c2ac ("x86/jump-labels: Macrofy inline assembly code to
    work around GCC inlining bugs")

    This was partially reverted because it made good cleanups
    irrespective of the inlining issue; the error message is still
    unneeded, and the conversion to STATIC_BRANCH_{NOP,JUMP} should
    be kept.

[2] d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to
    work around GCC inlining bugs")

[3] 0474d5d9d2f7 ("x86/extable: Macrofy inline assembly code to work
    around GCC inlining bugs")

[4] 494b5168f2de ("x86/paravirt: Work around GCC inlining bugs when
    compiling paravirt ops")

[5] f81f8ad56fd1 ("x86/bug: Macrofy the BUG table section handling,
    to work around GCC inlining bugs")

[6] 77f48ec28e4c ("x86/alternatives: Macrofy lock prefixes to work
   around GCC inlining bugs")

[7] 9e1725b41059 ("x86/refcount: Work around GCC inlining bug")

    Resolved conflicts in arch/x86/include/asm/refcount.h caused by
    288e4521f0f6 ("x86/asm: 'Simplify' GEN_*_RMWcc() macros").

[8] c06c4d809051 ("x86/objtool: Use asm macros to work around GCC
    inlining bugs")

[9] 77b0bf55bc67 ("kbuild/Makefile: Prepare for using macros in inline
    assembly code to work around asm() related GCC inlining bugs")

A few days after those commits applied, discussion started to solve
the issue more elegantly with the help of compiler:

  https://lkml.org/lkml/2018/10/7/92

The new syntax "asm inline" was implemented by Segher Boessenkool, and
now queued up for GCC 9. (People were positive even for back-porting it
to older compilers).

Since the in-kernel workarounds merged, some issues have been reported:
breakage of building with distcc/icecc, breakage of distro packages for
module building. (More fundamentally, we cannot build external modules
after 'make clean'.)

I do not want to mess up the build system any more.

Given that this issue will be solved in a cleaner way sooner or later,
let's revert the in-kernel workarounds, and wait for GCC 9.

Reported-by: Logan Gunthorpe <logang@deltatee.com> # distcc
Reported-by: Sedat Dilek <sedat.dilek@gmail.com> # deb/rpm package
Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
Cc: Nadav Amit <namit@vmware.com>
Cc: Segher Boessenkool <segher@kernel.crashing.org>
---

Please consider this for v4.20 release.
Currently, distro package build is broken.


Changes in v2:
  - Revive clean-ups made by 5bdcd510c2ac (per Peter Zijlstra)
  - Fix commit quoting style (per Peter Zijlstra)

 Makefile                               |  9 +---
 arch/x86/Makefile                      |  7 ---
 arch/x86/include/asm/alternative-asm.h | 20 +++----
 arch/x86/include/asm/alternative.h     | 11 +++-
 arch/x86/include/asm/asm.h             | 53 +++++++++++-------
 arch/x86/include/asm/bug.h             | 98 +++++++++++++++-------------------
 arch/x86/include/asm/cpufeature.h      | 82 ++++++++++++----------------
 arch/x86/include/asm/jump_label.h      | 22 ++++++--
 arch/x86/include/asm/paravirt_types.h  | 56 +++++++++----------
 arch/x86/include/asm/refcount.h        | 81 ++++++++++++----------------
 arch/x86/kernel/macros.S               | 16 ------
 include/asm-generic/bug.h              |  8 +--
 include/linux/compiler.h               | 56 +++++--------------
 scripts/Kbuild.include                 |  4 +-
 scripts/mod/Makefile                   |  2 -
 15 files changed, 224 insertions(+), 301 deletions(-)
 delete mode 100644 arch/x86/kernel/macros.S

diff --git a/Makefile b/Makefile
index f2c3423..4cf4c5b 100644
--- a/Makefile
+++ b/Makefile
@@ -1081,7 +1081,7 @@ scripts: scripts_basic scripts_dtc asm-generic gcc-plugins $(autoksyms_h)
 # version.h and scripts_basic is processed / created.
 
 # Listed in dependency order
-PHONY += prepare archprepare macroprepare prepare0 prepare1 prepare2 prepare3
+PHONY += prepare archprepare prepare0 prepare1 prepare2 prepare3
 
 # prepare3 is used to check if we are building in a separate output directory,
 # and if so do:
@@ -1104,9 +1104,7 @@ prepare2: prepare3 outputmakefile asm-generic
 prepare1: prepare2 $(version_h) $(autoksyms_h) include/generated/utsrelease.h
 	$(cmd_crmodverdir)
 
-macroprepare: prepare1 archmacros
-
-archprepare: archheaders archscripts macroprepare scripts_basic
+archprepare: archheaders archscripts prepare1 scripts_basic
 
 prepare0: archprepare gcc-plugins
 	$(Q)$(MAKE) $(build)=.
@@ -1174,9 +1172,6 @@ archheaders:
 PHONY += archscripts
 archscripts:
 
-PHONY += archmacros
-archmacros:
-
 PHONY += __headers
 __headers: $(version_h) scripts_basic uapi-asm-generic archheaders archscripts
 	$(Q)$(MAKE) $(build)=scripts build_unifdef
diff --git a/arch/x86/Makefile b/arch/x86/Makefile
index 75ef499..85a66c4 100644
--- a/arch/x86/Makefile
+++ b/arch/x86/Makefile
@@ -232,13 +232,6 @@ archscripts: scripts_basic
 archheaders:
 	$(Q)$(MAKE) $(build)=arch/x86/entry/syscalls all
 
-archmacros:
-	$(Q)$(MAKE) $(build)=arch/x86/kernel arch/x86/kernel/macros.s
-
-ASM_MACRO_FLAGS = -Wa,arch/x86/kernel/macros.s
-export ASM_MACRO_FLAGS
-KBUILD_CFLAGS += $(ASM_MACRO_FLAGS)
-
 ###
 # Kernel objects
 
diff --git a/arch/x86/include/asm/alternative-asm.h b/arch/x86/include/asm/alternative-asm.h
index 8e4ea39..31b627b 100644
--- a/arch/x86/include/asm/alternative-asm.h
+++ b/arch/x86/include/asm/alternative-asm.h
@@ -7,24 +7,16 @@
 #include <asm/asm.h>
 
 #ifdef CONFIG_SMP
-.macro LOCK_PREFIX_HERE
+	.macro LOCK_PREFIX
+672:	lock
 	.pushsection .smp_locks,"a"
 	.balign 4
-	.long 671f - .		# offset
+	.long 672b - .
 	.popsection
-671:
-.endm
-
-.macro LOCK_PREFIX insn:vararg
-	LOCK_PREFIX_HERE
-	lock \insn
-.endm
+	.endm
 #else
-.macro LOCK_PREFIX_HERE
-.endm
-
-.macro LOCK_PREFIX insn:vararg
-.endm
+	.macro LOCK_PREFIX
+	.endm
 #endif
 
 /*
diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h
index d7faa16..4cd6a3b 100644
--- a/arch/x86/include/asm/alternative.h
+++ b/arch/x86/include/asm/alternative.h
@@ -31,8 +31,15 @@
  */
 
 #ifdef CONFIG_SMP
-#define LOCK_PREFIX_HERE "LOCK_PREFIX_HERE\n\t"
-#define LOCK_PREFIX "LOCK_PREFIX "
+#define LOCK_PREFIX_HERE \
+		".pushsection .smp_locks,\"a\"\n"	\
+		".balign 4\n"				\
+		".long 671f - .\n" /* offset */		\
+		".popsection\n"				\
+		"671:"
+
+#define LOCK_PREFIX LOCK_PREFIX_HERE "\n\tlock; "
+
 #else /* ! CONFIG_SMP */
 #define LOCK_PREFIX_HERE ""
 #define LOCK_PREFIX ""
diff --git a/arch/x86/include/asm/asm.h b/arch/x86/include/asm/asm.h
index 21b0867..6467757b 100644
--- a/arch/x86/include/asm/asm.h
+++ b/arch/x86/include/asm/asm.h
@@ -120,25 +120,12 @@
 /* Exception table entry */
 #ifdef __ASSEMBLY__
 # define _ASM_EXTABLE_HANDLE(from, to, handler)			\
-	ASM_EXTABLE_HANDLE from to handler
-
-.macro ASM_EXTABLE_HANDLE from:req to:req handler:req
-	.pushsection "__ex_table","a"
-	.balign 4
-	.long (\from) - .
-	.long (\to) - .
-	.long (\handler) - .
+	.pushsection "__ex_table","a" ;				\
+	.balign 4 ;						\
+	.long (from) - . ;					\
+	.long (to) - . ;					\
+	.long (handler) - . ;					\
 	.popsection
-.endm
-#else /* __ASSEMBLY__ */
-
-# define _ASM_EXTABLE_HANDLE(from, to, handler)			\
-	"ASM_EXTABLE_HANDLE from=" #from " to=" #to		\
-	" handler=\"" #handler "\"\n\t"
-
-/* For C file, we already have NOKPROBE_SYMBOL macro */
-
-#endif /* __ASSEMBLY__ */
 
 # define _ASM_EXTABLE(from, to)					\
 	_ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
@@ -161,7 +148,6 @@
 	_ASM_PTR (entry);					\
 	.popsection
 
-#ifdef __ASSEMBLY__
 .macro ALIGN_DESTINATION
 	/* check for bad alignment of destination */
 	movl %edi,%ecx
@@ -185,7 +171,34 @@
 	_ASM_EXTABLE_UA(100b, 103b)
 	_ASM_EXTABLE_UA(101b, 103b)
 	.endm
-#endif /* __ASSEMBLY__ */
+
+#else
+# define _EXPAND_EXTABLE_HANDLE(x) #x
+# define _ASM_EXTABLE_HANDLE(from, to, handler)			\
+	" .pushsection \"__ex_table\",\"a\"\n"			\
+	" .balign 4\n"						\
+	" .long (" #from ") - .\n"				\
+	" .long (" #to ") - .\n"				\
+	" .long (" _EXPAND_EXTABLE_HANDLE(handler) ") - .\n"	\
+	" .popsection\n"
+
+# define _ASM_EXTABLE(from, to)					\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
+
+# define _ASM_EXTABLE_UA(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_uaccess)
+
+# define _ASM_EXTABLE_FAULT(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_fault)
+
+# define _ASM_EXTABLE_EX(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_ext)
+
+# define _ASM_EXTABLE_REFCOUNT(from, to)			\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_refcount)
+
+/* For C file, we already have NOKPROBE_SYMBOL macro */
+#endif
 
 #ifndef __ASSEMBLY__
 /*
diff --git a/arch/x86/include/asm/bug.h b/arch/x86/include/asm/bug.h
index 5090035..6804d66 100644
--- a/arch/x86/include/asm/bug.h
+++ b/arch/x86/include/asm/bug.h
@@ -4,8 +4,6 @@
 
 #include <linux/stringify.h>
 
-#ifndef __ASSEMBLY__
-
 /*
  * Despite that some emulators terminate on UD2, we use it for WARN().
  *
@@ -22,15 +20,53 @@
 
 #define LEN_UD2		2
 
+#ifdef CONFIG_GENERIC_BUG
+
+#ifdef CONFIG_X86_32
+# define __BUG_REL(val)	".long " __stringify(val)
+#else
+# define __BUG_REL(val)	".long " __stringify(val) " - 2b"
+#endif
+
+#ifdef CONFIG_DEBUG_BUGVERBOSE
+
+#define _BUG_FLAGS(ins, flags)						\
+do {									\
+	asm volatile("1:\t" ins "\n"					\
+		     ".pushsection __bug_table,\"aw\"\n"		\
+		     "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"	\
+		     "\t"  __BUG_REL(%c0) "\t# bug_entry::file\n"	\
+		     "\t.word %c1"        "\t# bug_entry::line\n"	\
+		     "\t.word %c2"        "\t# bug_entry::flags\n"	\
+		     "\t.org 2b+%c3\n"					\
+		     ".popsection"					\
+		     : : "i" (__FILE__), "i" (__LINE__),		\
+			 "i" (flags),					\
+			 "i" (sizeof(struct bug_entry)));		\
+} while (0)
+
+#else /* !CONFIG_DEBUG_BUGVERBOSE */
+
 #define _BUG_FLAGS(ins, flags)						\
 do {									\
-	asm volatile("ASM_BUG ins=\"" ins "\" file=%c0 line=%c1 "	\
-		     "flags=%c2 size=%c3"				\
-		     : : "i" (__FILE__), "i" (__LINE__),                \
-			 "i" (flags),                                   \
+	asm volatile("1:\t" ins "\n"					\
+		     ".pushsection __bug_table,\"aw\"\n"		\
+		     "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"	\
+		     "\t.word %c0"        "\t# bug_entry::flags\n"	\
+		     "\t.org 2b+%c1\n"					\
+		     ".popsection"					\
+		     : : "i" (flags),					\
 			 "i" (sizeof(struct bug_entry)));		\
 } while (0)
 
+#endif /* CONFIG_DEBUG_BUGVERBOSE */
+
+#else
+
+#define _BUG_FLAGS(ins, flags)  asm volatile(ins)
+
+#endif /* CONFIG_GENERIC_BUG */
+
 #define HAVE_ARCH_BUG
 #define BUG()							\
 do {								\
@@ -46,54 +82,4 @@ do {								\
 
 #include <asm-generic/bug.h>
 
-#else /* __ASSEMBLY__ */
-
-#ifdef CONFIG_GENERIC_BUG
-
-#ifdef CONFIG_X86_32
-.macro __BUG_REL val:req
-	.long \val
-.endm
-#else
-.macro __BUG_REL val:req
-	.long \val - 2b
-.endm
-#endif
-
-#ifdef CONFIG_DEBUG_BUGVERBOSE
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-1:	\ins
-	.pushsection __bug_table,"aw"
-2:	__BUG_REL val=1b	# bug_entry::bug_addr
-	__BUG_REL val=\file	# bug_entry::file
-	.word \line		# bug_entry::line
-	.word \flags		# bug_entry::flags
-	.org 2b+\size
-	.popsection
-.endm
-
-#else /* !CONFIG_DEBUG_BUGVERBOSE */
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-1:	\ins
-	.pushsection __bug_table,"aw"
-2:	__BUG_REL val=1b	# bug_entry::bug_addr
-	.word \flags		# bug_entry::flags
-	.org 2b+\size
-	.popsection
-.endm
-
-#endif /* CONFIG_DEBUG_BUGVERBOSE */
-
-#else /* CONFIG_GENERIC_BUG */
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-	\ins
-.endm
-
-#endif /* CONFIG_GENERIC_BUG */
-
-#endif /* __ASSEMBLY__ */
-
 #endif /* _ASM_X86_BUG_H */
diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h
index 7d44272..aced6c9 100644
--- a/arch/x86/include/asm/cpufeature.h
+++ b/arch/x86/include/asm/cpufeature.h
@@ -2,10 +2,10 @@
 #ifndef _ASM_X86_CPUFEATURE_H
 #define _ASM_X86_CPUFEATURE_H
 
-#ifdef __KERNEL__
-#ifndef __ASSEMBLY__
-
 #include <asm/processor.h>
+
+#if defined(__KERNEL__) && !defined(__ASSEMBLY__)
+
 #include <asm/asm.h>
 #include <linux/bitops.h>
 
@@ -161,10 +161,37 @@ extern void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int bit);
  */
 static __always_inline __pure bool _static_cpu_has(u16 bit)
 {
-	asm_volatile_goto("STATIC_CPU_HAS bitnum=%[bitnum] "
-			  "cap_byte=\"%[cap_byte]\" "
-			  "feature=%P[feature] t_yes=%l[t_yes] "
-			  "t_no=%l[t_no] always=%P[always]"
+	asm_volatile_goto("1: jmp 6f\n"
+		 "2:\n"
+		 ".skip -(((5f-4f) - (2b-1b)) > 0) * "
+			 "((5f-4f) - (2b-1b)),0x90\n"
+		 "3:\n"
+		 ".section .altinstructions,\"a\"\n"
+		 " .long 1b - .\n"		/* src offset */
+		 " .long 4f - .\n"		/* repl offset */
+		 " .word %P[always]\n"		/* always replace */
+		 " .byte 3b - 1b\n"		/* src len */
+		 " .byte 5f - 4f\n"		/* repl len */
+		 " .byte 3b - 2b\n"		/* pad len */
+		 ".previous\n"
+		 ".section .altinstr_replacement,\"ax\"\n"
+		 "4: jmp %l[t_no]\n"
+		 "5:\n"
+		 ".previous\n"
+		 ".section .altinstructions,\"a\"\n"
+		 " .long 1b - .\n"		/* src offset */
+		 " .long 0\n"			/* no replacement */
+		 " .word %P[feature]\n"		/* feature bit */
+		 " .byte 3b - 1b\n"		/* src len */
+		 " .byte 0\n"			/* repl len */
+		 " .byte 0\n"			/* pad len */
+		 ".previous\n"
+		 ".section .altinstr_aux,\"ax\"\n"
+		 "6:\n"
+		 " testb %[bitnum],%[cap_byte]\n"
+		 " jnz %l[t_yes]\n"
+		 " jmp %l[t_no]\n"
+		 ".previous\n"
 		 : : [feature]  "i" (bit),
 		     [always]   "i" (X86_FEATURE_ALWAYS),
 		     [bitnum]   "i" (1 << (bit & 7)),
@@ -199,44 +226,5 @@ static __always_inline __pure bool _static_cpu_has(u16 bit)
 #define CPU_FEATURE_TYPEVAL		boot_cpu_data.x86_vendor, boot_cpu_data.x86, \
 					boot_cpu_data.x86_model
 
-#else /* __ASSEMBLY__ */
-
-.macro STATIC_CPU_HAS bitnum:req cap_byte:req feature:req t_yes:req t_no:req always:req
-1:
-	jmp 6f
-2:
-	.skip -(((5f-4f) - (2b-1b)) > 0) * ((5f-4f) - (2b-1b)),0x90
-3:
-	.section .altinstructions,"a"
-	.long 1b - .		/* src offset */
-	.long 4f - .		/* repl offset */
-	.word \always		/* always replace */
-	.byte 3b - 1b		/* src len */
-	.byte 5f - 4f		/* repl len */
-	.byte 3b - 2b		/* pad len */
-	.previous
-	.section .altinstr_replacement,"ax"
-4:
-	jmp \t_no
-5:
-	.previous
-	.section .altinstructions,"a"
-	.long 1b - .		/* src offset */
-	.long 0			/* no replacement */
-	.word \feature		/* feature bit */
-	.byte 3b - 1b		/* src len */
-	.byte 0			/* repl len */
-	.byte 0			/* pad len */
-	.previous
-	.section .altinstr_aux,"ax"
-6:
-	testb \bitnum,\cap_byte
-	jnz \t_yes
-	jmp \t_no
-	.previous
-.endm
-
-#endif /* __ASSEMBLY__ */
-
-#endif /* __KERNEL__ */
+#endif /* defined(__KERNEL__) && !defined(__ASSEMBLY__) */
 #endif /* _ASM_X86_CPUFEATURE_H */
diff --git a/arch/x86/include/asm/jump_label.h b/arch/x86/include/asm/jump_label.h
index a5fb34f..cf88ebf 100644
--- a/arch/x86/include/asm/jump_label.h
+++ b/arch/x86/include/asm/jump_label.h
@@ -20,9 +20,15 @@
 
 static __always_inline bool arch_static_branch(struct static_key *key, bool branch)
 {
-	asm_volatile_goto("STATIC_BRANCH_NOP l_yes=\"%l[l_yes]\" key=\"%c0\" "
-			  "branch=\"%c1\""
-			: :  "i" (key), "i" (branch) : : l_yes);
+	asm_volatile_goto("1:"
+		".byte " __stringify(STATIC_KEY_INIT_NOP) "\n\t"
+		".pushsection __jump_table,  \"aw\" \n\t"
+		_ASM_ALIGN "\n\t"
+		".long 1b - ., %l[l_yes] - . \n\t"
+		_ASM_PTR "%c0 + %c1 - .\n\t"
+		".popsection \n\t"
+		: :  "i" (key), "i" (branch) : : l_yes);
+
 	return false;
 l_yes:
 	return true;
@@ -30,8 +36,14 @@ static __always_inline bool arch_static_branch(struct static_key *key, bool bran
 
 static __always_inline bool arch_static_branch_jump(struct static_key *key, bool branch)
 {
-	asm_volatile_goto("STATIC_BRANCH_JMP l_yes=\"%l[l_yes]\" key=\"%c0\" "
-			  "branch=\"%c1\""
+	asm_volatile_goto("1:"
+		".byte 0xe9\n\t .long %l[l_yes] - 2f\n\t"
+		"2:\n\t"
+		".pushsection __jump_table,  \"aw\" \n\t"
+		_ASM_ALIGN "\n\t"
+		".long 1b - ., %l[l_yes] - . \n\t"
+		_ASM_PTR "%c0 + %c1 - .\n\t"
+		".popsection \n\t"
 		: :  "i" (key), "i" (branch) : : l_yes);
 
 	return false;
diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h
index 26942ad..488c596 100644
--- a/arch/x86/include/asm/paravirt_types.h
+++ b/arch/x86/include/asm/paravirt_types.h
@@ -348,11 +348,23 @@ extern struct paravirt_patch_template pv_ops;
 #define paravirt_clobber(clobber)		\
 	[paravirt_clobber] "i" (clobber)
 
+/*
+ * Generate some code, and mark it as patchable by the
+ * apply_paravirt() alternate instruction patcher.
+ */
+#define _paravirt_alt(insn_string, type, clobber)	\
+	"771:\n\t" insn_string "\n" "772:\n"		\
+	".pushsection .parainstructions,\"a\"\n"	\
+	_ASM_ALIGN "\n"					\
+	_ASM_PTR " 771b\n"				\
+	"  .byte " type "\n"				\
+	"  .byte 772b-771b\n"				\
+	"  .short " clobber "\n"			\
+	".popsection\n"
+
 /* Generate patchable code, with the default asm parameters. */
-#define paravirt_call							\
-	"PARAVIRT_CALL type=\"%c[paravirt_typenum]\""			\
-	" clobber=\"%c[paravirt_clobber]\""				\
-	" pv_opptr=\"%c[paravirt_opptr]\";"
+#define paravirt_alt(insn_string)					\
+	_paravirt_alt(insn_string, "%c[paravirt_typenum]", "%c[paravirt_clobber]")
 
 /* Simple instruction patching code. */
 #define NATIVE_LABEL(a,x,b) "\n\t.globl " a #x "_" #b "\n" a #x "_" #b ":\n\t"
@@ -373,6 +385,16 @@ unsigned native_patch(u8 type, void *ibuf, unsigned long addr, unsigned len);
 int paravirt_disable_iospace(void);
 
 /*
+ * This generates an indirect call based on the operation type number.
+ * The type number, computed in PARAVIRT_PATCH, is derived from the
+ * offset into the paravirt_patch_template structure, and can therefore be
+ * freely converted back into a structure offset.
+ */
+#define PARAVIRT_CALL					\
+	ANNOTATE_RETPOLINE_SAFE				\
+	"call *%c[paravirt_opptr];"
+
+/*
  * These macros are intended to wrap calls through one of the paravirt
  * ops structs, so that they can be later identified and patched at
  * runtime.
@@ -509,7 +531,7 @@ int paravirt_disable_iospace(void);
 		/* since this condition will never hold */		\
 		if (sizeof(rettype) > sizeof(unsigned long)) {		\
 			asm volatile(pre				\
-				     paravirt_call			\
+				     paravirt_alt(PARAVIRT_CALL)	\
 				     post				\
 				     : call_clbr, ASM_CALL_CONSTRAINT	\
 				     : paravirt_type(op),		\
@@ -519,7 +541,7 @@ int paravirt_disable_iospace(void);
 			__ret = (rettype)((((u64)__edx) << 32) | __eax); \
 		} else {						\
 			asm volatile(pre				\
-				     paravirt_call			\
+				     paravirt_alt(PARAVIRT_CALL)	\
 				     post				\
 				     : call_clbr, ASM_CALL_CONSTRAINT	\
 				     : paravirt_type(op),		\
@@ -546,7 +568,7 @@ int paravirt_disable_iospace(void);
 		PVOP_VCALL_ARGS;					\
 		PVOP_TEST_NULL(op);					\
 		asm volatile(pre					\
-			     paravirt_call				\
+			     paravirt_alt(PARAVIRT_CALL)		\
 			     post					\
 			     : call_clbr, ASM_CALL_CONSTRAINT		\
 			     : paravirt_type(op),			\
@@ -664,26 +686,6 @@ struct paravirt_patch_site {
 extern struct paravirt_patch_site __parainstructions[],
 	__parainstructions_end[];
 
-#else	/* __ASSEMBLY__ */
-
-/*
- * This generates an indirect call based on the operation type number.
- * The type number, computed in PARAVIRT_PATCH, is derived from the
- * offset into the paravirt_patch_template structure, and can therefore be
- * freely converted back into a structure offset.
- */
-.macro PARAVIRT_CALL type:req clobber:req pv_opptr:req
-771:	ANNOTATE_RETPOLINE_SAFE
-	call *\pv_opptr
-772:	.pushsection .parainstructions,"a"
-	_ASM_ALIGN
-	_ASM_PTR 771b
-	.byte \type
-	.byte 772b-771b
-	.short \clobber
-	.popsection
-.endm
-
 #endif	/* __ASSEMBLY__ */
 
 #endif	/* _ASM_X86_PARAVIRT_TYPES_H */
diff --git a/arch/x86/include/asm/refcount.h b/arch/x86/include/asm/refcount.h
index a8b5e1e..dbaed55 100644
--- a/arch/x86/include/asm/refcount.h
+++ b/arch/x86/include/asm/refcount.h
@@ -4,41 +4,6 @@
  * x86-specific implementation of refcount_t. Based on PAX_REFCOUNT from
  * PaX/grsecurity.
  */
-
-#ifdef __ASSEMBLY__
-
-#include <asm/asm.h>
-#include <asm/bug.h>
-
-.macro REFCOUNT_EXCEPTION counter:req
-	.pushsection .text..refcount
-111:	lea \counter, %_ASM_CX
-112:	ud2
-	ASM_UNREACHABLE
-	.popsection
-113:	_ASM_EXTABLE_REFCOUNT(112b, 113b)
-.endm
-
-/* Trigger refcount exception if refcount result is negative. */
-.macro REFCOUNT_CHECK_LT_ZERO counter:req
-	js 111f
-	REFCOUNT_EXCEPTION counter="\counter"
-.endm
-
-/* Trigger refcount exception if refcount result is zero or negative. */
-.macro REFCOUNT_CHECK_LE_ZERO counter:req
-	jz 111f
-	REFCOUNT_CHECK_LT_ZERO counter="\counter"
-.endm
-
-/* Trigger refcount exception unconditionally. */
-.macro REFCOUNT_ERROR counter:req
-	jmp 111f
-	REFCOUNT_EXCEPTION counter="\counter"
-.endm
-
-#else /* __ASSEMBLY__ */
-
 #include <linux/refcount.h>
 #include <asm/bug.h>
 
@@ -50,12 +15,35 @@
  * central refcount exception. The fixup address for the exception points
  * back to the regular execution flow in .text.
  */
+#define _REFCOUNT_EXCEPTION				\
+	".pushsection .text..refcount\n"		\
+	"111:\tlea %[var], %%" _ASM_CX "\n"		\
+	"112:\t" ASM_UD2 "\n"				\
+	ASM_UNREACHABLE					\
+	".popsection\n"					\
+	"113:\n"					\
+	_ASM_EXTABLE_REFCOUNT(112b, 113b)
+
+/* Trigger refcount exception if refcount result is negative. */
+#define REFCOUNT_CHECK_LT_ZERO				\
+	"js 111f\n\t"					\
+	_REFCOUNT_EXCEPTION
+
+/* Trigger refcount exception if refcount result is zero or negative. */
+#define REFCOUNT_CHECK_LE_ZERO				\
+	"jz 111f\n\t"					\
+	REFCOUNT_CHECK_LT_ZERO
+
+/* Trigger refcount exception unconditionally. */
+#define REFCOUNT_ERROR					\
+	"jmp 111f\n\t"					\
+	_REFCOUNT_EXCEPTION
 
 static __always_inline void refcount_add(unsigned int i, refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "addl %1,%0\n\t"
-		"REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LT_ZERO
+		: [var] "+m" (r->refs.counter)
 		: "ir" (i)
 		: "cc", "cx");
 }
@@ -63,32 +51,31 @@ static __always_inline void refcount_add(unsigned int i, refcount_t *r)
 static __always_inline void refcount_inc(refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "incl %0\n\t"
-		"REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LT_ZERO
+		: [var] "+m" (r->refs.counter)
 		: : "cc", "cx");
 }
 
 static __always_inline void refcount_dec(refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "decl %0\n\t"
-		"REFCOUNT_CHECK_LE_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LE_ZERO
+		: [var] "+m" (r->refs.counter)
 		: : "cc", "cx");
 }
 
 static __always_inline __must_check
 bool refcount_sub_and_test(unsigned int i, refcount_t *r)
 {
-
 	return GEN_BINARY_SUFFIXED_RMWcc(LOCK_PREFIX "subl",
-					 "REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
+					 REFCOUNT_CHECK_LT_ZERO,
 					 r->refs.counter, e, "er", i, "cx");
 }
 
 static __always_inline __must_check bool refcount_dec_and_test(refcount_t *r)
 {
 	return GEN_UNARY_SUFFIXED_RMWcc(LOCK_PREFIX "decl",
-					"REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
+					REFCOUNT_CHECK_LT_ZERO,
 					r->refs.counter, e, "cx");
 }
 
@@ -106,8 +93,8 @@ bool refcount_add_not_zero(unsigned int i, refcount_t *r)
 
 		/* Did we try to increment from/to an undesirable state? */
 		if (unlikely(c < 0 || c == INT_MAX || result < c)) {
-			asm volatile("REFCOUNT_ERROR counter=\"%[counter]\""
-				     : : [counter] "m" (r->refs.counter)
+			asm volatile(REFCOUNT_ERROR
+				     : : [var] "m" (r->refs.counter)
 				     : "cc", "cx");
 			break;
 		}
@@ -122,6 +109,4 @@ static __always_inline __must_check bool refcount_inc_not_zero(refcount_t *r)
 	return refcount_add_not_zero(1, r);
 }
 
-#endif /* __ASSEMBLY__ */
-
 #endif
diff --git a/arch/x86/kernel/macros.S b/arch/x86/kernel/macros.S
deleted file mode 100644
index 161c950..0000000
--- a/arch/x86/kernel/macros.S
+++ /dev/null
@@ -1,16 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-
-/*
- * This file includes headers whose assembly part includes macros which are
- * commonly used. The macros are precompiled into assmebly file which is later
- * assembled together with each compiled file.
- */
-
-#include <linux/compiler.h>
-#include <asm/refcount.h>
-#include <asm/alternative-asm.h>
-#include <asm/bug.h>
-#include <asm/paravirt.h>
-#include <asm/asm.h>
-#include <asm/cpufeature.h>
-#include <asm/jump_label.h>
diff --git a/include/asm-generic/bug.h b/include/asm-generic/bug.h
index cdafa5e..20561a6 100644
--- a/include/asm-generic/bug.h
+++ b/include/asm-generic/bug.h
@@ -17,8 +17,10 @@
 #ifndef __ASSEMBLY__
 #include <linux/kernel.h>
 
-struct bug_entry {
+#ifdef CONFIG_BUG
+
 #ifdef CONFIG_GENERIC_BUG
+struct bug_entry {
 #ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS
 	unsigned long	bug_addr;
 #else
@@ -33,10 +35,8 @@ struct bug_entry {
 	unsigned short	line;
 #endif
 	unsigned short	flags;
-#endif	/* CONFIG_GENERIC_BUG */
 };
-
-#ifdef CONFIG_BUG
+#endif	/* CONFIG_GENERIC_BUG */
 
 /*
  * Don't use BUG() or BUG_ON() unless there's really no way out; one
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 06396c1..fc5004a 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -99,13 +99,22 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
  * unique, to convince GCC not to merge duplicate inline asm statements.
  */
 #define annotate_reachable() ({						\
-	asm volatile("ANNOTATE_REACHABLE counter=%c0"			\
-		     : : "i" (__COUNTER__));				\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.reachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
 #define annotate_unreachable() ({					\
-	asm volatile("ANNOTATE_UNREACHABLE counter=%c0"			\
-		     : : "i" (__COUNTER__));				\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.unreachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
+#define ASM_UNREACHABLE							\
+	"999:\n\t"							\
+	".pushsection .discard.unreachable\n\t"				\
+	".long 999b - .\n\t"						\
+	".popsection\n\t"
 #else
 #define annotate_reachable()
 #define annotate_unreachable()
@@ -293,45 +302,6 @@ static inline void *offset_to_ptr(const int *off)
 	return (void *)((unsigned long)off + *off);
 }
 
-#else /* __ASSEMBLY__ */
-
-#ifdef __KERNEL__
-#ifndef LINKER_SCRIPT
-
-#ifdef CONFIG_STACK_VALIDATION
-.macro ANNOTATE_UNREACHABLE counter:req
-\counter:
-	.pushsection .discard.unreachable
-	.long \counter\()b -.
-	.popsection
-.endm
-
-.macro ANNOTATE_REACHABLE counter:req
-\counter:
-	.pushsection .discard.reachable
-	.long \counter\()b -.
-	.popsection
-.endm
-
-.macro ASM_UNREACHABLE
-999:
-	.pushsection .discard.unreachable
-	.long 999b - .
-	.popsection
-.endm
-#else /* CONFIG_STACK_VALIDATION */
-.macro ANNOTATE_UNREACHABLE counter:req
-.endm
-
-.macro ANNOTATE_REACHABLE counter:req
-.endm
-
-.macro ASM_UNREACHABLE
-.endm
-#endif /* CONFIG_STACK_VALIDATION */
-
-#endif /* LINKER_SCRIPT */
-#endif /* __KERNEL__ */
 #endif /* __ASSEMBLY__ */
 
 /* Compile time object size, -1 for unknown */
diff --git a/scripts/Kbuild.include b/scripts/Kbuild.include
index bb01555..3d09844 100644
--- a/scripts/Kbuild.include
+++ b/scripts/Kbuild.include
@@ -115,9 +115,7 @@ __cc-option = $(call try-run,\
 
 # Do not attempt to build with gcc plugins during cc-option tests.
 # (And this uses delayed resolution so the flags will be up to date.)
-# In addition, do not include the asm macros which are built later.
-CC_OPTION_FILTERED = $(GCC_PLUGINS_CFLAGS) $(ASM_MACRO_FLAGS)
-CC_OPTION_CFLAGS = $(filter-out $(CC_OPTION_FILTERED),$(KBUILD_CFLAGS))
+CC_OPTION_CFLAGS = $(filter-out $(GCC_PLUGINS_CFLAGS),$(KBUILD_CFLAGS))
 
 # cc-option
 # Usage: cflags-y += $(call cc-option,-march=winchip-c6,-march=i586)
diff --git a/scripts/mod/Makefile b/scripts/mod/Makefile
index a5b4af4..42c5d50 100644
--- a/scripts/mod/Makefile
+++ b/scripts/mod/Makefile
@@ -4,8 +4,6 @@ OBJECT_FILES_NON_STANDARD := y
 hostprogs-y	:= modpost mk_elfconfig
 always		:= $(hostprogs-y) empty.o
 
-CFLAGS_REMOVE_empty.o := $(ASM_MACRO_FLAGS)
-
 modpost-objs	:= modpost.o file2alias.o sumversion.o
 
 devicetable-offsets-file := devicetable-offsets.h
-- 
2.7.4

================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kernel
Subject: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Sun, 16 Dec 2018 02:50:32 +0000
Message-ID: <1544928632-9717-1-git-send-email-yamada.masahiro () socionext ! com>
--------------------
Revert the following 9 commits:

[1] 5bdcd510c2ac ("x86/jump-labels: Macrofy inline assembly code to
    work around GCC inlining bugs")

    This was partially reverted because it made good cleanups
    irrespective of the inlining issue; the error message is still
    unneeded, and the conversion to STATIC_BRANCH_{NOP,JUMP} should
    be kept.

[2] d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to
    work around GCC inlining bugs")

[3] 0474d5d9d2f7 ("x86/extable: Macrofy inline assembly code to work
    around GCC inlining bugs")

[4] 494b5168f2de ("x86/paravirt: Work around GCC inlining bugs when
    compiling paravirt ops")

[5] f81f8ad56fd1 ("x86/bug: Macrofy the BUG table section handling,
    to work around GCC inlining bugs")

[6] 77f48ec28e4c ("x86/alternatives: Macrofy lock prefixes to work
   around GCC inlining bugs")

[7] 9e1725b41059 ("x86/refcount: Work around GCC inlining bug")

    Resolved conflicts in arch/x86/include/asm/refcount.h caused by
    288e4521f0f6 ("x86/asm: 'Simplify' GEN_*_RMWcc() macros").

[8] c06c4d809051 ("x86/objtool: Use asm macros to work around GCC
    inlining bugs")

[9] 77b0bf55bc67 ("kbuild/Makefile: Prepare for using macros in inline
    assembly code to work around asm() related GCC inlining bugs")

A few days after those commits applied, discussion started to solve
the issue more elegantly with the help of compiler:

  https://lkml.org/lkml/2018/10/7/92

The new syntax "asm inline" was implemented by Segher Boessenkool, and
now queued up for GCC 9. (People were positive even for back-porting it
to older compilers).

Since the in-kernel workarounds merged, some issues have been reported:
breakage of building with distcc/icecc, breakage of distro packages for
module building. (More fundamentally, we cannot build external modules
after 'make clean'.)

I do not want to mess up the build system any more.

Given that this issue will be solved in a cleaner way sooner or later,
let's revert the in-kernel workarounds, and wait for GCC 9.

Reported-by: Logan Gunthorpe <logang@deltatee.com> # distcc
Reported-by: Sedat Dilek <sedat.dilek@gmail.com> # deb/rpm package
Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
Cc: Nadav Amit <namit@vmware.com>
Cc: Segher Boessenkool <segher@kernel.crashing.org>
---

Please consider this for v4.20 release.
Currently, distro package build is broken.


Changes in v2:
  - Revive clean-ups made by 5bdcd510c2ac (per Peter Zijlstra)
  - Fix commit quoting style (per Peter Zijlstra)

 Makefile                               |  9 +---
 arch/x86/Makefile                      |  7 ---
 arch/x86/include/asm/alternative-asm.h | 20 +++----
 arch/x86/include/asm/alternative.h     | 11 +++-
 arch/x86/include/asm/asm.h             | 53 +++++++++++-------
 arch/x86/include/asm/bug.h             | 98 +++++++++++++++-------------------
 arch/x86/include/asm/cpufeature.h      | 82 ++++++++++++----------------
 arch/x86/include/asm/jump_label.h      | 22 ++++++--
 arch/x86/include/asm/paravirt_types.h  | 56 +++++++++----------
 arch/x86/include/asm/refcount.h        | 81 ++++++++++++----------------
 arch/x86/kernel/macros.S               | 16 ------
 include/asm-generic/bug.h              |  8 +--
 include/linux/compiler.h               | 56 +++++--------------
 scripts/Kbuild.include                 |  4 +-
 scripts/mod/Makefile                   |  2 -
 15 files changed, 224 insertions(+), 301 deletions(-)
 delete mode 100644 arch/x86/kernel/macros.S

diff --git a/Makefile b/Makefile
index f2c3423..4cf4c5b 100644
--- a/Makefile
+++ b/Makefile
@@ -1081,7 +1081,7 @@ scripts: scripts_basic scripts_dtc asm-generic gcc-plugins $(autoksyms_h)
 # version.h and scripts_basic is processed / created.
 
 # Listed in dependency order
-PHONY += prepare archprepare macroprepare prepare0 prepare1 prepare2 prepare3
+PHONY += prepare archprepare prepare0 prepare1 prepare2 prepare3
 
 # prepare3 is used to check if we are building in a separate output directory,
 # and if so do:
@@ -1104,9 +1104,7 @@ prepare2: prepare3 outputmakefile asm-generic
 prepare1: prepare2 $(version_h) $(autoksyms_h) include/generated/utsrelease.h
 	$(cmd_crmodverdir)
 
-macroprepare: prepare1 archmacros
-
-archprepare: archheaders archscripts macroprepare scripts_basic
+archprepare: archheaders archscripts prepare1 scripts_basic
 
 prepare0: archprepare gcc-plugins
 	$(Q)$(MAKE) $(build)=.
@@ -1174,9 +1172,6 @@ archheaders:
 PHONY += archscripts
 archscripts:
 
-PHONY += archmacros
-archmacros:
-
 PHONY += __headers
 __headers: $(version_h) scripts_basic uapi-asm-generic archheaders archscripts
 	$(Q)$(MAKE) $(build)=scripts build_unifdef
diff --git a/arch/x86/Makefile b/arch/x86/Makefile
index 75ef499..85a66c4 100644
--- a/arch/x86/Makefile
+++ b/arch/x86/Makefile
@@ -232,13 +232,6 @@ archscripts: scripts_basic
 archheaders:
 	$(Q)$(MAKE) $(build)=arch/x86/entry/syscalls all
 
-archmacros:
-	$(Q)$(MAKE) $(build)=arch/x86/kernel arch/x86/kernel/macros.s
-
-ASM_MACRO_FLAGS = -Wa,arch/x86/kernel/macros.s
-export ASM_MACRO_FLAGS
-KBUILD_CFLAGS += $(ASM_MACRO_FLAGS)
-
 ###
 # Kernel objects
 
diff --git a/arch/x86/include/asm/alternative-asm.h b/arch/x86/include/asm/alternative-asm.h
index 8e4ea39..31b627b 100644
--- a/arch/x86/include/asm/alternative-asm.h
+++ b/arch/x86/include/asm/alternative-asm.h
@@ -7,24 +7,16 @@
 #include <asm/asm.h>
 
 #ifdef CONFIG_SMP
-.macro LOCK_PREFIX_HERE
+	.macro LOCK_PREFIX
+672:	lock
 	.pushsection .smp_locks,"a"
 	.balign 4
-	.long 671f - .		# offset
+	.long 672b - .
 	.popsection
-671:
-.endm
-
-.macro LOCK_PREFIX insn:vararg
-	LOCK_PREFIX_HERE
-	lock \insn
-.endm
+	.endm
 #else
-.macro LOCK_PREFIX_HERE
-.endm
-
-.macro LOCK_PREFIX insn:vararg
-.endm
+	.macro LOCK_PREFIX
+	.endm
 #endif
 
 /*
diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h
index d7faa16..4cd6a3b 100644
--- a/arch/x86/include/asm/alternative.h
+++ b/arch/x86/include/asm/alternative.h
@@ -31,8 +31,15 @@
  */
 
 #ifdef CONFIG_SMP
-#define LOCK_PREFIX_HERE "LOCK_PREFIX_HERE\n\t"
-#define LOCK_PREFIX "LOCK_PREFIX "
+#define LOCK_PREFIX_HERE \
+		".pushsection .smp_locks,\"a\"\n"	\
+		".balign 4\n"				\
+		".long 671f - .\n" /* offset */		\
+		".popsection\n"				\
+		"671:"
+
+#define LOCK_PREFIX LOCK_PREFIX_HERE "\n\tlock; "
+
 #else /* ! CONFIG_SMP */
 #define LOCK_PREFIX_HERE ""
 #define LOCK_PREFIX ""
diff --git a/arch/x86/include/asm/asm.h b/arch/x86/include/asm/asm.h
index 21b0867..6467757b 100644
--- a/arch/x86/include/asm/asm.h
+++ b/arch/x86/include/asm/asm.h
@@ -120,25 +120,12 @@
 /* Exception table entry */
 #ifdef __ASSEMBLY__
 # define _ASM_EXTABLE_HANDLE(from, to, handler)			\
-	ASM_EXTABLE_HANDLE from to handler
-
-.macro ASM_EXTABLE_HANDLE from:req to:req handler:req
-	.pushsection "__ex_table","a"
-	.balign 4
-	.long (\from) - .
-	.long (\to) - .
-	.long (\handler) - .
+	.pushsection "__ex_table","a" ;				\
+	.balign 4 ;						\
+	.long (from) - . ;					\
+	.long (to) - . ;					\
+	.long (handler) - . ;					\
 	.popsection
-.endm
-#else /* __ASSEMBLY__ */
-
-# define _ASM_EXTABLE_HANDLE(from, to, handler)			\
-	"ASM_EXTABLE_HANDLE from=" #from " to=" #to		\
-	" handler=\"" #handler "\"\n\t"
-
-/* For C file, we already have NOKPROBE_SYMBOL macro */
-
-#endif /* __ASSEMBLY__ */
 
 # define _ASM_EXTABLE(from, to)					\
 	_ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
@@ -161,7 +148,6 @@
 	_ASM_PTR (entry);					\
 	.popsection
 
-#ifdef __ASSEMBLY__
 .macro ALIGN_DESTINATION
 	/* check for bad alignment of destination */
 	movl %edi,%ecx
@@ -185,7 +171,34 @@
 	_ASM_EXTABLE_UA(100b, 103b)
 	_ASM_EXTABLE_UA(101b, 103b)
 	.endm
-#endif /* __ASSEMBLY__ */
+
+#else
+# define _EXPAND_EXTABLE_HANDLE(x) #x
+# define _ASM_EXTABLE_HANDLE(from, to, handler)			\
+	" .pushsection \"__ex_table\",\"a\"\n"			\
+	" .balign 4\n"						\
+	" .long (" #from ") - .\n"				\
+	" .long (" #to ") - .\n"				\
+	" .long (" _EXPAND_EXTABLE_HANDLE(handler) ") - .\n"	\
+	" .popsection\n"
+
+# define _ASM_EXTABLE(from, to)					\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
+
+# define _ASM_EXTABLE_UA(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_uaccess)
+
+# define _ASM_EXTABLE_FAULT(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_fault)
+
+# define _ASM_EXTABLE_EX(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_ext)
+
+# define _ASM_EXTABLE_REFCOUNT(from, to)			\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_refcount)
+
+/* For C file, we already have NOKPROBE_SYMBOL macro */
+#endif
 
 #ifndef __ASSEMBLY__
 /*
diff --git a/arch/x86/include/asm/bug.h b/arch/x86/include/asm/bug.h
index 5090035..6804d66 100644
--- a/arch/x86/include/asm/bug.h
+++ b/arch/x86/include/asm/bug.h
@@ -4,8 +4,6 @@
 
 #include <linux/stringify.h>
 
-#ifndef __ASSEMBLY__
-
 /*
  * Despite that some emulators terminate on UD2, we use it for WARN().
  *
@@ -22,15 +20,53 @@
 
 #define LEN_UD2		2
 
+#ifdef CONFIG_GENERIC_BUG
+
+#ifdef CONFIG_X86_32
+# define __BUG_REL(val)	".long " __stringify(val)
+#else
+# define __BUG_REL(val)	".long " __stringify(val) " - 2b"
+#endif
+
+#ifdef CONFIG_DEBUG_BUGVERBOSE
+
+#define _BUG_FLAGS(ins, flags)						\
+do {									\
+	asm volatile("1:\t" ins "\n"					\
+		     ".pushsection __bug_table,\"aw\"\n"		\
+		     "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"	\
+		     "\t"  __BUG_REL(%c0) "\t# bug_entry::file\n"	\
+		     "\t.word %c1"        "\t# bug_entry::line\n"	\
+		     "\t.word %c2"        "\t# bug_entry::flags\n"	\
+		     "\t.org 2b+%c3\n"					\
+		     ".popsection"					\
+		     : : "i" (__FILE__), "i" (__LINE__),		\
+			 "i" (flags),					\
+			 "i" (sizeof(struct bug_entry)));		\
+} while (0)
+
+#else /* !CONFIG_DEBUG_BUGVERBOSE */
+
 #define _BUG_FLAGS(ins, flags)						\
 do {									\
-	asm volatile("ASM_BUG ins=\"" ins "\" file=%c0 line=%c1 "	\
-		     "flags=%c2 size=%c3"				\
-		     : : "i" (__FILE__), "i" (__LINE__),                \
-			 "i" (flags),                                   \
+	asm volatile("1:\t" ins "\n"					\
+		     ".pushsection __bug_table,\"aw\"\n"		\
+		     "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"	\
+		     "\t.word %c0"        "\t# bug_entry::flags\n"	\
+		     "\t.org 2b+%c1\n"					\
+		     ".popsection"					\
+		     : : "i" (flags),					\
 			 "i" (sizeof(struct bug_entry)));		\
 } while (0)
 
+#endif /* CONFIG_DEBUG_BUGVERBOSE */
+
+#else
+
+#define _BUG_FLAGS(ins, flags)  asm volatile(ins)
+
+#endif /* CONFIG_GENERIC_BUG */
+
 #define HAVE_ARCH_BUG
 #define BUG()							\
 do {								\
@@ -46,54 +82,4 @@ do {								\
 
 #include <asm-generic/bug.h>
 
-#else /* __ASSEMBLY__ */
-
-#ifdef CONFIG_GENERIC_BUG
-
-#ifdef CONFIG_X86_32
-.macro __BUG_REL val:req
-	.long \val
-.endm
-#else
-.macro __BUG_REL val:req
-	.long \val - 2b
-.endm
-#endif
-
-#ifdef CONFIG_DEBUG_BUGVERBOSE
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-1:	\ins
-	.pushsection __bug_table,"aw"
-2:	__BUG_REL val=1b	# bug_entry::bug_addr
-	__BUG_REL val=\file	# bug_entry::file
-	.word \line		# bug_entry::line
-	.word \flags		# bug_entry::flags
-	.org 2b+\size
-	.popsection
-.endm
-
-#else /* !CONFIG_DEBUG_BUGVERBOSE */
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-1:	\ins
-	.pushsection __bug_table,"aw"
-2:	__BUG_REL val=1b	# bug_entry::bug_addr
-	.word \flags		# bug_entry::flags
-	.org 2b+\size
-	.popsection
-.endm
-
-#endif /* CONFIG_DEBUG_BUGVERBOSE */
-
-#else /* CONFIG_GENERIC_BUG */
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-	\ins
-.endm
-
-#endif /* CONFIG_GENERIC_BUG */
-
-#endif /* __ASSEMBLY__ */
-
 #endif /* _ASM_X86_BUG_H */
diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h
index 7d44272..aced6c9 100644
--- a/arch/x86/include/asm/cpufeature.h
+++ b/arch/x86/include/asm/cpufeature.h
@@ -2,10 +2,10 @@
 #ifndef _ASM_X86_CPUFEATURE_H
 #define _ASM_X86_CPUFEATURE_H
 
-#ifdef __KERNEL__
-#ifndef __ASSEMBLY__
-
 #include <asm/processor.h>
+
+#if defined(__KERNEL__) && !defined(__ASSEMBLY__)
+
 #include <asm/asm.h>
 #include <linux/bitops.h>
 
@@ -161,10 +161,37 @@ extern void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int bit);
  */
 static __always_inline __pure bool _static_cpu_has(u16 bit)
 {
-	asm_volatile_goto("STATIC_CPU_HAS bitnum=%[bitnum] "
-			  "cap_byte=\"%[cap_byte]\" "
-			  "feature=%P[feature] t_yes=%l[t_yes] "
-			  "t_no=%l[t_no] always=%P[always]"
+	asm_volatile_goto("1: jmp 6f\n"
+		 "2:\n"
+		 ".skip -(((5f-4f) - (2b-1b)) > 0) * "
+			 "((5f-4f) - (2b-1b)),0x90\n"
+		 "3:\n"
+		 ".section .altinstructions,\"a\"\n"
+		 " .long 1b - .\n"		/* src offset */
+		 " .long 4f - .\n"		/* repl offset */
+		 " .word %P[always]\n"		/* always replace */
+		 " .byte 3b - 1b\n"		/* src len */
+		 " .byte 5f - 4f\n"		/* repl len */
+		 " .byte 3b - 2b\n"		/* pad len */
+		 ".previous\n"
+		 ".section .altinstr_replacement,\"ax\"\n"
+		 "4: jmp %l[t_no]\n"
+		 "5:\n"
+		 ".previous\n"
+		 ".section .altinstructions,\"a\"\n"
+		 " .long 1b - .\n"		/* src offset */
+		 " .long 0\n"			/* no replacement */
+		 " .word %P[feature]\n"		/* feature bit */
+		 " .byte 3b - 1b\n"		/* src len */
+		 " .byte 0\n"			/* repl len */
+		 " .byte 0\n"			/* pad len */
+		 ".previous\n"
+		 ".section .altinstr_aux,\"ax\"\n"
+		 "6:\n"
+		 " testb %[bitnum],%[cap_byte]\n"
+		 " jnz %l[t_yes]\n"
+		 " jmp %l[t_no]\n"
+		 ".previous\n"
 		 : : [feature]  "i" (bit),
 		     [always]   "i" (X86_FEATURE_ALWAYS),
 		     [bitnum]   "i" (1 << (bit & 7)),
@@ -199,44 +226,5 @@ static __always_inline __pure bool _static_cpu_has(u16 bit)
 #define CPU_FEATURE_TYPEVAL		boot_cpu_data.x86_vendor, boot_cpu_data.x86, \
 					boot_cpu_data.x86_model
 
-#else /* __ASSEMBLY__ */
-
-.macro STATIC_CPU_HAS bitnum:req cap_byte:req feature:req t_yes:req t_no:req always:req
-1:
-	jmp 6f
-2:
-	.skip -(((5f-4f) - (2b-1b)) > 0) * ((5f-4f) - (2b-1b)),0x90
-3:
-	.section .altinstructions,"a"
-	.long 1b - .		/* src offset */
-	.long 4f - .		/* repl offset */
-	.word \always		/* always replace */
-	.byte 3b - 1b		/* src len */
-	.byte 5f - 4f		/* repl len */
-	.byte 3b - 2b		/* pad len */
-	.previous
-	.section .altinstr_replacement,"ax"
-4:
-	jmp \t_no
-5:
-	.previous
-	.section .altinstructions,"a"
-	.long 1b - .		/* src offset */
-	.long 0			/* no replacement */
-	.word \feature		/* feature bit */
-	.byte 3b - 1b		/* src len */
-	.byte 0			/* repl len */
-	.byte 0			/* pad len */
-	.previous
-	.section .altinstr_aux,"ax"
-6:
-	testb \bitnum,\cap_byte
-	jnz \t_yes
-	jmp \t_no
-	.previous
-.endm
-
-#endif /* __ASSEMBLY__ */
-
-#endif /* __KERNEL__ */
+#endif /* defined(__KERNEL__) && !defined(__ASSEMBLY__) */
 #endif /* _ASM_X86_CPUFEATURE_H */
diff --git a/arch/x86/include/asm/jump_label.h b/arch/x86/include/asm/jump_label.h
index a5fb34f..cf88ebf 100644
--- a/arch/x86/include/asm/jump_label.h
+++ b/arch/x86/include/asm/jump_label.h
@@ -20,9 +20,15 @@
 
 static __always_inline bool arch_static_branch(struct static_key *key, bool branch)
 {
-	asm_volatile_goto("STATIC_BRANCH_NOP l_yes=\"%l[l_yes]\" key=\"%c0\" "
-			  "branch=\"%c1\""
-			: :  "i" (key), "i" (branch) : : l_yes);
+	asm_volatile_goto("1:"
+		".byte " __stringify(STATIC_KEY_INIT_NOP) "\n\t"
+		".pushsection __jump_table,  \"aw\" \n\t"
+		_ASM_ALIGN "\n\t"
+		".long 1b - ., %l[l_yes] - . \n\t"
+		_ASM_PTR "%c0 + %c1 - .\n\t"
+		".popsection \n\t"
+		: :  "i" (key), "i" (branch) : : l_yes);
+
 	return false;
 l_yes:
 	return true;
@@ -30,8 +36,14 @@ static __always_inline bool arch_static_branch(struct static_key *key, bool bran
 
 static __always_inline bool arch_static_branch_jump(struct static_key *key, bool branch)
 {
-	asm_volatile_goto("STATIC_BRANCH_JMP l_yes=\"%l[l_yes]\" key=\"%c0\" "
-			  "branch=\"%c1\""
+	asm_volatile_goto("1:"
+		".byte 0xe9\n\t .long %l[l_yes] - 2f\n\t"
+		"2:\n\t"
+		".pushsection __jump_table,  \"aw\" \n\t"
+		_ASM_ALIGN "\n\t"
+		".long 1b - ., %l[l_yes] - . \n\t"
+		_ASM_PTR "%c0 + %c1 - .\n\t"
+		".popsection \n\t"
 		: :  "i" (key), "i" (branch) : : l_yes);
 
 	return false;
diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h
index 26942ad..488c596 100644
--- a/arch/x86/include/asm/paravirt_types.h
+++ b/arch/x86/include/asm/paravirt_types.h
@@ -348,11 +348,23 @@ extern struct paravirt_patch_template pv_ops;
 #define paravirt_clobber(clobber)		\
 	[paravirt_clobber] "i" (clobber)
 
+/*
+ * Generate some code, and mark it as patchable by the
+ * apply_paravirt() alternate instruction patcher.
+ */
+#define _paravirt_alt(insn_string, type, clobber)	\
+	"771:\n\t" insn_string "\n" "772:\n"		\
+	".pushsection .parainstructions,\"a\"\n"	\
+	_ASM_ALIGN "\n"					\
+	_ASM_PTR " 771b\n"				\
+	"  .byte " type "\n"				\
+	"  .byte 772b-771b\n"				\
+	"  .short " clobber "\n"			\
+	".popsection\n"
+
 /* Generate patchable code, with the default asm parameters. */
-#define paravirt_call							\
-	"PARAVIRT_CALL type=\"%c[paravirt_typenum]\""			\
-	" clobber=\"%c[paravirt_clobber]\""				\
-	" pv_opptr=\"%c[paravirt_opptr]\";"
+#define paravirt_alt(insn_string)					\
+	_paravirt_alt(insn_string, "%c[paravirt_typenum]", "%c[paravirt_clobber]")
 
 /* Simple instruction patching code. */
 #define NATIVE_LABEL(a,x,b) "\n\t.globl " a #x "_" #b "\n" a #x "_" #b ":\n\t"
@@ -373,6 +385,16 @@ unsigned native_patch(u8 type, void *ibuf, unsigned long addr, unsigned len);
 int paravirt_disable_iospace(void);
 
 /*
+ * This generates an indirect call based on the operation type number.
+ * The type number, computed in PARAVIRT_PATCH, is derived from the
+ * offset into the paravirt_patch_template structure, and can therefore be
+ * freely converted back into a structure offset.
+ */
+#define PARAVIRT_CALL					\
+	ANNOTATE_RETPOLINE_SAFE				\
+	"call *%c[paravirt_opptr];"
+
+/*
  * These macros are intended to wrap calls through one of the paravirt
  * ops structs, so that they can be later identified and patched at
  * runtime.
@@ -509,7 +531,7 @@ int paravirt_disable_iospace(void);
 		/* since this condition will never hold */		\
 		if (sizeof(rettype) > sizeof(unsigned long)) {		\
 			asm volatile(pre				\
-				     paravirt_call			\
+				     paravirt_alt(PARAVIRT_CALL)	\
 				     post				\
 				     : call_clbr, ASM_CALL_CONSTRAINT	\
 				     : paravirt_type(op),		\
@@ -519,7 +541,7 @@ int paravirt_disable_iospace(void);
 			__ret = (rettype)((((u64)__edx) << 32) | __eax); \
 		} else {						\
 			asm volatile(pre				\
-				     paravirt_call			\
+				     paravirt_alt(PARAVIRT_CALL)	\
 				     post				\
 				     : call_clbr, ASM_CALL_CONSTRAINT	\
 				     : paravirt_type(op),		\
@@ -546,7 +568,7 @@ int paravirt_disable_iospace(void);
 		PVOP_VCALL_ARGS;					\
 		PVOP_TEST_NULL(op);					\
 		asm volatile(pre					\
-			     paravirt_call				\
+			     paravirt_alt(PARAVIRT_CALL)		\
 			     post					\
 			     : call_clbr, ASM_CALL_CONSTRAINT		\
 			     : paravirt_type(op),			\
@@ -664,26 +686,6 @@ struct paravirt_patch_site {
 extern struct paravirt_patch_site __parainstructions[],
 	__parainstructions_end[];
 
-#else	/* __ASSEMBLY__ */
-
-/*
- * This generates an indirect call based on the operation type number.
- * The type number, computed in PARAVIRT_PATCH, is derived from the
- * offset into the paravirt_patch_template structure, and can therefore be
- * freely converted back into a structure offset.
- */
-.macro PARAVIRT_CALL type:req clobber:req pv_opptr:req
-771:	ANNOTATE_RETPOLINE_SAFE
-	call *\pv_opptr
-772:	.pushsection .parainstructions,"a"
-	_ASM_ALIGN
-	_ASM_PTR 771b
-	.byte \type
-	.byte 772b-771b
-	.short \clobber
-	.popsection
-.endm
-
 #endif	/* __ASSEMBLY__ */
 
 #endif	/* _ASM_X86_PARAVIRT_TYPES_H */
diff --git a/arch/x86/include/asm/refcount.h b/arch/x86/include/asm/refcount.h
index a8b5e1e..dbaed55 100644
--- a/arch/x86/include/asm/refcount.h
+++ b/arch/x86/include/asm/refcount.h
@@ -4,41 +4,6 @@
  * x86-specific implementation of refcount_t. Based on PAX_REFCOUNT from
  * PaX/grsecurity.
  */
-
-#ifdef __ASSEMBLY__
-
-#include <asm/asm.h>
-#include <asm/bug.h>
-
-.macro REFCOUNT_EXCEPTION counter:req
-	.pushsection .text..refcount
-111:	lea \counter, %_ASM_CX
-112:	ud2
-	ASM_UNREACHABLE
-	.popsection
-113:	_ASM_EXTABLE_REFCOUNT(112b, 113b)
-.endm
-
-/* Trigger refcount exception if refcount result is negative. */
-.macro REFCOUNT_CHECK_LT_ZERO counter:req
-	js 111f
-	REFCOUNT_EXCEPTION counter="\counter"
-.endm
-
-/* Trigger refcount exception if refcount result is zero or negative. */
-.macro REFCOUNT_CHECK_LE_ZERO counter:req
-	jz 111f
-	REFCOUNT_CHECK_LT_ZERO counter="\counter"
-.endm
-
-/* Trigger refcount exception unconditionally. */
-.macro REFCOUNT_ERROR counter:req
-	jmp 111f
-	REFCOUNT_EXCEPTION counter="\counter"
-.endm
-
-#else /* __ASSEMBLY__ */
-
 #include <linux/refcount.h>
 #include <asm/bug.h>
 
@@ -50,12 +15,35 @@
  * central refcount exception. The fixup address for the exception points
  * back to the regular execution flow in .text.
  */
+#define _REFCOUNT_EXCEPTION				\
+	".pushsection .text..refcount\n"		\
+	"111:\tlea %[var], %%" _ASM_CX "\n"		\
+	"112:\t" ASM_UD2 "\n"				\
+	ASM_UNREACHABLE					\
+	".popsection\n"					\
+	"113:\n"					\
+	_ASM_EXTABLE_REFCOUNT(112b, 113b)
+
+/* Trigger refcount exception if refcount result is negative. */
+#define REFCOUNT_CHECK_LT_ZERO				\
+	"js 111f\n\t"					\
+	_REFCOUNT_EXCEPTION
+
+/* Trigger refcount exception if refcount result is zero or negative. */
+#define REFCOUNT_CHECK_LE_ZERO				\
+	"jz 111f\n\t"					\
+	REFCOUNT_CHECK_LT_ZERO
+
+/* Trigger refcount exception unconditionally. */
+#define REFCOUNT_ERROR					\
+	"jmp 111f\n\t"					\
+	_REFCOUNT_EXCEPTION
 
 static __always_inline void refcount_add(unsigned int i, refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "addl %1,%0\n\t"
-		"REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LT_ZERO
+		: [var] "+m" (r->refs.counter)
 		: "ir" (i)
 		: "cc", "cx");
 }
@@ -63,32 +51,31 @@ static __always_inline void refcount_add(unsigned int i, refcount_t *r)
 static __always_inline void refcount_inc(refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "incl %0\n\t"
-		"REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LT_ZERO
+		: [var] "+m" (r->refs.counter)
 		: : "cc", "cx");
 }
 
 static __always_inline void refcount_dec(refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "decl %0\n\t"
-		"REFCOUNT_CHECK_LE_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LE_ZERO
+		: [var] "+m" (r->refs.counter)
 		: : "cc", "cx");
 }
 
 static __always_inline __must_check
 bool refcount_sub_and_test(unsigned int i, refcount_t *r)
 {
-
 	return GEN_BINARY_SUFFIXED_RMWcc(LOCK_PREFIX "subl",
-					 "REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
+					 REFCOUNT_CHECK_LT_ZERO,
 					 r->refs.counter, e, "er", i, "cx");
 }
 
 static __always_inline __must_check bool refcount_dec_and_test(refcount_t *r)
 {
 	return GEN_UNARY_SUFFIXED_RMWcc(LOCK_PREFIX "decl",
-					"REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
+					REFCOUNT_CHECK_LT_ZERO,
 					r->refs.counter, e, "cx");
 }
 
@@ -106,8 +93,8 @@ bool refcount_add_not_zero(unsigned int i, refcount_t *r)
 
 		/* Did we try to increment from/to an undesirable state? */
 		if (unlikely(c < 0 || c == INT_MAX || result < c)) {
-			asm volatile("REFCOUNT_ERROR counter=\"%[counter]\""
-				     : : [counter] "m" (r->refs.counter)
+			asm volatile(REFCOUNT_ERROR
+				     : : [var] "m" (r->refs.counter)
 				     : "cc", "cx");
 			break;
 		}
@@ -122,6 +109,4 @@ static __always_inline __must_check bool refcount_inc_not_zero(refcount_t *r)
 	return refcount_add_not_zero(1, r);
 }
 
-#endif /* __ASSEMBLY__ */
-
 #endif
diff --git a/arch/x86/kernel/macros.S b/arch/x86/kernel/macros.S
deleted file mode 100644
index 161c950..0000000
--- a/arch/x86/kernel/macros.S
+++ /dev/null
@@ -1,16 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-
-/*
- * This file includes headers whose assembly part includes macros which are
- * commonly used. The macros are precompiled into assmebly file which is later
- * assembled together with each compiled file.
- */
-
-#include <linux/compiler.h>
-#include <asm/refcount.h>
-#include <asm/alternative-asm.h>
-#include <asm/bug.h>
-#include <asm/paravirt.h>
-#include <asm/asm.h>
-#include <asm/cpufeature.h>
-#include <asm/jump_label.h>
diff --git a/include/asm-generic/bug.h b/include/asm-generic/bug.h
index cdafa5e..20561a6 100644
--- a/include/asm-generic/bug.h
+++ b/include/asm-generic/bug.h
@@ -17,8 +17,10 @@
 #ifndef __ASSEMBLY__
 #include <linux/kernel.h>
 
-struct bug_entry {
+#ifdef CONFIG_BUG
+
 #ifdef CONFIG_GENERIC_BUG
+struct bug_entry {
 #ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS
 	unsigned long	bug_addr;
 #else
@@ -33,10 +35,8 @@ struct bug_entry {
 	unsigned short	line;
 #endif
 	unsigned short	flags;
-#endif	/* CONFIG_GENERIC_BUG */
 };
-
-#ifdef CONFIG_BUG
+#endif	/* CONFIG_GENERIC_BUG */
 
 /*
  * Don't use BUG() or BUG_ON() unless there's really no way out; one
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 06396c1..fc5004a 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -99,13 +99,22 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
  * unique, to convince GCC not to merge duplicate inline asm statements.
  */
 #define annotate_reachable() ({						\
-	asm volatile("ANNOTATE_REACHABLE counter=%c0"			\
-		     : : "i" (__COUNTER__));				\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.reachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
 #define annotate_unreachable() ({					\
-	asm volatile("ANNOTATE_UNREACHABLE counter=%c0"			\
-		     : : "i" (__COUNTER__));				\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.unreachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
+#define ASM_UNREACHABLE							\
+	"999:\n\t"							\
+	".pushsection .discard.unreachable\n\t"				\
+	".long 999b - .\n\t"						\
+	".popsection\n\t"
 #else
 #define annotate_reachable()
 #define annotate_unreachable()
@@ -293,45 +302,6 @@ static inline void *offset_to_ptr(const int *off)
 	return (void *)((unsigned long)off + *off);
 }
 
-#else /* __ASSEMBLY__ */
-
-#ifdef __KERNEL__
-#ifndef LINKER_SCRIPT
-
-#ifdef CONFIG_STACK_VALIDATION
-.macro ANNOTATE_UNREACHABLE counter:req
-\counter:
-	.pushsection .discard.unreachable
-	.long \counter\()b -.
-	.popsection
-.endm
-
-.macro ANNOTATE_REACHABLE counter:req
-\counter:
-	.pushsection .discard.reachable
-	.long \counter\()b -.
-	.popsection
-.endm
-
-.macro ASM_UNREACHABLE
-999:
-	.pushsection .discard.unreachable
-	.long 999b - .
-	.popsection
-.endm
-#else /* CONFIG_STACK_VALIDATION */
-.macro ANNOTATE_UNREACHABLE counter:req
-.endm
-
-.macro ANNOTATE_REACHABLE counter:req
-.endm
-
-.macro ASM_UNREACHABLE
-.endm
-#endif /* CONFIG_STACK_VALIDATION */
-
-#endif /* LINKER_SCRIPT */
-#endif /* __KERNEL__ */
 #endif /* __ASSEMBLY__ */
 
 /* Compile time object size, -1 for unknown */
diff --git a/scripts/Kbuild.include b/scripts/Kbuild.include
index bb01555..3d09844 100644
--- a/scripts/Kbuild.include
+++ b/scripts/Kbuild.include
@@ -115,9 +115,7 @@ __cc-option = $(call try-run,\
 
 # Do not attempt to build with gcc plugins during cc-option tests.
 # (And this uses delayed resolution so the flags will be up to date.)
-# In addition, do not include the asm macros which are built later.
-CC_OPTION_FILTERED = $(GCC_PLUGINS_CFLAGS) $(ASM_MACRO_FLAGS)
-CC_OPTION_CFLAGS = $(filter-out $(CC_OPTION_FILTERED),$(KBUILD_CFLAGS))
+CC_OPTION_CFLAGS = $(filter-out $(GCC_PLUGINS_CFLAGS),$(KBUILD_CFLAGS))
 
 # cc-option
 # Usage: cflags-y += $(call cc-option,-march=winchip-c6,-march=i586)
diff --git a/scripts/mod/Makefile b/scripts/mod/Makefile
index a5b4af4..42c5d50 100644
--- a/scripts/mod/Makefile
+++ b/scripts/mod/Makefile
@@ -4,8 +4,6 @@ OBJECT_FILES_NON_STANDARD := y
 hostprogs-y	:= modpost mk_elfconfig
 always		:= $(hostprogs-y) empty.o
 
-CFLAGS_REMOVE_empty.o := $(ASM_MACRO_FLAGS)
-
 modpost-objs	:= modpost.o file2alias.o sumversion.o
 
 devicetable-offsets-file := devicetable-offsets.h
-- 
2.7.4

================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-arch
Subject: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Sun, 16 Dec 2018 02:50:32 +0000
Message-ID: <1544928632-9717-1-git-send-email-yamada.masahiro () socionext ! com>
--------------------
Revert the following 9 commits:

[1] 5bdcd510c2ac ("x86/jump-labels: Macrofy inline assembly code to
    work around GCC inlining bugs")

    This was partially reverted because it made good cleanups
    irrespective of the inlining issue; the error message is still
    unneeded, and the conversion to STATIC_BRANCH_{NOP,JUMP} should
    be kept.

[2] d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to
    work around GCC inlining bugs")

[3] 0474d5d9d2f7 ("x86/extable: Macrofy inline assembly code to work
    around GCC inlining bugs")

[4] 494b5168f2de ("x86/paravirt: Work around GCC inlining bugs when
    compiling paravirt ops")

[5] f81f8ad56fd1 ("x86/bug: Macrofy the BUG table section handling,
    to work around GCC inlining bugs")

[6] 77f48ec28e4c ("x86/alternatives: Macrofy lock prefixes to work
   around GCC inlining bugs")

[7] 9e1725b41059 ("x86/refcount: Work around GCC inlining bug")

    Resolved conflicts in arch/x86/include/asm/refcount.h caused by
    288e4521f0f6 ("x86/asm: 'Simplify' GEN_*_RMWcc() macros").

[8] c06c4d809051 ("x86/objtool: Use asm macros to work around GCC
    inlining bugs")

[9] 77b0bf55bc67 ("kbuild/Makefile: Prepare for using macros in inline
    assembly code to work around asm() related GCC inlining bugs")

A few days after those commits applied, discussion started to solve
the issue more elegantly with the help of compiler:

  https://lkml.org/lkml/2018/10/7/92

The new syntax "asm inline" was implemented by Segher Boessenkool, and
now queued up for GCC 9. (People were positive even for back-porting it
to older compilers).

Since the in-kernel workarounds merged, some issues have been reported:
breakage of building with distcc/icecc, breakage of distro packages for
module building. (More fundamentally, we cannot build external modules
after 'make clean'.)

I do not want to mess up the build system any more.

Given that this issue will be solved in a cleaner way sooner or later,
let's revert the in-kernel workarounds, and wait for GCC 9.

Reported-by: Logan Gunthorpe <logang@deltatee.com> # distcc
Reported-by: Sedat Dilek <sedat.dilek@gmail.com> # deb/rpm package
Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
Cc: Nadav Amit <namit@vmware.com>
Cc: Segher Boessenkool <segher@kernel.crashing.org>
---

Please consider this for v4.20 release.
Currently, distro package build is broken.


Changes in v2:
  - Revive clean-ups made by 5bdcd510c2ac (per Peter Zijlstra)
  - Fix commit quoting style (per Peter Zijlstra)

 Makefile                               |  9 +---
 arch/x86/Makefile                      |  7 ---
 arch/x86/include/asm/alternative-asm.h | 20 +++----
 arch/x86/include/asm/alternative.h     | 11 +++-
 arch/x86/include/asm/asm.h             | 53 +++++++++++-------
 arch/x86/include/asm/bug.h             | 98 +++++++++++++++-------------------
 arch/x86/include/asm/cpufeature.h      | 82 ++++++++++++----------------
 arch/x86/include/asm/jump_label.h      | 22 ++++++--
 arch/x86/include/asm/paravirt_types.h  | 56 +++++++++----------
 arch/x86/include/asm/refcount.h        | 81 ++++++++++++----------------
 arch/x86/kernel/macros.S               | 16 ------
 include/asm-generic/bug.h              |  8 +--
 include/linux/compiler.h               | 56 +++++--------------
 scripts/Kbuild.include                 |  4 +-
 scripts/mod/Makefile                   |  2 -
 15 files changed, 224 insertions(+), 301 deletions(-)
 delete mode 100644 arch/x86/kernel/macros.S

diff --git a/Makefile b/Makefile
index f2c3423..4cf4c5b 100644
--- a/Makefile
+++ b/Makefile
@@ -1081,7 +1081,7 @@ scripts: scripts_basic scripts_dtc asm-generic gcc-plugins $(autoksyms_h)
 # version.h and scripts_basic is processed / created.
 
 # Listed in dependency order
-PHONY += prepare archprepare macroprepare prepare0 prepare1 prepare2 prepare3
+PHONY += prepare archprepare prepare0 prepare1 prepare2 prepare3
 
 # prepare3 is used to check if we are building in a separate output directory,
 # and if so do:
@@ -1104,9 +1104,7 @@ prepare2: prepare3 outputmakefile asm-generic
 prepare1: prepare2 $(version_h) $(autoksyms_h) include/generated/utsrelease.h
 	$(cmd_crmodverdir)
 
-macroprepare: prepare1 archmacros
-
-archprepare: archheaders archscripts macroprepare scripts_basic
+archprepare: archheaders archscripts prepare1 scripts_basic
 
 prepare0: archprepare gcc-plugins
 	$(Q)$(MAKE) $(build)=.
@@ -1174,9 +1172,6 @@ archheaders:
 PHONY += archscripts
 archscripts:
 
-PHONY += archmacros
-archmacros:
-
 PHONY += __headers
 __headers: $(version_h) scripts_basic uapi-asm-generic archheaders archscripts
 	$(Q)$(MAKE) $(build)=scripts build_unifdef
diff --git a/arch/x86/Makefile b/arch/x86/Makefile
index 75ef499..85a66c4 100644
--- a/arch/x86/Makefile
+++ b/arch/x86/Makefile
@@ -232,13 +232,6 @@ archscripts: scripts_basic
 archheaders:
 	$(Q)$(MAKE) $(build)=arch/x86/entry/syscalls all
 
-archmacros:
-	$(Q)$(MAKE) $(build)=arch/x86/kernel arch/x86/kernel/macros.s
-
-ASM_MACRO_FLAGS = -Wa,arch/x86/kernel/macros.s
-export ASM_MACRO_FLAGS
-KBUILD_CFLAGS += $(ASM_MACRO_FLAGS)
-
 ###
 # Kernel objects
 
diff --git a/arch/x86/include/asm/alternative-asm.h b/arch/x86/include/asm/alternative-asm.h
index 8e4ea39..31b627b 100644
--- a/arch/x86/include/asm/alternative-asm.h
+++ b/arch/x86/include/asm/alternative-asm.h
@@ -7,24 +7,16 @@
 #include <asm/asm.h>
 
 #ifdef CONFIG_SMP
-.macro LOCK_PREFIX_HERE
+	.macro LOCK_PREFIX
+672:	lock
 	.pushsection .smp_locks,"a"
 	.balign 4
-	.long 671f - .		# offset
+	.long 672b - .
 	.popsection
-671:
-.endm
-
-.macro LOCK_PREFIX insn:vararg
-	LOCK_PREFIX_HERE
-	lock \insn
-.endm
+	.endm
 #else
-.macro LOCK_PREFIX_HERE
-.endm
-
-.macro LOCK_PREFIX insn:vararg
-.endm
+	.macro LOCK_PREFIX
+	.endm
 #endif
 
 /*
diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h
index d7faa16..4cd6a3b 100644
--- a/arch/x86/include/asm/alternative.h
+++ b/arch/x86/include/asm/alternative.h
@@ -31,8 +31,15 @@
  */
 
 #ifdef CONFIG_SMP
-#define LOCK_PREFIX_HERE "LOCK_PREFIX_HERE\n\t"
-#define LOCK_PREFIX "LOCK_PREFIX "
+#define LOCK_PREFIX_HERE \
+		".pushsection .smp_locks,\"a\"\n"	\
+		".balign 4\n"				\
+		".long 671f - .\n" /* offset */		\
+		".popsection\n"				\
+		"671:"
+
+#define LOCK_PREFIX LOCK_PREFIX_HERE "\n\tlock; "
+
 #else /* ! CONFIG_SMP */
 #define LOCK_PREFIX_HERE ""
 #define LOCK_PREFIX ""
diff --git a/arch/x86/include/asm/asm.h b/arch/x86/include/asm/asm.h
index 21b0867..6467757b 100644
--- a/arch/x86/include/asm/asm.h
+++ b/arch/x86/include/asm/asm.h
@@ -120,25 +120,12 @@
 /* Exception table entry */
 #ifdef __ASSEMBLY__
 # define _ASM_EXTABLE_HANDLE(from, to, handler)			\
-	ASM_EXTABLE_HANDLE from to handler
-
-.macro ASM_EXTABLE_HANDLE from:req to:req handler:req
-	.pushsection "__ex_table","a"
-	.balign 4
-	.long (\from) - .
-	.long (\to) - .
-	.long (\handler) - .
+	.pushsection "__ex_table","a" ;				\
+	.balign 4 ;						\
+	.long (from) - . ;					\
+	.long (to) - . ;					\
+	.long (handler) - . ;					\
 	.popsection
-.endm
-#else /* __ASSEMBLY__ */
-
-# define _ASM_EXTABLE_HANDLE(from, to, handler)			\
-	"ASM_EXTABLE_HANDLE from=" #from " to=" #to		\
-	" handler=\"" #handler "\"\n\t"
-
-/* For C file, we already have NOKPROBE_SYMBOL macro */
-
-#endif /* __ASSEMBLY__ */
 
 # define _ASM_EXTABLE(from, to)					\
 	_ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
@@ -161,7 +148,6 @@
 	_ASM_PTR (entry);					\
 	.popsection
 
-#ifdef __ASSEMBLY__
 .macro ALIGN_DESTINATION
 	/* check for bad alignment of destination */
 	movl %edi,%ecx
@@ -185,7 +171,34 @@
 	_ASM_EXTABLE_UA(100b, 103b)
 	_ASM_EXTABLE_UA(101b, 103b)
 	.endm
-#endif /* __ASSEMBLY__ */
+
+#else
+# define _EXPAND_EXTABLE_HANDLE(x) #x
+# define _ASM_EXTABLE_HANDLE(from, to, handler)			\
+	" .pushsection \"__ex_table\",\"a\"\n"			\
+	" .balign 4\n"						\
+	" .long (" #from ") - .\n"				\
+	" .long (" #to ") - .\n"				\
+	" .long (" _EXPAND_EXTABLE_HANDLE(handler) ") - .\n"	\
+	" .popsection\n"
+
+# define _ASM_EXTABLE(from, to)					\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
+
+# define _ASM_EXTABLE_UA(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_uaccess)
+
+# define _ASM_EXTABLE_FAULT(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_fault)
+
+# define _ASM_EXTABLE_EX(from, to)				\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_ext)
+
+# define _ASM_EXTABLE_REFCOUNT(from, to)			\
+	_ASM_EXTABLE_HANDLE(from, to, ex_handler_refcount)
+
+/* For C file, we already have NOKPROBE_SYMBOL macro */
+#endif
 
 #ifndef __ASSEMBLY__
 /*
diff --git a/arch/x86/include/asm/bug.h b/arch/x86/include/asm/bug.h
index 5090035..6804d66 100644
--- a/arch/x86/include/asm/bug.h
+++ b/arch/x86/include/asm/bug.h
@@ -4,8 +4,6 @@
 
 #include <linux/stringify.h>
 
-#ifndef __ASSEMBLY__
-
 /*
  * Despite that some emulators terminate on UD2, we use it for WARN().
  *
@@ -22,15 +20,53 @@
 
 #define LEN_UD2		2
 
+#ifdef CONFIG_GENERIC_BUG
+
+#ifdef CONFIG_X86_32
+# define __BUG_REL(val)	".long " __stringify(val)
+#else
+# define __BUG_REL(val)	".long " __stringify(val) " - 2b"
+#endif
+
+#ifdef CONFIG_DEBUG_BUGVERBOSE
+
+#define _BUG_FLAGS(ins, flags)						\
+do {									\
+	asm volatile("1:\t" ins "\n"					\
+		     ".pushsection __bug_table,\"aw\"\n"		\
+		     "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"	\
+		     "\t"  __BUG_REL(%c0) "\t# bug_entry::file\n"	\
+		     "\t.word %c1"        "\t# bug_entry::line\n"	\
+		     "\t.word %c2"        "\t# bug_entry::flags\n"	\
+		     "\t.org 2b+%c3\n"					\
+		     ".popsection"					\
+		     : : "i" (__FILE__), "i" (__LINE__),		\
+			 "i" (flags),					\
+			 "i" (sizeof(struct bug_entry)));		\
+} while (0)
+
+#else /* !CONFIG_DEBUG_BUGVERBOSE */
+
 #define _BUG_FLAGS(ins, flags)						\
 do {									\
-	asm volatile("ASM_BUG ins=\"" ins "\" file=%c0 line=%c1 "	\
-		     "flags=%c2 size=%c3"				\
-		     : : "i" (__FILE__), "i" (__LINE__),                \
-			 "i" (flags),                                   \
+	asm volatile("1:\t" ins "\n"					\
+		     ".pushsection __bug_table,\"aw\"\n"		\
+		     "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"	\
+		     "\t.word %c0"        "\t# bug_entry::flags\n"	\
+		     "\t.org 2b+%c1\n"					\
+		     ".popsection"					\
+		     : : "i" (flags),					\
 			 "i" (sizeof(struct bug_entry)));		\
 } while (0)
 
+#endif /* CONFIG_DEBUG_BUGVERBOSE */
+
+#else
+
+#define _BUG_FLAGS(ins, flags)  asm volatile(ins)
+
+#endif /* CONFIG_GENERIC_BUG */
+
 #define HAVE_ARCH_BUG
 #define BUG()							\
 do {								\
@@ -46,54 +82,4 @@ do {								\
 
 #include <asm-generic/bug.h>
 
-#else /* __ASSEMBLY__ */
-
-#ifdef CONFIG_GENERIC_BUG
-
-#ifdef CONFIG_X86_32
-.macro __BUG_REL val:req
-	.long \val
-.endm
-#else
-.macro __BUG_REL val:req
-	.long \val - 2b
-.endm
-#endif
-
-#ifdef CONFIG_DEBUG_BUGVERBOSE
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-1:	\ins
-	.pushsection __bug_table,"aw"
-2:	__BUG_REL val=1b	# bug_entry::bug_addr
-	__BUG_REL val=\file	# bug_entry::file
-	.word \line		# bug_entry::line
-	.word \flags		# bug_entry::flags
-	.org 2b+\size
-	.popsection
-.endm
-
-#else /* !CONFIG_DEBUG_BUGVERBOSE */
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-1:	\ins
-	.pushsection __bug_table,"aw"
-2:	__BUG_REL val=1b	# bug_entry::bug_addr
-	.word \flags		# bug_entry::flags
-	.org 2b+\size
-	.popsection
-.endm
-
-#endif /* CONFIG_DEBUG_BUGVERBOSE */
-
-#else /* CONFIG_GENERIC_BUG */
-
-.macro ASM_BUG ins:req file:req line:req flags:req size:req
-	\ins
-.endm
-
-#endif /* CONFIG_GENERIC_BUG */
-
-#endif /* __ASSEMBLY__ */
-
 #endif /* _ASM_X86_BUG_H */
diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h
index 7d44272..aced6c9 100644
--- a/arch/x86/include/asm/cpufeature.h
+++ b/arch/x86/include/asm/cpufeature.h
@@ -2,10 +2,10 @@
 #ifndef _ASM_X86_CPUFEATURE_H
 #define _ASM_X86_CPUFEATURE_H
 
-#ifdef __KERNEL__
-#ifndef __ASSEMBLY__
-
 #include <asm/processor.h>
+
+#if defined(__KERNEL__) && !defined(__ASSEMBLY__)
+
 #include <asm/asm.h>
 #include <linux/bitops.h>
 
@@ -161,10 +161,37 @@ extern void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int bit);
  */
 static __always_inline __pure bool _static_cpu_has(u16 bit)
 {
-	asm_volatile_goto("STATIC_CPU_HAS bitnum=%[bitnum] "
-			  "cap_byte=\"%[cap_byte]\" "
-			  "feature=%P[feature] t_yes=%l[t_yes] "
-			  "t_no=%l[t_no] always=%P[always]"
+	asm_volatile_goto("1: jmp 6f\n"
+		 "2:\n"
+		 ".skip -(((5f-4f) - (2b-1b)) > 0) * "
+			 "((5f-4f) - (2b-1b)),0x90\n"
+		 "3:\n"
+		 ".section .altinstructions,\"a\"\n"
+		 " .long 1b - .\n"		/* src offset */
+		 " .long 4f - .\n"		/* repl offset */
+		 " .word %P[always]\n"		/* always replace */
+		 " .byte 3b - 1b\n"		/* src len */
+		 " .byte 5f - 4f\n"		/* repl len */
+		 " .byte 3b - 2b\n"		/* pad len */
+		 ".previous\n"
+		 ".section .altinstr_replacement,\"ax\"\n"
+		 "4: jmp %l[t_no]\n"
+		 "5:\n"
+		 ".previous\n"
+		 ".section .altinstructions,\"a\"\n"
+		 " .long 1b - .\n"		/* src offset */
+		 " .long 0\n"			/* no replacement */
+		 " .word %P[feature]\n"		/* feature bit */
+		 " .byte 3b - 1b\n"		/* src len */
+		 " .byte 0\n"			/* repl len */
+		 " .byte 0\n"			/* pad len */
+		 ".previous\n"
+		 ".section .altinstr_aux,\"ax\"\n"
+		 "6:\n"
+		 " testb %[bitnum],%[cap_byte]\n"
+		 " jnz %l[t_yes]\n"
+		 " jmp %l[t_no]\n"
+		 ".previous\n"
 		 : : [feature]  "i" (bit),
 		     [always]   "i" (X86_FEATURE_ALWAYS),
 		     [bitnum]   "i" (1 << (bit & 7)),
@@ -199,44 +226,5 @@ static __always_inline __pure bool _static_cpu_has(u16 bit)
 #define CPU_FEATURE_TYPEVAL		boot_cpu_data.x86_vendor, boot_cpu_data.x86, \
 					boot_cpu_data.x86_model
 
-#else /* __ASSEMBLY__ */
-
-.macro STATIC_CPU_HAS bitnum:req cap_byte:req feature:req t_yes:req t_no:req always:req
-1:
-	jmp 6f
-2:
-	.skip -(((5f-4f) - (2b-1b)) > 0) * ((5f-4f) - (2b-1b)),0x90
-3:
-	.section .altinstructions,"a"
-	.long 1b - .		/* src offset */
-	.long 4f - .		/* repl offset */
-	.word \always		/* always replace */
-	.byte 3b - 1b		/* src len */
-	.byte 5f - 4f		/* repl len */
-	.byte 3b - 2b		/* pad len */
-	.previous
-	.section .altinstr_replacement,"ax"
-4:
-	jmp \t_no
-5:
-	.previous
-	.section .altinstructions,"a"
-	.long 1b - .		/* src offset */
-	.long 0			/* no replacement */
-	.word \feature		/* feature bit */
-	.byte 3b - 1b		/* src len */
-	.byte 0			/* repl len */
-	.byte 0			/* pad len */
-	.previous
-	.section .altinstr_aux,"ax"
-6:
-	testb \bitnum,\cap_byte
-	jnz \t_yes
-	jmp \t_no
-	.previous
-.endm
-
-#endif /* __ASSEMBLY__ */
-
-#endif /* __KERNEL__ */
+#endif /* defined(__KERNEL__) && !defined(__ASSEMBLY__) */
 #endif /* _ASM_X86_CPUFEATURE_H */
diff --git a/arch/x86/include/asm/jump_label.h b/arch/x86/include/asm/jump_label.h
index a5fb34f..cf88ebf 100644
--- a/arch/x86/include/asm/jump_label.h
+++ b/arch/x86/include/asm/jump_label.h
@@ -20,9 +20,15 @@
 
 static __always_inline bool arch_static_branch(struct static_key *key, bool branch)
 {
-	asm_volatile_goto("STATIC_BRANCH_NOP l_yes=\"%l[l_yes]\" key=\"%c0\" "
-			  "branch=\"%c1\""
-			: :  "i" (key), "i" (branch) : : l_yes);
+	asm_volatile_goto("1:"
+		".byte " __stringify(STATIC_KEY_INIT_NOP) "\n\t"
+		".pushsection __jump_table,  \"aw\" \n\t"
+		_ASM_ALIGN "\n\t"
+		".long 1b - ., %l[l_yes] - . \n\t"
+		_ASM_PTR "%c0 + %c1 - .\n\t"
+		".popsection \n\t"
+		: :  "i" (key), "i" (branch) : : l_yes);
+
 	return false;
 l_yes:
 	return true;
@@ -30,8 +36,14 @@ static __always_inline bool arch_static_branch(struct static_key *key, bool bran
 
 static __always_inline bool arch_static_branch_jump(struct static_key *key, bool branch)
 {
-	asm_volatile_goto("STATIC_BRANCH_JMP l_yes=\"%l[l_yes]\" key=\"%c0\" "
-			  "branch=\"%c1\""
+	asm_volatile_goto("1:"
+		".byte 0xe9\n\t .long %l[l_yes] - 2f\n\t"
+		"2:\n\t"
+		".pushsection __jump_table,  \"aw\" \n\t"
+		_ASM_ALIGN "\n\t"
+		".long 1b - ., %l[l_yes] - . \n\t"
+		_ASM_PTR "%c0 + %c1 - .\n\t"
+		".popsection \n\t"
 		: :  "i" (key), "i" (branch) : : l_yes);
 
 	return false;
diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h
index 26942ad..488c596 100644
--- a/arch/x86/include/asm/paravirt_types.h
+++ b/arch/x86/include/asm/paravirt_types.h
@@ -348,11 +348,23 @@ extern struct paravirt_patch_template pv_ops;
 #define paravirt_clobber(clobber)		\
 	[paravirt_clobber] "i" (clobber)
 
+/*
+ * Generate some code, and mark it as patchable by the
+ * apply_paravirt() alternate instruction patcher.
+ */
+#define _paravirt_alt(insn_string, type, clobber)	\
+	"771:\n\t" insn_string "\n" "772:\n"		\
+	".pushsection .parainstructions,\"a\"\n"	\
+	_ASM_ALIGN "\n"					\
+	_ASM_PTR " 771b\n"				\
+	"  .byte " type "\n"				\
+	"  .byte 772b-771b\n"				\
+	"  .short " clobber "\n"			\
+	".popsection\n"
+
 /* Generate patchable code, with the default asm parameters. */
-#define paravirt_call							\
-	"PARAVIRT_CALL type=\"%c[paravirt_typenum]\""			\
-	" clobber=\"%c[paravirt_clobber]\""				\
-	" pv_opptr=\"%c[paravirt_opptr]\";"
+#define paravirt_alt(insn_string)					\
+	_paravirt_alt(insn_string, "%c[paravirt_typenum]", "%c[paravirt_clobber]")
 
 /* Simple instruction patching code. */
 #define NATIVE_LABEL(a,x,b) "\n\t.globl " a #x "_" #b "\n" a #x "_" #b ":\n\t"
@@ -373,6 +385,16 @@ unsigned native_patch(u8 type, void *ibuf, unsigned long addr, unsigned len);
 int paravirt_disable_iospace(void);
 
 /*
+ * This generates an indirect call based on the operation type number.
+ * The type number, computed in PARAVIRT_PATCH, is derived from the
+ * offset into the paravirt_patch_template structure, and can therefore be
+ * freely converted back into a structure offset.
+ */
+#define PARAVIRT_CALL					\
+	ANNOTATE_RETPOLINE_SAFE				\
+	"call *%c[paravirt_opptr];"
+
+/*
  * These macros are intended to wrap calls through one of the paravirt
  * ops structs, so that they can be later identified and patched at
  * runtime.
@@ -509,7 +531,7 @@ int paravirt_disable_iospace(void);
 		/* since this condition will never hold */		\
 		if (sizeof(rettype) > sizeof(unsigned long)) {		\
 			asm volatile(pre				\
-				     paravirt_call			\
+				     paravirt_alt(PARAVIRT_CALL)	\
 				     post				\
 				     : call_clbr, ASM_CALL_CONSTRAINT	\
 				     : paravirt_type(op),		\
@@ -519,7 +541,7 @@ int paravirt_disable_iospace(void);
 			__ret = (rettype)((((u64)__edx) << 32) | __eax); \
 		} else {						\
 			asm volatile(pre				\
-				     paravirt_call			\
+				     paravirt_alt(PARAVIRT_CALL)	\
 				     post				\
 				     : call_clbr, ASM_CALL_CONSTRAINT	\
 				     : paravirt_type(op),		\
@@ -546,7 +568,7 @@ int paravirt_disable_iospace(void);
 		PVOP_VCALL_ARGS;					\
 		PVOP_TEST_NULL(op);					\
 		asm volatile(pre					\
-			     paravirt_call				\
+			     paravirt_alt(PARAVIRT_CALL)		\
 			     post					\
 			     : call_clbr, ASM_CALL_CONSTRAINT		\
 			     : paravirt_type(op),			\
@@ -664,26 +686,6 @@ struct paravirt_patch_site {
 extern struct paravirt_patch_site __parainstructions[],
 	__parainstructions_end[];
 
-#else	/* __ASSEMBLY__ */
-
-/*
- * This generates an indirect call based on the operation type number.
- * The type number, computed in PARAVIRT_PATCH, is derived from the
- * offset into the paravirt_patch_template structure, and can therefore be
- * freely converted back into a structure offset.
- */
-.macro PARAVIRT_CALL type:req clobber:req pv_opptr:req
-771:	ANNOTATE_RETPOLINE_SAFE
-	call *\pv_opptr
-772:	.pushsection .parainstructions,"a"
-	_ASM_ALIGN
-	_ASM_PTR 771b
-	.byte \type
-	.byte 772b-771b
-	.short \clobber
-	.popsection
-.endm
-
 #endif	/* __ASSEMBLY__ */
 
 #endif	/* _ASM_X86_PARAVIRT_TYPES_H */
diff --git a/arch/x86/include/asm/refcount.h b/arch/x86/include/asm/refcount.h
index a8b5e1e..dbaed55 100644
--- a/arch/x86/include/asm/refcount.h
+++ b/arch/x86/include/asm/refcount.h
@@ -4,41 +4,6 @@
  * x86-specific implementation of refcount_t. Based on PAX_REFCOUNT from
  * PaX/grsecurity.
  */
-
-#ifdef __ASSEMBLY__
-
-#include <asm/asm.h>
-#include <asm/bug.h>
-
-.macro REFCOUNT_EXCEPTION counter:req
-	.pushsection .text..refcount
-111:	lea \counter, %_ASM_CX
-112:	ud2
-	ASM_UNREACHABLE
-	.popsection
-113:	_ASM_EXTABLE_REFCOUNT(112b, 113b)
-.endm
-
-/* Trigger refcount exception if refcount result is negative. */
-.macro REFCOUNT_CHECK_LT_ZERO counter:req
-	js 111f
-	REFCOUNT_EXCEPTION counter="\counter"
-.endm
-
-/* Trigger refcount exception if refcount result is zero or negative. */
-.macro REFCOUNT_CHECK_LE_ZERO counter:req
-	jz 111f
-	REFCOUNT_CHECK_LT_ZERO counter="\counter"
-.endm
-
-/* Trigger refcount exception unconditionally. */
-.macro REFCOUNT_ERROR counter:req
-	jmp 111f
-	REFCOUNT_EXCEPTION counter="\counter"
-.endm
-
-#else /* __ASSEMBLY__ */
-
 #include <linux/refcount.h>
 #include <asm/bug.h>
 
@@ -50,12 +15,35 @@
  * central refcount exception. The fixup address for the exception points
  * back to the regular execution flow in .text.
  */
+#define _REFCOUNT_EXCEPTION				\
+	".pushsection .text..refcount\n"		\
+	"111:\tlea %[var], %%" _ASM_CX "\n"		\
+	"112:\t" ASM_UD2 "\n"				\
+	ASM_UNREACHABLE					\
+	".popsection\n"					\
+	"113:\n"					\
+	_ASM_EXTABLE_REFCOUNT(112b, 113b)
+
+/* Trigger refcount exception if refcount result is negative. */
+#define REFCOUNT_CHECK_LT_ZERO				\
+	"js 111f\n\t"					\
+	_REFCOUNT_EXCEPTION
+
+/* Trigger refcount exception if refcount result is zero or negative. */
+#define REFCOUNT_CHECK_LE_ZERO				\
+	"jz 111f\n\t"					\
+	REFCOUNT_CHECK_LT_ZERO
+
+/* Trigger refcount exception unconditionally. */
+#define REFCOUNT_ERROR					\
+	"jmp 111f\n\t"					\
+	_REFCOUNT_EXCEPTION
 
 static __always_inline void refcount_add(unsigned int i, refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "addl %1,%0\n\t"
-		"REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LT_ZERO
+		: [var] "+m" (r->refs.counter)
 		: "ir" (i)
 		: "cc", "cx");
 }
@@ -63,32 +51,31 @@ static __always_inline void refcount_add(unsigned int i, refcount_t *r)
 static __always_inline void refcount_inc(refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "incl %0\n\t"
-		"REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LT_ZERO
+		: [var] "+m" (r->refs.counter)
 		: : "cc", "cx");
 }
 
 static __always_inline void refcount_dec(refcount_t *r)
 {
 	asm volatile(LOCK_PREFIX "decl %0\n\t"
-		"REFCOUNT_CHECK_LE_ZERO counter=\"%[counter]\""
-		: [counter] "+m" (r->refs.counter)
+		REFCOUNT_CHECK_LE_ZERO
+		: [var] "+m" (r->refs.counter)
 		: : "cc", "cx");
 }
 
 static __always_inline __must_check
 bool refcount_sub_and_test(unsigned int i, refcount_t *r)
 {
-
 	return GEN_BINARY_SUFFIXED_RMWcc(LOCK_PREFIX "subl",
-					 "REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
+					 REFCOUNT_CHECK_LT_ZERO,
 					 r->refs.counter, e, "er", i, "cx");
 }
 
 static __always_inline __must_check bool refcount_dec_and_test(refcount_t *r)
 {
 	return GEN_UNARY_SUFFIXED_RMWcc(LOCK_PREFIX "decl",
-					"REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
+					REFCOUNT_CHECK_LT_ZERO,
 					r->refs.counter, e, "cx");
 }
 
@@ -106,8 +93,8 @@ bool refcount_add_not_zero(unsigned int i, refcount_t *r)
 
 		/* Did we try to increment from/to an undesirable state? */
 		if (unlikely(c < 0 || c == INT_MAX || result < c)) {
-			asm volatile("REFCOUNT_ERROR counter=\"%[counter]\""
-				     : : [counter] "m" (r->refs.counter)
+			asm volatile(REFCOUNT_ERROR
+				     : : [var] "m" (r->refs.counter)
 				     : "cc", "cx");
 			break;
 		}
@@ -122,6 +109,4 @@ static __always_inline __must_check bool refcount_inc_not_zero(refcount_t *r)
 	return refcount_add_not_zero(1, r);
 }
 
-#endif /* __ASSEMBLY__ */
-
 #endif
diff --git a/arch/x86/kernel/macros.S b/arch/x86/kernel/macros.S
deleted file mode 100644
index 161c950..0000000
--- a/arch/x86/kernel/macros.S
+++ /dev/null
@@ -1,16 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-
-/*
- * This file includes headers whose assembly part includes macros which are
- * commonly used. The macros are precompiled into assmebly file which is later
- * assembled together with each compiled file.
- */
-
-#include <linux/compiler.h>
-#include <asm/refcount.h>
-#include <asm/alternative-asm.h>
-#include <asm/bug.h>
-#include <asm/paravirt.h>
-#include <asm/asm.h>
-#include <asm/cpufeature.h>
-#include <asm/jump_label.h>
diff --git a/include/asm-generic/bug.h b/include/asm-generic/bug.h
index cdafa5e..20561a6 100644
--- a/include/asm-generic/bug.h
+++ b/include/asm-generic/bug.h
@@ -17,8 +17,10 @@
 #ifndef __ASSEMBLY__
 #include <linux/kernel.h>
 
-struct bug_entry {
+#ifdef CONFIG_BUG
+
 #ifdef CONFIG_GENERIC_BUG
+struct bug_entry {
 #ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS
 	unsigned long	bug_addr;
 #else
@@ -33,10 +35,8 @@ struct bug_entry {
 	unsigned short	line;
 #endif
 	unsigned short	flags;
-#endif	/* CONFIG_GENERIC_BUG */
 };
-
-#ifdef CONFIG_BUG
+#endif	/* CONFIG_GENERIC_BUG */
 
 /*
  * Don't use BUG() or BUG_ON() unless there's really no way out; one
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 06396c1..fc5004a 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -99,13 +99,22 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
  * unique, to convince GCC not to merge duplicate inline asm statements.
  */
 #define annotate_reachable() ({						\
-	asm volatile("ANNOTATE_REACHABLE counter=%c0"			\
-		     : : "i" (__COUNTER__));				\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.reachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
 #define annotate_unreachable() ({					\
-	asm volatile("ANNOTATE_UNREACHABLE counter=%c0"			\
-		     : : "i" (__COUNTER__));				\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.unreachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
+#define ASM_UNREACHABLE							\
+	"999:\n\t"							\
+	".pushsection .discard.unreachable\n\t"				\
+	".long 999b - .\n\t"						\
+	".popsection\n\t"
 #else
 #define annotate_reachable()
 #define annotate_unreachable()
@@ -293,45 +302,6 @@ static inline void *offset_to_ptr(const int *off)
 	return (void *)((unsigned long)off + *off);
 }
 
-#else /* __ASSEMBLY__ */
-
-#ifdef __KERNEL__
-#ifndef LINKER_SCRIPT
-
-#ifdef CONFIG_STACK_VALIDATION
-.macro ANNOTATE_UNREACHABLE counter:req
-\counter:
-	.pushsection .discard.unreachable
-	.long \counter\()b -.
-	.popsection
-.endm
-
-.macro ANNOTATE_REACHABLE counter:req
-\counter:
-	.pushsection .discard.reachable
-	.long \counter\()b -.
-	.popsection
-.endm
-
-.macro ASM_UNREACHABLE
-999:
-	.pushsection .discard.unreachable
-	.long 999b - .
-	.popsection
-.endm
-#else /* CONFIG_STACK_VALIDATION */
-.macro ANNOTATE_UNREACHABLE counter:req
-.endm
-
-.macro ANNOTATE_REACHABLE counter:req
-.endm
-
-.macro ASM_UNREACHABLE
-.endm
-#endif /* CONFIG_STACK_VALIDATION */
-
-#endif /* LINKER_SCRIPT */
-#endif /* __KERNEL__ */
 #endif /* __ASSEMBLY__ */
 
 /* Compile time object size, -1 for unknown */
diff --git a/scripts/Kbuild.include b/scripts/Kbuild.include
index bb01555..3d09844 100644
--- a/scripts/Kbuild.include
+++ b/scripts/Kbuild.include
@@ -115,9 +115,7 @@ __cc-option = $(call try-run,\
 
 # Do not attempt to build with gcc plugins during cc-option tests.
 # (And this uses delayed resolution so the flags will be up to date.)
-# In addition, do not include the asm macros which are built later.
-CC_OPTION_FILTERED = $(GCC_PLUGINS_CFLAGS) $(ASM_MACRO_FLAGS)
-CC_OPTION_CFLAGS = $(filter-out $(CC_OPTION_FILTERED),$(KBUILD_CFLAGS))
+CC_OPTION_CFLAGS = $(filter-out $(GCC_PLUGINS_CFLAGS),$(KBUILD_CFLAGS))
 
 # cc-option
 # Usage: cflags-y += $(call cc-option,-march=winchip-c6,-march=i586)
diff --git a/scripts/mod/Makefile b/scripts/mod/Makefile
index a5b4af4..42c5d50 100644
--- a/scripts/mod/Makefile
+++ b/scripts/mod/Makefile
@@ -4,8 +4,6 @@ OBJECT_FILES_NON_STANDARD := y
 hostprogs-y	:= modpost mk_elfconfig
 always		:= $(hostprogs-y) empty.o
 
-CFLAGS_REMOVE_empty.o := $(ASM_MACRO_FLAGS)
-
 modpost-objs	:= modpost.o file2alias.o sumversion.o
 
 devicetable-offsets-file := devicetable-offsets.h
-- 
2.7.4

================================================================================

From: Nadav Amit <namit () vmware ! com>
To: linux-kernel
Subject: Re: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Sun, 16 Dec 2018 03:28:23 +0000
Message-ID: <9691ECB6-D659-4E88-B8AF-4947B9431AF1 () vmware ! com>
--------------------
PiBPbiBEZWMgMTUsIDIwMTgsIGF0IDY6NTAgUE0sIE1hc2FoaXJvIFlhbWFkYSA8eWFtYWRhLm1h
c2FoaXJvQHNvY2lvbmV4dC5jb20+IHdyb3RlOg0KPiANCj4gUmV2ZXJ0IHRoZSBmb2xsb3dpbmcg
OSBjb21taXRzOg0KPiANCj4gWzFdIDViZGNkNTEwYzJhYyAoIng4Ni9qdW1wLWxhYmVsczogTWFj
cm9meSBpbmxpbmUgYXNzZW1ibHkgY29kZSB0bw0KPiAgICB3b3JrIGFyb3VuZCBHQ0MgaW5saW5p
bmcgYnVncyIpDQo+IA0KPiAgICBUaGlzIHdhcyBwYXJ0aWFsbHkgcmV2ZXJ0ZWQgYmVjYXVzZSBp
dCBtYWRlIGdvb2QgY2xlYW51cHMNCj4gICAgaXJyZXNwZWN0aXZlIG9mIHRoZSBpbmxpbmluZyBp
c3N1ZTsgdGhlIGVycm9yIG1lc3NhZ2UgaXMgc3RpbGwNCj4gICAgdW5uZWVkZWQsIGFuZCB0aGUg
Y29udmVyc2lvbiB0byBTVEFUSUNfQlJBTkNIX3tOT1AsSlVNUH0gc2hvdWxkDQo+ICAgIGJlIGtl
cHQuDQo+IA0KPiBbMl0gZDVhNTgxZDg0YWU2ICgieDg2L2NwdWZlYXR1cmU6IE1hY3JvZnkgaW5s
aW5lIGFzc2VtYmx5IGNvZGUgdG8NCj4gICAgd29yayBhcm91bmQgR0NDIGlubGluaW5nIGJ1Z3Mi
KQ0KPiANCj4gWzNdIDA0NzRkNWQ5ZDJmNyAoIng4Ni9leHRhYmxlOiBNYWNyb2Z5IGlubGluZSBh
c3NlbWJseSBjb2RlIHRvIHdvcmsNCj4gICAgYXJvdW5kIEdDQyBpbmxpbmluZyBidWdzIikNCj4g
DQo+IFs0XSA0OTRiNTE2OGYyZGUgKCJ4ODYvcGFyYXZpcnQ6IFdvcmsgYXJvdW5kIEdDQyBpbmxp
bmluZyBidWdzIHdoZW4NCj4gICAgY29tcGlsaW5nIHBhcmF2aXJ0IG9wcyIpDQo+IA0KPiBbNV0g
ZjgxZjhhZDU2ZmQxICgieDg2L2J1ZzogTWFjcm9meSB0aGUgQlVHIHRhYmxlIHNlY3Rpb24gaGFu
ZGxpbmcsDQo+ICAgIHRvIHdvcmsgYXJvdW5kIEdDQyBpbmxpbmluZyBidWdzIikNCj4gDQo+IFs2
XSA3N2Y0OGVjMjhlNGMgKCJ4ODYvYWx0ZXJuYXRpdmVzOiBNYWNyb2Z5IGxvY2sgcHJlZml4ZXMg
dG8gd29yaw0KPiAgIGFyb3VuZCBHQ0MgaW5saW5pbmcgYnVncyIpDQo+IA0KPiBbN10gOWUxNzI1
YjQxMDU5ICgieDg2L3JlZmNvdW50OiBXb3JrIGFyb3VuZCBHQ0MgaW5saW5pbmcgYnVnIikNCj4g
DQo+ICAgIFJlc29sdmVkIGNvbmZsaWN0cyBpbiBhcmNoL3g4Ni9pbmNsdWRlL2FzbS9yZWZjb3Vu
dC5oIGNhdXNlZCBieQ0KPiAgICAyODhlNDUyMWYwZjYgKCJ4ODYvYXNtOiAnU2ltcGxpZnknIEdF
Tl8qX1JNV2NjKCkgbWFjcm9zIikuDQo+IA0KPiBbOF0gYzA2YzRkODA5MDUxICgieDg2L29ianRv
b2w6IFVzZSBhc20gbWFjcm9zIHRvIHdvcmsgYXJvdW5kIEdDQw0KPiAgICBpbmxpbmluZyBidWdz
IikNCj4gDQo+IFs5XSA3N2IwYmY1NWJjNjcgKCJrYnVpbGQvTWFrZWZpbGU6IFByZXBhcmUgZm9y
IHVzaW5nIG1hY3JvcyBpbiBpbmxpbmUNCj4gICAgYXNzZW1ibHkgY29kZSB0byB3b3JrIGFyb3Vu
ZCBhc20oKSByZWxhdGVkIEdDQyBpbmxpbmluZyBidWdzIikNCj4gDQo+IEEgZmV3IGRheXMgYWZ0
ZXIgdGhvc2UgY29tbWl0cyBhcHBsaWVkLCBkaXNjdXNzaW9uIHN0YXJ0ZWQgdG8gc29sdmUNCj4g
dGhlIGlzc3VlIG1vcmUgZWxlZ2FudGx5IHdpdGggdGhlIGhlbHAgb2YgY29tcGlsZXI6DQo+IA0K
PiAgaHR0cHM6Ly9uYTAxLnNhZmVsaW5rcy5wcm90ZWN0aW9uLm91dGxvb2suY29tLz91cmw9aHR0
cHMlM0ElMkYlMkZsa21sLm9yZyUyRmxrbWwlMkYyMDE4JTJGMTAlMkY3JTJGOTImYW1wO2RhdGE9
MDIlN0MwMSU3Q25hbWl0JTQwdm13YXJlLmNvbSU3Q2U4OTNjZTg4MDY1ZTRjNTkyMzYzMDhkNjYz
MDE5NDI0JTdDYjM5MTM4Y2EzY2VlNGI0YWE0ZDZjZDgzZDlkZDYyZjAlN0MwJTdDMCU3QzYzNjgw
NTI1NTc4NzYwNzE3OCZhbXA7c2RhdGE9bWlpVW5kbVBmR05LdnJ6RDVtdHRDMSUyQm42ck5hb0lG
ZWJqWk9Ba0JyMjRZJTNEJmFtcDtyZXNlcnZlZD0wDQo+IA0KPiBUaGUgbmV3IHN5bnRheCAiYXNt
IGlubGluZSIgd2FzIGltcGxlbWVudGVkIGJ5IFNlZ2hlciBCb2Vzc2Vua29vbCwgYW5kDQo+IG5v
dyBxdWV1ZWQgdXAgZm9yIEdDQyA5LiAoUGVvcGxlIHdlcmUgcG9zaXRpdmUgZXZlbiBmb3IgYmFj
ay1wb3J0aW5nIGl0DQo+IHRvIG9sZGVyIGNvbXBpbGVycykuDQo+IA0KPiBTaW5jZSB0aGUgaW4t
a2VybmVsIHdvcmthcm91bmRzIG1lcmdlZCwgc29tZSBpc3N1ZXMgaGF2ZSBiZWVuIHJlcG9ydGVk
Og0KPiBicmVha2FnZSBvZiBidWlsZGluZyB3aXRoIGRpc3RjYy9pY2VjYywgYnJlYWthZ2Ugb2Yg
ZGlzdHJvIHBhY2thZ2VzIGZvcg0KPiBtb2R1bGUgYnVpbGRpbmcuIChNb3JlIGZ1bmRhbWVudGFs
bHksIHdlIGNhbm5vdCBidWlsZCBleHRlcm5hbCBtb2R1bGVzDQo+IGFmdGVyICdtYWtlIGNsZWFu
Jy4pDQo+IA0KPiBJIGRvIG5vdCB3YW50IHRvIG1lc3MgdXAgdGhlIGJ1aWxkIHN5c3RlbSBhbnkg
bW9yZS4NCj4gDQo+IEdpdmVuIHRoYXQgdGhpcyBpc3N1ZSB3aWxsIGJlIHNvbHZlZCBpbiBhIGNs
ZWFuZXIgd2F5IHNvb25lciBvciBsYXRlciwNCj4gbGV0J3MgcmV2ZXJ0IHRoZSBpbi1rZXJuZWwg
d29ya2Fyb3VuZHMsIGFuZCB3YWl0IGZvciBHQ0MgOS4NCj4gDQo+IFJlcG9ydGVkLWJ5OiBMb2dh
biBHdW50aG9ycGUgPGxvZ2FuZ0BkZWx0YXRlZS5jb20+ICMgZGlzdGNjDQo+IFJlcG9ydGVkLWJ5
OiBTZWRhdCBEaWxlayA8c2VkYXQuZGlsZWtAZ21haWwuY29tPiAjIGRlYi9ycG0gcGFja2FnZQ0K
DQpJdCBpcyBjdXN0b21hcnkgdG8gY2MgdGhvc2Ugd2hvIHJlcG9ydCBhbiBpc3N1ZS4NCg0KVGhl
IGRpc3RjYyBpc3N1ZSBoYXMgYWxyZWFkeSBiZWVuIHJlc29sdmVkIGJvdGggaW4gZGlzdGNjIGFu
ZCBpbiB0aGUgcGF0Y2hlcw0KSeKAmXZlIHNlbnQ6IGh0dHBzOi8vbGttbC5vcmcvbGttbC8yMDE4
LzExLzE1LzQ2NyAuIFNvIEkgY2Fubm90IHVuZGVyc3RhbmQgd2h5DQppdCBpcyBtZW50aW9uZWQg
YXMgYSBtb3RpdmF0aW9uLg0KDQpJdCBzb3VuZHMgdGhhdCB0aGUgZXh0ZXJuYWwgbW9kdWxlcyBj
YW4gZWFzaWx5IGJlIHJlc29sdmVkLiBDYW4geW91IHBsZWFzZQ0KcHJvdmlkZSBhIGxpbmsgZm9y
IHRoZSBidWcgcmVwb3J0Pw0KDQpQbGVhc2UgcmVnYXJkIG15IGNvbW1lbnRzIHJlZ2FyZGluZyB2
MS4gSSBtdXN0IGFkbWl0IHRoYXQgSeKAmW0gdmVyeSBzdXJwcmlzZWQNCnRoYXQgeW91IGRvbuKA
mXQgbGlrZSB0aGUgcGF0Y2hlcyBzaW5jZSB5b3UgYWNr4oCZZCB0aGUgb3JpZ2luYWwgcGF0Y2gt
c2V0IChhbmQNCmFjdHVhbGx5IGFzc2lzdGVkIG1lIGluIGNoYW5naW5nIHRoZSBNYWtlZmlsZSku
DQoNCg==
================================================================================

From: Nadav Amit <namit () vmware ! com>
To: linux-arch
Subject: Re: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Sun, 16 Dec 2018 03:28:23 +0000
Message-ID: <9691ECB6-D659-4E88-B8AF-4947B9431AF1 () vmware ! com>
--------------------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================================================================================

From: Nadav Amit <namit () vmware ! com>
To: linux-sparse
Subject: Re: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Sun, 16 Dec 2018 03:28:23 +0000
Message-ID: <9691ECB6-D659-4E88-B8AF-4947B9431AF1 () vmware ! com>
--------------------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================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kernel
Subject: Re: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Mon, 17 Dec 2018 02:54:13 +0000
Message-ID: <CAK7LNARcSYvc-3xhdwFMk=q20LYW2i1ahscofYsYka2hg0Oszw () mail ! gmail ! com>
--------------------
On Sun, Dec 16, 2018 at 12:29 PM Nadav Amit <namit@vmware.com> wrote:
>
> > On Dec 15, 2018, at 6:50 PM, Masahiro Yamada <yamada.masahiro@socionext=
.com> wrote:
> >
> > Revert the following 9 commits:
> >
> > [1] 5bdcd510c2ac ("x86/jump-labels: Macrofy inline assembly code to
> >    work around GCC inlining bugs")
> >
> >    This was partially reverted because it made good cleanups
> >    irrespective of the inlining issue; the error message is still
> >    unneeded, and the conversion to STATIC_BRANCH_{NOP,JUMP} should
> >    be kept.
> >
> > [2] d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to
> >    work around GCC inlining bugs")
> >
> > [3] 0474d5d9d2f7 ("x86/extable: Macrofy inline assembly code to work
> >    around GCC inlining bugs")
> >
> > [4] 494b5168f2de ("x86/paravirt: Work around GCC inlining bugs when
> >    compiling paravirt ops")
> >
> > [5] f81f8ad56fd1 ("x86/bug: Macrofy the BUG table section handling,
> >    to work around GCC inlining bugs")
> >
> > [6] 77f48ec28e4c ("x86/alternatives: Macrofy lock prefixes to work
> >   around GCC inlining bugs")
> >
> > [7] 9e1725b41059 ("x86/refcount: Work around GCC inlining bug")
> >
> >    Resolved conflicts in arch/x86/include/asm/refcount.h caused by
> >    288e4521f0f6 ("x86/asm: 'Simplify' GEN_*_RMWcc() macros").
> >
> > [8] c06c4d809051 ("x86/objtool: Use asm macros to work around GCC
> >    inlining bugs")
> >
> > [9] 77b0bf55bc67 ("kbuild/Makefile: Prepare for using macros in inline
> >    assembly code to work around asm() related GCC inlining bugs")
> >
> > A few days after those commits applied, discussion started to solve
> > the issue more elegantly with the help of compiler:
> >
> >  https://na01.safelinks.protection.outlook.com/?url=3Dhttps%3A%2F%2Flkm=
l.org%2Flkml%2F2018%2F10%2F7%2F92&amp;data=3D02%7C01%7Cnamit%40vmware.com%7=
Ce893ce88065e4c59236308d663019424%7Cb39138ca3cee4b4aa4d6cd83d9dd62f0%7C0%7C=
0%7C636805255787607178&amp;sdata=3DmiiUndmPfGNKvrzD5mttC1%2Bn6rNaoIFebjZOAk=
Br24Y%3D&amp;reserved=3D0
> >
> > The new syntax "asm inline" was implemented by Segher Boessenkool, and
> > now queued up for GCC 9. (People were positive even for back-porting it
> > to older compilers).
> >
> > Since the in-kernel workarounds merged, some issues have been reported:
> > breakage of building with distcc/icecc, breakage of distro packages for
> > module building. (More fundamentally, we cannot build external modules
> > after 'make clean'.)
> >
> > I do not want to mess up the build system any more.
> >
> > Given that this issue will be solved in a cleaner way sooner or later,
> > let's revert the in-kernel workarounds, and wait for GCC 9.
> >
> > Reported-by: Logan Gunthorpe <logang@deltatee.com> # distcc
> > Reported-by: Sedat Dilek <sedat.dilek@gmail.com> # deb/rpm package
>
> It is customary to cc those who report an issue.

OK.

> The distcc issue has already been resolved both in distcc

Precisely, the fix-up was submitted,
but not pulled yet as of writing.
https://github.com/distcc/distcc/pull/313


> and in the patches
> I=E2=80=99ve sent: https://lkml.org/lkml/2018/11/15/467 .

I was scared by this ugly fix-up, so I rejected it.



> So I cannot understand why
> it is mentioned as a motivation.
>
> It sounds that the external modules can easily be resolved. Can you pleas=
e
> provide a link for the bug report?


https://www.spinics.net/lists/linux-kbuild/msg20037.html

We can fix it under circumstances "we can do anything"
although I am scared by endless Makefile hacks.



> Please regard my comments regarding v1.

I will try my best, although I felt some of your requests were too much.
I am not an x86 developer.


I posted this so people can play with 'asm inline'
https://lore.kernel.org/patchwork/patch/1024590/

You can confirm vmlinux size is increased,
and some symbols disappears.



> I must admit that I=E2=80=99m very surprised
> that you don=E2=80=99t like the patches since you ack=E2=80=99d the origi=
nal patch-set


I think ack and "I like it" are different.

There are situations where we had to accept something reluctantly.


Without my ack, your patch series would not have been
merged via x86 tree.

There was no other solution at that time.
Also, I could not predict potential problems, which turned out later.

So I let it go.

It would have been better if
the following discussion had stared earlier.
https://lkml.org/lkml/2018/10/7/92


Now, we got a much cleaner solution.

I believe we should replace the workarounds with it.



> (and
> actually assisted me in changing the Makefile).

You were clearly breaking the build system.
So, I provided a less ugly solution.

Again, the in-kernel hack was the only solution at that time,
but the situation has changed then.



--=20
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-arch
Subject: Re: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Mon, 17 Dec 2018 02:54:13 +0000
Message-ID: <CAK7LNARcSYvc-3xhdwFMk=q20LYW2i1ahscofYsYka2hg0Oszw () mail ! gmail ! com>
--------------------
On Sun, Dec 16, 2018 at 12:29 PM Nadav Amit <namit@vmware.com> wrote:
>
> > On Dec 15, 2018, at 6:50 PM, Masahiro Yamada <yamada.masahiro@socionext=
.com> wrote:
> >
> > Revert the following 9 commits:
> >
> > [1] 5bdcd510c2ac ("x86/jump-labels: Macrofy inline assembly code to
> >    work around GCC inlining bugs")
> >
> >    This was partially reverted because it made good cleanups
> >    irrespective of the inlining issue; the error message is still
> >    unneeded, and the conversion to STATIC_BRANCH_{NOP,JUMP} should
> >    be kept.
> >
> > [2] d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to
> >    work around GCC inlining bugs")
> >
> > [3] 0474d5d9d2f7 ("x86/extable: Macrofy inline assembly code to work
> >    around GCC inlining bugs")
> >
> > [4] 494b5168f2de ("x86/paravirt: Work around GCC inlining bugs when
> >    compiling paravirt ops")
> >
> > [5] f81f8ad56fd1 ("x86/bug: Macrofy the BUG table section handling,
> >    to work around GCC inlining bugs")
> >
> > [6] 77f48ec28e4c ("x86/alternatives: Macrofy lock prefixes to work
> >   around GCC inlining bugs")
> >
> > [7] 9e1725b41059 ("x86/refcount: Work around GCC inlining bug")
> >
> >    Resolved conflicts in arch/x86/include/asm/refcount.h caused by
> >    288e4521f0f6 ("x86/asm: 'Simplify' GEN_*_RMWcc() macros").
> >
> > [8] c06c4d809051 ("x86/objtool: Use asm macros to work around GCC
> >    inlining bugs")
> >
> > [9] 77b0bf55bc67 ("kbuild/Makefile: Prepare for using macros in inline
> >    assembly code to work around asm() related GCC inlining bugs")
> >
> > A few days after those commits applied, discussion started to solve
> > the issue more elegantly with the help of compiler:
> >
> >  https://na01.safelinks.protection.outlook.com/?url=3Dhttps%3A%2F%2Flkm=
l.org%2Flkml%2F2018%2F10%2F7%2F92&amp;data=3D02%7C01%7Cnamit%40vmware.com%7=
Ce893ce88065e4c59236308d663019424%7Cb39138ca3cee4b4aa4d6cd83d9dd62f0%7C0%7C=
0%7C636805255787607178&amp;sdata=3DmiiUndmPfGNKvrzD5mttC1%2Bn6rNaoIFebjZOAk=
Br24Y%3D&amp;reserved=3D0
> >
> > The new syntax "asm inline" was implemented by Segher Boessenkool, and
> > now queued up for GCC 9. (People were positive even for back-porting it
> > to older compilers).
> >
> > Since the in-kernel workarounds merged, some issues have been reported:
> > breakage of building with distcc/icecc, breakage of distro packages for
> > module building. (More fundamentally, we cannot build external modules
> > after 'make clean'.)
> >
> > I do not want to mess up the build system any more.
> >
> > Given that this issue will be solved in a cleaner way sooner or later,
> > let's revert the in-kernel workarounds, and wait for GCC 9.
> >
> > Reported-by: Logan Gunthorpe <logang@deltatee.com> # distcc
> > Reported-by: Sedat Dilek <sedat.dilek@gmail.com> # deb/rpm package
>
> It is customary to cc those who report an issue.

OK.

> The distcc issue has already been resolved both in distcc

Precisely, the fix-up was submitted,
but not pulled yet as of writing.
https://github.com/distcc/distcc/pull/313


> and in the patches
> I=E2=80=99ve sent: https://lkml.org/lkml/2018/11/15/467 .

I was scared by this ugly fix-up, so I rejected it.



> So I cannot understand why
> it is mentioned as a motivation.
>
> It sounds that the external modules can easily be resolved. Can you pleas=
e
> provide a link for the bug report?


https://www.spinics.net/lists/linux-kbuild/msg20037.html

We can fix it under circumstances "we can do anything"
although I am scared by endless Makefile hacks.



> Please regard my comments regarding v1.

I will try my best, although I felt some of your requests were too much.
I am not an x86 developer.


I posted this so people can play with 'asm inline'
https://lore.kernel.org/patchwork/patch/1024590/

You can confirm vmlinux size is increased,
and some symbols disappears.



> I must admit that I=E2=80=99m very surprised
> that you don=E2=80=99t like the patches since you ack=E2=80=99d the origi=
nal patch-set


I think ack and "I like it" are different.

There are situations where we had to accept something reluctantly.


Without my ack, your patch series would not have been
merged via x86 tree.

There was no other solution at that time.
Also, I could not predict potential problems, which turned out later.

So I let it go.

It would have been better if
the following discussion had stared earlier.
https://lkml.org/lkml/2018/10/7/92


Now, we got a much cleaner solution.

I believe we should replace the workarounds with it.



> (and
> actually assisted me in changing the Makefile).

You were clearly breaking the build system.
So, I provided a less ugly solution.

Again, the in-kernel hack was the only solution at that time,
but the situation has changed then.



--=20
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-virtualization
Subject: Re: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Mon, 17 Dec 2018 02:54:13 +0000
Message-ID: <CAK7LNARcSYvc-3xhdwFMk=q20LYW2i1ahscofYsYka2hg0Oszw () mail ! gmail ! com>
--------------------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================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kbuild
Subject: Re: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Mon, 17 Dec 2018 02:54:13 +0000
Message-ID: <CAK7LNARcSYvc-3xhdwFMk=q20LYW2i1ahscofYsYka2hg0Oszw () mail ! gmail ! com>
--------------------
On Sun, Dec 16, 2018 at 12:29 PM Nadav Amit <namit@vmware.com> wrote:
>
> > On Dec 15, 2018, at 6:50 PM, Masahiro Yamada <yamada.masahiro@socionext=
.com> wrote:
> >
> > Revert the following 9 commits:
> >
> > [1] 5bdcd510c2ac ("x86/jump-labels: Macrofy inline assembly code to
> >    work around GCC inlining bugs")
> >
> >    This was partially reverted because it made good cleanups
> >    irrespective of the inlining issue; the error message is still
> >    unneeded, and the conversion to STATIC_BRANCH_{NOP,JUMP} should
> >    be kept.
> >
> > [2] d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to
> >    work around GCC inlining bugs")
> >
> > [3] 0474d5d9d2f7 ("x86/extable: Macrofy inline assembly code to work
> >    around GCC inlining bugs")
> >
> > [4] 494b5168f2de ("x86/paravirt: Work around GCC inlining bugs when
> >    compiling paravirt ops")
> >
> > [5] f81f8ad56fd1 ("x86/bug: Macrofy the BUG table section handling,
> >    to work around GCC inlining bugs")
> >
> > [6] 77f48ec28e4c ("x86/alternatives: Macrofy lock prefixes to work
> >   around GCC inlining bugs")
> >
> > [7] 9e1725b41059 ("x86/refcount: Work around GCC inlining bug")
> >
> >    Resolved conflicts in arch/x86/include/asm/refcount.h caused by
> >    288e4521f0f6 ("x86/asm: 'Simplify' GEN_*_RMWcc() macros").
> >
> > [8] c06c4d809051 ("x86/objtool: Use asm macros to work around GCC
> >    inlining bugs")
> >
> > [9] 77b0bf55bc67 ("kbuild/Makefile: Prepare for using macros in inline
> >    assembly code to work around asm() related GCC inlining bugs")
> >
> > A few days after those commits applied, discussion started to solve
> > the issue more elegantly with the help of compiler:
> >
> >  https://na01.safelinks.protection.outlook.com/?url=3Dhttps%3A%2F%2Flkm=
l.org%2Flkml%2F2018%2F10%2F7%2F92&amp;data=3D02%7C01%7Cnamit%40vmware.com%7=
Ce893ce88065e4c59236308d663019424%7Cb39138ca3cee4b4aa4d6cd83d9dd62f0%7C0%7C=
0%7C636805255787607178&amp;sdata=3DmiiUndmPfGNKvrzD5mttC1%2Bn6rNaoIFebjZOAk=
Br24Y%3D&amp;reserved=3D0
> >
> > The new syntax "asm inline" was implemented by Segher Boessenkool, and
> > now queued up for GCC 9. (People were positive even for back-porting it
> > to older compilers).
> >
> > Since the in-kernel workarounds merged, some issues have been reported:
> > breakage of building with distcc/icecc, breakage of distro packages for
> > module building. (More fundamentally, we cannot build external modules
> > after 'make clean'.)
> >
> > I do not want to mess up the build system any more.
> >
> > Given that this issue will be solved in a cleaner way sooner or later,
> > let's revert the in-kernel workarounds, and wait for GCC 9.
> >
> > Reported-by: Logan Gunthorpe <logang@deltatee.com> # distcc
> > Reported-by: Sedat Dilek <sedat.dilek@gmail.com> # deb/rpm package
>
> It is customary to cc those who report an issue.

OK.

> The distcc issue has already been resolved both in distcc

Precisely, the fix-up was submitted,
but not pulled yet as of writing.
https://github.com/distcc/distcc/pull/313


> and in the patches
> I=E2=80=99ve sent: https://lkml.org/lkml/2018/11/15/467 .

I was scared by this ugly fix-up, so I rejected it.



> So I cannot understand why
> it is mentioned as a motivation.
>
> It sounds that the external modules can easily be resolved. Can you pleas=
e
> provide a link for the bug report?


https://www.spinics.net/lists/linux-kbuild/msg20037.html

We can fix it under circumstances "we can do anything"
although I am scared by endless Makefile hacks.



> Please regard my comments regarding v1.

I will try my best, although I felt some of your requests were too much.
I am not an x86 developer.


I posted this so people can play with 'asm inline'
https://lore.kernel.org/patchwork/patch/1024590/

You can confirm vmlinux size is increased,
and some symbols disappears.



> I must admit that I=E2=80=99m very surprised
> that you don=E2=80=99t like the patches since you ack=E2=80=99d the origi=
nal patch-set


I think ack and "I like it" are different.

There are situations where we had to accept something reluctantly.


Without my ack, your patch series would not have been
merged via x86 tree.

There was no other solution at that time.
Also, I could not predict potential problems, which turned out later.

So I let it go.

It would have been better if
the following discussion had stared earlier.
https://lkml.org/lkml/2018/10/7/92


Now, we got a much cleaner solution.

I believe we should replace the workarounds with it.



> (and
> actually assisted me in changing the Makefile).

You were clearly breaking the build system.
So, I provided a less ugly solution.

Again, the in-kernel hack was the only solution at that time,
but the situation has changed then.



--=20
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Mon, 17 Dec 2018 02:54:13 +0000
Message-ID: <CAK7LNARcSYvc-3xhdwFMk=q20LYW2i1ahscofYsYka2hg0Oszw () mail ! gmail ! com>
--------------------
On Sun, Dec 16, 2018 at 12:29 PM Nadav Amit <namit@vmware.com> wrote:
>
> > On Dec 15, 2018, at 6:50 PM, Masahiro Yamada <yamada.masahiro@socionext=
.com> wrote:
> >
> > Revert the following 9 commits:
> >
> > [1] 5bdcd510c2ac ("x86/jump-labels: Macrofy inline assembly code to
> >    work around GCC inlining bugs")
> >
> >    This was partially reverted because it made good cleanups
> >    irrespective of the inlining issue; the error message is still
> >    unneeded, and the conversion to STATIC_BRANCH_{NOP,JUMP} should
> >    be kept.
> >
> > [2] d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to
> >    work around GCC inlining bugs")
> >
> > [3] 0474d5d9d2f7 ("x86/extable: Macrofy inline assembly code to work
> >    around GCC inlining bugs")
> >
> > [4] 494b5168f2de ("x86/paravirt: Work around GCC inlining bugs when
> >    compiling paravirt ops")
> >
> > [5] f81f8ad56fd1 ("x86/bug: Macrofy the BUG table section handling,
> >    to work around GCC inlining bugs")
> >
> > [6] 77f48ec28e4c ("x86/alternatives: Macrofy lock prefixes to work
> >   around GCC inlining bugs")
> >
> > [7] 9e1725b41059 ("x86/refcount: Work around GCC inlining bug")
> >
> >    Resolved conflicts in arch/x86/include/asm/refcount.h caused by
> >    288e4521f0f6 ("x86/asm: 'Simplify' GEN_*_RMWcc() macros").
> >
> > [8] c06c4d809051 ("x86/objtool: Use asm macros to work around GCC
> >    inlining bugs")
> >
> > [9] 77b0bf55bc67 ("kbuild/Makefile: Prepare for using macros in inline
> >    assembly code to work around asm() related GCC inlining bugs")
> >
> > A few days after those commits applied, discussion started to solve
> > the issue more elegantly with the help of compiler:
> >
> >  https://na01.safelinks.protection.outlook.com/?url=3Dhttps%3A%2F%2Flkm=
l.org%2Flkml%2F2018%2F10%2F7%2F92&amp;data=3D02%7C01%7Cnamit%40vmware.com%7=
Ce893ce88065e4c59236308d663019424%7Cb39138ca3cee4b4aa4d6cd83d9dd62f0%7C0%7C=
0%7C636805255787607178&amp;sdata=3DmiiUndmPfGNKvrzD5mttC1%2Bn6rNaoIFebjZOAk=
Br24Y%3D&amp;reserved=3D0
> >
> > The new syntax "asm inline" was implemented by Segher Boessenkool, and
> > now queued up for GCC 9. (People were positive even for back-porting it
> > to older compilers).
> >
> > Since the in-kernel workarounds merged, some issues have been reported:
> > breakage of building with distcc/icecc, breakage of distro packages for
> > module building. (More fundamentally, we cannot build external modules
> > after 'make clean'.)
> >
> > I do not want to mess up the build system any more.
> >
> > Given that this issue will be solved in a cleaner way sooner or later,
> > let's revert the in-kernel workarounds, and wait for GCC 9.
> >
> > Reported-by: Logan Gunthorpe <logang@deltatee.com> # distcc
> > Reported-by: Sedat Dilek <sedat.dilek@gmail.com> # deb/rpm package
>
> It is customary to cc those who report an issue.

OK.

> The distcc issue has already been resolved both in distcc

Precisely, the fix-up was submitted,
but not pulled yet as of writing.
https://github.com/distcc/distcc/pull/313


> and in the patches
> I=E2=80=99ve sent: https://lkml.org/lkml/2018/11/15/467 .

I was scared by this ugly fix-up, so I rejected it.



> So I cannot understand why
> it is mentioned as a motivation.
>
> It sounds that the external modules can easily be resolved. Can you pleas=
e
> provide a link for the bug report?


https://www.spinics.net/lists/linux-kbuild/msg20037.html

We can fix it under circumstances "we can do anything"
although I am scared by endless Makefile hacks.



> Please regard my comments regarding v1.

I will try my best, although I felt some of your requests were too much.
I am not an x86 developer.


I posted this so people can play with 'asm inline'
https://lore.kernel.org/patchwork/patch/1024590/

You can confirm vmlinux size is increased,
and some symbols disappears.



> I must admit that I=E2=80=99m very surprised
> that you don=E2=80=99t like the patches since you ack=E2=80=99d the origi=
nal patch-set


I think ack and "I like it" are different.

There are situations where we had to accept something reluctantly.


Without my ack, your patch series would not have been
merged via x86 tree.

There was no other solution at that time.
Also, I could not predict potential problems, which turned out later.

So I let it go.

It would have been better if
the following discussion had stared earlier.
https://lkml.org/lkml/2018/10/7/92


Now, we got a much cleaner solution.

I believe we should replace the workarounds with it.



> (and
> actually assisted me in changing the Makefile).

You were clearly breaking the build system.
So, I provided a less ugly solution.

Again, the in-kernel hack was the only solution at that time,
but the situation has changed then.



--=20
Best Regards
Masahiro Yamada
================================================================================

From: Andi Kleen <ak () linux ! intel ! com>
To: linux-kbuild
Subject: Re: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Fri, 21 Dec 2018 18:44:28 +0000
Message-ID: <87woo23ek3.fsf () linux ! intel ! com>
--------------------
Masahiro Yamada <yamada.masahiro@socionext.com> writes:

> Revert the following 9 commits:

FWIW the -Wa additional also broke LTO builds because it doesn't really
support -Wa for individual files.

So I'm glad they got reverted.

-Andi
================================================================================

From: Andi Kleen <ak () linux ! intel ! com>
To: linux-sparse
Subject: Re: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Fri, 21 Dec 2018 18:44:28 +0000
Message-ID: <87woo23ek3.fsf () linux ! intel ! com>
--------------------
Masahiro Yamada <yamada.masahiro@socionext.com> writes:

> Revert the following 9 commits:

FWIW the -Wa additional also broke LTO builds because it doesn't really
support -Wa for individual files.

So I'm glad they got reverted.

-Andi
================================================================================

From: Andi Kleen <ak () linux ! intel ! com>
To: linux-arch
Subject: Re: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Fri, 21 Dec 2018 18:44:28 +0000
Message-ID: <87woo23ek3.fsf () linux ! intel ! com>
--------------------
Masahiro Yamada <yamada.masahiro@socionext.com> writes:

> Revert the following 9 commits:

FWIW the -Wa additional also broke LTO builds because it doesn't really
support -Wa for individual files.

So I'm glad they got reverted.

-Andi
================================================================================

From: Andi Kleen <ak () linux ! intel ! com>
To: linux-virtualization
Subject: Re: [PATCH v2] x86, kbuild: revert macrofying inline assembly code
Date: Fri, 21 Dec 2018 18:44:28 +0000
Message-ID: <87woo23ek3.fsf () linux ! intel ! com>
--------------------
Masahiro Yamada <yamada.masahiro@socionext.com> writes:

> Revert the following 9 commits:

FWIW the -Wa additional also broke LTO builds because it doesn't really
support -Wa for individual files.

So I'm glad they got reverted.

-Andi
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================


################################################################################

=== Thread: [PATCH v3 0/4] kernel hacking: GCC optimization for better debug experience (-Og) ===

From: Changbin Du <changbin.du () gmail ! com>
To: linux-sparse
Subject: [PATCH v3 0/4] kernel hacking: GCC optimization for better debug experience (-Og)
Date: Sun, 28 Oct 2018 13:09:41 +0000
Message-ID: <20181028130945.23581-1-changbin.du () gmail ! com>
--------------------
Hi all,
I have posted this series several months ago but interrupted by personal
affairs. Now I get time to complete this task. Thanks for all of the
reviewers.

I know some kernel developers was searching for a method to dissable GCC
optimizations, probably they want to apply GCC '-O0' option. But since Linux
kernel relies on GCC optimization to remove some dead code, so '-O0' just
breaks the build. They do need this because they want to debug kernel with
qemu, simics, kgtp or kgdb.

Thanks for the GCC '-Og' optimization level introduced in GCC 4.8, which
offers a reasonable level of optimization while maintaining fast compilation
and a good debugging experience. It is similar to '-O1' while perferring to
keep debug ability over runtime speed. With '-Og', we can build a kernel with
better debug ability and little performance drop after some simple change.

In this series, firstly introduce a new config CONFIG_NO_AUTO_INLINE after two
fixes for this new option. With this option, only functions explicitly marked
with "inline" will  be inlined. This will allow the function tracer to trace
more functions because it only traces functions that the compiler has not
inlined.

Then introduce new config CC_OPTIMIZE_FOR_DEBUGGING which apply '-Og'
optimization level for whole kernel, with a simple fix in fix_to_virt().
Currently I have only tested this option on x86 and ARM platform. Other
platforms should also work but probably need some compiling fixes as what
having done in this series. I leave that to who want to try this debug
option.

Comparison of vmlinux size: a bit smaller.

    w/o CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ size vmlinux
       text    data     bss     dec     hex filename
    22665554   9709674  2920908 35296136        21a9388 vmlinux

    w/ CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ size vmlinux
       text    data     bss     dec     hex filename
    21499032   10102758 2920908 34522698        20ec64a vmlinux


Comparison of system performance: a bit drop (~6%).
    This benchmark of kernel compilation is suggested by Ingo Molnar.
    https://lkml.org/lkml/2018/5/2/74

    Preparation: Set cpufreq to 'performance'.
    for ((cpu=0; cpu<120; cpu++)); do
      G=/sys/devices/system/cpu/cpu$cpu/cpufreq/scaling_governor
      [ -f $G ] && echo performance > $G
    done

    w/o CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ perf stat --repeat 5 --null --pre                 '\
        cp -a kernel ../kernel.copy.$(date +%s);         \
        rm -rf *;                                        \
        git checkout .;                                  \
        echo 1 > /proc/sys/vm/drop_caches;               \
        find ../kernel* -type f | xargs cat >/dev/null;  \
        make -j kernel >/dev/null;                       \
        make clean >/dev/null 2>&1;                      \
        sync                                            '\
                                                         \
        make -j8 >/dev/null

     Performance counter stats for 'make -j8' (5 runs):

        219.764246652 seconds time elapsed                   ( +-  0.78% )

    w/ CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ perf stat --repeat 5 --null --pre                 '\
        cp -a kernel ../kernel.copy.$(date +%s);         \
        rm -rf *;                                        \
        git checkout .;                                  \
        echo 1 > /proc/sys/vm/drop_caches;               \
        find ../kernel* -type f | xargs cat >/dev/null;  \
        make -j kernel >/dev/null;                       \
        make clean >/dev/null 2>&1;                      \
        sync                                            '\
                                                         \
        make -j8 >/dev/null

    Performance counter stats for 'make -j8' (5 runs):

         233.574187771 seconds time elapsed                  ( +-  0.19% )

v3:
  o Take suggestions from Masahiro Yamada.
v2:
  o rebase on top of mainline.

Changbin Du (4):
  x86/mm: declare check_la57_support() as inline
  kernel hacking: new config NO_AUTO_INLINE to disable compiler
    auto-inline optimizations
  ARM: mm: fix build error in fix_to_virt with
    CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
  kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og
    optimization

 Makefile                     | 11 +++++++++++
 arch/arm/mm/mmu.c            |  2 +-
 arch/x86/kernel/head64.c     |  2 +-
 include/linux/compiler-gcc.h |  2 +-
 include/linux/compiler.h     |  2 +-
 init/Kconfig                 | 20 ++++++++++++++++++++
 kernel/configs/tiny.config   |  1 +
 lib/Kconfig.debug            | 17 +++++++++++++++++
 8 files changed, 53 insertions(+), 4 deletions(-)

-- 
2.17.1

================================================================================


################################################################################

=== Thread: [PATCH v3 00/12] x86, kbuild: revert macrofying inline assembly code ===

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: [PATCH v3 00/12] x86, kbuild: revert macrofying inline assembly code
Date: Wed, 19 Dec 2018 14:33:59 +0000
Message-ID: <CAK7LNAQjZP8mYD02+=uvzO135fTB5GyU7_snV7ymFAjMBsep8w () mail ! gmail ! com>
--------------------
On Wed, Dec 19, 2018 at 9:44 PM Ingo Molnar <mingo@kernel.org> wrote:
>
>
> * Masahiro Yamada <yamada.masahiro@socionext.com> wrote:
>
> > This series reverts the in-kernel workarounds for inlining issues.
> >
> > The commit description of 77b0bf55bc67 mentioned
> > "We also hope that GCC will eventually get fixed,..."
> >
> > Now, GCC provides a solution.
> >
> > https://gcc.gnu.org/onlinedocs/gcc/Extended-Asm.html
> > explains the new "asm inline" syntax.
> >
> > The performance issue will be eventually solved.
> >
> > [About Code cleanups]
> >
> > I know Nadam Amit is opposed to the full revert.
> > He also claims his motivation for macrofying was not only
> > performance, but also cleanups.
> >
> > IIUC, the criticism addresses the code duplication between C and ASM.
> >
> > If so, I'd like to suggest a different approach for cleanups.
> > Please see the last 3 patches.
> > IMHO, preprocessor approach is more straight-forward, and readable.
> > Basically, this idea should work because it is what we already do for
> > __ASM_FORM() etc.
> >
> > [Quick Test of "asm inline" of GCC 9]
> >
> > If you want to try "asm inline" feature, the patch is available:
> > https://lore.kernel.org/patchwork/patch/1024590/
> >
> > The number of symbols for arch/x86/configs/x86_64_defconfig:
> >
> >                             nr_symbols
> >   [1]    v4.20-rc7       :   96502
> >   [2]    [1]+full revert :   96705   (+203)
> >   [3]    [2]+"asm inline":   96568   (-137)
> >
> > [3]: apply my patch, then replace "asm" -> "asm_inline"
> >     for _BUG_FLAGS(), refcount_add(), refcount_inc(), refcount_dec(),
> >         annotate_reachable(), annotate_unreachable()
> >
> >
> > Changes in v3:
> >   - Split into per-commit revert (per Nadav Amit)
> >   - Add some cleanups with preprocessor approach
> >
> > Changes in v2:
> >   - Revive clean-ups made by 5bdcd510c2ac (per Peter Zijlstra)
> >   - Fix commit quoting style (per Peter Zijlstra)
> >
> > Masahiro Yamada (12):
> >   Revert "x86/jump-labels: Macrofy inline assembly code to work around
> >     GCC inlining bugs"
> >   Revert "x86/cpufeature: Macrofy inline assembly code to work around
> >     GCC inlining bugs"
> >   Revert "x86/extable: Macrofy inline assembly code to work around GCC
> >     inlining bugs"
> >   Revert "x86/paravirt: Work around GCC inlining bugs when compiling
> >     paravirt ops"
> >   Revert "x86/bug: Macrofy the BUG table section handling, to work
> >     around GCC inlining bugs"
> >   Revert "x86/alternatives: Macrofy lock prefixes to work around GCC
> >     inlining bugs"
> >   Revert "x86/refcount: Work around GCC inlining bug"
> >   Revert "x86/objtool: Use asm macros to work around GCC inlining bugs"
> >   Revert "kbuild/Makefile: Prepare for using macros in inline assembly
> >     code to work around asm() related GCC inlining bugs"
> >   linux/linkage: add ASM() macro to reduce duplication between C/ASM
> >     code
> >   x86/alternatives: consolidate LOCK_PREFIX macro
> >   x86/asm: consolidate ASM_EXTABLE_* macros
> >
> >  Makefile                                  |  9 +--
> >  arch/x86/Makefile                         |  7 ---
> >  arch/x86/include/asm/alternative-asm.h    | 22 +------
> >  arch/x86/include/asm/alternative-common.h | 47 +++++++++++++++
> >  arch/x86/include/asm/alternative.h        | 30 +---------
> >  arch/x86/include/asm/asm.h                | 46 +++++----------
> >  arch/x86/include/asm/bug.h                | 98 +++++++++++++------------------
> >  arch/x86/include/asm/cpufeature.h         | 82 +++++++++++---------------
> >  arch/x86/include/asm/jump_label.h         | 22 +++++--
> >  arch/x86/include/asm/paravirt_types.h     | 56 +++++++++---------
> >  arch/x86/include/asm/refcount.h           | 81 +++++++++++--------------
> >  arch/x86/kernel/macros.S                  | 16 -----
> >  include/asm-generic/bug.h                 |  8 +--
> >  include/linux/compiler.h                  | 56 ++++--------------
> >  include/linux/linkage.h                   |  8 +++
> >  scripts/Kbuild.include                    |  4 +-
> >  scripts/mod/Makefile                      |  2 -
> >  17 files changed, 249 insertions(+), 345 deletions(-)
> >  create mode 100644 arch/x86/include/asm/alternative-common.h
> >  delete mode 100644 arch/x86/kernel/macros.S
>
> I absolutely agree that this needs to be resolved in v4.20.
>
> So I did the 1-9 reverts manually myself as well, because I think the
> first commit should be reverted fully to get as close to the starting
> point as possible (we are late in the cycle) - and came to the attached
> interdiff between your series and mine.
>
> Does this approach look OK to you, or did I miss something?


It looks OK to me.

I thought the diff was a good cleanup part,
but we can deal with it later on,
so I do not mind it.

Thanks!



> Thanks,
>
>         Ingo
>
> =============>
>
>  entry/calling.h          |    2 -
>  include/asm/jump_label.h |   50 ++++++++++++++++++++++++++++++++++-------------
>  2 files changed, 38 insertions(+), 14 deletions(-)
>
> diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h
> index 25e5a6bda8c3..20d0885b00fb 100644
> --- a/arch/x86/entry/calling.h
> +++ b/arch/x86/entry/calling.h
> @@ -352,7 +352,7 @@ For 32-bit we have the following conventions - kernel is built with
>  .macro CALL_enter_from_user_mode
>  #ifdef CONFIG_CONTEXT_TRACKING
>  #ifdef HAVE_JUMP_LABEL
> -       STATIC_BRANCH_JMP l_yes=.Lafter_call_\@, key=context_tracking_enabled, branch=1
> +       STATIC_JUMP_IF_FALSE .Lafter_call_\@, context_tracking_enabled, def=0
>  #endif
>         call enter_from_user_mode
>  .Lafter_call_\@:
> diff --git a/arch/x86/include/asm/jump_label.h b/arch/x86/include/asm/jump_label.h
> index cf88ebf9a4ca..21efc9d07ed9 100644
> --- a/arch/x86/include/asm/jump_label.h
> +++ b/arch/x86/include/asm/jump_label.h
> @@ -2,6 +2,19 @@
>  #ifndef _ASM_X86_JUMP_LABEL_H
>  #define _ASM_X86_JUMP_LABEL_H
>
> +#ifndef HAVE_JUMP_LABEL
> +/*
> + * For better or for worse, if jump labels (the gcc extension) are missing,
> + * then the entire static branch patching infrastructure is compiled out.
> + * If that happens, the code in here will malfunction.  Raise a compiler
> + * error instead.
> + *
> + * In theory, jump labels and the static branch patching infrastructure
> + * could be decoupled to fix this.
> + */
> +#error asm/jump_label.h included on a non-jump-label kernel
> +#endif
> +
>  #define JUMP_LABEL_NOP_SIZE 5
>
>  #ifdef CONFIG_X86_64
> @@ -53,26 +66,37 @@ static __always_inline bool arch_static_branch_jump(struct static_key *key, bool
>
>  #else  /* __ASSEMBLY__ */
>
> -.macro STATIC_BRANCH_NOP l_yes:req key:req branch:req
> -.Lstatic_branch_nop_\@:
> -       .byte STATIC_KEY_INIT_NOP
> -.Lstatic_branch_no_after_\@:
> +.macro STATIC_JUMP_IF_TRUE target, key, def
> +.Lstatic_jump_\@:
> +       .if \def
> +       /* Equivalent to "jmp.d32 \target" */
> +       .byte           0xe9
> +       .long           \target - .Lstatic_jump_after_\@
> +.Lstatic_jump_after_\@:
> +       .else
> +       .byte           STATIC_KEY_INIT_NOP
> +       .endif
>         .pushsection __jump_table, "aw"
>         _ASM_ALIGN
> -       .long           .Lstatic_branch_nop_\@ - ., \l_yes - .
> -       _ASM_PTR        \key + \branch - .
> +       .long           .Lstatic_jump_\@ - ., \target - .
> +       _ASM_PTR        \key - .
>         .popsection
>  .endm
>
> -.macro STATIC_BRANCH_JMP l_yes:req key:req branch:req
> -.Lstatic_branch_jmp_\@:
> -       .byte 0xe9
> -       .long \l_yes - .Lstatic_branch_jmp_after_\@
> -.Lstatic_branch_jmp_after_\@:
> +.macro STATIC_JUMP_IF_FALSE target, key, def
> +.Lstatic_jump_\@:
> +       .if \def
> +       .byte           STATIC_KEY_INIT_NOP
> +       .else
> +       /* Equivalent to "jmp.d32 \target" */
> +       .byte           0xe9
> +       .long           \target - .Lstatic_jump_after_\@
> +.Lstatic_jump_after_\@:
> +       .endif
>         .pushsection __jump_table, "aw"
>         _ASM_ALIGN
> -       .long           .Lstatic_branch_jmp_\@ - ., \l_yes - .
> -       _ASM_PTR        \key + \branch - .
> +       .long           .Lstatic_jump_\@ - ., \target - .
> +       _ASM_PTR        \key + 1 - .
>         .popsection
>  .endm
>
>


-- 
Best Regards
Masahiro Yamada
================================================================================


################################################################################

=== Thread: [PATCH v3 08/12] Revert "x86/objtool: Use asm macros to work around GCC inlining bugs" ===

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: [PATCH v3 08/12] Revert "x86/objtool: Use asm macros to work around GCC inlining bugs"
Date: Mon, 17 Dec 2018 16:03:23 +0000
Message-ID: <1545062607-8599-9-git-send-email-yamada.masahiro () socionext ! com>
--------------------
This reverts commit c06c4d8090513f2974dfdbed2ac98634357ac475.

The in-kernel workarounds will be replaced with GCC's new
"asm inline" syntax.

Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
---

 arch/x86/kernel/macros.S |  2 --
 include/linux/compiler.h | 56 +++++++++++-------------------------------------
 2 files changed, 13 insertions(+), 45 deletions(-)

diff --git a/arch/x86/kernel/macros.S b/arch/x86/kernel/macros.S
index cee28c3..cfc1c7d 100644
--- a/arch/x86/kernel/macros.S
+++ b/arch/x86/kernel/macros.S
@@ -5,5 +5,3 @@
  * commonly used. The macros are precompiled into assmebly file which is later
  * assembled together with each compiled file.
  */
-
-#include <linux/compiler.h>
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 06396c1..fc5004a 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -99,13 +99,22 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
  * unique, to convince GCC not to merge duplicate inline asm statements.
  */
 #define annotate_reachable() ({						\
-	asm volatile("ANNOTATE_REACHABLE counter=%c0"			\
-		     : : "i" (__COUNTER__));				\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.reachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
 #define annotate_unreachable() ({					\
-	asm volatile("ANNOTATE_UNREACHABLE counter=%c0"			\
-		     : : "i" (__COUNTER__));				\
+	asm volatile("%c0:\n\t"						\
+		     ".pushsection .discard.unreachable\n\t"		\
+		     ".long %c0b - .\n\t"				\
+		     ".popsection\n\t" : : "i" (__COUNTER__));		\
 })
+#define ASM_UNREACHABLE							\
+	"999:\n\t"							\
+	".pushsection .discard.unreachable\n\t"				\
+	".long 999b - .\n\t"						\
+	".popsection\n\t"
 #else
 #define annotate_reachable()
 #define annotate_unreachable()
@@ -293,45 +302,6 @@ static inline void *offset_to_ptr(const int *off)
 	return (void *)((unsigned long)off + *off);
 }
 
-#else /* __ASSEMBLY__ */
-
-#ifdef __KERNEL__
-#ifndef LINKER_SCRIPT
-
-#ifdef CONFIG_STACK_VALIDATION
-.macro ANNOTATE_UNREACHABLE counter:req
-\counter:
-	.pushsection .discard.unreachable
-	.long \counter\()b -.
-	.popsection
-.endm
-
-.macro ANNOTATE_REACHABLE counter:req
-\counter:
-	.pushsection .discard.reachable
-	.long \counter\()b -.
-	.popsection
-.endm
-
-.macro ASM_UNREACHABLE
-999:
-	.pushsection .discard.unreachable
-	.long 999b - .
-	.popsection
-.endm
-#else /* CONFIG_STACK_VALIDATION */
-.macro ANNOTATE_UNREACHABLE counter:req
-.endm
-
-.macro ANNOTATE_REACHABLE counter:req
-.endm
-
-.macro ASM_UNREACHABLE
-.endm
-#endif /* CONFIG_STACK_VALIDATION */
-
-#endif /* LINKER_SCRIPT */
-#endif /* __KERNEL__ */
 #endif /* __ASSEMBLY__ */
 
 /* Compile time object size, -1 for unknown */
-- 
2.7.4

================================================================================


################################################################################

=== Thread: [PATCH v3 1/2] Compiler Attributes: naked can be shared ===

From: Matthias Kaehlcke <mka () chromium ! org>
To: linux-sparse
Subject: Re: [PATCH v3 1/2] Compiler Attributes: naked can be shared
Date: Tue, 25 Sep 2018 19:03:49 +0000
Message-ID: <20180925190349.GB22824 () google ! com>
--------------------
Sorry, this series got messed up. Apparently the branch was still
rebasing when I ran 'git send-email' :/

On Tue, Sep 25, 2018 at 11:53:38AM -0700, Matthias Kaehlcke wrote:
> From: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
> 
> The naked attribute is supported by at least gcc >= 4.6 (for ARM,
> which is the only current user), gcc >= 8 (for x86), clang >= 3.1
> and icc >= 13. See https://godbolt.org/z/350Dyc
> 
> Therefore, move it out of compiler-gcc.h so that the definition
> is shared by all compilers.
> 
> This also fixes Clang support for ARM32 --- 815f0ddb346c
> ("include/linux/compiler*.h: make compiler-*.h mutually exclusive").
> 
> Fixes: 815f0ddb346c ("include/linux/compiler*.h: make compiler-*.h mutually exclusive")
> Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
> Cc: Eli Friedman <efriedma@codeaurora.org>
> Cc: Christopher Li <sparse@chrisli.org>
> Cc: Kees Cook <keescook@chromium.org>
> Cc: Ingo Molnar <mingo@kernel.org>
> Cc: Geert Uytterhoeven <geert@linux-m68k.org>
> Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
> Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
> Cc: Joe Perches <joe@perches.com>
> Cc: Dominique Martinet <asmadeus@codewreck.org>
> Cc: Linus Torvalds <torvalds@linux-foundation.org>
> Cc: linux-sparse@vger.kernel.org
> Suggested-by: Arnd Bergmann <arnd@arndb.de>
> Tested-by: Stefan Agner <stefan@agner.ch>
> Reviewed-by: Stefan Agner <stefan@agner.ch>
> Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
> Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
> Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
> ---
>  include/linux/compiler-gcc.h   | 8 --------
>  include/linux/compiler_types.h | 8 ++++++++
>  2 files changed, 8 insertions(+), 8 deletions(-)
> 
> diff --git a/include/linux/compiler-gcc.h b/include/linux/compiler-gcc.h
> index 25d3dd6b2702..4d36b27214fd 100644
> --- a/include/linux/compiler-gcc.h
> +++ b/include/linux/compiler-gcc.h
> @@ -79,14 +79,6 @@
>  #define __noretpoline __attribute__((indirect_branch("keep")))
>  #endif
>  
> -/*
> - * it doesn't make sense on ARM (currently the only user of __naked)
> - * to trace naked functions because then mcount is called without
> - * stack and frame pointer being set up and there is no chance to
> - * restore the lr register to the value before mcount was called.
> - */
> -#define __naked		__attribute__((naked)) notrace
> -
>  #define __UNIQUE_ID(prefix) __PASTE(__PASTE(__UNIQUE_ID_, prefix), __COUNTER__)
>  
>  #define __optimize(level)	__attribute__((__optimize__(level)))
> diff --git a/include/linux/compiler_types.h b/include/linux/compiler_types.h
> index 3525c179698c..db192becfec4 100644
> --- a/include/linux/compiler_types.h
> +++ b/include/linux/compiler_types.h
> @@ -226,6 +226,14 @@ struct ftrace_likely_data {
>  #define notrace			__attribute__((no_instrument_function))
>  #endif
>  
> +/*
> + * it doesn't make sense on ARM (currently the only user of __naked)
> + * to trace naked functions because then mcount is called without
> + * stack and frame pointer being set up and there is no chance to
> + * restore the lr register to the value before mcount was called.
> + */
> +#define __naked			__attribute__((naked)) notrace
> +
>  #define __compiler_offsetof(a, b)	__builtin_offsetof(a, b)
>  
>  /*
================================================================================


################################################################################

=== Thread: [PATCH v3 1/4] x86/mm: declare check_la57_support() as inline ===

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: [PATCH v3 1/4] x86/mm: declare check_la57_support() as inline
Date: Mon, 29 Oct 2018 13:04:46 +0000
Message-ID: <CAK7LNATixykqHqdu5W3NpfXqBXMNsP0OLKjNdsFc_EzJ_diOxA () mail ! gmail ! com>
--------------------
On Mon, Oct 29, 2018 at 3:09 AM Steven Rostedt <rostedt@goodmis.org> wrote:
>
> On Sun, 28 Oct 2018 13:09:42 +0000
> Changbin Du <changbin.du@gmail.com> wrote:
>
> > The level4_kernel_pgt is only defined when X86_5LEVEL is enabled.
> > So declare check_la57_support() as inline to make sure the code
> > referring to level4_kernel_pgt is optimized out. This is a preparation
> > for CONFIG_CC_OPTIMIZE_FOR_DEBUGGING.
> >
> > Signed-off-by: Changbin Du <changbin.du@gmail.com>
>
> Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
>
> -- Steve
>


Applied to linux-kbuild.


-- 
Best Regards
Masahiro Yamada
================================================================================


################################################################################

=== Thread: [PATCH v3 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimiz ===

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kbuild
Subject: Re: [PATCH v3 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimiz
Date: Mon, 29 Oct 2018 13:03:38 +0000
Message-ID: <CAK7LNASiGPar_Aj1KDu8E+S=NKRf7mEJ8gwe2=OMUGggZmtfNQ () mail ! gmail ! com>
--------------------
On Sun, Oct 28, 2018 at 10:13 PM Changbin Du <changbin.du@gmail.com> wrote:
>
> This patch add a new kernel hacking option NO_AUTO_INLINE. Selecting


I fixed "This patch add ..." to "This patch adds ..."
then applied to linux-kbuild.




> this option will prevent the compiler from optimizing the kernel by
> auto-inlining functions not marked with the inline keyword.
>
> With this option, only functions explicitly marked with "inline" will
> be inlined. This will allow the function tracer to trace more functions
> because it only traces functions that the compiler has not inlined.
>
> Signed-off-by: Changbin Du <changbin.du@gmail.com>
> Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
> ---
>  Makefile          |  6 ++++++
>  lib/Kconfig.debug | 17 +++++++++++++++++
>  2 files changed, 23 insertions(+)
>
> diff --git a/Makefile b/Makefile
> index 7d4ba5196010..04beb822ddfc 100644
> --- a/Makefile
> +++ b/Makefile
> @@ -749,6 +749,12 @@ KBUILD_CFLAGS      += $(call cc-option, -femit-struct-debug-baseonly) \
>                    $(call cc-option,-fno-var-tracking)
>  endif
>
> +ifdef CONFIG_NO_AUTO_INLINE
> +KBUILD_CFLAGS   += $(call cc-option, -fno-inline-functions) \
> +                  $(call cc-option, -fno-inline-small-functions) \
> +                  $(call cc-option, -fno-inline-functions-called-once)
> +endif
> +
>  ifdef CONFIG_FUNCTION_TRACER
>  ifdef CONFIG_FTRACE_MCOUNT_RECORD
>    # gcc 5 supports generating the mcount tables directly
> diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
> index 04adfc3b185e..d50711b41dad 100644
> --- a/lib/Kconfig.debug
> +++ b/lib/Kconfig.debug
> @@ -211,6 +211,23 @@ config GDB_SCRIPTS
>           instance. See Documentation/dev-tools/gdb-kernel-debugging.rst
>           for further details.
>
> +config NO_AUTO_INLINE
> +       bool "Disable compiler auto-inline optimizations"
> +       help
> +         This will prevent the compiler from optimizing the kernel by
> +         auto-inlining functions not marked with the inline keyword.
> +         With this option, only functions explicitly marked with
> +         "inline" will be inlined. This will allow the function tracer
> +         to trace more functions because it only traces functions that
> +         the compiler has not inlined.
> +
> +         Enabling this function can help debugging a kernel if using
> +         the function tracer. But it can also change how the kernel
> +         works, because inlining functions may change the timing,
> +         which could make it difficult while debugging race conditions.
> +
> +         If unsure, select N.
> +
>  config ENABLE_MUST_CHECK
>         bool "Enable __must_check logic"
>         default y
> --
> 2.17.1
>


-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kernel
Subject: Re: [PATCH v3 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimiz
Date: Mon, 29 Oct 2018 13:03:38 +0000
Message-ID: <CAK7LNASiGPar_Aj1KDu8E+S=NKRf7mEJ8gwe2=OMUGggZmtfNQ () mail ! gmail ! com>
--------------------
On Sun, Oct 28, 2018 at 10:13 PM Changbin Du <changbin.du@gmail.com> wrote:
>
> This patch add a new kernel hacking option NO_AUTO_INLINE. Selecting


I fixed "This patch add ..." to "This patch adds ..."
then applied to linux-kbuild.




> this option will prevent the compiler from optimizing the kernel by
> auto-inlining functions not marked with the inline keyword.
>
> With this option, only functions explicitly marked with "inline" will
> be inlined. This will allow the function tracer to trace more functions
> because it only traces functions that the compiler has not inlined.
>
> Signed-off-by: Changbin Du <changbin.du@gmail.com>
> Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
> ---
>  Makefile          |  6 ++++++
>  lib/Kconfig.debug | 17 +++++++++++++++++
>  2 files changed, 23 insertions(+)
>
> diff --git a/Makefile b/Makefile
> index 7d4ba5196010..04beb822ddfc 100644
> --- a/Makefile
> +++ b/Makefile
> @@ -749,6 +749,12 @@ KBUILD_CFLAGS      += $(call cc-option, -femit-struct-debug-baseonly) \
>                    $(call cc-option,-fno-var-tracking)
>  endif
>
> +ifdef CONFIG_NO_AUTO_INLINE
> +KBUILD_CFLAGS   += $(call cc-option, -fno-inline-functions) \
> +                  $(call cc-option, -fno-inline-small-functions) \
> +                  $(call cc-option, -fno-inline-functions-called-once)
> +endif
> +
>  ifdef CONFIG_FUNCTION_TRACER
>  ifdef CONFIG_FTRACE_MCOUNT_RECORD
>    # gcc 5 supports generating the mcount tables directly
> diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
> index 04adfc3b185e..d50711b41dad 100644
> --- a/lib/Kconfig.debug
> +++ b/lib/Kconfig.debug
> @@ -211,6 +211,23 @@ config GDB_SCRIPTS
>           instance. See Documentation/dev-tools/gdb-kernel-debugging.rst
>           for further details.
>
> +config NO_AUTO_INLINE
> +       bool "Disable compiler auto-inline optimizations"
> +       help
> +         This will prevent the compiler from optimizing the kernel by
> +         auto-inlining functions not marked with the inline keyword.
> +         With this option, only functions explicitly marked with
> +         "inline" will be inlined. This will allow the function tracer
> +         to trace more functions because it only traces functions that
> +         the compiler has not inlined.
> +
> +         Enabling this function can help debugging a kernel if using
> +         the function tracer. But it can also change how the kernel
> +         works, because inlining functions may change the timing,
> +         which could make it difficult while debugging race conditions.
> +
> +         If unsure, select N.
> +
>  config ENABLE_MUST_CHECK
>         bool "Enable __must_check logic"
>         default y
> --
> 2.17.1
>


-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: [PATCH v3 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimiz
Date: Mon, 29 Oct 2018 13:03:38 +0000
Message-ID: <CAK7LNASiGPar_Aj1KDu8E+S=NKRf7mEJ8gwe2=OMUGggZmtfNQ () mail ! gmail ! com>
--------------------
On Sun, Oct 28, 2018 at 10:13 PM Changbin Du <changbin.du@gmail.com> wrote:
>
> This patch add a new kernel hacking option NO_AUTO_INLINE. Selecting


I fixed "This patch add ..." to "This patch adds ..."
then applied to linux-kbuild.




> this option will prevent the compiler from optimizing the kernel by
> auto-inlining functions not marked with the inline keyword.
>
> With this option, only functions explicitly marked with "inline" will
> be inlined. This will allow the function tracer to trace more functions
> because it only traces functions that the compiler has not inlined.
>
> Signed-off-by: Changbin Du <changbin.du@gmail.com>
> Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
> ---
>  Makefile          |  6 ++++++
>  lib/Kconfig.debug | 17 +++++++++++++++++
>  2 files changed, 23 insertions(+)
>
> diff --git a/Makefile b/Makefile
> index 7d4ba5196010..04beb822ddfc 100644
> --- a/Makefile
> +++ b/Makefile
> @@ -749,6 +749,12 @@ KBUILD_CFLAGS      += $(call cc-option, -femit-struct-debug-baseonly) \
>                    $(call cc-option,-fno-var-tracking)
>  endif
>
> +ifdef CONFIG_NO_AUTO_INLINE
> +KBUILD_CFLAGS   += $(call cc-option, -fno-inline-functions) \
> +                  $(call cc-option, -fno-inline-small-functions) \
> +                  $(call cc-option, -fno-inline-functions-called-once)
> +endif
> +
>  ifdef CONFIG_FUNCTION_TRACER
>  ifdef CONFIG_FTRACE_MCOUNT_RECORD
>    # gcc 5 supports generating the mcount tables directly
> diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
> index 04adfc3b185e..d50711b41dad 100644
> --- a/lib/Kconfig.debug
> +++ b/lib/Kconfig.debug
> @@ -211,6 +211,23 @@ config GDB_SCRIPTS
>           instance. See Documentation/dev-tools/gdb-kernel-debugging.rst
>           for further details.
>
> +config NO_AUTO_INLINE
> +       bool "Disable compiler auto-inline optimizations"
> +       help
> +         This will prevent the compiler from optimizing the kernel by
> +         auto-inlining functions not marked with the inline keyword.
> +         With this option, only functions explicitly marked with
> +         "inline" will be inlined. This will allow the function tracer
> +         to trace more functions because it only traces functions that
> +         the compiler has not inlined.
> +
> +         Enabling this function can help debugging a kernel if using
> +         the function tracer. But it can also change how the kernel
> +         works, because inlining functions may change the timing,
> +         which could make it difficult while debugging race conditions.
> +
> +         If unsure, select N.
> +
>  config ENABLE_MUST_CHECK
>         bool "Enable __must_check logic"
>         default y
> --
> 2.17.1
>


-- 
Best Regards
Masahiro Yamada
================================================================================


################################################################################

=== Thread: [PATCH v3 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimizatio ===

From: Changbin Du <changbin.du () gmail ! com>
To: linux-sparse
Subject: [PATCH v3 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimizatio
Date: Sun, 28 Oct 2018 13:09:43 +0000
Message-ID: <20181028130945.23581-3-changbin.du () gmail ! com>
--------------------
This patch add a new kernel hacking option NO_AUTO_INLINE. Selecting
this option will prevent the compiler from optimizing the kernel by
auto-inlining functions not marked with the inline keyword.

With this option, only functions explicitly marked with "inline" will
be inlined. This will allow the function tracer to trace more functions
because it only traces functions that the compiler has not inlined.

Signed-off-by: Changbin Du <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
---
 Makefile          |  6 ++++++
 lib/Kconfig.debug | 17 +++++++++++++++++
 2 files changed, 23 insertions(+)

diff --git a/Makefile b/Makefile
index 7d4ba5196010..04beb822ddfc 100644
--- a/Makefile
+++ b/Makefile
@@ -749,6 +749,12 @@ KBUILD_CFLAGS 	+= $(call cc-option, -femit-struct-debug-baseonly) \
 		   $(call cc-option,-fno-var-tracking)
 endif
 
+ifdef CONFIG_NO_AUTO_INLINE
+KBUILD_CFLAGS   += $(call cc-option, -fno-inline-functions) \
+		   $(call cc-option, -fno-inline-small-functions) \
+		   $(call cc-option, -fno-inline-functions-called-once)
+endif
+
 ifdef CONFIG_FUNCTION_TRACER
 ifdef CONFIG_FTRACE_MCOUNT_RECORD
   # gcc 5 supports generating the mcount tables directly
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 04adfc3b185e..d50711b41dad 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -211,6 +211,23 @@ config GDB_SCRIPTS
 	  instance. See Documentation/dev-tools/gdb-kernel-debugging.rst
 	  for further details.
 
+config NO_AUTO_INLINE
+	bool "Disable compiler auto-inline optimizations"
+	help
+	  This will prevent the compiler from optimizing the kernel by
+	  auto-inlining functions not marked with the inline keyword.
+	  With this option, only functions explicitly marked with
+	  "inline" will be inlined. This will allow the function tracer
+	  to trace more functions because it only traces functions that
+	  the compiler has not inlined.
+
+	  Enabling this function can help debugging a kernel if using
+	  the function tracer. But it can also change how the kernel
+	  works, because inlining functions may change the timing,
+	  which could make it difficult while debugging race conditions.
+
+	  If unsure, select N.
+
 config ENABLE_MUST_CHECK
 	bool "Enable __must_check logic"
 	default y
-- 
2.17.1

================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-kernel
Subject: [PATCH v3 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimizatio
Date: Sun, 28 Oct 2018 13:09:43 +0000
Message-ID: <20181028130945.23581-3-changbin.du () gmail ! com>
--------------------
This patch add a new kernel hacking option NO_AUTO_INLINE. Selecting
this option will prevent the compiler from optimizing the kernel by
auto-inlining functions not marked with the inline keyword.

With this option, only functions explicitly marked with "inline" will
be inlined. This will allow the function tracer to trace more functions
because it only traces functions that the compiler has not inlined.

Signed-off-by: Changbin Du <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
---
 Makefile          |  6 ++++++
 lib/Kconfig.debug | 17 +++++++++++++++++
 2 files changed, 23 insertions(+)

diff --git a/Makefile b/Makefile
index 7d4ba5196010..04beb822ddfc 100644
--- a/Makefile
+++ b/Makefile
@@ -749,6 +749,12 @@ KBUILD_CFLAGS 	+= $(call cc-option, -femit-struct-debug-baseonly) \
 		   $(call cc-option,-fno-var-tracking)
 endif
 
+ifdef CONFIG_NO_AUTO_INLINE
+KBUILD_CFLAGS   += $(call cc-option, -fno-inline-functions) \
+		   $(call cc-option, -fno-inline-small-functions) \
+		   $(call cc-option, -fno-inline-functions-called-once)
+endif
+
 ifdef CONFIG_FUNCTION_TRACER
 ifdef CONFIG_FTRACE_MCOUNT_RECORD
   # gcc 5 supports generating the mcount tables directly
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 04adfc3b185e..d50711b41dad 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -211,6 +211,23 @@ config GDB_SCRIPTS
 	  instance. See Documentation/dev-tools/gdb-kernel-debugging.rst
 	  for further details.
 
+config NO_AUTO_INLINE
+	bool "Disable compiler auto-inline optimizations"
+	help
+	  This will prevent the compiler from optimizing the kernel by
+	  auto-inlining functions not marked with the inline keyword.
+	  With this option, only functions explicitly marked with
+	  "inline" will be inlined. This will allow the function tracer
+	  to trace more functions because it only traces functions that
+	  the compiler has not inlined.
+
+	  Enabling this function can help debugging a kernel if using
+	  the function tracer. But it can also change how the kernel
+	  works, because inlining functions may change the timing,
+	  which could make it difficult while debugging race conditions.
+
+	  If unsure, select N.
+
 config ENABLE_MUST_CHECK
 	bool "Enable __must_check logic"
 	default y
-- 
2.17.1

================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-arm-kernel
Subject: [PATCH v3 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimizatio
Date: Sun, 28 Oct 2018 13:09:43 +0000
Message-ID: <20181028130945.23581-3-changbin.du () gmail ! com>
--------------------
This patch add a new kernel hacking option NO_AUTO_INLINE. Selecting
this option will prevent the compiler from optimizing the kernel by
auto-inlining functions not marked with the inline keyword.

With this option, only functions explicitly marked with "inline" will
be inlined. This will allow the function tracer to trace more functions
because it only traces functions that the compiler has not inlined.

Signed-off-by: Changbin Du <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
---
 Makefile          |  6 ++++++
 lib/Kconfig.debug | 17 +++++++++++++++++
 2 files changed, 23 insertions(+)

diff --git a/Makefile b/Makefile
index 7d4ba5196010..04beb822ddfc 100644
--- a/Makefile
+++ b/Makefile
@@ -749,6 +749,12 @@ KBUILD_CFLAGS 	+= $(call cc-option, -femit-struct-debug-baseonly) \
 		   $(call cc-option,-fno-var-tracking)
 endif
 
+ifdef CONFIG_NO_AUTO_INLINE
+KBUILD_CFLAGS   += $(call cc-option, -fno-inline-functions) \
+		   $(call cc-option, -fno-inline-small-functions) \
+		   $(call cc-option, -fno-inline-functions-called-once)
+endif
+
 ifdef CONFIG_FUNCTION_TRACER
 ifdef CONFIG_FTRACE_MCOUNT_RECORD
   # gcc 5 supports generating the mcount tables directly
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 04adfc3b185e..d50711b41dad 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -211,6 +211,23 @@ config GDB_SCRIPTS
 	  instance. See Documentation/dev-tools/gdb-kernel-debugging.rst
 	  for further details.
 
+config NO_AUTO_INLINE
+	bool "Disable compiler auto-inline optimizations"
+	help
+	  This will prevent the compiler from optimizing the kernel by
+	  auto-inlining functions not marked with the inline keyword.
+	  With this option, only functions explicitly marked with
+	  "inline" will be inlined. This will allow the function tracer
+	  to trace more functions because it only traces functions that
+	  the compiler has not inlined.
+
+	  Enabling this function can help debugging a kernel if using
+	  the function tracer. But it can also change how the kernel
+	  works, because inlining functions may change the timing,
+	  which could make it difficult while debugging race conditions.
+
+	  If unsure, select N.
+
 config ENABLE_MUST_CHECK
 	bool "Enable __must_check logic"
 	default y
-- 
2.17.1


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-kbuild
Subject: [PATCH v3 2/4] kernel hacking: new config NO_AUTO_INLINE to disable compiler auto-inline optimizatio
Date: Sun, 28 Oct 2018 13:09:43 +0000
Message-ID: <20181028130945.23581-3-changbin.du () gmail ! com>
--------------------
This patch add a new kernel hacking option NO_AUTO_INLINE. Selecting
this option will prevent the compiler from optimizing the kernel by
auto-inlining functions not marked with the inline keyword.

With this option, only functions explicitly marked with "inline" will
be inlined. This will allow the function tracer to trace more functions
because it only traces functions that the compiler has not inlined.

Signed-off-by: Changbin Du <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
---
 Makefile          |  6 ++++++
 lib/Kconfig.debug | 17 +++++++++++++++++
 2 files changed, 23 insertions(+)

diff --git a/Makefile b/Makefile
index 7d4ba5196010..04beb822ddfc 100644
--- a/Makefile
+++ b/Makefile
@@ -749,6 +749,12 @@ KBUILD_CFLAGS 	+= $(call cc-option, -femit-struct-debug-baseonly) \
 		   $(call cc-option,-fno-var-tracking)
 endif
 
+ifdef CONFIG_NO_AUTO_INLINE
+KBUILD_CFLAGS   += $(call cc-option, -fno-inline-functions) \
+		   $(call cc-option, -fno-inline-small-functions) \
+		   $(call cc-option, -fno-inline-functions-called-once)
+endif
+
 ifdef CONFIG_FUNCTION_TRACER
 ifdef CONFIG_FTRACE_MCOUNT_RECORD
   # gcc 5 supports generating the mcount tables directly
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 04adfc3b185e..d50711b41dad 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -211,6 +211,23 @@ config GDB_SCRIPTS
 	  instance. See Documentation/dev-tools/gdb-kernel-debugging.rst
 	  for further details.
 
+config NO_AUTO_INLINE
+	bool "Disable compiler auto-inline optimizations"
+	help
+	  This will prevent the compiler from optimizing the kernel by
+	  auto-inlining functions not marked with the inline keyword.
+	  With this option, only functions explicitly marked with
+	  "inline" will be inlined. This will allow the function tracer
+	  to trace more functions because it only traces functions that
+	  the compiler has not inlined.
+
+	  Enabling this function can help debugging a kernel if using
+	  the function tracer. But it can also change how the kernel
+	  works, because inlining functions may change the timing,
+	  which could make it difficult while debugging race conditions.
+
+	  If unsure, select N.
+
 config ENABLE_MUST_CHECK
 	bool "Enable __must_check logic"
 	default y
-- 
2.17.1

================================================================================


################################################################################

=== Thread: [PATCH v3 3/4] ARM: mm: fix build error in fix_to_virt with CONFIG_CC_OPTIMIZE_FOR_DEBUGGING ===

From: Changbin Du <changbin.du () gmail ! com>
To: linux-kbuild
Subject: [PATCH v3 3/4] ARM: mm: fix build error in fix_to_virt with CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
Date: Sun, 28 Oct 2018 13:09:44 +0000
Message-ID: <20181028130945.23581-4-changbin.du () gmail ! com>
--------------------
With '-Og' optimization level, GCC would not optimize a count for a loop
as a constant value. But BUILD_BUG_ON() only accept compile-time constant
values. Let's use __fix_to_virt() to avoid the error.

arch/arm/mm/mmu.o: In function `fix_to_virt':
/home/changbin/work/linux/./include/asm-generic/fixmap.h:31: undefined reference to `__compiletime_assert_31'
Makefile:1051: recipe for target 'vmlinux' failed
make: *** [vmlinux] Error 1

Signed-off-by: Changbin Du <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
---
 arch/arm/mm/mmu.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e46a6a446cdd..c08d74e76714 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1599,7 +1599,7 @@ static void __init early_fixmap_shutdown(void)
 		pte_t *pte;
 		struct map_desc map;
 
-		map.virtual = fix_to_virt(i);
+		map.virtual = __fix_to_virt(i);
 		pte = pte_offset_early_fixmap(pmd_off_k(map.virtual), map.virtual);
 
 		/* Only i/o device mappings are supported ATM */
-- 
2.17.1

================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-sparse
Subject: [PATCH v3 3/4] ARM: mm: fix build error in fix_to_virt with CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
Date: Sun, 28 Oct 2018 13:09:44 +0000
Message-ID: <20181028130945.23581-4-changbin.du () gmail ! com>
--------------------
With '-Og' optimization level, GCC would not optimize a count for a loop
as a constant value. But BUILD_BUG_ON() only accept compile-time constant
values. Let's use __fix_to_virt() to avoid the error.

arch/arm/mm/mmu.o: In function `fix_to_virt':
/home/changbin/work/linux/./include/asm-generic/fixmap.h:31: undefined reference to `__compiletime_assert_31'
Makefile:1051: recipe for target 'vmlinux' failed
make: *** [vmlinux] Error 1

Signed-off-by: Changbin Du <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
---
 arch/arm/mm/mmu.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e46a6a446cdd..c08d74e76714 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1599,7 +1599,7 @@ static void __init early_fixmap_shutdown(void)
 		pte_t *pte;
 		struct map_desc map;
 
-		map.virtual = fix_to_virt(i);
+		map.virtual = __fix_to_virt(i);
 		pte = pte_offset_early_fixmap(pmd_off_k(map.virtual), map.virtual);
 
 		/* Only i/o device mappings are supported ATM */
-- 
2.17.1

================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-kernel
Subject: [PATCH v3 3/4] ARM: mm: fix build error in fix_to_virt with CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
Date: Sun, 28 Oct 2018 13:09:44 +0000
Message-ID: <20181028130945.23581-4-changbin.du () gmail ! com>
--------------------
With '-Og' optimization level, GCC would not optimize a count for a loop
as a constant value. But BUILD_BUG_ON() only accept compile-time constant
values. Let's use __fix_to_virt() to avoid the error.

arch/arm/mm/mmu.o: In function `fix_to_virt':
/home/changbin/work/linux/./include/asm-generic/fixmap.h:31: undefined reference to `__compiletime_assert_31'
Makefile:1051: recipe for target 'vmlinux' failed
make: *** [vmlinux] Error 1

Signed-off-by: Changbin Du <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
---
 arch/arm/mm/mmu.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e46a6a446cdd..c08d74e76714 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1599,7 +1599,7 @@ static void __init early_fixmap_shutdown(void)
 		pte_t *pte;
 		struct map_desc map;
 
-		map.virtual = fix_to_virt(i);
+		map.virtual = __fix_to_virt(i);
 		pte = pte_offset_early_fixmap(pmd_off_k(map.virtual), map.virtual);
 
 		/* Only i/o device mappings are supported ATM */
-- 
2.17.1

================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-arm-kernel
Subject: [PATCH v3 3/4] ARM: mm: fix build error in fix_to_virt with CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
Date: Sun, 28 Oct 2018 13:09:44 +0000
Message-ID: <20181028130945.23581-4-changbin.du () gmail ! com>
--------------------
With '-Og' optimization level, GCC would not optimize a count for a loop
as a constant value. But BUILD_BUG_ON() only accept compile-time constant
values. Let's use __fix_to_virt() to avoid the error.

arch/arm/mm/mmu.o: In function `fix_to_virt':
/home/changbin/work/linux/./include/asm-generic/fixmap.h:31: undefined reference to `__compiletime_assert_31'
Makefile:1051: recipe for target 'vmlinux' failed
make: *** [vmlinux] Error 1

Signed-off-by: Changbin Du <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
---
 arch/arm/mm/mmu.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e46a6a446cdd..c08d74e76714 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1599,7 +1599,7 @@ static void __init early_fixmap_shutdown(void)
 		pte_t *pte;
 		struct map_desc map;
 
-		map.virtual = fix_to_virt(i);
+		map.virtual = __fix_to_virt(i);
 		pte = pte_offset_early_fixmap(pmd_off_k(map.virtual), map.virtual);
 
 		/* Only i/o device mappings are supported ATM */
-- 
2.17.1


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kernel
Subject: Re: [PATCH v3 3/4] ARM: mm: fix build error in fix_to_virt with CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
Date: Mon, 29 Oct 2018 13:05:44 +0000
Message-ID: <CAK7LNAQP6SgtbYcxZ7ApGg006omEZErNcDAVtPR=2RibxN8Ruw () mail ! gmail ! com>
--------------------
On Sun, Oct 28, 2018 at 10:13 PM Changbin Du <changbin.du@gmail.com> wrote:
>
> With '-Og' optimization level, GCC would not optimize a count for a loop
> as a constant value. But BUILD_BUG_ON() only accept compile-time constant
> values. Let's use __fix_to_virt() to avoid the error.
>
> arch/arm/mm/mmu.o: In function `fix_to_virt':
> /home/changbin/work/linux/./include/asm-generic/fixmap.h:31: undefined reference to `__compiletime_assert_31'
> Makefile:1051: recipe for target 'vmlinux' failed
> make: *** [vmlinux] Error 1
>
> Signed-off-by: Changbin Du <changbin.du@gmail.com>
> Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
> ---

Applied to linux-kbuild





-- 
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v3 3/4] ARM: mm: fix build error in fix_to_virt with CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
Date: Mon, 29 Oct 2018 13:05:44 +0000
Message-ID: <CAK7LNAQP6SgtbYcxZ7ApGg006omEZErNcDAVtPR=2RibxN8Ruw () mail ! gmail ! com>
--------------------
On Sun, Oct 28, 2018 at 10:13 PM Changbin Du <changbin.du@gmail.com> wrote:
>
> With '-Og' optimization level, GCC would not optimize a count for a loop
> as a constant value. But BUILD_BUG_ON() only accept compile-time constant
> values. Let's use __fix_to_virt() to avoid the error.
>
> arch/arm/mm/mmu.o: In function `fix_to_virt':
> /home/changbin/work/linux/./include/asm-generic/fixmap.h:31: undefined reference to `__compiletime_assert_31'
> Makefile:1051: recipe for target 'vmlinux' failed
> make: *** [vmlinux] Error 1
>
> Signed-off-by: Changbin Du <changbin.du@gmail.com>
> Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
> ---

Applied to linux-kbuild





-- 
Best Regards
Masahiro Yamada

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================


################################################################################

=== Thread: [PATCH v3 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimizatio ===

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kbuild
Subject: Re: [PATCH v3 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimizatio
Date: Mon, 29 Oct 2018 13:16:02 +0000
Message-ID: <CAK7LNAS8UPXBbOpG9Zy_aP7an2jbq5H7uQGymUAn+hv46WQ4Bw () mail ! gmail ! com>
--------------------
On Sun, Oct 28, 2018 at 10:11 PM Changbin Du <changbin.du@gmail.com> wrote:
>
> This will apply GCC '-Og' optimization level which is supported
> since GCC 4.8. This optimization level offers a reasonable level
> of optimization while maintaining fast compilation and a good
> debugging experience. It is similar to '-O1' while perferring
> to keep debug ability over runtime speed.
>
> If enabling this option breaks your kernel, you should either
> disable this or find a fix (mostly in the arch code). Currently
> this option has only been tested on x86_64 and arm platform.
>
> This option can satisfy people who was searching for a method
> to disable compiler optimizations so to achieve better kernel
> debugging experience with kgdb or qemu.
>
> The main problem of '-Og' is we must not use __attribute__((error(msg))).
> The compiler will report error though the call to error function
> still can be optimize out. So we must fallback to array tricky.


I removed the sentence "So we must fallback to array tricky."

Commit 81b45683487a51b0f4d3b29d37f20d6d078544e4
killed the fallback to the negative array trick.


I also resolved a conflict.

Your series is now available in the following branch.

git://git.kernel.org/pub/scm/linux/kernel/git/masahiroy/linux-kbuild kbuild


Please double check if I did it correctly.


--
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: [PATCH v3 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimizatio
Date: Mon, 29 Oct 2018 13:16:02 +0000
Message-ID: <CAK7LNAS8UPXBbOpG9Zy_aP7an2jbq5H7uQGymUAn+hv46WQ4Bw () mail ! gmail ! com>
--------------------
On Sun, Oct 28, 2018 at 10:11 PM Changbin Du <changbin.du@gmail.com> wrote:
>
> This will apply GCC '-Og' optimization level which is supported
> since GCC 4.8. This optimization level offers a reasonable level
> of optimization while maintaining fast compilation and a good
> debugging experience. It is similar to '-O1' while perferring
> to keep debug ability over runtime speed.
>
> If enabling this option breaks your kernel, you should either
> disable this or find a fix (mostly in the arch code). Currently
> this option has only been tested on x86_64 and arm platform.
>
> This option can satisfy people who was searching for a method
> to disable compiler optimizations so to achieve better kernel
> debugging experience with kgdb or qemu.
>
> The main problem of '-Og' is we must not use __attribute__((error(msg))).
> The compiler will report error though the call to error function
> still can be optimize out. So we must fallback to array tricky.


I removed the sentence "So we must fallback to array tricky."

Commit 81b45683487a51b0f4d3b29d37f20d6d078544e4
killed the fallback to the negative array trick.


I also resolved a conflict.

Your series is now available in the following branch.

git://git.kernel.org/pub/scm/linux/kernel/git/masahiroy/linux-kbuild kbuild


Please double check if I did it correctly.


--
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v3 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimizatio
Date: Mon, 29 Oct 2018 13:16:02 +0000
Message-ID: <CAK7LNAS8UPXBbOpG9Zy_aP7an2jbq5H7uQGymUAn+hv46WQ4Bw () mail ! gmail ! com>
--------------------
On Sun, Oct 28, 2018 at 10:11 PM Changbin Du <changbin.du@gmail.com> wrote:
>
> This will apply GCC '-Og' optimization level which is supported
> since GCC 4.8. This optimization level offers a reasonable level
> of optimization while maintaining fast compilation and a good
> debugging experience. It is similar to '-O1' while perferring
> to keep debug ability over runtime speed.
>
> If enabling this option breaks your kernel, you should either
> disable this or find a fix (mostly in the arch code). Currently
> this option has only been tested on x86_64 and arm platform.
>
> This option can satisfy people who was searching for a method
> to disable compiler optimizations so to achieve better kernel
> debugging experience with kgdb or qemu.
>
> The main problem of '-Og' is we must not use __attribute__((error(msg))).
> The compiler will report error though the call to error function
> still can be optimize out. So we must fallback to array tricky.


I removed the sentence "So we must fallback to array tricky."

Commit 81b45683487a51b0f4d3b29d37f20d6d078544e4
killed the fallback to the negative array trick.


I also resolved a conflict.

Your series is now available in the following branch.

git://git.kernel.org/pub/scm/linux/kernel/git/masahiroy/linux-kbuild kbuild


Please double check if I did it correctly.


--
Best Regards
Masahiro Yamada

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kernel
Subject: Re: [PATCH v3 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimizatio
Date: Mon, 29 Oct 2018 13:16:02 +0000
Message-ID: <CAK7LNAS8UPXBbOpG9Zy_aP7an2jbq5H7uQGymUAn+hv46WQ4Bw () mail ! gmail ! com>
--------------------
On Sun, Oct 28, 2018 at 10:11 PM Changbin Du <changbin.du@gmail.com> wrote:
>
> This will apply GCC '-Og' optimization level which is supported
> since GCC 4.8. This optimization level offers a reasonable level
> of optimization while maintaining fast compilation and a good
> debugging experience. It is similar to '-O1' while perferring
> to keep debug ability over runtime speed.
>
> If enabling this option breaks your kernel, you should either
> disable this or find a fix (mostly in the arch code). Currently
> this option has only been tested on x86_64 and arm platform.
>
> This option can satisfy people who was searching for a method
> to disable compiler optimizations so to achieve better kernel
> debugging experience with kgdb or qemu.
>
> The main problem of '-Og' is we must not use __attribute__((error(msg))).
> The compiler will report error though the call to error function
> still can be optimize out. So we must fallback to array tricky.


I removed the sentence "So we must fallback to array tricky."

Commit 81b45683487a51b0f4d3b29d37f20d6d078544e4
killed the fallback to the negative array trick.


I also resolved a conflict.

Your series is now available in the following branch.

git://git.kernel.org/pub/scm/linux/kernel/git/masahiroy/linux-kbuild kbuild


Please double check if I did it correctly.


--
Best Regards
Masahiro Yamada
================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-kernel
Subject: Re: [PATCH v3 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimizatio
Date: Mon, 29 Oct 2018 14:54:51 +0000
Message-ID: <20181029145320.xjt4ydr5gierv57n () mail ! google ! com>
--------------------
On Mon, Oct 29, 2018 at 10:16:02PM +0900, Masahiro Yamada wrote:
> On Sun, Oct 28, 2018 at 10:11 PM Changbin Du <changbin.du@gmail.com> wrote:
> >
> > This will apply GCC '-Og' optimization level which is supported
> > since GCC 4.8. This optimization level offers a reasonable level
> > of optimization while maintaining fast compilation and a good
> > debugging experience. It is similar to '-O1' while perferring
> > to keep debug ability over runtime speed.
> >
> > If enabling this option breaks your kernel, you should either
> > disable this or find a fix (mostly in the arch code). Currently
> > this option has only been tested on x86_64 and arm platform.
> >
> > This option can satisfy people who was searching for a method
> > to disable compiler optimizations so to achieve better kernel
> > debugging experience with kgdb or qemu.
> >
> > The main problem of '-Og' is we must not use __attribute__((error(msg))).
> > The compiler will report error though the call to error function
> > still can be optimize out. So we must fallback to array tricky.
> 
> 
> I removed the sentence "So we must fallback to array tricky."
> 
> Commit 81b45683487a51b0f4d3b29d37f20d6d078544e4
> killed the fallback to the negative array trick.
> 
> 
> I also resolved a conflict.
> 
> Your series is now available in the following branch.
> 
> git://git.kernel.org/pub/scm/linux/kernel/git/masahiroy/linux-kbuild kbuild
> 
> 
> Please double check if I did it correctly.
>
I have tested your kbuild branch and no issues found. Thanks for checking this
series!

> 
> --
> Best Regards
> Masahiro Yamada

-- 
Thanks,
Changbin Du
================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v3 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimizatio
Date: Mon, 29 Oct 2018 14:54:51 +0000
Message-ID: <20181029145320.xjt4ydr5gierv57n () mail ! google ! com>
--------------------
On Mon, Oct 29, 2018 at 10:16:02PM +0900, Masahiro Yamada wrote:
> On Sun, Oct 28, 2018 at 10:11 PM Changbin Du <changbin.du@gmail.com> wrote:
> >
> > This will apply GCC '-Og' optimization level which is supported
> > since GCC 4.8. This optimization level offers a reasonable level
> > of optimization while maintaining fast compilation and a good
> > debugging experience. It is similar to '-O1' while perferring
> > to keep debug ability over runtime speed.
> >
> > If enabling this option breaks your kernel, you should either
> > disable this or find a fix (mostly in the arch code). Currently
> > this option has only been tested on x86_64 and arm platform.
> >
> > This option can satisfy people who was searching for a method
> > to disable compiler optimizations so to achieve better kernel
> > debugging experience with kgdb or qemu.
> >
> > The main problem of '-Og' is we must not use __attribute__((error(msg))).
> > The compiler will report error though the call to error function
> > still can be optimize out. So we must fallback to array tricky.
> 
> 
> I removed the sentence "So we must fallback to array tricky."
> 
> Commit 81b45683487a51b0f4d3b29d37f20d6d078544e4
> killed the fallback to the negative array trick.
> 
> 
> I also resolved a conflict.
> 
> Your series is now available in the following branch.
> 
> git://git.kernel.org/pub/scm/linux/kernel/git/masahiroy/linux-kbuild kbuild
> 
> 
> Please double check if I did it correctly.
>
I have tested your kbuild branch and no issues found. Thanks for checking this
series!

> 
> --
> Best Regards
> Masahiro Yamada

-- 
Thanks,
Changbin Du
================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-kbuild
Subject: Re: [PATCH v3 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimizatio
Date: Mon, 29 Oct 2018 14:54:51 +0000
Message-ID: <20181029145320.xjt4ydr5gierv57n () mail ! google ! com>
--------------------
On Mon, Oct 29, 2018 at 10:16:02PM +0900, Masahiro Yamada wrote:
> On Sun, Oct 28, 2018 at 10:11 PM Changbin Du <changbin.du@gmail.com> wrote:
> >
> > This will apply GCC '-Og' optimization level which is supported
> > since GCC 4.8. This optimization level offers a reasonable level
> > of optimization while maintaining fast compilation and a good
> > debugging experience. It is similar to '-O1' while perferring
> > to keep debug ability over runtime speed.
> >
> > If enabling this option breaks your kernel, you should either
> > disable this or find a fix (mostly in the arch code). Currently
> > this option has only been tested on x86_64 and arm platform.
> >
> > This option can satisfy people who was searching for a method
> > to disable compiler optimizations so to achieve better kernel
> > debugging experience with kgdb or qemu.
> >
> > The main problem of '-Og' is we must not use __attribute__((error(msg))).
> > The compiler will report error though the call to error function
> > still can be optimize out. So we must fallback to array tricky.
> 
> 
> I removed the sentence "So we must fallback to array tricky."
> 
> Commit 81b45683487a51b0f4d3b29d37f20d6d078544e4
> killed the fallback to the negative array trick.
> 
> 
> I also resolved a conflict.
> 
> Your series is now available in the following branch.
> 
> git://git.kernel.org/pub/scm/linux/kernel/git/masahiroy/linux-kbuild kbuild
> 
> 
> Please double check if I did it correctly.
>
I have tested your kbuild branch and no issues found. Thanks for checking this
series!

> 
> --
> Best Regards
> Masahiro Yamada

-- 
Thanks,
Changbin Du
================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v3 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimizatio
Date: Mon, 29 Oct 2018 14:54:51 +0000
Message-ID: <20181029145320.xjt4ydr5gierv57n () mail ! google ! com>
--------------------
On Mon, Oct 29, 2018 at 10:16:02PM +0900, Masahiro Yamada wrote:
> On Sun, Oct 28, 2018 at 10:11 PM Changbin Du <changbin.du@gmail.com> wrote:
> >
> > This will apply GCC '-Og' optimization level which is supported
> > since GCC 4.8. This optimization level offers a reasonable level
> > of optimization while maintaining fast compilation and a good
> > debugging experience. It is similar to '-O1' while perferring
> > to keep debug ability over runtime speed.
> >
> > If enabling this option breaks your kernel, you should either
> > disable this or find a fix (mostly in the arch code). Currently
> > this option has only been tested on x86_64 and arm platform.
> >
> > This option can satisfy people who was searching for a method
> > to disable compiler optimizations so to achieve better kernel
> > debugging experience with kgdb or qemu.
> >
> > The main problem of '-Og' is we must not use __attribute__((error(msg))).
> > The compiler will report error though the call to error function
> > still can be optimize out. So we must fallback to array tricky.
> 
> 
> I removed the sentence "So we must fallback to array tricky."
> 
> Commit 81b45683487a51b0f4d3b29d37f20d6d078544e4
> killed the fallback to the negative array trick.
> 
> 
> I also resolved a conflict.
> 
> Your series is now available in the following branch.
> 
> git://git.kernel.org/pub/scm/linux/kernel/git/masahiroy/linux-kbuild kbuild
> 
> 
> Please double check if I did it correctly.
>
I have tested your kbuild branch and no issues found. Thanks for checking this
series!

> 
> --
> Best Regards
> Masahiro Yamada

-- 
Thanks,
Changbin Du

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================


################################################################################

=== Thread: [PATCH v3 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimization ===

From: Changbin Du <changbin.du () gmail ! com>
To: linux-sparse
Subject: [PATCH v3 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimization
Date: Sun, 28 Oct 2018 13:09:45 +0000
Message-ID: <20181028130945.23581-5-changbin.du () gmail ! com>
--------------------
This will apply GCC '-Og' optimization level which is supported
since GCC 4.8. This optimization level offers a reasonable level
of optimization while maintaining fast compilation and a good
debugging experience. It is similar to '-O1' while perferring
to keep debug ability over runtime speed.

If enabling this option breaks your kernel, you should either
disable this or find a fix (mostly in the arch code). Currently
this option has only been tested on x86_64 and arm platform.

This option can satisfy people who was searching for a method
to disable compiler optimizations so to achieve better kernel
debugging experience with kgdb or qemu.

The main problem of '-Og' is we must not use __attribute__((error(msg))).
The compiler will report error though the call to error function
still can be optimize out. So we must fallback to array tricky.

Comparison of vmlinux size: a bit smaller.

    w/o CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ size vmlinux
       text    data     bss     dec     hex filename
    22665554   9709674  2920908 35296136        21a9388 vmlinux

    w/ CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ size vmlinux
       text    data     bss     dec     hex filename
    21499032   10102758 2920908 34522698        20ec64a vmlinux

Comparison of system performance: a bit drop (~6%).
    This benchmark of kernel compilation is suggested by Ingo Molnar.
    https://lkml.org/lkml/2018/5/2/74

    Preparation: Set cpufreq to 'performance'.
    for ((cpu=0; cpu<120; cpu++)); do
      G=/sys/devices/system/cpu/cpu$cpu/cpufreq/scaling_governor
      [ -f $G ] && echo performance > $G
    done

    w/o CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ perf stat --repeat 5 --null --pre                 '\
        cp -a kernel ../kernel.copy.$(date +%s);         \
        rm -rf *;                                        \
        git checkout .;                                  \
        echo 1 > /proc/sys/vm/drop_caches;               \
        find ../kernel* -type f | xargs cat >/dev/null;  \
        make -j kernel >/dev/null;                       \
        make clean >/dev/null 2>&1;                      \
        sync                                            '\
                                                         \
        make -j8 >/dev/null

    Performance counter stats for 'make -j8' (5 runs):

        219.764246652 seconds time elapsed                   ( +-  0.78% )

    w/ CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ perf stat --repeat 5 --null --pre                 '\
        cp -a kernel ../kernel.copy.$(date +%s);         \
        rm -rf *;                                        \
        git checkout .;                                  \
        echo 1 > /proc/sys/vm/drop_caches;               \
        find ../kernel* -type f | xargs cat >/dev/null;  \
        make -j kernel >/dev/null;                       \
        make clean >/dev/null 2>&1;                      \
        sync                                            '\
                                                         \
        make -j8 >/dev/null

    Performance counter stats for 'make -j8' (5 runs):

         233.574187771 seconds time elapsed                  ( +-  0.19% )

Signed-off-by: Changbin Du <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
Cc: Masahiro Yamada <yamada.masahiro@socionext.com>

---
v3:
  o make CC_OPTIMIZE_FOR_DEBUGGING depends on $(cc-option,-Og)
  o reflect CONFIG_CC_OPTIMIZE_FOR_DEBUGGING in tiny.config
---
 Makefile                     |  5 +++++
 include/linux/compiler-gcc.h |  2 +-
 include/linux/compiler.h     |  2 +-
 init/Kconfig                 | 20 ++++++++++++++++++++
 kernel/configs/tiny.config   |  1 +
 5 files changed, 28 insertions(+), 2 deletions(-)

diff --git a/Makefile b/Makefile
index 04beb822ddfc..10de245a3325 100644
--- a/Makefile
+++ b/Makefile
@@ -657,6 +657,10 @@ KBUILD_CFLAGS	+= $(call cc-disable-warning, format-truncation)
 KBUILD_CFLAGS	+= $(call cc-disable-warning, format-overflow)
 KBUILD_CFLAGS	+= $(call cc-disable-warning, int-in-bool-context)
 
+ifdef CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
+KBUILD_CFLAGS	+= -Og
+KBUILD_CFLAGS	+= $(call cc-disable-warning,maybe-uninitialized,)
+else
 ifdef CONFIG_CC_OPTIMIZE_FOR_SIZE
 KBUILD_CFLAGS	+= $(call cc-option,-Oz,-Os)
 KBUILD_CFLAGS	+= $(call cc-disable-warning,maybe-uninitialized,)
@@ -667,6 +671,7 @@ else
 KBUILD_CFLAGS   += -O2
 endif
 endif
+endif
 
 KBUILD_CFLAGS += $(call cc-ifversion, -lt, 0409, \
 			$(call cc-disable-warning,maybe-uninitialized,))
diff --git a/include/linux/compiler-gcc.h b/include/linux/compiler-gcc.h
index 90ddfefb6c2b..4832c98c7885 100644
--- a/include/linux/compiler-gcc.h
+++ b/include/linux/compiler-gcc.h
@@ -85,7 +85,7 @@
 
 #define __compiletime_object_size(obj) __builtin_object_size(obj, 0)
 
-#ifndef __CHECKER__
+#if !defined(__CHECKER__) && !defined(CONFIG_CC_OPTIMIZE_FOR_DEBUGGING)
 #define __compiletime_warning(message) __attribute__((warning(message)))
 #define __compiletime_error(message) __attribute__((error(message)))
 
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 1921545c6351..3836397bf477 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -350,7 +350,7 @@ static inline void *offset_to_ptr(const int *off)
  * sparse see a constant array size without breaking compiletime_assert on old
  * versions of GCC (e.g. 4.2.4), so hide the array from sparse altogether.
  */
-# ifndef __CHECKER__
+# if !defined(__CHECKER__) && !defined(CONFIG_CC_OPTIMIZE_FOR_DEBUGGING)
 #  define __compiletime_error_fallback(condition) \
 	do { ((void)sizeof(char[1 - 2 * condition])); } while (0)
 # endif
diff --git a/init/Kconfig b/init/Kconfig
index a4112e95724a..0fb9c0b5f1a1 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1105,6 +1105,26 @@ config CC_OPTIMIZE_FOR_SIZE
 
 	  If unsure, say N.
 
+config CC_OPTIMIZE_FOR_DEBUGGING
+	bool "Optimize for better debugging experience (-Og)"
+	depends on $(cc-option,-Og)
+	select NO_AUTO_INLINE
+	help
+	  This will apply GCC '-Og' optimization level which is supported
+	  since GCC 4.8. This optimization level offers a reasonable level
+	  of optimization while maintaining fast compilation and a good
+	  debugging experience. It is similar to '-O1' while preferring to
+	  keep debug ability over runtime speed. The overall performance
+	  will drop a bit (~6%).
+
+	  Use only if you want to debug the kernel, especially if you want
+	  to have better kernel debugging experience with gdb facilities
+	  like kgdb or qemu. If enabling this option breaks your kernel,
+	  you should either disable this or find a fix (mostly in the arch
+	  code).
+
+	  If unsure, select N.
+
 endchoice
 
 config HAVE_LD_DEAD_CODE_DATA_ELIMINATION
diff --git a/kernel/configs/tiny.config b/kernel/configs/tiny.config
index 7fa0c4ae6394..599ea86b0800 100644
--- a/kernel/configs/tiny.config
+++ b/kernel/configs/tiny.config
@@ -1,5 +1,6 @@
 # CONFIG_CC_OPTIMIZE_FOR_PERFORMANCE is not set
 CONFIG_CC_OPTIMIZE_FOR_SIZE=y
+# CONFIG_CC_OPTIMIZE_FOR_DEBUGGING is not set
 # CONFIG_KERNEL_GZIP is not set
 # CONFIG_KERNEL_BZIP2 is not set
 # CONFIG_KERNEL_LZMA is not set
-- 
2.17.1

================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-kbuild
Subject: [PATCH v3 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimization
Date: Sun, 28 Oct 2018 13:09:45 +0000
Message-ID: <20181028130945.23581-5-changbin.du () gmail ! com>
--------------------
This will apply GCC '-Og' optimization level which is supported
since GCC 4.8. This optimization level offers a reasonable level
of optimization while maintaining fast compilation and a good
debugging experience. It is similar to '-O1' while perferring
to keep debug ability over runtime speed.

If enabling this option breaks your kernel, you should either
disable this or find a fix (mostly in the arch code). Currently
this option has only been tested on x86_64 and arm platform.

This option can satisfy people who was searching for a method
to disable compiler optimizations so to achieve better kernel
debugging experience with kgdb or qemu.

The main problem of '-Og' is we must not use __attribute__((error(msg))).
The compiler will report error though the call to error function
still can be optimize out. So we must fallback to array tricky.

Comparison of vmlinux size: a bit smaller.

    w/o CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ size vmlinux
       text    data     bss     dec     hex filename
    22665554   9709674  2920908 35296136        21a9388 vmlinux

    w/ CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ size vmlinux
       text    data     bss     dec     hex filename
    21499032   10102758 2920908 34522698        20ec64a vmlinux

Comparison of system performance: a bit drop (~6%).
    This benchmark of kernel compilation is suggested by Ingo Molnar.
    https://lkml.org/lkml/2018/5/2/74

    Preparation: Set cpufreq to 'performance'.
    for ((cpu=0; cpu<120; cpu++)); do
      G=/sys/devices/system/cpu/cpu$cpu/cpufreq/scaling_governor
      [ -f $G ] && echo performance > $G
    done

    w/o CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ perf stat --repeat 5 --null --pre                 '\
        cp -a kernel ../kernel.copy.$(date +%s);         \
        rm -rf *;                                        \
        git checkout .;                                  \
        echo 1 > /proc/sys/vm/drop_caches;               \
        find ../kernel* -type f | xargs cat >/dev/null;  \
        make -j kernel >/dev/null;                       \
        make clean >/dev/null 2>&1;                      \
        sync                                            '\
                                                         \
        make -j8 >/dev/null

    Performance counter stats for 'make -j8' (5 runs):

        219.764246652 seconds time elapsed                   ( +-  0.78% )

    w/ CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ perf stat --repeat 5 --null --pre                 '\
        cp -a kernel ../kernel.copy.$(date +%s);         \
        rm -rf *;                                        \
        git checkout .;                                  \
        echo 1 > /proc/sys/vm/drop_caches;               \
        find ../kernel* -type f | xargs cat >/dev/null;  \
        make -j kernel >/dev/null;                       \
        make clean >/dev/null 2>&1;                      \
        sync                                            '\
                                                         \
        make -j8 >/dev/null

    Performance counter stats for 'make -j8' (5 runs):

         233.574187771 seconds time elapsed                  ( +-  0.19% )

Signed-off-by: Changbin Du <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
Cc: Masahiro Yamada <yamada.masahiro@socionext.com>

---
v3:
  o make CC_OPTIMIZE_FOR_DEBUGGING depends on $(cc-option,-Og)
  o reflect CONFIG_CC_OPTIMIZE_FOR_DEBUGGING in tiny.config
---
 Makefile                     |  5 +++++
 include/linux/compiler-gcc.h |  2 +-
 include/linux/compiler.h     |  2 +-
 init/Kconfig                 | 20 ++++++++++++++++++++
 kernel/configs/tiny.config   |  1 +
 5 files changed, 28 insertions(+), 2 deletions(-)

diff --git a/Makefile b/Makefile
index 04beb822ddfc..10de245a3325 100644
--- a/Makefile
+++ b/Makefile
@@ -657,6 +657,10 @@ KBUILD_CFLAGS	+= $(call cc-disable-warning, format-truncation)
 KBUILD_CFLAGS	+= $(call cc-disable-warning, format-overflow)
 KBUILD_CFLAGS	+= $(call cc-disable-warning, int-in-bool-context)
 
+ifdef CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
+KBUILD_CFLAGS	+= -Og
+KBUILD_CFLAGS	+= $(call cc-disable-warning,maybe-uninitialized,)
+else
 ifdef CONFIG_CC_OPTIMIZE_FOR_SIZE
 KBUILD_CFLAGS	+= $(call cc-option,-Oz,-Os)
 KBUILD_CFLAGS	+= $(call cc-disable-warning,maybe-uninitialized,)
@@ -667,6 +671,7 @@ else
 KBUILD_CFLAGS   += -O2
 endif
 endif
+endif
 
 KBUILD_CFLAGS += $(call cc-ifversion, -lt, 0409, \
 			$(call cc-disable-warning,maybe-uninitialized,))
diff --git a/include/linux/compiler-gcc.h b/include/linux/compiler-gcc.h
index 90ddfefb6c2b..4832c98c7885 100644
--- a/include/linux/compiler-gcc.h
+++ b/include/linux/compiler-gcc.h
@@ -85,7 +85,7 @@
 
 #define __compiletime_object_size(obj) __builtin_object_size(obj, 0)
 
-#ifndef __CHECKER__
+#if !defined(__CHECKER__) && !defined(CONFIG_CC_OPTIMIZE_FOR_DEBUGGING)
 #define __compiletime_warning(message) __attribute__((warning(message)))
 #define __compiletime_error(message) __attribute__((error(message)))
 
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 1921545c6351..3836397bf477 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -350,7 +350,7 @@ static inline void *offset_to_ptr(const int *off)
  * sparse see a constant array size without breaking compiletime_assert on old
  * versions of GCC (e.g. 4.2.4), so hide the array from sparse altogether.
  */
-# ifndef __CHECKER__
+# if !defined(__CHECKER__) && !defined(CONFIG_CC_OPTIMIZE_FOR_DEBUGGING)
 #  define __compiletime_error_fallback(condition) \
 	do { ((void)sizeof(char[1 - 2 * condition])); } while (0)
 # endif
diff --git a/init/Kconfig b/init/Kconfig
index a4112e95724a..0fb9c0b5f1a1 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1105,6 +1105,26 @@ config CC_OPTIMIZE_FOR_SIZE
 
 	  If unsure, say N.
 
+config CC_OPTIMIZE_FOR_DEBUGGING
+	bool "Optimize for better debugging experience (-Og)"
+	depends on $(cc-option,-Og)
+	select NO_AUTO_INLINE
+	help
+	  This will apply GCC '-Og' optimization level which is supported
+	  since GCC 4.8. This optimization level offers a reasonable level
+	  of optimization while maintaining fast compilation and a good
+	  debugging experience. It is similar to '-O1' while preferring to
+	  keep debug ability over runtime speed. The overall performance
+	  will drop a bit (~6%).
+
+	  Use only if you want to debug the kernel, especially if you want
+	  to have better kernel debugging experience with gdb facilities
+	  like kgdb or qemu. If enabling this option breaks your kernel,
+	  you should either disable this or find a fix (mostly in the arch
+	  code).
+
+	  If unsure, select N.
+
 endchoice
 
 config HAVE_LD_DEAD_CODE_DATA_ELIMINATION
diff --git a/kernel/configs/tiny.config b/kernel/configs/tiny.config
index 7fa0c4ae6394..599ea86b0800 100644
--- a/kernel/configs/tiny.config
+++ b/kernel/configs/tiny.config
@@ -1,5 +1,6 @@
 # CONFIG_CC_OPTIMIZE_FOR_PERFORMANCE is not set
 CONFIG_CC_OPTIMIZE_FOR_SIZE=y
+# CONFIG_CC_OPTIMIZE_FOR_DEBUGGING is not set
 # CONFIG_KERNEL_GZIP is not set
 # CONFIG_KERNEL_BZIP2 is not set
 # CONFIG_KERNEL_LZMA is not set
-- 
2.17.1

================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-kernel
Subject: [PATCH v3 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimization
Date: Sun, 28 Oct 2018 13:09:45 +0000
Message-ID: <20181028130945.23581-5-changbin.du () gmail ! com>
--------------------
This will apply GCC '-Og' optimization level which is supported
since GCC 4.8. This optimization level offers a reasonable level
of optimization while maintaining fast compilation and a good
debugging experience. It is similar to '-O1' while perferring
to keep debug ability over runtime speed.

If enabling this option breaks your kernel, you should either
disable this or find a fix (mostly in the arch code). Currently
this option has only been tested on x86_64 and arm platform.

This option can satisfy people who was searching for a method
to disable compiler optimizations so to achieve better kernel
debugging experience with kgdb or qemu.

The main problem of '-Og' is we must not use __attribute__((error(msg))).
The compiler will report error though the call to error function
still can be optimize out. So we must fallback to array tricky.

Comparison of vmlinux size: a bit smaller.

    w/o CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ size vmlinux
       text    data     bss     dec     hex filename
    22665554   9709674  2920908 35296136        21a9388 vmlinux

    w/ CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ size vmlinux
       text    data     bss     dec     hex filename
    21499032   10102758 2920908 34522698        20ec64a vmlinux

Comparison of system performance: a bit drop (~6%).
    This benchmark of kernel compilation is suggested by Ingo Molnar.
    https://lkml.org/lkml/2018/5/2/74

    Preparation: Set cpufreq to 'performance'.
    for ((cpu=0; cpu<120; cpu++)); do
      G=/sys/devices/system/cpu/cpu$cpu/cpufreq/scaling_governor
      [ -f $G ] && echo performance > $G
    done

    w/o CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ perf stat --repeat 5 --null --pre                 '\
        cp -a kernel ../kernel.copy.$(date +%s);         \
        rm -rf *;                                        \
        git checkout .;                                  \
        echo 1 > /proc/sys/vm/drop_caches;               \
        find ../kernel* -type f | xargs cat >/dev/null;  \
        make -j kernel >/dev/null;                       \
        make clean >/dev/null 2>&1;                      \
        sync                                            '\
                                                         \
        make -j8 >/dev/null

    Performance counter stats for 'make -j8' (5 runs):

        219.764246652 seconds time elapsed                   ( +-  0.78% )

    w/ CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ perf stat --repeat 5 --null --pre                 '\
        cp -a kernel ../kernel.copy.$(date +%s);         \
        rm -rf *;                                        \
        git checkout .;                                  \
        echo 1 > /proc/sys/vm/drop_caches;               \
        find ../kernel* -type f | xargs cat >/dev/null;  \
        make -j kernel >/dev/null;                       \
        make clean >/dev/null 2>&1;                      \
        sync                                            '\
                                                         \
        make -j8 >/dev/null

    Performance counter stats for 'make -j8' (5 runs):

         233.574187771 seconds time elapsed                  ( +-  0.19% )

Signed-off-by: Changbin Du <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
Cc: Masahiro Yamada <yamada.masahiro@socionext.com>

---
v3:
  o make CC_OPTIMIZE_FOR_DEBUGGING depends on $(cc-option,-Og)
  o reflect CONFIG_CC_OPTIMIZE_FOR_DEBUGGING in tiny.config
---
 Makefile                     |  5 +++++
 include/linux/compiler-gcc.h |  2 +-
 include/linux/compiler.h     |  2 +-
 init/Kconfig                 | 20 ++++++++++++++++++++
 kernel/configs/tiny.config   |  1 +
 5 files changed, 28 insertions(+), 2 deletions(-)

diff --git a/Makefile b/Makefile
index 04beb822ddfc..10de245a3325 100644
--- a/Makefile
+++ b/Makefile
@@ -657,6 +657,10 @@ KBUILD_CFLAGS	+= $(call cc-disable-warning, format-truncation)
 KBUILD_CFLAGS	+= $(call cc-disable-warning, format-overflow)
 KBUILD_CFLAGS	+= $(call cc-disable-warning, int-in-bool-context)
 
+ifdef CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
+KBUILD_CFLAGS	+= -Og
+KBUILD_CFLAGS	+= $(call cc-disable-warning,maybe-uninitialized,)
+else
 ifdef CONFIG_CC_OPTIMIZE_FOR_SIZE
 KBUILD_CFLAGS	+= $(call cc-option,-Oz,-Os)
 KBUILD_CFLAGS	+= $(call cc-disable-warning,maybe-uninitialized,)
@@ -667,6 +671,7 @@ else
 KBUILD_CFLAGS   += -O2
 endif
 endif
+endif
 
 KBUILD_CFLAGS += $(call cc-ifversion, -lt, 0409, \
 			$(call cc-disable-warning,maybe-uninitialized,))
diff --git a/include/linux/compiler-gcc.h b/include/linux/compiler-gcc.h
index 90ddfefb6c2b..4832c98c7885 100644
--- a/include/linux/compiler-gcc.h
+++ b/include/linux/compiler-gcc.h
@@ -85,7 +85,7 @@
 
 #define __compiletime_object_size(obj) __builtin_object_size(obj, 0)
 
-#ifndef __CHECKER__
+#if !defined(__CHECKER__) && !defined(CONFIG_CC_OPTIMIZE_FOR_DEBUGGING)
 #define __compiletime_warning(message) __attribute__((warning(message)))
 #define __compiletime_error(message) __attribute__((error(message)))
 
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 1921545c6351..3836397bf477 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -350,7 +350,7 @@ static inline void *offset_to_ptr(const int *off)
  * sparse see a constant array size without breaking compiletime_assert on old
  * versions of GCC (e.g. 4.2.4), so hide the array from sparse altogether.
  */
-# ifndef __CHECKER__
+# if !defined(__CHECKER__) && !defined(CONFIG_CC_OPTIMIZE_FOR_DEBUGGING)
 #  define __compiletime_error_fallback(condition) \
 	do { ((void)sizeof(char[1 - 2 * condition])); } while (0)
 # endif
diff --git a/init/Kconfig b/init/Kconfig
index a4112e95724a..0fb9c0b5f1a1 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1105,6 +1105,26 @@ config CC_OPTIMIZE_FOR_SIZE
 
 	  If unsure, say N.
 
+config CC_OPTIMIZE_FOR_DEBUGGING
+	bool "Optimize for better debugging experience (-Og)"
+	depends on $(cc-option,-Og)
+	select NO_AUTO_INLINE
+	help
+	  This will apply GCC '-Og' optimization level which is supported
+	  since GCC 4.8. This optimization level offers a reasonable level
+	  of optimization while maintaining fast compilation and a good
+	  debugging experience. It is similar to '-O1' while preferring to
+	  keep debug ability over runtime speed. The overall performance
+	  will drop a bit (~6%).
+
+	  Use only if you want to debug the kernel, especially if you want
+	  to have better kernel debugging experience with gdb facilities
+	  like kgdb or qemu. If enabling this option breaks your kernel,
+	  you should either disable this or find a fix (mostly in the arch
+	  code).
+
+	  If unsure, select N.
+
 endchoice
 
 config HAVE_LD_DEAD_CODE_DATA_ELIMINATION
diff --git a/kernel/configs/tiny.config b/kernel/configs/tiny.config
index 7fa0c4ae6394..599ea86b0800 100644
--- a/kernel/configs/tiny.config
+++ b/kernel/configs/tiny.config
@@ -1,5 +1,6 @@
 # CONFIG_CC_OPTIMIZE_FOR_PERFORMANCE is not set
 CONFIG_CC_OPTIMIZE_FOR_SIZE=y
+# CONFIG_CC_OPTIMIZE_FOR_DEBUGGING is not set
 # CONFIG_KERNEL_GZIP is not set
 # CONFIG_KERNEL_BZIP2 is not set
 # CONFIG_KERNEL_LZMA is not set
-- 
2.17.1

================================================================================

From: Changbin Du <changbin.du () gmail ! com>
To: linux-arm-kernel
Subject: [PATCH v3 4/4] kernel hacking: new config CC_OPTIMIZE_FOR_DEBUGGING to apply GCC -Og optimization
Date: Sun, 28 Oct 2018 13:09:45 +0000
Message-ID: <20181028130945.23581-5-changbin.du () gmail ! com>
--------------------
This will apply GCC '-Og' optimization level which is supported
since GCC 4.8. This optimization level offers a reasonable level
of optimization while maintaining fast compilation and a good
debugging experience. It is similar to '-O1' while perferring
to keep debug ability over runtime speed.

If enabling this option breaks your kernel, you should either
disable this or find a fix (mostly in the arch code). Currently
this option has only been tested on x86_64 and arm platform.

This option can satisfy people who was searching for a method
to disable compiler optimizations so to achieve better kernel
debugging experience with kgdb or qemu.

The main problem of '-Og' is we must not use __attribute__((error(msg))).
The compiler will report error though the call to error function
still can be optimize out. So we must fallback to array tricky.

Comparison of vmlinux size: a bit smaller.

    w/o CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ size vmlinux
       text    data     bss     dec     hex filename
    22665554   9709674  2920908 35296136        21a9388 vmlinux

    w/ CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ size vmlinux
       text    data     bss     dec     hex filename
    21499032   10102758 2920908 34522698        20ec64a vmlinux

Comparison of system performance: a bit drop (~6%).
    This benchmark of kernel compilation is suggested by Ingo Molnar.
    https://lkml.org/lkml/2018/5/2/74

    Preparation: Set cpufreq to 'performance'.
    for ((cpu=0; cpu<120; cpu++)); do
      G=/sys/devices/system/cpu/cpu$cpu/cpufreq/scaling_governor
      [ -f $G ] && echo performance > $G
    done

    w/o CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ perf stat --repeat 5 --null --pre                 '\
        cp -a kernel ../kernel.copy.$(date +%s);         \
        rm -rf *;                                        \
        git checkout .;                                  \
        echo 1 > /proc/sys/vm/drop_caches;               \
        find ../kernel* -type f | xargs cat >/dev/null;  \
        make -j kernel >/dev/null;                       \
        make clean >/dev/null 2>&1;                      \
        sync                                            '\
                                                         \
        make -j8 >/dev/null

    Performance counter stats for 'make -j8' (5 runs):

        219.764246652 seconds time elapsed                   ( +-  0.78% )

    w/ CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
    $ perf stat --repeat 5 --null --pre                 '\
        cp -a kernel ../kernel.copy.$(date +%s);         \
        rm -rf *;                                        \
        git checkout .;                                  \
        echo 1 > /proc/sys/vm/drop_caches;               \
        find ../kernel* -type f | xargs cat >/dev/null;  \
        make -j kernel >/dev/null;                       \
        make clean >/dev/null 2>&1;                      \
        sync                                            '\
                                                         \
        make -j8 >/dev/null

    Performance counter stats for 'make -j8' (5 runs):

         233.574187771 seconds time elapsed                  ( +-  0.19% )

Signed-off-by: Changbin Du <changbin.du@gmail.com>
Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
Cc: Masahiro Yamada <yamada.masahiro@socionext.com>

---
v3:
  o make CC_OPTIMIZE_FOR_DEBUGGING depends on $(cc-option,-Og)
  o reflect CONFIG_CC_OPTIMIZE_FOR_DEBUGGING in tiny.config
---
 Makefile                     |  5 +++++
 include/linux/compiler-gcc.h |  2 +-
 include/linux/compiler.h     |  2 +-
 init/Kconfig                 | 20 ++++++++++++++++++++
 kernel/configs/tiny.config   |  1 +
 5 files changed, 28 insertions(+), 2 deletions(-)

diff --git a/Makefile b/Makefile
index 04beb822ddfc..10de245a3325 100644
--- a/Makefile
+++ b/Makefile
@@ -657,6 +657,10 @@ KBUILD_CFLAGS	+= $(call cc-disable-warning, format-truncation)
 KBUILD_CFLAGS	+= $(call cc-disable-warning, format-overflow)
 KBUILD_CFLAGS	+= $(call cc-disable-warning, int-in-bool-context)
 
+ifdef CONFIG_CC_OPTIMIZE_FOR_DEBUGGING
+KBUILD_CFLAGS	+= -Og
+KBUILD_CFLAGS	+= $(call cc-disable-warning,maybe-uninitialized,)
+else
 ifdef CONFIG_CC_OPTIMIZE_FOR_SIZE
 KBUILD_CFLAGS	+= $(call cc-option,-Oz,-Os)
 KBUILD_CFLAGS	+= $(call cc-disable-warning,maybe-uninitialized,)
@@ -667,6 +671,7 @@ else
 KBUILD_CFLAGS   += -O2
 endif
 endif
+endif
 
 KBUILD_CFLAGS += $(call cc-ifversion, -lt, 0409, \
 			$(call cc-disable-warning,maybe-uninitialized,))
diff --git a/include/linux/compiler-gcc.h b/include/linux/compiler-gcc.h
index 90ddfefb6c2b..4832c98c7885 100644
--- a/include/linux/compiler-gcc.h
+++ b/include/linux/compiler-gcc.h
@@ -85,7 +85,7 @@
 
 #define __compiletime_object_size(obj) __builtin_object_size(obj, 0)
 
-#ifndef __CHECKER__
+#if !defined(__CHECKER__) && !defined(CONFIG_CC_OPTIMIZE_FOR_DEBUGGING)
 #define __compiletime_warning(message) __attribute__((warning(message)))
 #define __compiletime_error(message) __attribute__((error(message)))
 
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 1921545c6351..3836397bf477 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -350,7 +350,7 @@ static inline void *offset_to_ptr(const int *off)
  * sparse see a constant array size without breaking compiletime_assert on old
  * versions of GCC (e.g. 4.2.4), so hide the array from sparse altogether.
  */
-# ifndef __CHECKER__
+# if !defined(__CHECKER__) && !defined(CONFIG_CC_OPTIMIZE_FOR_DEBUGGING)
 #  define __compiletime_error_fallback(condition) \
 	do { ((void)sizeof(char[1 - 2 * condition])); } while (0)
 # endif
diff --git a/init/Kconfig b/init/Kconfig
index a4112e95724a..0fb9c0b5f1a1 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1105,6 +1105,26 @@ config CC_OPTIMIZE_FOR_SIZE
 
 	  If unsure, say N.
 
+config CC_OPTIMIZE_FOR_DEBUGGING
+	bool "Optimize for better debugging experience (-Og)"
+	depends on $(cc-option,-Og)
+	select NO_AUTO_INLINE
+	help
+	  This will apply GCC '-Og' optimization level which is supported
+	  since GCC 4.8. This optimization level offers a reasonable level
+	  of optimization while maintaining fast compilation and a good
+	  debugging experience. It is similar to '-O1' while preferring to
+	  keep debug ability over runtime speed. The overall performance
+	  will drop a bit (~6%).
+
+	  Use only if you want to debug the kernel, especially if you want
+	  to have better kernel debugging experience with gdb facilities
+	  like kgdb or qemu. If enabling this option breaks your kernel,
+	  you should either disable this or find a fix (mostly in the arch
+	  code).
+
+	  If unsure, select N.
+
 endchoice
 
 config HAVE_LD_DEAD_CODE_DATA_ELIMINATION
diff --git a/kernel/configs/tiny.config b/kernel/configs/tiny.config
index 7fa0c4ae6394..599ea86b0800 100644
--- a/kernel/configs/tiny.config
+++ b/kernel/configs/tiny.config
@@ -1,5 +1,6 @@
 # CONFIG_CC_OPTIMIZE_FOR_PERFORMANCE is not set
 CONFIG_CC_OPTIMIZE_FOR_SIZE=y
+# CONFIG_CC_OPTIMIZE_FOR_DEBUGGING is not set
 # CONFIG_KERNEL_GZIP is not set
 # CONFIG_KERNEL_BZIP2 is not set
 # CONFIG_KERNEL_LZMA is not set
-- 
2.17.1


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================


################################################################################

=== Thread: [PATCH v4 0/4] predefined macros for intmax_t/intptr_t/... ===

From: Ramsay Jones <ramsay () ramsayjones ! plus ! com>
To: linux-sparse
Subject: Re: [PATCH v4 0/4] predefined macros for intmax_t/intptr_t/...
Date: Mon, 17 Dec 2018 18:08:28 +0000
Message-ID: <b4fb5d26-72ca-ef40-f30e-53dfa8a5a29b () ramsayjones ! plus ! com>
--------------------


On 17/12/2018 00:02, Luc Van Oostenryck wrote:
> Some types have already their TYPE/SIZEOF/MAX macros.
> These patches add them for the missing types: ptrdiff,
> int{ptr,max,64,32,16,8}_t and their unsigned version.
> 
> Note: some of the types vary a lot depending on the
>       architecture, OS & exact ABI used. This is
>       specially the case for for int32_t.
>       The definition in these patches should now be correct
>       for the most common archs & ABI used for the kernel.
> 
>       It seems now to be correct and complete (for Linux)
>       on x86-64, i386, ppc64 & arm64.
> 
> Changes since v1:
> * correct _MAX value of unsigned types (+ testing)
> * fix definition PTYPE_WIDTH/PTYPE_TYPE
> * fix inverted type for INT8/UINT8
> * define shortcur PTYPE_ALL_T
> * add tests for PTRDIFF/SIZE/INTMAX/INT{8,16,32,64}
> * add definitions for wint_t, char16_t & char32t
> 
> Changes since v2:
> * mv slong_ctype's entry in typenames after long_ctype
> * use the type for predefined_max()
> * add a temptative arch_mach initialized with the native
>   architecture and use it to set more correctly wchar_t,
>   int32_t, ...
> * fix size of long double, especially on x86-64
> 
> Change since v3:
> * SCHAR must refer to schar_ctype, not the plain char_ctype
> * remove now unneeded #ifdefery to initialize int32_t
> * add predefine for __CHAR_UNSIGNED__
> * add predefine for __{WCHAR,WINT}_MIN__
> 
> To make clearer what changed since v3, only the delta and new
> patches are posted.

Ah, sorry Luc, but I didn't manage to do much testing this
weekend after all (_many_ higher priority interrupts!).

I did manage _some_ testing, first with the v3 patches based
on the 'master' branch from a couple of days ago. Then I saw
the 'master' branch gained some additional patches, so I rebased
the v3 patches on top (of 'master' @ 5532461), which had a
minor text conflict (which was automatically resolved by rebase).

So, I will add these on top tonight ... (but I still have some
other things I need to do as well :( ).

So, with the limited testing of v3, I noticed that (on Linux) the
gcc '-mx32' mode differed from sparse in the size of a 'long double',
which was 16 on gcc and 12 on sparse.

As previously noted, on cygwin WCHAR is an 'unsigned short'. Also, the
gcc '-mx32' mode on cygwin is useless (probably unsupported/not defined).
Indeed, the '-mx32' mode on Linux requires kernel support, which the
fedora project are talking about removing soon. (apparently, nobody
uses it anyway!).

I will try to get to this testing soon. (but I would not be unhappy
if you pushed this out, as it stands, and go 'incremental' with any
additional 'fixes'). :-D

ATB,
Ramsay Jones


================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v4 0/4] predefined macros for intmax_t/intptr_t/...
Date: Mon, 17 Dec 2018 20:45:52 +0000
Message-ID: <20181217204551.ydiu4ztyxxcn225j () ltop ! local>
--------------------
On Mon, Dec 17, 2018 at 06:08:28PM +0000, Ramsay Jones wrote:
> On 17/12/2018 00:02, Luc Van Oostenryck wrote:
> > Some types have already their TYPE/SIZEOF/MAX macros.
> > These patches add them for the missing types: ptrdiff,
> > int{ptr,max,64,32,16,8}_t and their unsigned version.
...
> > Change since v3:
> > * SCHAR must refer to schar_ctype, not the plain char_ctype
> > * remove now unneeded #ifdefery to initialize int32_t
> > * add predefine for __CHAR_UNSIGNED__
> > * add predefine for __{WCHAR,WINT}_MIN__
> > 
> > To make clearer what changed since v3, only the delta and new
> > patches are posted.
> 
> Ah, sorry Luc, but I didn't manage to do much testing this
> weekend after all (_many_ higher priority interrupts!).

No problem, really.
 
> I did manage _some_ testing, first with the v3 patches based
> on the 'master' branch from a couple of days ago. Then I saw
> the 'master' branch gained some additional patches, so I rebased
> the v3 patches on top (of 'master' @ 5532461), which had a
> minor text conflict (which was automatically resolved by rebase).

Ah OK :)
I purposely not rebase the different versions of a series in
its final stages to not change the context, be distracted by
unrelated changes when testing but well ... :)

> So, I will add these on top tonight ... (but I still have some
> other things I need to do as well :( ).

No worries.

> So, with the limited testing of v3, I noticed that (on Linux) the
> gcc '-mx32' mode differed from sparse in the size of a 'long double',
> which was 16 on gcc and 12 on sparse.

Mmmm yes, it wasn't intended.

> As previously noted, on cygwin WCHAR is an 'unsigned short'.

Yes, in fact kinda I expected a patch from you for it because I
don't have such platform on hand to test it. But no problem I can
just add this for WCHAR.

> Also, the
> gcc '-mx32' mode on cygwin is useless (probably unsupported/not defined).
> Indeed, the '-mx32' mode on Linux requires kernel support, which the
> fedora project are talking about removing soon. (apparently, nobody
> uses it anyway!).

Yes, I think also that -mx32 on cygwin is useless.
To be honest, the whole x86-x32 really annoys me because:
* it creates lots of complications for something that is seldom used
* worse, while I know that some people are working on it, it also
  looks as it is still broken in a lot of context
* Debian's (unofficial port) install images for it are still broken
  for me (it starts but network is never detected) and are dated
  from 2015-2016.
So, I don't want to spend/waste much time on it.

> I will try to get to this testing soon. (but I would not be unhappy
> if you pushed this out, as it stands, and go 'incremental' with any
> additional 'fixes'). :-D

Yes, it's was essentially my intention and why I only posted a delta
for -v4.

Best regards,
-- Luc
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v4 0/4] predefined macros for intmax_t/intptr_t/...
Date: Mon, 17 Dec 2018 22:26:18 +0000
Message-ID: <20181217222617.ffbvoy235lpznehw () ltop ! local>
--------------------
On Mon, Dec 17, 2018 at 09:17:56PM +0000, Ramsay Jones wrote:
> On 17/12/2018 20:45, Luc Van Oostenryck wrote:
> > On Mon, Dec 17, 2018 at 06:08:28PM +0000, Ramsay Jones wrote:
> 
> >> I will try to get to this testing soon. (but I would not be unhappy
> >> if you pushed this out, as it stands, and go 'incremental' with any
> >> additional 'fixes'). :-D
> > 
> > Yes, it's was essentially my intention and why I only posted a delta
> > for -v4.
> 
> Great! BTW, I noticed a typo in one commit message for patch titled
> 'add predefined macros for [u]int32_t', namely s/[u]loing/[u]long/.
> Also, in the patch titled 'fix the size of long double', I think
> that you meant s/The odd on here/The odd one here/?

Yup. Thanks.
I've pushed it now.

--Luc
================================================================================


################################################################################

=== Thread: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer ===

From: Andrew Morton <akpm () linux-foundation ! org>
To: linux-sparse
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Sat, 30 Jun 2018 02:41:17 +0000
Message-ID: <20180629194117.01b2d31e805808eee5c97b4d () linux-foundation ! org>
--------------------
On Fri, 29 Jun 2018 14:45:08 +0200 Andrey Konovalov <andreyknvl@google.com> wrote:

> >> What kind of memory consumption testing would you like to see?
> >
> > Well, 100kb or so is a teeny amount on virtually any machine.  I'm
> > assuming the savings are (much) more significant once the machine gets
> > loaded up and doing work?
> 
> So with clean kernel after boot we get 40 kb memory usage. With KASAN
> it is ~120 kb, which is 200% overhead. With KHWASAN it's 50 kb, which
> is 25% overhead. This should approximately scale to any amounts of
> used slab memory. For example with 100 mb memory usage we would get
> +200 mb for KASAN and +25 mb with KHWASAN. (And KASAN also requires
> quarantine for better use-after-free detection). I can explicitly
> mention the overhead in %s in the changelog.
> 
> If you think it makes sense, I can also make separate measurements
> with some workload. What kind of workload should I use?

Whatever workload people were running when they encountered problems
with KASAN memory consumption ;)

I dunno, something simple.  `find / > /dev/null'?

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-kbuild
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Thu, 02 Aug 2018 14:11:18 +0000
Message-ID: <da873a0d-3c75-0a00-974e-824f1eb49de6 () virtuozzo ! com>
--------------------
On 08/02/2018 04:52 PM, Catalin Marinas wrote:
> 
>> If somebody has a practical idea how to detect these statically, let's
>> do it. Otherwise let's go with the traditional solution to this --
>> dynamic testing. The patch series show that the problem is not a
>> disaster and we won't need to change just every line of kernel code.
> 
> It's indeed not a disaster but we had to do this exercise to find out
> whether there are better ways of detecting where untagging is necessary.
> 
> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled. I wouldn't say it's
> a blocker for khwasan, more like something to be aware of.
> 
> The awareness is a bit of a problem as the normal programmer would have
> to pay more attention to conversions between pointer and long. Given
> that this is an arm64-only feature, we have a risk of khwasan-triggered
> bugs being introduced in generic code in the future (hence the
> suggestion of some static checker, if possible).
 
I don't see how we can implement such checker. There is no simple rule which defines when we need
to remove the tag and when we can leave it in place.
The cast to long have nothing to do with the need to remove the tag. If pointers compared for sorting objects,
or lock ordering, than having tags is fine and it doesn't matter whether pointers compared as 'unsigned long'
or as 'void *'.
If developer needs to check whether the pointer is in linear mapping, than tag has to be removed.
But again, it doesn't matter if pointer is 'unsigned long' or 'void *'. Either way, the tag has to go away.
--
To unsubscribe from this list: send the line "unsubscribe linux-kbuild" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Thu, 02 Aug 2018 14:11:18 +0000
Message-ID: <da873a0d-3c75-0a00-974e-824f1eb49de6 () virtuozzo ! com>
--------------------
On 08/02/2018 04:52 PM, Catalin Marinas wrote:
> 
>> If somebody has a practical idea how to detect these statically, let's
>> do it. Otherwise let's go with the traditional solution to this --
>> dynamic testing. The patch series show that the problem is not a
>> disaster and we won't need to change just every line of kernel code.
> 
> It's indeed not a disaster but we had to do this exercise to find out
> whether there are better ways of detecting where untagging is necessary.
> 
> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled. I wouldn't say it's
> a blocker for khwasan, more like something to be aware of.
> 
> The awareness is a bit of a problem as the normal programmer would have
> to pay more attention to conversions between pointer and long. Given
> that this is an arm64-only feature, we have a risk of khwasan-triggered
> bugs being introduced in generic code in the future (hence the
> suggestion of some static checker, if possible).
 
I don't see how we can implement such checker. There is no simple rule which defines when we need
to remove the tag and when we can leave it in place.
The cast to long have nothing to do with the need to remove the tag. If pointers compared for sorting objects,
or lock ordering, than having tags is fine and it doesn't matter whether pointers compared as 'unsigned long'
or as 'void *'.
If developer needs to check whether the pointer is in linear mapping, than tag has to be removed.
But again, it doesn't matter if pointer is 'unsigned long' or 'void *'. Either way, the tag has to go away.
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-doc
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Thu, 02 Aug 2018 14:11:18 +0000
Message-ID: <da873a0d-3c75-0a00-974e-824f1eb49de6 () virtuozzo ! com>
--------------------
On 08/02/2018 04:52 PM, Catalin Marinas wrote:
> 
>> If somebody has a practical idea how to detect these statically, let's
>> do it. Otherwise let's go with the traditional solution to this --
>> dynamic testing. The patch series show that the problem is not a
>> disaster and we won't need to change just every line of kernel code.
> 
> It's indeed not a disaster but we had to do this exercise to find out
> whether there are better ways of detecting where untagging is necessary.
> 
> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled. I wouldn't say it's
> a blocker for khwasan, more like something to be aware of.
> 
> The awareness is a bit of a problem as the normal programmer would have
> to pay more attention to conversions between pointer and long. Given
> that this is an arm64-only feature, we have a risk of khwasan-triggered
> bugs being introduced in generic code in the future (hence the
> suggestion of some static checker, if possible).
 
I don't see how we can implement such checker. There is no simple rule which defines when we need
to remove the tag and when we can leave it in place.
The cast to long have nothing to do with the need to remove the tag. If pointers compared for sorting objects,
or lock ordering, than having tags is fine and it doesn't matter whether pointers compared as 'unsigned long'
or as 'void *'.
If developer needs to check whether the pointer is in linear mapping, than tag has to be removed.
But again, it doesn't matter if pointer is 'unsigned long' or 'void *'. Either way, the tag has to go away.
--
To unsubscribe from this list: send the line "unsubscribe linux-doc" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Thu, 02 Aug 2018 14:11:18 +0000
Message-ID: <da873a0d-3c75-0a00-974e-824f1eb49de6 () virtuozzo ! com>
--------------------
On 08/02/2018 04:52 PM, Catalin Marinas wrote:
> 
>> If somebody has a practical idea how to detect these statically, let's
>> do it. Otherwise let's go with the traditional solution to this --
>> dynamic testing. The patch series show that the problem is not a
>> disaster and we won't need to change just every line of kernel code.
> 
> It's indeed not a disaster but we had to do this exercise to find out
> whether there are better ways of detecting where untagging is necessary.
> 
> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled. I wouldn't say it's
> a blocker for khwasan, more like something to be aware of.
> 
> The awareness is a bit of a problem as the normal programmer would have
> to pay more attention to conversions between pointer and long. Given
> that this is an arm64-only feature, we have a risk of khwasan-triggered
> bugs being introduced in generic code in the future (hence the
> suggestion of some static checker, if possible).
 
I don't see how we can implement such checker. There is no simple rule which defines when we need
to remove the tag and when we can leave it in place.
The cast to long have nothing to do with the need to remove the tag. If pointers compared for sorting objects,
or lock ordering, than having tags is fine and it doesn't matter whether pointers compared as 'unsigned long'
or as 'void *'.
If developer needs to check whether the pointer is in linear mapping, than tag has to be removed.
But again, it doesn't matter if pointer is 'unsigned long' or 'void *'. Either way, the tag has to go away.

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-kbuild
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Thu, 02 Aug 2018 14:11:18 +0000
Message-ID: <da873a0d-3c75-0a00-974e-824f1eb49de6 () virtuozzo ! com>
--------------------
On 08/02/2018 04:52 PM, Catalin Marinas wrote:
> 
>> If somebody has a practical idea how to detect these statically, let's
>> do it. Otherwise let's go with the traditional solution to this --
>> dynamic testing. The patch series show that the problem is not a
>> disaster and we won't need to change just every line of kernel code.
> 
> It's indeed not a disaster but we had to do this exercise to find out
> whether there are better ways of detecting where untagging is necessary.
> 
> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled. I wouldn't say it's
> a blocker for khwasan, more like something to be aware of.
> 
> The awareness is a bit of a problem as the normal programmer would have
> to pay more attention to conversions between pointer and long. Given
> that this is an arm64-only feature, we have a risk of khwasan-triggered
> bugs being introduced in generic code in the future (hence the
> suggestion of some static checker, if possible).
 
I don't see how we can implement such checker. There is no simple rule which defines when we need
to remove the tag and when we can leave it in place.
The cast to long have nothing to do with the need to remove the tag. If pointers compared for sorting objects,
or lock ordering, than having tags is fine and it doesn't matter whether pointers compared as 'unsigned long'
or as 'void *'.
If developer needs to check whether the pointer is in linear mapping, than tag has to be removed.
But again, it doesn't matter if pointer is 'unsigned long' or 'void *'. Either way, the tag has to go away.
--
To unsubscribe from this list: send the line "unsubscribe linux-kbuild" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-mm
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Thu, 02 Aug 2018 14:11:18 +0000
Message-ID: <da873a0d-3c75-0a00-974e-824f1eb49de6 () virtuozzo ! com>
--------------------
On 08/02/2018 04:52 PM, Catalin Marinas wrote:
> 
>> If somebody has a practical idea how to detect these statically, let's
>> do it. Otherwise let's go with the traditional solution to this --
>> dynamic testing. The patch series show that the problem is not a
>> disaster and we won't need to change just every line of kernel code.
> 
> It's indeed not a disaster but we had to do this exercise to find out
> whether there are better ways of detecting where untagging is necessary.
> 
> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled. I wouldn't say it's
> a blocker for khwasan, more like something to be aware of.
> 
> The awareness is a bit of a problem as the normal programmer would have
> to pay more attention to conversions between pointer and long. Given
> that this is an arm64-only feature, we have a risk of khwasan-triggered
> bugs being introduced in generic code in the future (hence the
> suggestion of some static checker, if possible).
 
I don't see how we can implement such checker. There is no simple rule which defines when we need
to remove the tag and when we can leave it in place.
The cast to long have nothing to do with the need to remove the tag. If pointers compared for sorting objects,
or lock ordering, than having tags is fine and it doesn't matter whether pointers compared as 'unsigned long'
or as 'void *'.
If developer needs to check whether the pointer is in linear mapping, than tag has to be removed.
But again, it doesn't matter if pointer is 'unsigned long' or 'void *'. Either way, the tag has to go away.

================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-sparse
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Thu, 02 Aug 2018 14:11:18 +0000
Message-ID: <da873a0d-3c75-0a00-974e-824f1eb49de6 () virtuozzo ! com>
--------------------
On 08/02/2018 04:52 PM, Catalin Marinas wrote:
> 
>> If somebody has a practical idea how to detect these statically, let's
>> do it. Otherwise let's go with the traditional solution to this --
>> dynamic testing. The patch series show that the problem is not a
>> disaster and we won't need to change just every line of kernel code.
> 
> It's indeed not a disaster but we had to do this exercise to find out
> whether there are better ways of detecting where untagging is necessary.
> 
> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled. I wouldn't say it's
> a blocker for khwasan, more like something to be aware of.
> 
> The awareness is a bit of a problem as the normal programmer would have
> to pay more attention to conversions between pointer and long. Given
> that this is an arm64-only feature, we have a risk of khwasan-triggered
> bugs being introduced in generic code in the future (hence the
> suggestion of some static checker, if possible).
 
I don't see how we can implement such checker. There is no simple rule which defines when we need
to remove the tag and when we can leave it in place.
The cast to long have nothing to do with the need to remove the tag. If pointers compared for sorting objects,
or lock ordering, than having tags is fine and it doesn't matter whether pointers compared as 'unsigned long'
or as 'void *'.
If developer needs to check whether the pointer is in linear mapping, than tag has to be removed.
But again, it doesn't matter if pointer is 'unsigned long' or 'void *'. Either way, the tag has to go away.
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-doc
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Thu, 02 Aug 2018 14:11:18 +0000
Message-ID: <da873a0d-3c75-0a00-974e-824f1eb49de6 () virtuozzo ! com>
--------------------
On 08/02/2018 04:52 PM, Catalin Marinas wrote:
> 
>> If somebody has a practical idea how to detect these statically, let's
>> do it. Otherwise let's go with the traditional solution to this --
>> dynamic testing. The patch series show that the problem is not a
>> disaster and we won't need to change just every line of kernel code.
> 
> It's indeed not a disaster but we had to do this exercise to find out
> whether there are better ways of detecting where untagging is necessary.
> 
> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled. I wouldn't say it's
> a blocker for khwasan, more like something to be aware of.
> 
> The awareness is a bit of a problem as the normal programmer would have
> to pay more attention to conversions between pointer and long. Given
> that this is an arm64-only feature, we have a risk of khwasan-triggered
> bugs being introduced in generic code in the future (hence the
> suggestion of some static checker, if possible).
 
I don't see how we can implement such checker. There is no simple rule which defines when we need
to remove the tag and when we can leave it in place.
The cast to long have nothing to do with the need to remove the tag. If pointers compared for sorting objects,
or lock ordering, than having tags is fine and it doesn't matter whether pointers compared as 'unsigned long'
or as 'void *'.
If developer needs to check whether the pointer is in linear mapping, than tag has to be removed.
But again, it doesn't matter if pointer is 'unsigned long' or 'void *'. Either way, the tag has to go away.
--
To unsubscribe from this list: send the line "unsubscribe linux-doc" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Thu, 02 Aug 2018 14:11:18 +0000
Message-ID: <da873a0d-3c75-0a00-974e-824f1eb49de6 () virtuozzo ! com>
--------------------
On 08/02/2018 04:52 PM, Catalin Marinas wrote:
> 
>> If somebody has a practical idea how to detect these statically, let's
>> do it. Otherwise let's go with the traditional solution to this --
>> dynamic testing. The patch series show that the problem is not a
>> disaster and we won't need to change just every line of kernel code.
> 
> It's indeed not a disaster but we had to do this exercise to find out
> whether there are better ways of detecting where untagging is necessary.
> 
> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled. I wouldn't say it's
> a blocker for khwasan, more like something to be aware of.
> 
> The awareness is a bit of a problem as the normal programmer would have
> to pay more attention to conversions between pointer and long. Given
> that this is an arm64-only feature, we have a risk of khwasan-triggered
> bugs being introduced in generic code in the future (hence the
> suggestion of some static checker, if possible).
 
I don't see how we can implement such checker. There is no simple rule which defines when we need
to remove the tag and when we can leave it in place.
The cast to long have nothing to do with the need to remove the tag. If pointers compared for sorting objects,
or lock ordering, than having tags is fine and it doesn't matter whether pointers compared as 'unsigned long'
or as 'void *'.
If developer needs to check whether the pointer is in linear mapping, than tag has to be removed.
But again, it doesn't matter if pointer is 'unsigned long' or 'void *'. Either way, the tag has to go away.

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-doc
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:23:13 +0000
Message-ID: <20180803092312.GA17798 () arm ! com>
--------------------
On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> > Thanks for tracking these cases down and going through each of them. The
> > obvious follow-up question is: how do we ensure that we keep on top of
> > this in mainline? Are you going to repeat your experiment at every kernel
> > release or every -rc or something else? I really can't see how we can
> > maintain this in the long run, especially given that the coverage we have
> > is only dynamic -- do you have an idea of how much coverage you're actually
> > getting for, say, a defconfig+modules build?
> >
> > I'd really like to enable pointer tagging in the kernel, I'm just still
> > failing to see how we can do it in a controlled manner where we can reason
> > about the semantic changes using something other than a best-effort,
> > case-by-case basis which is likely to be fragile and error-prone.
> > Unfortunately, if that's all we have, then this gets relegated to a
> > debug feature, which sort of defeats the point in my opinion.
> 
> Well, in some cases there is no other way as resorting to dynamic testing.
> How do we ensure that kernel does not dereference NULL pointers, does
> not access objects after free or out of bounds? Nohow. And, yes, it's
> constant maintenance burden resolved via dynamic testing.

... and the advantage of NULL pointer issues is that you're likely to see
them as a synchronous exception at runtime, regardless of architecture and
regardless of Kconfig options. With pointer tagging, that's certainly not
the case, and so I don't think we can just treat issues there like we do for
NULL pointers.

Will
--
To unsubscribe from this list: send the line "unsubscribe linux-doc" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-mm
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:23:13 +0000
Message-ID: <20180803092312.GA17798 () arm ! com>
--------------------
On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> > Thanks for tracking these cases down and going through each of them. The
> > obvious follow-up question is: how do we ensure that we keep on top of
> > this in mainline? Are you going to repeat your experiment at every kernel
> > release or every -rc or something else? I really can't see how we can
> > maintain this in the long run, especially given that the coverage we have
> > is only dynamic -- do you have an idea of how much coverage you're actually
> > getting for, say, a defconfig+modules build?
> >
> > I'd really like to enable pointer tagging in the kernel, I'm just still
> > failing to see how we can do it in a controlled manner where we can reason
> > about the semantic changes using something other than a best-effort,
> > case-by-case basis which is likely to be fragile and error-prone.
> > Unfortunately, if that's all we have, then this gets relegated to a
> > debug feature, which sort of defeats the point in my opinion.
> 
> Well, in some cases there is no other way as resorting to dynamic testing.
> How do we ensure that kernel does not dereference NULL pointers, does
> not access objects after free or out of bounds? Nohow. And, yes, it's
> constant maintenance burden resolved via dynamic testing.

... and the advantage of NULL pointer issues is that you're likely to see
them as a synchronous exception at runtime, regardless of architecture and
regardless of Kconfig options. With pointer tagging, that's certainly not
the case, and so I don't think we can just treat issues there like we do for
NULL pointers.

Will

================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:23:13 +0000
Message-ID: <20180803092312.GA17798 () arm ! com>
--------------------
On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> > Thanks for tracking these cases down and going through each of them. The
> > obvious follow-up question is: how do we ensure that we keep on top of
> > this in mainline? Are you going to repeat your experiment at every kernel
> > release or every -rc or something else? I really can't see how we can
> > maintain this in the long run, especially given that the coverage we have
> > is only dynamic -- do you have an idea of how much coverage you're actually
> > getting for, say, a defconfig+modules build?
> >
> > I'd really like to enable pointer tagging in the kernel, I'm just still
> > failing to see how we can do it in a controlled manner where we can reason
> > about the semantic changes using something other than a best-effort,
> > case-by-case basis which is likely to be fragile and error-prone.
> > Unfortunately, if that's all we have, then this gets relegated to a
> > debug feature, which sort of defeats the point in my opinion.
> 
> Well, in some cases there is no other way as resorting to dynamic testing.
> How do we ensure that kernel does not dereference NULL pointers, does
> not access objects after free or out of bounds? Nohow. And, yes, it's
> constant maintenance burden resolved via dynamic testing.

... and the advantage of NULL pointer issues is that you're likely to see
them as a synchronous exception at runtime, regardless of architecture and
regardless of Kconfig options. With pointer tagging, that's certainly not
the case, and so I don't think we can just treat issues there like we do for
NULL pointers.

Will
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-sparse
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:23:13 +0000
Message-ID: <20180803092312.GA17798 () arm ! com>
--------------------
On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> > Thanks for tracking these cases down and going through each of them. The
> > obvious follow-up question is: how do we ensure that we keep on top of
> > this in mainline? Are you going to repeat your experiment at every kernel
> > release or every -rc or something else? I really can't see how we can
> > maintain this in the long run, especially given that the coverage we have
> > is only dynamic -- do you have an idea of how much coverage you're actually
> > getting for, say, a defconfig+modules build?
> >
> > I'd really like to enable pointer tagging in the kernel, I'm just still
> > failing to see how we can do it in a controlled manner where we can reason
> > about the semantic changes using something other than a best-effort,
> > case-by-case basis which is likely to be fragile and error-prone.
> > Unfortunately, if that's all we have, then this gets relegated to a
> > debug feature, which sort of defeats the point in my opinion.
> 
> Well, in some cases there is no other way as resorting to dynamic testing.
> How do we ensure that kernel does not dereference NULL pointers, does
> not access objects after free or out of bounds? Nohow. And, yes, it's
> constant maintenance burden resolved via dynamic testing.

... and the advantage of NULL pointer issues is that you're likely to see
them as a synchronous exception at runtime, regardless of architecture and
regardless of Kconfig options. With pointer tagging, that's certainly not
the case, and so I don't think we can just treat issues there like we do for
NULL pointers.

Will
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:23:13 +0000
Message-ID: <20180803092312.GA17798 () arm ! com>
--------------------
On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> > Thanks for tracking these cases down and going through each of them. The
> > obvious follow-up question is: how do we ensure that we keep on top of
> > this in mainline? Are you going to repeat your experiment at every kernel
> > release or every -rc or something else? I really can't see how we can
> > maintain this in the long run, especially given that the coverage we have
> > is only dynamic -- do you have an idea of how much coverage you're actually
> > getting for, say, a defconfig+modules build?
> >
> > I'd really like to enable pointer tagging in the kernel, I'm just still
> > failing to see how we can do it in a controlled manner where we can reason
> > about the semantic changes using something other than a best-effort,
> > case-by-case basis which is likely to be fragile and error-prone.
> > Unfortunately, if that's all we have, then this gets relegated to a
> > debug feature, which sort of defeats the point in my opinion.
> 
> Well, in some cases there is no other way as resorting to dynamic testing.
> How do we ensure that kernel does not dereference NULL pointers, does
> not access objects after free or out of bounds? Nohow. And, yes, it's
> constant maintenance burden resolved via dynamic testing.

... and the advantage of NULL pointer issues is that you're likely to see
them as a synchronous exception at runtime, regardless of architecture and
regardless of Kconfig options. With pointer tagging, that's certainly not
the case, and so I don't think we can just treat issues there like we do for
NULL pointers.

Will

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:23:13 +0000
Message-ID: <20180803092312.GA17798 () arm ! com>
--------------------
On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> > Thanks for tracking these cases down and going through each of them. The
> > obvious follow-up question is: how do we ensure that we keep on top of
> > this in mainline? Are you going to repeat your experiment at every kernel
> > release or every -rc or something else? I really can't see how we can
> > maintain this in the long run, especially given that the coverage we have
> > is only dynamic -- do you have an idea of how much coverage you're actually
> > getting for, say, a defconfig+modules build?
> >
> > I'd really like to enable pointer tagging in the kernel, I'm just still
> > failing to see how we can do it in a controlled manner where we can reason
> > about the semantic changes using something other than a best-effort,
> > case-by-case basis which is likely to be fragile and error-prone.
> > Unfortunately, if that's all we have, then this gets relegated to a
> > debug feature, which sort of defeats the point in my opinion.
> 
> Well, in some cases there is no other way as resorting to dynamic testing.
> How do we ensure that kernel does not dereference NULL pointers, does
> not access objects after free or out of bounds? Nohow. And, yes, it's
> constant maintenance burden resolved via dynamic testing.

... and the advantage of NULL pointer issues is that you're likely to see
them as a synchronous exception at runtime, regardless of architecture and
regardless of Kconfig options. With pointer tagging, that's certainly not
the case, and so I don't think we can just treat issues there like we do for
NULL pointers.

Will
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-sparse
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:23:13 +0000
Message-ID: <20180803092312.GA17798 () arm ! com>
--------------------
On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> > Thanks for tracking these cases down and going through each of them. The
> > obvious follow-up question is: how do we ensure that we keep on top of
> > this in mainline? Are you going to repeat your experiment at every kernel
> > release or every -rc or something else? I really can't see how we can
> > maintain this in the long run, especially given that the coverage we have
> > is only dynamic -- do you have an idea of how much coverage you're actually
> > getting for, say, a defconfig+modules build?
> >
> > I'd really like to enable pointer tagging in the kernel, I'm just still
> > failing to see how we can do it in a controlled manner where we can reason
> > about the semantic changes using something other than a best-effort,
> > case-by-case basis which is likely to be fragile and error-prone.
> > Unfortunately, if that's all we have, then this gets relegated to a
> > debug feature, which sort of defeats the point in my opinion.
> 
> Well, in some cases there is no other way as resorting to dynamic testing.
> How do we ensure that kernel does not dereference NULL pointers, does
> not access objects after free or out of bounds? Nohow. And, yes, it's
> constant maintenance burden resolved via dynamic testing.

... and the advantage of NULL pointer issues is that you're likely to see
them as a synchronous exception at runtime, regardless of architecture and
regardless of Kconfig options. With pointer tagging, that's certainly not
the case, and so I don't think we can just treat issues there like we do for
NULL pointers.

Will
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-kbuild
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:23:13 +0000
Message-ID: <20180803092312.GA17798 () arm ! com>
--------------------
On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> > Thanks for tracking these cases down and going through each of them. The
> > obvious follow-up question is: how do we ensure that we keep on top of
> > this in mainline? Are you going to repeat your experiment at every kernel
> > release or every -rc or something else? I really can't see how we can
> > maintain this in the long run, especially given that the coverage we have
> > is only dynamic -- do you have an idea of how much coverage you're actually
> > getting for, say, a defconfig+modules build?
> >
> > I'd really like to enable pointer tagging in the kernel, I'm just still
> > failing to see how we can do it in a controlled manner where we can reason
> > about the semantic changes using something other than a best-effort,
> > case-by-case basis which is likely to be fragile and error-prone.
> > Unfortunately, if that's all we have, then this gets relegated to a
> > debug feature, which sort of defeats the point in my opinion.
> 
> Well, in some cases there is no other way as resorting to dynamic testing.
> How do we ensure that kernel does not dereference NULL pointers, does
> not access objects after free or out of bounds? Nohow. And, yes, it's
> constant maintenance burden resolved via dynamic testing.

... and the advantage of NULL pointer issues is that you're likely to see
them as a synchronous exception at runtime, regardless of architecture and
regardless of Kconfig options. With pointer tagging, that's certainly not
the case, and so I don't think we can just treat issues there like we do for
NULL pointers.

Will
--
To unsubscribe from this list: send the line "unsubscribe linux-kbuild" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:42:32 +0000
Message-ID: <CACT4Y+bCen+ccU8awYyx_Tw14JNZhaP4D-jNq-WZy7itW+vpYg () mail ! gmail ! com>
--------------------
On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
>> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
>> > Thanks for tracking these cases down and going through each of them. The
>> > obvious follow-up question is: how do we ensure that we keep on top of
>> > this in mainline? Are you going to repeat your experiment at every kernel
>> > release or every -rc or something else? I really can't see how we can
>> > maintain this in the long run, especially given that the coverage we have
>> > is only dynamic -- do you have an idea of how much coverage you're actually
>> > getting for, say, a defconfig+modules build?
>> >
>> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> > failing to see how we can do it in a controlled manner where we can reason
>> > about the semantic changes using something other than a best-effort,
>> > case-by-case basis which is likely to be fragile and error-prone.
>> > Unfortunately, if that's all we have, then this gets relegated to a
>> > debug feature, which sort of defeats the point in my opinion.
>>
>> Well, in some cases there is no other way as resorting to dynamic testing.
>> How do we ensure that kernel does not dereference NULL pointers, does
>> not access objects after free or out of bounds? Nohow. And, yes, it's
>> constant maintenance burden resolved via dynamic testing.
>
> ... and the advantage of NULL pointer issues is that you're likely to see
> them as a synchronous exception at runtime, regardless of architecture and
> regardless of Kconfig options. With pointer tagging, that's certainly not
> the case, and so I don't think we can just treat issues there like we do for
> NULL pointers.

Well, let's take use-after-frees, out-of-bounds, info leaks, data
races is a good example, deadlocks and just logical bugs...

> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled.

This is true for just any change in configs, sysctls or just a
different workload. Any of this can enable new code, exiting code
working differently, or just working with data in new states. And we
have tens of thousands of bugs, so blindly deploying anything new to
production without proper testing is a bad idea. It's not specific to
HWASAN in any way. And when you enable HWASAN you actually do mean to
retest everything as hard as possible.

And in the end we do not seem to have any action points here, right?
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-mm
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:42:32 +0000
Message-ID: <CACT4Y+bCen+ccU8awYyx_Tw14JNZhaP4D-jNq-WZy7itW+vpYg () mail ! gmail ! com>
--------------------
On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
>> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
>> > Thanks for tracking these cases down and going through each of them. The
>> > obvious follow-up question is: how do we ensure that we keep on top of
>> > this in mainline? Are you going to repeat your experiment at every kernel
>> > release or every -rc or something else? I really can't see how we can
>> > maintain this in the long run, especially given that the coverage we have
>> > is only dynamic -- do you have an idea of how much coverage you're actually
>> > getting for, say, a defconfig+modules build?
>> >
>> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> > failing to see how we can do it in a controlled manner where we can reason
>> > about the semantic changes using something other than a best-effort,
>> > case-by-case basis which is likely to be fragile and error-prone.
>> > Unfortunately, if that's all we have, then this gets relegated to a
>> > debug feature, which sort of defeats the point in my opinion.
>>
>> Well, in some cases there is no other way as resorting to dynamic testing.
>> How do we ensure that kernel does not dereference NULL pointers, does
>> not access objects after free or out of bounds? Nohow. And, yes, it's
>> constant maintenance burden resolved via dynamic testing.
>
> ... and the advantage of NULL pointer issues is that you're likely to see
> them as a synchronous exception at runtime, regardless of architecture and
> regardless of Kconfig options. With pointer tagging, that's certainly not
> the case, and so I don't think we can just treat issues there like we do for
> NULL pointers.

Well, let's take use-after-frees, out-of-bounds, info leaks, data
races is a good example, deadlocks and just logical bugs...

> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled.

This is true for just any change in configs, sysctls or just a
different workload. Any of this can enable new code, exiting code
working differently, or just working with data in new states. And we
have tens of thousands of bugs, so blindly deploying anything new to
production without proper testing is a bad idea. It's not specific to
HWASAN in any way. And when you enable HWASAN you actually do mean to
retest everything as hard as possible.

And in the end we do not seem to have any action points here, right?

================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-doc
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:42:32 +0000
Message-ID: <CACT4Y+bCen+ccU8awYyx_Tw14JNZhaP4D-jNq-WZy7itW+vpYg () mail ! gmail ! com>
--------------------
On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
>> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
>> > Thanks for tracking these cases down and going through each of them. The
>> > obvious follow-up question is: how do we ensure that we keep on top of
>> > this in mainline? Are you going to repeat your experiment at every kernel
>> > release or every -rc or something else? I really can't see how we can
>> > maintain this in the long run, especially given that the coverage we have
>> > is only dynamic -- do you have an idea of how much coverage you're actually
>> > getting for, say, a defconfig+modules build?
>> >
>> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> > failing to see how we can do it in a controlled manner where we can reason
>> > about the semantic changes using something other than a best-effort,
>> > case-by-case basis which is likely to be fragile and error-prone.
>> > Unfortunately, if that's all we have, then this gets relegated to a
>> > debug feature, which sort of defeats the point in my opinion.
>>
>> Well, in some cases there is no other way as resorting to dynamic testing.
>> How do we ensure that kernel does not dereference NULL pointers, does
>> not access objects after free or out of bounds? Nohow. And, yes, it's
>> constant maintenance burden resolved via dynamic testing.
>
> ... and the advantage of NULL pointer issues is that you're likely to see
> them as a synchronous exception at runtime, regardless of architecture and
> regardless of Kconfig options. With pointer tagging, that's certainly not
> the case, and so I don't think we can just treat issues there like we do for
> NULL pointers.

Well, let's take use-after-frees, out-of-bounds, info leaks, data
races is a good example, deadlocks and just logical bugs...

> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled.

This is true for just any change in configs, sysctls or just a
different workload. Any of this can enable new code, exiting code
working differently, or just working with data in new states. And we
have tens of thousands of bugs, so blindly deploying anything new to
production without proper testing is a bad idea. It's not specific to
HWASAN in any way. And when you enable HWASAN you actually do mean to
retest everything as hard as possible.

And in the end we do not seem to have any action points here, right?
--
To unsubscribe from this list: send the line "unsubscribe linux-doc" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:42:32 +0000
Message-ID: <CACT4Y+bCen+ccU8awYyx_Tw14JNZhaP4D-jNq-WZy7itW+vpYg () mail ! gmail ! com>
--------------------
On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
>> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
>> > Thanks for tracking these cases down and going through each of them. The
>> > obvious follow-up question is: how do we ensure that we keep on top of
>> > this in mainline? Are you going to repeat your experiment at every kernel
>> > release or every -rc or something else? I really can't see how we can
>> > maintain this in the long run, especially given that the coverage we have
>> > is only dynamic -- do you have an idea of how much coverage you're actually
>> > getting for, say, a defconfig+modules build?
>> >
>> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> > failing to see how we can do it in a controlled manner where we can reason
>> > about the semantic changes using something other than a best-effort,
>> > case-by-case basis which is likely to be fragile and error-prone.
>> > Unfortunately, if that's all we have, then this gets relegated to a
>> > debug feature, which sort of defeats the point in my opinion.
>>
>> Well, in some cases there is no other way as resorting to dynamic testing.
>> How do we ensure that kernel does not dereference NULL pointers, does
>> not access objects after free or out of bounds? Nohow. And, yes, it's
>> constant maintenance burden resolved via dynamic testing.
>
> ... and the advantage of NULL pointer issues is that you're likely to see
> them as a synchronous exception at runtime, regardless of architecture and
> regardless of Kconfig options. With pointer tagging, that's certainly not
> the case, and so I don't think we can just treat issues there like we do for
> NULL pointers.

Well, let's take use-after-frees, out-of-bounds, info leaks, data
races is a good example, deadlocks and just logical bugs...

> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled.

This is true for just any change in configs, sysctls or just a
different workload. Any of this can enable new code, exiting code
working differently, or just working with data in new states. And we
have tens of thousands of bugs, so blindly deploying anything new to
production without proper testing is a bad idea. It's not specific to
HWASAN in any way. And when you enable HWASAN you actually do mean to
retest everything as hard as possible.

And in the end we do not seem to have any action points here, right?

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-sparse
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:42:32 +0000
Message-ID: <CACT4Y+bCen+ccU8awYyx_Tw14JNZhaP4D-jNq-WZy7itW+vpYg () mail ! gmail ! com>
--------------------
On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
>> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
>> > Thanks for tracking these cases down and going through each of them. The
>> > obvious follow-up question is: how do we ensure that we keep on top of
>> > this in mainline? Are you going to repeat your experiment at every kernel
>> > release or every -rc or something else? I really can't see how we can
>> > maintain this in the long run, especially given that the coverage we have
>> > is only dynamic -- do you have an idea of how much coverage you're actually
>> > getting for, say, a defconfig+modules build?
>> >
>> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> > failing to see how we can do it in a controlled manner where we can reason
>> > about the semantic changes using something other than a best-effort,
>> > case-by-case basis which is likely to be fragile and error-prone.
>> > Unfortunately, if that's all we have, then this gets relegated to a
>> > debug feature, which sort of defeats the point in my opinion.
>>
>> Well, in some cases there is no other way as resorting to dynamic testing.
>> How do we ensure that kernel does not dereference NULL pointers, does
>> not access objects after free or out of bounds? Nohow. And, yes, it's
>> constant maintenance burden resolved via dynamic testing.
>
> ... and the advantage of NULL pointer issues is that you're likely to see
> them as a synchronous exception at runtime, regardless of architecture and
> regardless of Kconfig options. With pointer tagging, that's certainly not
> the case, and so I don't think we can just treat issues there like we do for
> NULL pointers.

Well, let's take use-after-frees, out-of-bounds, info leaks, data
races is a good example, deadlocks and just logical bugs...

> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled.

This is true for just any change in configs, sysctls or just a
different workload. Any of this can enable new code, exiting code
working differently, or just working with data in new states. And we
have tens of thousands of bugs, so blindly deploying anything new to
production without proper testing is a bad idea. It's not specific to
HWASAN in any way. And when you enable HWASAN you actually do mean to
retest everything as hard as possible.

And in the end we do not seem to have any action points here, right?
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:42:32 +0000
Message-ID: <CACT4Y+bCen+ccU8awYyx_Tw14JNZhaP4D-jNq-WZy7itW+vpYg () mail ! gmail ! com>
--------------------
On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
>> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
>> > Thanks for tracking these cases down and going through each of them. The
>> > obvious follow-up question is: how do we ensure that we keep on top of
>> > this in mainline? Are you going to repeat your experiment at every kernel
>> > release or every -rc or something else? I really can't see how we can
>> > maintain this in the long run, especially given that the coverage we have
>> > is only dynamic -- do you have an idea of how much coverage you're actually
>> > getting for, say, a defconfig+modules build?
>> >
>> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> > failing to see how we can do it in a controlled manner where we can reason
>> > about the semantic changes using something other than a best-effort,
>> > case-by-case basis which is likely to be fragile and error-prone.
>> > Unfortunately, if that's all we have, then this gets relegated to a
>> > debug feature, which sort of defeats the point in my opinion.
>>
>> Well, in some cases there is no other way as resorting to dynamic testing.
>> How do we ensure that kernel does not dereference NULL pointers, does
>> not access objects after free or out of bounds? Nohow. And, yes, it's
>> constant maintenance burden resolved via dynamic testing.
>
> ... and the advantage of NULL pointer issues is that you're likely to see
> them as a synchronous exception at runtime, regardless of architecture and
> regardless of Kconfig options. With pointer tagging, that's certainly not
> the case, and so I don't think we can just treat issues there like we do for
> NULL pointers.

Well, let's take use-after-frees, out-of-bounds, info leaks, data
races is a good example, deadlocks and just logical bugs...

> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled.

This is true for just any change in configs, sysctls or just a
different workload. Any of this can enable new code, exiting code
working differently, or just working with data in new states. And we
have tens of thousands of bugs, so blindly deploying anything new to
production without proper testing is a bad idea. It's not specific to
HWASAN in any way. And when you enable HWASAN you actually do mean to
retest everything as hard as possible.

And in the end we do not seem to have any action points here, right?
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-kbuild
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:42:32 +0000
Message-ID: <CACT4Y+bCen+ccU8awYyx_Tw14JNZhaP4D-jNq-WZy7itW+vpYg () mail ! gmail ! com>
--------------------
On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
>> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
>> > Thanks for tracking these cases down and going through each of them. The
>> > obvious follow-up question is: how do we ensure that we keep on top of
>> > this in mainline? Are you going to repeat your experiment at every kernel
>> > release or every -rc or something else? I really can't see how we can
>> > maintain this in the long run, especially given that the coverage we have
>> > is only dynamic -- do you have an idea of how much coverage you're actually
>> > getting for, say, a defconfig+modules build?
>> >
>> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> > failing to see how we can do it in a controlled manner where we can reason
>> > about the semantic changes using something other than a best-effort,
>> > case-by-case basis which is likely to be fragile and error-prone.
>> > Unfortunately, if that's all we have, then this gets relegated to a
>> > debug feature, which sort of defeats the point in my opinion.
>>
>> Well, in some cases there is no other way as resorting to dynamic testing.
>> How do we ensure that kernel does not dereference NULL pointers, does
>> not access objects after free or out of bounds? Nohow. And, yes, it's
>> constant maintenance burden resolved via dynamic testing.
>
> ... and the advantage of NULL pointer issues is that you're likely to see
> them as a synchronous exception at runtime, regardless of architecture and
> regardless of Kconfig options. With pointer tagging, that's certainly not
> the case, and so I don't think we can just treat issues there like we do for
> NULL pointers.

Well, let's take use-after-frees, out-of-bounds, info leaks, data
races is a good example, deadlocks and just logical bugs...

> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled.

This is true for just any change in configs, sysctls or just a
different workload. Any of this can enable new code, exiting code
working differently, or just working with data in new states. And we
have tens of thousands of bugs, so blindly deploying anything new to
production without proper testing is a bad idea. It's not specific to
HWASAN in any way. And when you enable HWASAN you actually do mean to
retest everything as hard as possible.

And in the end we do not seem to have any action points here, right?
--
To unsubscribe from this list: send the line "unsubscribe linux-kbuild" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-mm
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:42:32 +0000
Message-ID: <CACT4Y+bCen+ccU8awYyx_Tw14JNZhaP4D-jNq-WZy7itW+vpYg () mail ! gmail ! com>
--------------------
On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
>> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
>> > Thanks for tracking these cases down and going through each of them. The
>> > obvious follow-up question is: how do we ensure that we keep on top of
>> > this in mainline? Are you going to repeat your experiment at every kernel
>> > release or every -rc or something else? I really can't see how we can
>> > maintain this in the long run, especially given that the coverage we have
>> > is only dynamic -- do you have an idea of how much coverage you're actually
>> > getting for, say, a defconfig+modules build?
>> >
>> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> > failing to see how we can do it in a controlled manner where we can reason
>> > about the semantic changes using something other than a best-effort,
>> > case-by-case basis which is likely to be fragile and error-prone.
>> > Unfortunately, if that's all we have, then this gets relegated to a
>> > debug feature, which sort of defeats the point in my opinion.
>>
>> Well, in some cases there is no other way as resorting to dynamic testing.
>> How do we ensure that kernel does not dereference NULL pointers, does
>> not access objects after free or out of bounds? Nohow. And, yes, it's
>> constant maintenance burden resolved via dynamic testing.
>
> ... and the advantage of NULL pointer issues is that you're likely to see
> them as a synchronous exception at runtime, regardless of architecture and
> regardless of Kconfig options. With pointer tagging, that's certainly not
> the case, and so I don't think we can just treat issues there like we do for
> NULL pointers.

Well, let's take use-after-frees, out-of-bounds, info leaks, data
races is a good example, deadlocks and just logical bugs...

> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled.

This is true for just any change in configs, sysctls or just a
different workload. Any of this can enable new code, exiting code
working differently, or just working with data in new states. And we
have tens of thousands of bugs, so blindly deploying anything new to
production without proper testing is a bad idea. It's not specific to
HWASAN in any way. And when you enable HWASAN you actually do mean to
retest everything as hard as possible.

And in the end we do not seem to have any action points here, right?

================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Fri, 03 Aug 2018 09:42:32 +0000
Message-ID: <CACT4Y+bCen+ccU8awYyx_Tw14JNZhaP4D-jNq-WZy7itW+vpYg () mail ! gmail ! com>
--------------------
On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
>> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
>> > Thanks for tracking these cases down and going through each of them. The
>> > obvious follow-up question is: how do we ensure that we keep on top of
>> > this in mainline? Are you going to repeat your experiment at every kernel
>> > release or every -rc or something else? I really can't see how we can
>> > maintain this in the long run, especially given that the coverage we have
>> > is only dynamic -- do you have an idea of how much coverage you're actually
>> > getting for, say, a defconfig+modules build?
>> >
>> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> > failing to see how we can do it in a controlled manner where we can reason
>> > about the semantic changes using something other than a best-effort,
>> > case-by-case basis which is likely to be fragile and error-prone.
>> > Unfortunately, if that's all we have, then this gets relegated to a
>> > debug feature, which sort of defeats the point in my opinion.
>>
>> Well, in some cases there is no other way as resorting to dynamic testing.
>> How do we ensure that kernel does not dereference NULL pointers, does
>> not access objects after free or out of bounds? Nohow. And, yes, it's
>> constant maintenance burden resolved via dynamic testing.
>
> ... and the advantage of NULL pointer issues is that you're likely to see
> them as a synchronous exception at runtime, regardless of architecture and
> regardless of Kconfig options. With pointer tagging, that's certainly not
> the case, and so I don't think we can just treat issues there like we do for
> NULL pointers.

Well, let's take use-after-frees, out-of-bounds, info leaks, data
races is a good example, deadlocks and just logical bugs...

> If you want to enable khwasan in "production" and since enabling it
> could potentially change the behaviour of existing code paths, the
> run-time validation space doubles as we'd need to get the same code
> coverage with and without the feature being enabled.

This is true for just any change in configs, sysctls or just a
different workload. Any of this can enable new code, exiting code
working differently, or just working with data in new states. And we
have tens of thousands of bugs, so blindly deploying anything new to
production without proper testing is a bad idea. It's not specific to
HWASAN in any way. And when you enable HWASAN you actually do mean to
retest everything as hard as possible.

And in the end we do not seem to have any action points here, right?

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-sparse
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:27:53 +0000
Message-ID: <20180808162752.GA26592 () arm ! com>
--------------------
On Fri, Aug 03, 2018 at 11:42:32AM +0200, Dmitry Vyukov wrote:
> On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> > On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> >> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> >> > Thanks for tracking these cases down and going through each of them. The
> >> > obvious follow-up question is: how do we ensure that we keep on top of
> >> > this in mainline? Are you going to repeat your experiment at every kernel
> >> > release or every -rc or something else? I really can't see how we can
> >> > maintain this in the long run, especially given that the coverage we have
> >> > is only dynamic -- do you have an idea of how much coverage you're actually
> >> > getting for, say, a defconfig+modules build?
> >> >
> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
> >> > failing to see how we can do it in a controlled manner where we can reason
> >> > about the semantic changes using something other than a best-effort,
> >> > case-by-case basis which is likely to be fragile and error-prone.
> >> > Unfortunately, if that's all we have, then this gets relegated to a
> >> > debug feature, which sort of defeats the point in my opinion.
> >>
> >> Well, in some cases there is no other way as resorting to dynamic testing.
> >> How do we ensure that kernel does not dereference NULL pointers, does
> >> not access objects after free or out of bounds? Nohow. And, yes, it's
> >> constant maintenance burden resolved via dynamic testing.
> >
> > ... and the advantage of NULL pointer issues is that you're likely to see
> > them as a synchronous exception at runtime, regardless of architecture and
> > regardless of Kconfig options. With pointer tagging, that's certainly not
> > the case, and so I don't think we can just treat issues there like we do for
> > NULL pointers.
> 
> Well, let's take use-after-frees, out-of-bounds, info leaks, data
> races is a good example, deadlocks and just logical bugs...

Ok, but it was you that brought up NULL pointers, so there's some goalpost
moving here. And as with NULL pointers, all of the issues you mention above
apply to other architectures and the majority of their configurations, so my
concerns about this feature remain.

> > If you want to enable khwasan in "production" and since enabling it
> > could potentially change the behaviour of existing code paths, the
> > run-time validation space doubles as we'd need to get the same code
> > coverage with and without the feature being enabled.
> 
> This is true for just any change in configs, sysctls or just a
> different workload. Any of this can enable new code, exiting code
> working differently, or just working with data in new states. And we
> have tens of thousands of bugs, so blindly deploying anything new to
> production without proper testing is a bad idea. It's not specific to
> HWASAN in any way. And when you enable HWASAN you actually do mean to
> retest everything as hard as possible.

I suppose I'm trying to understand whether we have to resort to testing, or
whether we can do better. I'm really uncomfortable with testing as our only
means of getting this right because this is a non-standard, arm64-specific
option and I don't think it will get very much testing in mainline at all.
Rather, we'll get spurious bug reports from forks of -stable many releases
later and we'll actually be worse-off for it.

> And in the end we do not seem to have any action points here, right?

Right now, it feels like this series trades one set of bugs for another,
so I'd like to get to a position where this new set of bugs is genuinely
more manageable (i.e. detectable, fixable, preventable) than the old set.
Unfortunately, the only suggestion seems to be "testing", which I really
don't find convincing :(

Could we do things like:

  - Set up a dedicated arm64 test farm, running mainline and with a public
    frontend, aimed at getting maximum coverage of the kernel with KHWASAN
    enabled?

  - Have an implementation of KHWASAN for other architectures? (Is this even
    possible?)

  - Have a compiler plugin to clear out the tag for pointer arithmetic?
    Could we WARN if two pointers are compared with different tags?
    Could we manipulate the tag on cast-to-pointer so that a mismatch would
    be qualifier to say that pointer was created via a cast?

  - ...

?

Will
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-kbuild
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:27:53 +0000
Message-ID: <20180808162752.GA26592 () arm ! com>
--------------------
On Fri, Aug 03, 2018 at 11:42:32AM +0200, Dmitry Vyukov wrote:
> On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> > On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> >> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> >> > Thanks for tracking these cases down and going through each of them. The
> >> > obvious follow-up question is: how do we ensure that we keep on top of
> >> > this in mainline? Are you going to repeat your experiment at every kernel
> >> > release or every -rc or something else? I really can't see how we can
> >> > maintain this in the long run, especially given that the coverage we have
> >> > is only dynamic -- do you have an idea of how much coverage you're actually
> >> > getting for, say, a defconfig+modules build?
> >> >
> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
> >> > failing to see how we can do it in a controlled manner where we can reason
> >> > about the semantic changes using something other than a best-effort,
> >> > case-by-case basis which is likely to be fragile and error-prone.
> >> > Unfortunately, if that's all we have, then this gets relegated to a
> >> > debug feature, which sort of defeats the point in my opinion.
> >>
> >> Well, in some cases there is no other way as resorting to dynamic testing.
> >> How do we ensure that kernel does not dereference NULL pointers, does
> >> not access objects after free or out of bounds? Nohow. And, yes, it's
> >> constant maintenance burden resolved via dynamic testing.
> >
> > ... and the advantage of NULL pointer issues is that you're likely to see
> > them as a synchronous exception at runtime, regardless of architecture and
> > regardless of Kconfig options. With pointer tagging, that's certainly not
> > the case, and so I don't think we can just treat issues there like we do for
> > NULL pointers.
> 
> Well, let's take use-after-frees, out-of-bounds, info leaks, data
> races is a good example, deadlocks and just logical bugs...

Ok, but it was you that brought up NULL pointers, so there's some goalpost
moving here. And as with NULL pointers, all of the issues you mention above
apply to other architectures and the majority of their configurations, so my
concerns about this feature remain.

> > If you want to enable khwasan in "production" and since enabling it
> > could potentially change the behaviour of existing code paths, the
> > run-time validation space doubles as we'd need to get the same code
> > coverage with and without the feature being enabled.
> 
> This is true for just any change in configs, sysctls or just a
> different workload. Any of this can enable new code, exiting code
> working differently, or just working with data in new states. And we
> have tens of thousands of bugs, so blindly deploying anything new to
> production without proper testing is a bad idea. It's not specific to
> HWASAN in any way. And when you enable HWASAN you actually do mean to
> retest everything as hard as possible.

I suppose I'm trying to understand whether we have to resort to testing, or
whether we can do better. I'm really uncomfortable with testing as our only
means of getting this right because this is a non-standard, arm64-specific
option and I don't think it will get very much testing in mainline at all.
Rather, we'll get spurious bug reports from forks of -stable many releases
later and we'll actually be worse-off for it.

> And in the end we do not seem to have any action points here, right?

Right now, it feels like this series trades one set of bugs for another,
so I'd like to get to a position where this new set of bugs is genuinely
more manageable (i.e. detectable, fixable, preventable) than the old set.
Unfortunately, the only suggestion seems to be "testing", which I really
don't find convincing :(

Could we do things like:

  - Set up a dedicated arm64 test farm, running mainline and with a public
    frontend, aimed at getting maximum coverage of the kernel with KHWASAN
    enabled?

  - Have an implementation of KHWASAN for other architectures? (Is this even
    possible?)

  - Have a compiler plugin to clear out the tag for pointer arithmetic?
    Could we WARN if two pointers are compared with different tags?
    Could we manipulate the tag on cast-to-pointer so that a mismatch would
    be qualifier to say that pointer was created via a cast?

  - ...

?

Will
--
To unsubscribe from this list: send the line "unsubscribe linux-kbuild" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:27:53 +0000
Message-ID: <20180808162752.GA26592 () arm ! com>
--------------------
On Fri, Aug 03, 2018 at 11:42:32AM +0200, Dmitry Vyukov wrote:
> On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> > On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> >> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> >> > Thanks for tracking these cases down and going through each of them. The
> >> > obvious follow-up question is: how do we ensure that we keep on top of
> >> > this in mainline? Are you going to repeat your experiment at every kernel
> >> > release or every -rc or something else? I really can't see how we can
> >> > maintain this in the long run, especially given that the coverage we have
> >> > is only dynamic -- do you have an idea of how much coverage you're actually
> >> > getting for, say, a defconfig+modules build?
> >> >
> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
> >> > failing to see how we can do it in a controlled manner where we can reason
> >> > about the semantic changes using something other than a best-effort,
> >> > case-by-case basis which is likely to be fragile and error-prone.
> >> > Unfortunately, if that's all we have, then this gets relegated to a
> >> > debug feature, which sort of defeats the point in my opinion.
> >>
> >> Well, in some cases there is no other way as resorting to dynamic testing.
> >> How do we ensure that kernel does not dereference NULL pointers, does
> >> not access objects after free or out of bounds? Nohow. And, yes, it's
> >> constant maintenance burden resolved via dynamic testing.
> >
> > ... and the advantage of NULL pointer issues is that you're likely to see
> > them as a synchronous exception at runtime, regardless of architecture and
> > regardless of Kconfig options. With pointer tagging, that's certainly not
> > the case, and so I don't think we can just treat issues there like we do for
> > NULL pointers.
> 
> Well, let's take use-after-frees, out-of-bounds, info leaks, data
> races is a good example, deadlocks and just logical bugs...

Ok, but it was you that brought up NULL pointers, so there's some goalpost
moving here. And as with NULL pointers, all of the issues you mention above
apply to other architectures and the majority of their configurations, so my
concerns about this feature remain.

> > If you want to enable khwasan in "production" and since enabling it
> > could potentially change the behaviour of existing code paths, the
> > run-time validation space doubles as we'd need to get the same code
> > coverage with and without the feature being enabled.
> 
> This is true for just any change in configs, sysctls or just a
> different workload. Any of this can enable new code, exiting code
> working differently, or just working with data in new states. And we
> have tens of thousands of bugs, so blindly deploying anything new to
> production without proper testing is a bad idea. It's not specific to
> HWASAN in any way. And when you enable HWASAN you actually do mean to
> retest everything as hard as possible.

I suppose I'm trying to understand whether we have to resort to testing, or
whether we can do better. I'm really uncomfortable with testing as our only
means of getting this right because this is a non-standard, arm64-specific
option and I don't think it will get very much testing in mainline at all.
Rather, we'll get spurious bug reports from forks of -stable many releases
later and we'll actually be worse-off for it.

> And in the end we do not seem to have any action points here, right?

Right now, it feels like this series trades one set of bugs for another,
so I'd like to get to a position where this new set of bugs is genuinely
more manageable (i.e. detectable, fixable, preventable) than the old set.
Unfortunately, the only suggestion seems to be "testing", which I really
don't find convincing :(

Could we do things like:

  - Set up a dedicated arm64 test farm, running mainline and with a public
    frontend, aimed at getting maximum coverage of the kernel with KHWASAN
    enabled?

  - Have an implementation of KHWASAN for other architectures? (Is this even
    possible?)

  - Have a compiler plugin to clear out the tag for pointer arithmetic?
    Could we WARN if two pointers are compared with different tags?
    Could we manipulate the tag on cast-to-pointer so that a mismatch would
    be qualifier to say that pointer was created via a cast?

  - ...

?

Will
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:27:53 +0000
Message-ID: <20180808162752.GA26592 () arm ! com>
--------------------
On Fri, Aug 03, 2018 at 11:42:32AM +0200, Dmitry Vyukov wrote:
> On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> > On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> >> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> >> > Thanks for tracking these cases down and going through each of them. The
> >> > obvious follow-up question is: how do we ensure that we keep on top of
> >> > this in mainline? Are you going to repeat your experiment at every kernel
> >> > release or every -rc or something else? I really can't see how we can
> >> > maintain this in the long run, especially given that the coverage we have
> >> > is only dynamic -- do you have an idea of how much coverage you're actually
> >> > getting for, say, a defconfig+modules build?
> >> >
> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
> >> > failing to see how we can do it in a controlled manner where we can reason
> >> > about the semantic changes using something other than a best-effort,
> >> > case-by-case basis which is likely to be fragile and error-prone.
> >> > Unfortunately, if that's all we have, then this gets relegated to a
> >> > debug feature, which sort of defeats the point in my opinion.
> >>
> >> Well, in some cases there is no other way as resorting to dynamic testing.
> >> How do we ensure that kernel does not dereference NULL pointers, does
> >> not access objects after free or out of bounds? Nohow. And, yes, it's
> >> constant maintenance burden resolved via dynamic testing.
> >
> > ... and the advantage of NULL pointer issues is that you're likely to see
> > them as a synchronous exception at runtime, regardless of architecture and
> > regardless of Kconfig options. With pointer tagging, that's certainly not
> > the case, and so I don't think we can just treat issues there like we do for
> > NULL pointers.
> 
> Well, let's take use-after-frees, out-of-bounds, info leaks, data
> races is a good example, deadlocks and just logical bugs...

Ok, but it was you that brought up NULL pointers, so there's some goalpost
moving here. And as with NULL pointers, all of the issues you mention above
apply to other architectures and the majority of their configurations, so my
concerns about this feature remain.

> > If you want to enable khwasan in "production" and since enabling it
> > could potentially change the behaviour of existing code paths, the
> > run-time validation space doubles as we'd need to get the same code
> > coverage with and without the feature being enabled.
> 
> This is true for just any change in configs, sysctls or just a
> different workload. Any of this can enable new code, exiting code
> working differently, or just working with data in new states. And we
> have tens of thousands of bugs, so blindly deploying anything new to
> production without proper testing is a bad idea. It's not specific to
> HWASAN in any way. And when you enable HWASAN you actually do mean to
> retest everything as hard as possible.

I suppose I'm trying to understand whether we have to resort to testing, or
whether we can do better. I'm really uncomfortable with testing as our only
means of getting this right because this is a non-standard, arm64-specific
option and I don't think it will get very much testing in mainline at all.
Rather, we'll get spurious bug reports from forks of -stable many releases
later and we'll actually be worse-off for it.

> And in the end we do not seem to have any action points here, right?

Right now, it feels like this series trades one set of bugs for another,
so I'd like to get to a position where this new set of bugs is genuinely
more manageable (i.e. detectable, fixable, preventable) than the old set.
Unfortunately, the only suggestion seems to be "testing", which I really
don't find convincing :(

Could we do things like:

  - Set up a dedicated arm64 test farm, running mainline and with a public
    frontend, aimed at getting maximum coverage of the kernel with KHWASAN
    enabled?

  - Have an implementation of KHWASAN for other architectures? (Is this even
    possible?)

  - Have a compiler plugin to clear out the tag for pointer arithmetic?
    Could we WARN if two pointers are compared with different tags?
    Could we manipulate the tag on cast-to-pointer so that a mismatch would
    be qualifier to say that pointer was created via a cast?

  - ...

?

Will

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-sparse
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:27:53 +0000
Message-ID: <20180808162752.GA26592 () arm ! com>
--------------------
On Fri, Aug 03, 2018 at 11:42:32AM +0200, Dmitry Vyukov wrote:
> On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> > On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> >> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> >> > Thanks for tracking these cases down and going through each of them. The
> >> > obvious follow-up question is: how do we ensure that we keep on top of
> >> > this in mainline? Are you going to repeat your experiment at every kernel
> >> > release or every -rc or something else? I really can't see how we can
> >> > maintain this in the long run, especially given that the coverage we have
> >> > is only dynamic -- do you have an idea of how much coverage you're actually
> >> > getting for, say, a defconfig+modules build?
> >> >
> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
> >> > failing to see how we can do it in a controlled manner where we can reason
> >> > about the semantic changes using something other than a best-effort,
> >> > case-by-case basis which is likely to be fragile and error-prone.
> >> > Unfortunately, if that's all we have, then this gets relegated to a
> >> > debug feature, which sort of defeats the point in my opinion.
> >>
> >> Well, in some cases there is no other way as resorting to dynamic testing.
> >> How do we ensure that kernel does not dereference NULL pointers, does
> >> not access objects after free or out of bounds? Nohow. And, yes, it's
> >> constant maintenance burden resolved via dynamic testing.
> >
> > ... and the advantage of NULL pointer issues is that you're likely to see
> > them as a synchronous exception at runtime, regardless of architecture and
> > regardless of Kconfig options. With pointer tagging, that's certainly not
> > the case, and so I don't think we can just treat issues there like we do for
> > NULL pointers.
> 
> Well, let's take use-after-frees, out-of-bounds, info leaks, data
> races is a good example, deadlocks and just logical bugs...

Ok, but it was you that brought up NULL pointers, so there's some goalpost
moving here. And as with NULL pointers, all of the issues you mention above
apply to other architectures and the majority of their configurations, so my
concerns about this feature remain.

> > If you want to enable khwasan in "production" and since enabling it
> > could potentially change the behaviour of existing code paths, the
> > run-time validation space doubles as we'd need to get the same code
> > coverage with and without the feature being enabled.
> 
> This is true for just any change in configs, sysctls or just a
> different workload. Any of this can enable new code, exiting code
> working differently, or just working with data in new states. And we
> have tens of thousands of bugs, so blindly deploying anything new to
> production without proper testing is a bad idea. It's not specific to
> HWASAN in any way. And when you enable HWASAN you actually do mean to
> retest everything as hard as possible.

I suppose I'm trying to understand whether we have to resort to testing, or
whether we can do better. I'm really uncomfortable with testing as our only
means of getting this right because this is a non-standard, arm64-specific
option and I don't think it will get very much testing in mainline at all.
Rather, we'll get spurious bug reports from forks of -stable many releases
later and we'll actually be worse-off for it.

> And in the end we do not seem to have any action points here, right?

Right now, it feels like this series trades one set of bugs for another,
so I'd like to get to a position where this new set of bugs is genuinely
more manageable (i.e. detectable, fixable, preventable) than the old set.
Unfortunately, the only suggestion seems to be "testing", which I really
don't find convincing :(

Could we do things like:

  - Set up a dedicated arm64 test farm, running mainline and with a public
    frontend, aimed at getting maximum coverage of the kernel with KHWASAN
    enabled?

  - Have an implementation of KHWASAN for other architectures? (Is this even
    possible?)

  - Have a compiler plugin to clear out the tag for pointer arithmetic?
    Could we WARN if two pointers are compared with different tags?
    Could we manipulate the tag on cast-to-pointer so that a mismatch would
    be qualifier to say that pointer was created via a cast?

  - ...

?

Will
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-doc
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:27:53 +0000
Message-ID: <20180808162752.GA26592 () arm ! com>
--------------------
On Fri, Aug 03, 2018 at 11:42:32AM +0200, Dmitry Vyukov wrote:
> On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> > On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> >> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> >> > Thanks for tracking these cases down and going through each of them. The
> >> > obvious follow-up question is: how do we ensure that we keep on top of
> >> > this in mainline? Are you going to repeat your experiment at every kernel
> >> > release or every -rc or something else? I really can't see how we can
> >> > maintain this in the long run, especially given that the coverage we have
> >> > is only dynamic -- do you have an idea of how much coverage you're actually
> >> > getting for, say, a defconfig+modules build?
> >> >
> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
> >> > failing to see how we can do it in a controlled manner where we can reason
> >> > about the semantic changes using something other than a best-effort,
> >> > case-by-case basis which is likely to be fragile and error-prone.
> >> > Unfortunately, if that's all we have, then this gets relegated to a
> >> > debug feature, which sort of defeats the point in my opinion.
> >>
> >> Well, in some cases there is no other way as resorting to dynamic testing.
> >> How do we ensure that kernel does not dereference NULL pointers, does
> >> not access objects after free or out of bounds? Nohow. And, yes, it's
> >> constant maintenance burden resolved via dynamic testing.
> >
> > ... and the advantage of NULL pointer issues is that you're likely to see
> > them as a synchronous exception at runtime, regardless of architecture and
> > regardless of Kconfig options. With pointer tagging, that's certainly not
> > the case, and so I don't think we can just treat issues there like we do for
> > NULL pointers.
> 
> Well, let's take use-after-frees, out-of-bounds, info leaks, data
> races is a good example, deadlocks and just logical bugs...

Ok, but it was you that brought up NULL pointers, so there's some goalpost
moving here. And as with NULL pointers, all of the issues you mention above
apply to other architectures and the majority of their configurations, so my
concerns about this feature remain.

> > If you want to enable khwasan in "production" and since enabling it
> > could potentially change the behaviour of existing code paths, the
> > run-time validation space doubles as we'd need to get the same code
> > coverage with and without the feature being enabled.
> 
> This is true for just any change in configs, sysctls or just a
> different workload. Any of this can enable new code, exiting code
> working differently, or just working with data in new states. And we
> have tens of thousands of bugs, so blindly deploying anything new to
> production without proper testing is a bad idea. It's not specific to
> HWASAN in any way. And when you enable HWASAN you actually do mean to
> retest everything as hard as possible.

I suppose I'm trying to understand whether we have to resort to testing, or
whether we can do better. I'm really uncomfortable with testing as our only
means of getting this right because this is a non-standard, arm64-specific
option and I don't think it will get very much testing in mainline at all.
Rather, we'll get spurious bug reports from forks of -stable many releases
later and we'll actually be worse-off for it.

> And in the end we do not seem to have any action points here, right?

Right now, it feels like this series trades one set of bugs for another,
so I'd like to get to a position where this new set of bugs is genuinely
more manageable (i.e. detectable, fixable, preventable) than the old set.
Unfortunately, the only suggestion seems to be "testing", which I really
don't find convincing :(

Could we do things like:

  - Set up a dedicated arm64 test farm, running mainline and with a public
    frontend, aimed at getting maximum coverage of the kernel with KHWASAN
    enabled?

  - Have an implementation of KHWASAN for other architectures? (Is this even
    possible?)

  - Have a compiler plugin to clear out the tag for pointer arithmetic?
    Could we WARN if two pointers are compared with different tags?
    Could we manipulate the tag on cast-to-pointer so that a mismatch would
    be qualifier to say that pointer was created via a cast?

  - ...

?

Will
--
To unsubscribe from this list: send the line "unsubscribe linux-doc" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-kbuild
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:27:53 +0000
Message-ID: <20180808162752.GA26592 () arm ! com>
--------------------
On Fri, Aug 03, 2018 at 11:42:32AM +0200, Dmitry Vyukov wrote:
> On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> > On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> >> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> >> > Thanks for tracking these cases down and going through each of them. The
> >> > obvious follow-up question is: how do we ensure that we keep on top of
> >> > this in mainline? Are you going to repeat your experiment at every kernel
> >> > release or every -rc or something else? I really can't see how we can
> >> > maintain this in the long run, especially given that the coverage we have
> >> > is only dynamic -- do you have an idea of how much coverage you're actually
> >> > getting for, say, a defconfig+modules build?
> >> >
> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
> >> > failing to see how we can do it in a controlled manner where we can reason
> >> > about the semantic changes using something other than a best-effort,
> >> > case-by-case basis which is likely to be fragile and error-prone.
> >> > Unfortunately, if that's all we have, then this gets relegated to a
> >> > debug feature, which sort of defeats the point in my opinion.
> >>
> >> Well, in some cases there is no other way as resorting to dynamic testing.
> >> How do we ensure that kernel does not dereference NULL pointers, does
> >> not access objects after free or out of bounds? Nohow. And, yes, it's
> >> constant maintenance burden resolved via dynamic testing.
> >
> > ... and the advantage of NULL pointer issues is that you're likely to see
> > them as a synchronous exception at runtime, regardless of architecture and
> > regardless of Kconfig options. With pointer tagging, that's certainly not
> > the case, and so I don't think we can just treat issues there like we do for
> > NULL pointers.
> 
> Well, let's take use-after-frees, out-of-bounds, info leaks, data
> races is a good example, deadlocks and just logical bugs...

Ok, but it was you that brought up NULL pointers, so there's some goalpost
moving here. And as with NULL pointers, all of the issues you mention above
apply to other architectures and the majority of their configurations, so my
concerns about this feature remain.

> > If you want to enable khwasan in "production" and since enabling it
> > could potentially change the behaviour of existing code paths, the
> > run-time validation space doubles as we'd need to get the same code
> > coverage with and without the feature being enabled.
> 
> This is true for just any change in configs, sysctls or just a
> different workload. Any of this can enable new code, exiting code
> working differently, or just working with data in new states. And we
> have tens of thousands of bugs, so blindly deploying anything new to
> production without proper testing is a bad idea. It's not specific to
> HWASAN in any way. And when you enable HWASAN you actually do mean to
> retest everything as hard as possible.

I suppose I'm trying to understand whether we have to resort to testing, or
whether we can do better. I'm really uncomfortable with testing as our only
means of getting this right because this is a non-standard, arm64-specific
option and I don't think it will get very much testing in mainline at all.
Rather, we'll get spurious bug reports from forks of -stable many releases
later and we'll actually be worse-off for it.

> And in the end we do not seem to have any action points here, right?

Right now, it feels like this series trades one set of bugs for another,
so I'd like to get to a position where this new set of bugs is genuinely
more manageable (i.e. detectable, fixable, preventable) than the old set.
Unfortunately, the only suggestion seems to be "testing", which I really
don't find convincing :(

Could we do things like:

  - Set up a dedicated arm64 test farm, running mainline and with a public
    frontend, aimed at getting maximum coverage of the kernel with KHWASAN
    enabled?

  - Have an implementation of KHWASAN for other architectures? (Is this even
    possible?)

  - Have a compiler plugin to clear out the tag for pointer arithmetic?
    Could we WARN if two pointers are compared with different tags?
    Could we manipulate the tag on cast-to-pointer so that a mismatch would
    be qualifier to say that pointer was created via a cast?

  - ...

?

Will
--
To unsubscribe from this list: send the line "unsubscribe linux-kbuild" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Will Deacon <will.deacon () arm ! com>
To: linux-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:27:53 +0000
Message-ID: <20180808162752.GA26592 () arm ! com>
--------------------
On Fri, Aug 03, 2018 at 11:42:32AM +0200, Dmitry Vyukov wrote:
> On Fri, Aug 3, 2018 at 11:23 AM, Will Deacon <will.deacon@arm.com> wrote:
> > On Wed, Aug 01, 2018 at 06:52:09PM +0200, Dmitry Vyukov wrote:
> >> On Wed, Aug 1, 2018 at 6:35 PM, Will Deacon <will.deacon@arm.com> wrote:
> >> > Thanks for tracking these cases down and going through each of them. The
> >> > obvious follow-up question is: how do we ensure that we keep on top of
> >> > this in mainline? Are you going to repeat your experiment at every kernel
> >> > release or every -rc or something else? I really can't see how we can
> >> > maintain this in the long run, especially given that the coverage we have
> >> > is only dynamic -- do you have an idea of how much coverage you're actually
> >> > getting for, say, a defconfig+modules build?
> >> >
> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
> >> > failing to see how we can do it in a controlled manner where we can reason
> >> > about the semantic changes using something other than a best-effort,
> >> > case-by-case basis which is likely to be fragile and error-prone.
> >> > Unfortunately, if that's all we have, then this gets relegated to a
> >> > debug feature, which sort of defeats the point in my opinion.
> >>
> >> Well, in some cases there is no other way as resorting to dynamic testing.
> >> How do we ensure that kernel does not dereference NULL pointers, does
> >> not access objects after free or out of bounds? Nohow. And, yes, it's
> >> constant maintenance burden resolved via dynamic testing.
> >
> > ... and the advantage of NULL pointer issues is that you're likely to see
> > them as a synchronous exception at runtime, regardless of architecture and
> > regardless of Kconfig options. With pointer tagging, that's certainly not
> > the case, and so I don't think we can just treat issues there like we do for
> > NULL pointers.
> 
> Well, let's take use-after-frees, out-of-bounds, info leaks, data
> races is a good example, deadlocks and just logical bugs...

Ok, but it was you that brought up NULL pointers, so there's some goalpost
moving here. And as with NULL pointers, all of the issues you mention above
apply to other architectures and the majority of their configurations, so my
concerns about this feature remain.

> > If you want to enable khwasan in "production" and since enabling it
> > could potentially change the behaviour of existing code paths, the
> > run-time validation space doubles as we'd need to get the same code
> > coverage with and without the feature being enabled.
> 
> This is true for just any change in configs, sysctls or just a
> different workload. Any of this can enable new code, exiting code
> working differently, or just working with data in new states. And we
> have tens of thousands of bugs, so blindly deploying anything new to
> production without proper testing is a bad idea. It's not specific to
> HWASAN in any way. And when you enable HWASAN you actually do mean to
> retest everything as hard as possible.

I suppose I'm trying to understand whether we have to resort to testing, or
whether we can do better. I'm really uncomfortable with testing as our only
means of getting this right because this is a non-standard, arm64-specific
option and I don't think it will get very much testing in mainline at all.
Rather, we'll get spurious bug reports from forks of -stable many releases
later and we'll actually be worse-off for it.

> And in the end we do not seem to have any action points here, right?

Right now, it feels like this series trades one set of bugs for another,
so I'd like to get to a position where this new set of bugs is genuinely
more manageable (i.e. detectable, fixable, preventable) than the old set.
Unfortunately, the only suggestion seems to be "testing", which I really
don't find convincing :(

Could we do things like:

  - Set up a dedicated arm64 test farm, running mainline and with a public
    frontend, aimed at getting maximum coverage of the kernel with KHWASAN
    enabled?

  - Have an implementation of KHWASAN for other architectures? (Is this even
    possible?)

  - Have a compiler plugin to clear out the tag for pointer arithmetic?
    Could we WARN if two pointers are compared with different tags?
    Could we manipulate the tag on cast-to-pointer so that a mismatch would
    be qualifier to say that pointer was created via a cast?

  - ...

?

Will
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-kbuild
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:53:54 +0000
Message-ID: <CACT4Y+bQxwz2fcuGqtQYkCCV75chtLvOAfM9W-HBSjkTS1q=-w () mail ! gmail ! com>
--------------------
On Wed, Aug 8, 2018 at 6:27 PM, Will Deacon <will.deacon@arm.com> wrote:
>> >> > Thanks for tracking these cases down and going through each of them. The
>> >> > obvious follow-up question is: how do we ensure that we keep on top of
>> >> > this in mainline? Are you going to repeat your experiment at every kernel
>> >> > release or every -rc or something else? I really can't see how we can
>> >> > maintain this in the long run, especially given that the coverage we have
>> >> > is only dynamic -- do you have an idea of how much coverage you're actually
>> >> > getting for, say, a defconfig+modules build?
>> >> >
>> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> >> > failing to see how we can do it in a controlled manner where we can reason
>> >> > about the semantic changes using something other than a best-effort,
>> >> > case-by-case basis which is likely to be fragile and error-prone.
>> >> > Unfortunately, if that's all we have, then this gets relegated to a
>> >> > debug feature, which sort of defeats the point in my opinion.
>> >>
>> >> Well, in some cases there is no other way as resorting to dynamic testing.
>> >> How do we ensure that kernel does not dereference NULL pointers, does
>> >> not access objects after free or out of bounds? Nohow. And, yes, it's
>> >> constant maintenance burden resolved via dynamic testing.
>> >
>> > ... and the advantage of NULL pointer issues is that you're likely to see
>> > them as a synchronous exception at runtime, regardless of architecture and
>> > regardless of Kconfig options. With pointer tagging, that's certainly not
>> > the case, and so I don't think we can just treat issues there like we do for
>> > NULL pointers.
>>
>> Well, let's take use-after-frees, out-of-bounds, info leaks, data
>> races is a good example, deadlocks and just logical bugs...
>
> Ok, but it was you that brought up NULL pointers, so there's some goalpost
> moving here.

I moved it only because our views on bugs seems to be somewhat
different. I would put it all including NULL derefs into the same
bucket of bugs. But the point I wanted to make holds if we take NULL
derefs out of equation too, so I took them out so that we don't
concentrate on "synchronous exceptions" only.

> And as with NULL pointers, all of the issues you mention above
> apply to other architectures and the majority of their configurations, so my
> concerns about this feature remain.
>
>> > If you want to enable khwasan in "production" and since enabling it
>> > could potentially change the behaviour of existing code paths, the
>> > run-time validation space doubles as we'd need to get the same code
>> > coverage with and without the feature being enabled.
>>
>> This is true for just any change in configs, sysctls or just a
>> different workload. Any of this can enable new code, exiting code
>> working differently, or just working with data in new states. And we
>> have tens of thousands of bugs, so blindly deploying anything new to
>> production without proper testing is a bad idea. It's not specific to
>> HWASAN in any way. And when you enable HWASAN you actually do mean to
>> retest everything as hard as possible.
>
> I suppose I'm trying to understand whether we have to resort to testing, or
> whether we can do better. I'm really uncomfortable with testing as our only
> means of getting this right because this is a non-standard, arm64-specific
> option and I don't think it will get very much testing in mainline at all.
> Rather, we'll get spurious bug reports from forks of -stable many releases
> later and we'll actually be worse-off for it.
>
>> And in the end we do not seem to have any action points here, right?
>
> Right now, it feels like this series trades one set of bugs for another,
> so I'd like to get to a position where this new set of bugs is genuinely
> more manageable (i.e. detectable, fixable, preventable) than the old set.
> Unfortunately, the only suggestion seems to be "testing", which I really
> don't find convincing :(
>
> Could we do things like:
>
>   - Set up a dedicated arm64 test farm, running mainline and with a public
>     frontend, aimed at getting maximum coverage of the kernel with KHWASAN
>     enabled?

FWIW we could try to setup a syzbot instance with qemu/arm64
emulation. We run such combination few times, but I am not sure how
stable it will be wrt flaky timeouts/stalls/etc. If works, it will
give instant coverage of about 1MLOC.

>   - Have an implementation of KHWASAN for other architectures? (Is this even
>     possible?)
>
>   - Have a compiler plugin to clear out the tag for pointer arithmetic?
>     Could we WARN if two pointers are compared with different tags?
>     Could we manipulate the tag on cast-to-pointer so that a mismatch would
>     be qualifier to say that pointer was created via a cast?
>
>   - ...
>
> ?
>
> Will
--
To unsubscribe from this list: send the line "unsubscribe linux-kbuild" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:53:54 +0000
Message-ID: <CACT4Y+bQxwz2fcuGqtQYkCCV75chtLvOAfM9W-HBSjkTS1q=-w () mail ! gmail ! com>
--------------------
On Wed, Aug 8, 2018 at 6:27 PM, Will Deacon <will.deacon@arm.com> wrote:
>> >> > Thanks for tracking these cases down and going through each of them. The
>> >> > obvious follow-up question is: how do we ensure that we keep on top of
>> >> > this in mainline? Are you going to repeat your experiment at every kernel
>> >> > release or every -rc or something else? I really can't see how we can
>> >> > maintain this in the long run, especially given that the coverage we have
>> >> > is only dynamic -- do you have an idea of how much coverage you're actually
>> >> > getting for, say, a defconfig+modules build?
>> >> >
>> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> >> > failing to see how we can do it in a controlled manner where we can reason
>> >> > about the semantic changes using something other than a best-effort,
>> >> > case-by-case basis which is likely to be fragile and error-prone.
>> >> > Unfortunately, if that's all we have, then this gets relegated to a
>> >> > debug feature, which sort of defeats the point in my opinion.
>> >>
>> >> Well, in some cases there is no other way as resorting to dynamic testing.
>> >> How do we ensure that kernel does not dereference NULL pointers, does
>> >> not access objects after free or out of bounds? Nohow. And, yes, it's
>> >> constant maintenance burden resolved via dynamic testing.
>> >
>> > ... and the advantage of NULL pointer issues is that you're likely to see
>> > them as a synchronous exception at runtime, regardless of architecture and
>> > regardless of Kconfig options. With pointer tagging, that's certainly not
>> > the case, and so I don't think we can just treat issues there like we do for
>> > NULL pointers.
>>
>> Well, let's take use-after-frees, out-of-bounds, info leaks, data
>> races is a good example, deadlocks and just logical bugs...
>
> Ok, but it was you that brought up NULL pointers, so there's some goalpost
> moving here.

I moved it only because our views on bugs seems to be somewhat
different. I would put it all including NULL derefs into the same
bucket of bugs. But the point I wanted to make holds if we take NULL
derefs out of equation too, so I took them out so that we don't
concentrate on "synchronous exceptions" only.

> And as with NULL pointers, all of the issues you mention above
> apply to other architectures and the majority of their configurations, so my
> concerns about this feature remain.
>
>> > If you want to enable khwasan in "production" and since enabling it
>> > could potentially change the behaviour of existing code paths, the
>> > run-time validation space doubles as we'd need to get the same code
>> > coverage with and without the feature being enabled.
>>
>> This is true for just any change in configs, sysctls or just a
>> different workload. Any of this can enable new code, exiting code
>> working differently, or just working with data in new states. And we
>> have tens of thousands of bugs, so blindly deploying anything new to
>> production without proper testing is a bad idea. It's not specific to
>> HWASAN in any way. And when you enable HWASAN you actually do mean to
>> retest everything as hard as possible.
>
> I suppose I'm trying to understand whether we have to resort to testing, or
> whether we can do better. I'm really uncomfortable with testing as our only
> means of getting this right because this is a non-standard, arm64-specific
> option and I don't think it will get very much testing in mainline at all.
> Rather, we'll get spurious bug reports from forks of -stable many releases
> later and we'll actually be worse-off for it.
>
>> And in the end we do not seem to have any action points here, right?
>
> Right now, it feels like this series trades one set of bugs for another,
> so I'd like to get to a position where this new set of bugs is genuinely
> more manageable (i.e. detectable, fixable, preventable) than the old set.
> Unfortunately, the only suggestion seems to be "testing", which I really
> don't find convincing :(
>
> Could we do things like:
>
>   - Set up a dedicated arm64 test farm, running mainline and with a public
>     frontend, aimed at getting maximum coverage of the kernel with KHWASAN
>     enabled?

FWIW we could try to setup a syzbot instance with qemu/arm64
emulation. We run such combination few times, but I am not sure how
stable it will be wrt flaky timeouts/stalls/etc. If works, it will
give instant coverage of about 1MLOC.

>   - Have an implementation of KHWASAN for other architectures? (Is this even
>     possible?)
>
>   - Have a compiler plugin to clear out the tag for pointer arithmetic?
>     Could we WARN if two pointers are compared with different tags?
>     Could we manipulate the tag on cast-to-pointer so that a mismatch would
>     be qualifier to say that pointer was created via a cast?
>
>   - ...
>
> ?
>
> Will
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-sparse
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:53:54 +0000
Message-ID: <CACT4Y+bQxwz2fcuGqtQYkCCV75chtLvOAfM9W-HBSjkTS1q=-w () mail ! gmail ! com>
--------------------
On Wed, Aug 8, 2018 at 6:27 PM, Will Deacon <will.deacon@arm.com> wrote:
>> >> > Thanks for tracking these cases down and going through each of them. The
>> >> > obvious follow-up question is: how do we ensure that we keep on top of
>> >> > this in mainline? Are you going to repeat your experiment at every kernel
>> >> > release or every -rc or something else? I really can't see how we can
>> >> > maintain this in the long run, especially given that the coverage we have
>> >> > is only dynamic -- do you have an idea of how much coverage you're actually
>> >> > getting for, say, a defconfig+modules build?
>> >> >
>> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> >> > failing to see how we can do it in a controlled manner where we can reason
>> >> > about the semantic changes using something other than a best-effort,
>> >> > case-by-case basis which is likely to be fragile and error-prone.
>> >> > Unfortunately, if that's all we have, then this gets relegated to a
>> >> > debug feature, which sort of defeats the point in my opinion.
>> >>
>> >> Well, in some cases there is no other way as resorting to dynamic testing.
>> >> How do we ensure that kernel does not dereference NULL pointers, does
>> >> not access objects after free or out of bounds? Nohow. And, yes, it's
>> >> constant maintenance burden resolved via dynamic testing.
>> >
>> > ... and the advantage of NULL pointer issues is that you're likely to see
>> > them as a synchronous exception at runtime, regardless of architecture and
>> > regardless of Kconfig options. With pointer tagging, that's certainly not
>> > the case, and so I don't think we can just treat issues there like we do for
>> > NULL pointers.
>>
>> Well, let's take use-after-frees, out-of-bounds, info leaks, data
>> races is a good example, deadlocks and just logical bugs...
>
> Ok, but it was you that brought up NULL pointers, so there's some goalpost
> moving here.

I moved it only because our views on bugs seems to be somewhat
different. I would put it all including NULL derefs into the same
bucket of bugs. But the point I wanted to make holds if we take NULL
derefs out of equation too, so I took them out so that we don't
concentrate on "synchronous exceptions" only.

> And as with NULL pointers, all of the issues you mention above
> apply to other architectures and the majority of their configurations, so my
> concerns about this feature remain.
>
>> > If you want to enable khwasan in "production" and since enabling it
>> > could potentially change the behaviour of existing code paths, the
>> > run-time validation space doubles as we'd need to get the same code
>> > coverage with and without the feature being enabled.
>>
>> This is true for just any change in configs, sysctls or just a
>> different workload. Any of this can enable new code, exiting code
>> working differently, or just working with data in new states. And we
>> have tens of thousands of bugs, so blindly deploying anything new to
>> production without proper testing is a bad idea. It's not specific to
>> HWASAN in any way. And when you enable HWASAN you actually do mean to
>> retest everything as hard as possible.
>
> I suppose I'm trying to understand whether we have to resort to testing, or
> whether we can do better. I'm really uncomfortable with testing as our only
> means of getting this right because this is a non-standard, arm64-specific
> option and I don't think it will get very much testing in mainline at all.
> Rather, we'll get spurious bug reports from forks of -stable many releases
> later and we'll actually be worse-off for it.
>
>> And in the end we do not seem to have any action points here, right?
>
> Right now, it feels like this series trades one set of bugs for another,
> so I'd like to get to a position where this new set of bugs is genuinely
> more manageable (i.e. detectable, fixable, preventable) than the old set.
> Unfortunately, the only suggestion seems to be "testing", which I really
> don't find convincing :(
>
> Could we do things like:
>
>   - Set up a dedicated arm64 test farm, running mainline and with a public
>     frontend, aimed at getting maximum coverage of the kernel with KHWASAN
>     enabled?

FWIW we could try to setup a syzbot instance with qemu/arm64
emulation. We run such combination few times, but I am not sure how
stable it will be wrt flaky timeouts/stalls/etc. If works, it will
give instant coverage of about 1MLOC.

>   - Have an implementation of KHWASAN for other architectures? (Is this even
>     possible?)
>
>   - Have a compiler plugin to clear out the tag for pointer arithmetic?
>     Could we WARN if two pointers are compared with different tags?
>     Could we manipulate the tag on cast-to-pointer so that a mismatch would
>     be qualifier to say that pointer was created via a cast?
>
>   - ...
>
> ?
>
> Will
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-doc
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:53:54 +0000
Message-ID: <CACT4Y+bQxwz2fcuGqtQYkCCV75chtLvOAfM9W-HBSjkTS1q=-w () mail ! gmail ! com>
--------------------
On Wed, Aug 8, 2018 at 6:27 PM, Will Deacon <will.deacon@arm.com> wrote:
>> >> > Thanks for tracking these cases down and going through each of them. The
>> >> > obvious follow-up question is: how do we ensure that we keep on top of
>> >> > this in mainline? Are you going to repeat your experiment at every kernel
>> >> > release or every -rc or something else? I really can't see how we can
>> >> > maintain this in the long run, especially given that the coverage we have
>> >> > is only dynamic -- do you have an idea of how much coverage you're actually
>> >> > getting for, say, a defconfig+modules build?
>> >> >
>> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> >> > failing to see how we can do it in a controlled manner where we can reason
>> >> > about the semantic changes using something other than a best-effort,
>> >> > case-by-case basis which is likely to be fragile and error-prone.
>> >> > Unfortunately, if that's all we have, then this gets relegated to a
>> >> > debug feature, which sort of defeats the point in my opinion.
>> >>
>> >> Well, in some cases there is no other way as resorting to dynamic testing.
>> >> How do we ensure that kernel does not dereference NULL pointers, does
>> >> not access objects after free or out of bounds? Nohow. And, yes, it's
>> >> constant maintenance burden resolved via dynamic testing.
>> >
>> > ... and the advantage of NULL pointer issues is that you're likely to see
>> > them as a synchronous exception at runtime, regardless of architecture and
>> > regardless of Kconfig options. With pointer tagging, that's certainly not
>> > the case, and so I don't think we can just treat issues there like we do for
>> > NULL pointers.
>>
>> Well, let's take use-after-frees, out-of-bounds, info leaks, data
>> races is a good example, deadlocks and just logical bugs...
>
> Ok, but it was you that brought up NULL pointers, so there's some goalpost
> moving here.

I moved it only because our views on bugs seems to be somewhat
different. I would put it all including NULL derefs into the same
bucket of bugs. But the point I wanted to make holds if we take NULL
derefs out of equation too, so I took them out so that we don't
concentrate on "synchronous exceptions" only.

> And as with NULL pointers, all of the issues you mention above
> apply to other architectures and the majority of their configurations, so my
> concerns about this feature remain.
>
>> > If you want to enable khwasan in "production" and since enabling it
>> > could potentially change the behaviour of existing code paths, the
>> > run-time validation space doubles as we'd need to get the same code
>> > coverage with and without the feature being enabled.
>>
>> This is true for just any change in configs, sysctls or just a
>> different workload. Any of this can enable new code, exiting code
>> working differently, or just working with data in new states. And we
>> have tens of thousands of bugs, so blindly deploying anything new to
>> production without proper testing is a bad idea. It's not specific to
>> HWASAN in any way. And when you enable HWASAN you actually do mean to
>> retest everything as hard as possible.
>
> I suppose I'm trying to understand whether we have to resort to testing, or
> whether we can do better. I'm really uncomfortable with testing as our only
> means of getting this right because this is a non-standard, arm64-specific
> option and I don't think it will get very much testing in mainline at all.
> Rather, we'll get spurious bug reports from forks of -stable many releases
> later and we'll actually be worse-off for it.
>
>> And in the end we do not seem to have any action points here, right?
>
> Right now, it feels like this series trades one set of bugs for another,
> so I'd like to get to a position where this new set of bugs is genuinely
> more manageable (i.e. detectable, fixable, preventable) than the old set.
> Unfortunately, the only suggestion seems to be "testing", which I really
> don't find convincing :(
>
> Could we do things like:
>
>   - Set up a dedicated arm64 test farm, running mainline and with a public
>     frontend, aimed at getting maximum coverage of the kernel with KHWASAN
>     enabled?

FWIW we could try to setup a syzbot instance with qemu/arm64
emulation. We run such combination few times, but I am not sure how
stable it will be wrt flaky timeouts/stalls/etc. If works, it will
give instant coverage of about 1MLOC.

>   - Have an implementation of KHWASAN for other architectures? (Is this even
>     possible?)
>
>   - Have a compiler plugin to clear out the tag for pointer arithmetic?
>     Could we WARN if two pointers are compared with different tags?
>     Could we manipulate the tag on cast-to-pointer so that a mismatch would
>     be qualifier to say that pointer was created via a cast?
>
>   - ...
>
> ?
>
> Will
--
To unsubscribe from this list: send the line "unsubscribe linux-doc" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-kbuild
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:53:54 +0000
Message-ID: <CACT4Y+bQxwz2fcuGqtQYkCCV75chtLvOAfM9W-HBSjkTS1q=-w () mail ! gmail ! com>
--------------------
On Wed, Aug 8, 2018 at 6:27 PM, Will Deacon <will.deacon@arm.com> wrote:
>> >> > Thanks for tracking these cases down and going through each of them. The
>> >> > obvious follow-up question is: how do we ensure that we keep on top of
>> >> > this in mainline? Are you going to repeat your experiment at every kernel
>> >> > release or every -rc or something else? I really can't see how we can
>> >> > maintain this in the long run, especially given that the coverage we have
>> >> > is only dynamic -- do you have an idea of how much coverage you're actually
>> >> > getting for, say, a defconfig+modules build?
>> >> >
>> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> >> > failing to see how we can do it in a controlled manner where we can reason
>> >> > about the semantic changes using something other than a best-effort,
>> >> > case-by-case basis which is likely to be fragile and error-prone.
>> >> > Unfortunately, if that's all we have, then this gets relegated to a
>> >> > debug feature, which sort of defeats the point in my opinion.
>> >>
>> >> Well, in some cases there is no other way as resorting to dynamic testing.
>> >> How do we ensure that kernel does not dereference NULL pointers, does
>> >> not access objects after free or out of bounds? Nohow. And, yes, it's
>> >> constant maintenance burden resolved via dynamic testing.
>> >
>> > ... and the advantage of NULL pointer issues is that you're likely to see
>> > them as a synchronous exception at runtime, regardless of architecture and
>> > regardless of Kconfig options. With pointer tagging, that's certainly not
>> > the case, and so I don't think we can just treat issues there like we do for
>> > NULL pointers.
>>
>> Well, let's take use-after-frees, out-of-bounds, info leaks, data
>> races is a good example, deadlocks and just logical bugs...
>
> Ok, but it was you that brought up NULL pointers, so there's some goalpost
> moving here.

I moved it only because our views on bugs seems to be somewhat
different. I would put it all including NULL derefs into the same
bucket of bugs. But the point I wanted to make holds if we take NULL
derefs out of equation too, so I took them out so that we don't
concentrate on "synchronous exceptions" only.

> And as with NULL pointers, all of the issues you mention above
> apply to other architectures and the majority of their configurations, so my
> concerns about this feature remain.
>
>> > If you want to enable khwasan in "production" and since enabling it
>> > could potentially change the behaviour of existing code paths, the
>> > run-time validation space doubles as we'd need to get the same code
>> > coverage with and without the feature being enabled.
>>
>> This is true for just any change in configs, sysctls or just a
>> different workload. Any of this can enable new code, exiting code
>> working differently, or just working with data in new states. And we
>> have tens of thousands of bugs, so blindly deploying anything new to
>> production without proper testing is a bad idea. It's not specific to
>> HWASAN in any way. And when you enable HWASAN you actually do mean to
>> retest everything as hard as possible.
>
> I suppose I'm trying to understand whether we have to resort to testing, or
> whether we can do better. I'm really uncomfortable with testing as our only
> means of getting this right because this is a non-standard, arm64-specific
> option and I don't think it will get very much testing in mainline at all.
> Rather, we'll get spurious bug reports from forks of -stable many releases
> later and we'll actually be worse-off for it.
>
>> And in the end we do not seem to have any action points here, right?
>
> Right now, it feels like this series trades one set of bugs for another,
> so I'd like to get to a position where this new set of bugs is genuinely
> more manageable (i.e. detectable, fixable, preventable) than the old set.
> Unfortunately, the only suggestion seems to be "testing", which I really
> don't find convincing :(
>
> Could we do things like:
>
>   - Set up a dedicated arm64 test farm, running mainline and with a public
>     frontend, aimed at getting maximum coverage of the kernel with KHWASAN
>     enabled?

FWIW we could try to setup a syzbot instance with qemu/arm64
emulation. We run such combination few times, but I am not sure how
stable it will be wrt flaky timeouts/stalls/etc. If works, it will
give instant coverage of about 1MLOC.

>   - Have an implementation of KHWASAN for other architectures? (Is this even
>     possible?)
>
>   - Have a compiler plugin to clear out the tag for pointer arithmetic?
>     Could we WARN if two pointers are compared with different tags?
>     Could we manipulate the tag on cast-to-pointer so that a mismatch would
>     be qualifier to say that pointer was created via a cast?
>
>   - ...
>
> ?
>
> Will
--
To unsubscribe from this list: send the line "unsubscribe linux-kbuild" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-mm
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:53:54 +0000
Message-ID: <CACT4Y+bQxwz2fcuGqtQYkCCV75chtLvOAfM9W-HBSjkTS1q=-w () mail ! gmail ! com>
--------------------
On Wed, Aug 8, 2018 at 6:27 PM, Will Deacon <will.deacon@arm.com> wrote:
>> >> > Thanks for tracking these cases down and going through each of them. The
>> >> > obvious follow-up question is: how do we ensure that we keep on top of
>> >> > this in mainline? Are you going to repeat your experiment at every kernel
>> >> > release or every -rc or something else? I really can't see how we can
>> >> > maintain this in the long run, especially given that the coverage we have
>> >> > is only dynamic -- do you have an idea of how much coverage you're actually
>> >> > getting for, say, a defconfig+modules build?
>> >> >
>> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> >> > failing to see how we can do it in a controlled manner where we can reason
>> >> > about the semantic changes using something other than a best-effort,
>> >> > case-by-case basis which is likely to be fragile and error-prone.
>> >> > Unfortunately, if that's all we have, then this gets relegated to a
>> >> > debug feature, which sort of defeats the point in my opinion.
>> >>
>> >> Well, in some cases there is no other way as resorting to dynamic testing.
>> >> How do we ensure that kernel does not dereference NULL pointers, does
>> >> not access objects after free or out of bounds? Nohow. And, yes, it's
>> >> constant maintenance burden resolved via dynamic testing.
>> >
>> > ... and the advantage of NULL pointer issues is that you're likely to see
>> > them as a synchronous exception at runtime, regardless of architecture and
>> > regardless of Kconfig options. With pointer tagging, that's certainly not
>> > the case, and so I don't think we can just treat issues there like we do for
>> > NULL pointers.
>>
>> Well, let's take use-after-frees, out-of-bounds, info leaks, data
>> races is a good example, deadlocks and just logical bugs...
>
> Ok, but it was you that brought up NULL pointers, so there's some goalpost
> moving here.

I moved it only because our views on bugs seems to be somewhat
different. I would put it all including NULL derefs into the same
bucket of bugs. But the point I wanted to make holds if we take NULL
derefs out of equation too, so I took them out so that we don't
concentrate on "synchronous exceptions" only.

> And as with NULL pointers, all of the issues you mention above
> apply to other architectures and the majority of their configurations, so my
> concerns about this feature remain.
>
>> > If you want to enable khwasan in "production" and since enabling it
>> > could potentially change the behaviour of existing code paths, the
>> > run-time validation space doubles as we'd need to get the same code
>> > coverage with and without the feature being enabled.
>>
>> This is true for just any change in configs, sysctls or just a
>> different workload. Any of this can enable new code, exiting code
>> working differently, or just working with data in new states. And we
>> have tens of thousands of bugs, so blindly deploying anything new to
>> production without proper testing is a bad idea. It's not specific to
>> HWASAN in any way. And when you enable HWASAN you actually do mean to
>> retest everything as hard as possible.
>
> I suppose I'm trying to understand whether we have to resort to testing, or
> whether we can do better. I'm really uncomfortable with testing as our only
> means of getting this right because this is a non-standard, arm64-specific
> option and I don't think it will get very much testing in mainline at all.
> Rather, we'll get spurious bug reports from forks of -stable many releases
> later and we'll actually be worse-off for it.
>
>> And in the end we do not seem to have any action points here, right?
>
> Right now, it feels like this series trades one set of bugs for another,
> so I'd like to get to a position where this new set of bugs is genuinely
> more manageable (i.e. detectable, fixable, preventable) than the old set.
> Unfortunately, the only suggestion seems to be "testing", which I really
> don't find convincing :(
>
> Could we do things like:
>
>   - Set up a dedicated arm64 test farm, running mainline and with a public
>     frontend, aimed at getting maximum coverage of the kernel with KHWASAN
>     enabled?

FWIW we could try to setup a syzbot instance with qemu/arm64
emulation. We run such combination few times, but I am not sure how
stable it will be wrt flaky timeouts/stalls/etc. If works, it will
give instant coverage of about 1MLOC.

>   - Have an implementation of KHWASAN for other architectures? (Is this even
>     possible?)
>
>   - Have a compiler plugin to clear out the tag for pointer arithmetic?
>     Could we WARN if two pointers are compared with different tags?
>     Could we manipulate the tag on cast-to-pointer so that a mismatch would
>     be qualifier to say that pointer was created via a cast?
>
>   - ...
>
> ?
>
> Will

================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-sparse
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:53:54 +0000
Message-ID: <CACT4Y+bQxwz2fcuGqtQYkCCV75chtLvOAfM9W-HBSjkTS1q=-w () mail ! gmail ! com>
--------------------
On Wed, Aug 8, 2018 at 6:27 PM, Will Deacon <will.deacon@arm.com> wrote:
>> >> > Thanks for tracking these cases down and going through each of them. The
>> >> > obvious follow-up question is: how do we ensure that we keep on top of
>> >> > this in mainline? Are you going to repeat your experiment at every kernel
>> >> > release or every -rc or something else? I really can't see how we can
>> >> > maintain this in the long run, especially given that the coverage we have
>> >> > is only dynamic -- do you have an idea of how much coverage you're actually
>> >> > getting for, say, a defconfig+modules build?
>> >> >
>> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> >> > failing to see how we can do it in a controlled manner where we can reason
>> >> > about the semantic changes using something other than a best-effort,
>> >> > case-by-case basis which is likely to be fragile and error-prone.
>> >> > Unfortunately, if that's all we have, then this gets relegated to a
>> >> > debug feature, which sort of defeats the point in my opinion.
>> >>
>> >> Well, in some cases there is no other way as resorting to dynamic testing.
>> >> How do we ensure that kernel does not dereference NULL pointers, does
>> >> not access objects after free or out of bounds? Nohow. And, yes, it's
>> >> constant maintenance burden resolved via dynamic testing.
>> >
>> > ... and the advantage of NULL pointer issues is that you're likely to see
>> > them as a synchronous exception at runtime, regardless of architecture and
>> > regardless of Kconfig options. With pointer tagging, that's certainly not
>> > the case, and so I don't think we can just treat issues there like we do for
>> > NULL pointers.
>>
>> Well, let's take use-after-frees, out-of-bounds, info leaks, data
>> races is a good example, deadlocks and just logical bugs...
>
> Ok, but it was you that brought up NULL pointers, so there's some goalpost
> moving here.

I moved it only because our views on bugs seems to be somewhat
different. I would put it all including NULL derefs into the same
bucket of bugs. But the point I wanted to make holds if we take NULL
derefs out of equation too, so I took them out so that we don't
concentrate on "synchronous exceptions" only.

> And as with NULL pointers, all of the issues you mention above
> apply to other architectures and the majority of their configurations, so my
> concerns about this feature remain.
>
>> > If you want to enable khwasan in "production" and since enabling it
>> > could potentially change the behaviour of existing code paths, the
>> > run-time validation space doubles as we'd need to get the same code
>> > coverage with and without the feature being enabled.
>>
>> This is true for just any change in configs, sysctls or just a
>> different workload. Any of this can enable new code, exiting code
>> working differently, or just working with data in new states. And we
>> have tens of thousands of bugs, so blindly deploying anything new to
>> production without proper testing is a bad idea. It's not specific to
>> HWASAN in any way. And when you enable HWASAN you actually do mean to
>> retest everything as hard as possible.
>
> I suppose I'm trying to understand whether we have to resort to testing, or
> whether we can do better. I'm really uncomfortable with testing as our only
> means of getting this right because this is a non-standard, arm64-specific
> option and I don't think it will get very much testing in mainline at all.
> Rather, we'll get spurious bug reports from forks of -stable many releases
> later and we'll actually be worse-off for it.
>
>> And in the end we do not seem to have any action points here, right?
>
> Right now, it feels like this series trades one set of bugs for another,
> so I'd like to get to a position where this new set of bugs is genuinely
> more manageable (i.e. detectable, fixable, preventable) than the old set.
> Unfortunately, the only suggestion seems to be "testing", which I really
> don't find convincing :(
>
> Could we do things like:
>
>   - Set up a dedicated arm64 test farm, running mainline and with a public
>     frontend, aimed at getting maximum coverage of the kernel with KHWASAN
>     enabled?

FWIW we could try to setup a syzbot instance with qemu/arm64
emulation. We run such combination few times, but I am not sure how
stable it will be wrt flaky timeouts/stalls/etc. If works, it will
give instant coverage of about 1MLOC.

>   - Have an implementation of KHWASAN for other architectures? (Is this even
>     possible?)
>
>   - Have a compiler plugin to clear out the tag for pointer arithmetic?
>     Could we WARN if two pointers are compared with different tags?
>     Could we manipulate the tag on cast-to-pointer so that a mismatch would
>     be qualifier to say that pointer was created via a cast?
>
>   - ...
>
> ?
>
> Will
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v4 00/17] khwasan: kernel hardware assisted address sanitizer
Date: Wed, 08 Aug 2018 16:53:54 +0000
Message-ID: <CACT4Y+bQxwz2fcuGqtQYkCCV75chtLvOAfM9W-HBSjkTS1q=-w () mail ! gmail ! com>
--------------------
On Wed, Aug 8, 2018 at 6:27 PM, Will Deacon <will.deacon@arm.com> wrote:
>> >> > Thanks for tracking these cases down and going through each of them. The
>> >> > obvious follow-up question is: how do we ensure that we keep on top of
>> >> > this in mainline? Are you going to repeat your experiment at every kernel
>> >> > release or every -rc or something else? I really can't see how we can
>> >> > maintain this in the long run, especially given that the coverage we have
>> >> > is only dynamic -- do you have an idea of how much coverage you're actually
>> >> > getting for, say, a defconfig+modules build?
>> >> >
>> >> > I'd really like to enable pointer tagging in the kernel, I'm just still
>> >> > failing to see how we can do it in a controlled manner where we can reason
>> >> > about the semantic changes using something other than a best-effort,
>> >> > case-by-case basis which is likely to be fragile and error-prone.
>> >> > Unfortunately, if that's all we have, then this gets relegated to a
>> >> > debug feature, which sort of defeats the point in my opinion.
>> >>
>> >> Well, in some cases there is no other way as resorting to dynamic testing.
>> >> How do we ensure that kernel does not dereference NULL pointers, does
>> >> not access objects after free or out of bounds? Nohow. And, yes, it's
>> >> constant maintenance burden resolved via dynamic testing.
>> >
>> > ... and the advantage of NULL pointer issues is that you're likely to see
>> > them as a synchronous exception at runtime, regardless of architecture and
>> > regardless of Kconfig options. With pointer tagging, that's certainly not
>> > the case, and so I don't think we can just treat issues there like we do for
>> > NULL pointers.
>>
>> Well, let's take use-after-frees, out-of-bounds, info leaks, data
>> races is a good example, deadlocks and just logical bugs...
>
> Ok, but it was you that brought up NULL pointers, so there's some goalpost
> moving here.

I moved it only because our views on bugs seems to be somewhat
different. I would put it all including NULL derefs into the same
bucket of bugs. But the point I wanted to make holds if we take NULL
derefs out of equation too, so I took them out so that we don't
concentrate on "synchronous exceptions" only.

> And as with NULL pointers, all of the issues you mention above
> apply to other architectures and the majority of their configurations, so my
> concerns about this feature remain.
>
>> > If you want to enable khwasan in "production" and since enabling it
>> > could potentially change the behaviour of existing code paths, the
>> > run-time validation space doubles as we'd need to get the same code
>> > coverage with and without the feature being enabled.
>>
>> This is true for just any change in configs, sysctls or just a
>> different workload. Any of this can enable new code, exiting code
>> working differently, or just working with data in new states. And we
>> have tens of thousands of bugs, so blindly deploying anything new to
>> production without proper testing is a bad idea. It's not specific to
>> HWASAN in any way. And when you enable HWASAN you actually do mean to
>> retest everything as hard as possible.
>
> I suppose I'm trying to understand whether we have to resort to testing, or
> whether we can do better. I'm really uncomfortable with testing as our only
> means of getting this right because this is a non-standard, arm64-specific
> option and I don't think it will get very much testing in mainline at all.
> Rather, we'll get spurious bug reports from forks of -stable many releases
> later and we'll actually be worse-off for it.
>
>> And in the end we do not seem to have any action points here, right?
>
> Right now, it feels like this series trades one set of bugs for another,
> so I'd like to get to a position where this new set of bugs is genuinely
> more manageable (i.e. detectable, fixable, preventable) than the old set.
> Unfortunately, the only suggestion seems to be "testing", which I really
> don't find convincing :(
>
> Could we do things like:
>
>   - Set up a dedicated arm64 test farm, running mainline and with a public
>     frontend, aimed at getting maximum coverage of the kernel with KHWASAN
>     enabled?

FWIW we could try to setup a syzbot instance with qemu/arm64
emulation. We run such combination few times, but I am not sure how
stable it will be wrt flaky timeouts/stalls/etc. If works, it will
give instant coverage of about 1MLOC.

>   - Have an implementation of KHWASAN for other architectures? (Is this even
>     possible?)
>
>   - Have a compiler plugin to clear out the tag for pointer arithmetic?
>     Could we WARN if two pointers are compared with different tags?
>     Could we manipulate the tag on cast-to-pointer so that a mismatch would
>     be qualifier to say that pointer was created via a cast?
>
>   - ...
>
> ?
>
> Will

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================


################################################################################

=== Thread: [PATCH v4 01/17] khwasan, mm: change kasan hooks signatures ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v4 01/17] khwasan, mm: change kasan hooks signatures
Date: Tue, 26 Jun 2018 13:15:11 +0000
Message-ID: <5e472107368b64759f60987065bb557e79bdebe6.1530018818.git.andreyknvl () google ! com>
--------------------
KHWASAN will change the value of the top byte of pointers returned from the
kernel allocation functions (such as kmalloc). This patch updates KASAN
hooks signatures and their usage in SLAB and SLUB code to reflect that.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 include/linux/kasan.h | 34 +++++++++++++++++++++++-----------
 mm/kasan/kasan.c      | 24 ++++++++++++++----------
 mm/slab.c             | 12 ++++++------
 mm/slab.h             |  2 +-
 mm/slab_common.c      |  4 ++--
 mm/slub.c             | 15 +++++++--------
 6 files changed, 53 insertions(+), 38 deletions(-)

diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index de784fd11d12..cbdc54543803 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -53,14 +53,14 @@ void kasan_unpoison_object_data(struct kmem_cache *cache, void *object);
 void kasan_poison_object_data(struct kmem_cache *cache, void *object);
 void kasan_init_slab_obj(struct kmem_cache *cache, const void *object);
 
-void kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags);
+void *kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags);
 void kasan_kfree_large(void *ptr, unsigned long ip);
 void kasan_poison_kfree(void *ptr, unsigned long ip);
-void kasan_kmalloc(struct kmem_cache *s, const void *object, size_t size,
+void *kasan_kmalloc(struct kmem_cache *s, const void *object, size_t size,
 		  gfp_t flags);
-void kasan_krealloc(const void *object, size_t new_size, gfp_t flags);
+void *kasan_krealloc(const void *object, size_t new_size, gfp_t flags);
 
-void kasan_slab_alloc(struct kmem_cache *s, void *object, gfp_t flags);
+void *kasan_slab_alloc(struct kmem_cache *s, void *object, gfp_t flags);
 bool kasan_slab_free(struct kmem_cache *s, void *object, unsigned long ip);
 
 struct kasan_cache {
@@ -105,16 +105,28 @@ static inline void kasan_poison_object_data(struct kmem_cache *cache,
 static inline void kasan_init_slab_obj(struct kmem_cache *cache,
 				const void *object) {}
 
-static inline void kasan_kmalloc_large(void *ptr, size_t size, gfp_t flags) {}
+static inline void *kasan_kmalloc_large(void *ptr, size_t size, gfp_t flags)
+{
+	return ptr;
+}
 static inline void kasan_kfree_large(void *ptr, unsigned long ip) {}
 static inline void kasan_poison_kfree(void *ptr, unsigned long ip) {}
-static inline void kasan_kmalloc(struct kmem_cache *s, const void *object,
-				size_t size, gfp_t flags) {}
-static inline void kasan_krealloc(const void *object, size_t new_size,
-				 gfp_t flags) {}
+static inline void *kasan_kmalloc(struct kmem_cache *s, const void *object,
+				size_t size, gfp_t flags)
+{
+	return (void *)object;
+}
+static inline void *kasan_krealloc(const void *object, size_t new_size,
+				 gfp_t flags)
+{
+	return (void *)object;
+}
 
-static inline void kasan_slab_alloc(struct kmem_cache *s, void *object,
-				   gfp_t flags) {}
+static inline void *kasan_slab_alloc(struct kmem_cache *s, void *object,
+				   gfp_t flags)
+{
+	return object;
+}
 static inline bool kasan_slab_free(struct kmem_cache *s, void *object,
 				   unsigned long ip)
 {
diff --git a/mm/kasan/kasan.c b/mm/kasan/kasan.c
index f185455b3406..f04aa1e0ba48 100644
--- a/mm/kasan/kasan.c
+++ b/mm/kasan/kasan.c
@@ -485,9 +485,9 @@ void kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
 	__memset(alloc_info, 0, sizeof(*alloc_info));
 }
 
-void kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t flags)
+void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t flags)
 {
-	kasan_kmalloc(cache, object, cache->object_size, flags);
+	return kasan_kmalloc(cache, object, cache->object_size, flags);
 }
 
 static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
@@ -528,7 +528,7 @@ bool kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
 	return __kasan_slab_free(cache, object, ip, true);
 }
 
-void kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
+void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 		   gfp_t flags)
 {
 	unsigned long redzone_start;
@@ -538,7 +538,7 @@ void kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 		quarantine_reduce();
 
 	if (unlikely(object == NULL))
-		return;
+		return NULL;
 
 	redzone_start = round_up((unsigned long)(object + size),
 				KASAN_SHADOW_SCALE_SIZE);
@@ -551,10 +551,12 @@ void kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 
 	if (cache->flags & SLAB_KASAN)
 		set_track(&get_alloc_info(cache, object)->alloc_track, flags);
+
+	return (void *)object;
 }
 EXPORT_SYMBOL(kasan_kmalloc);
 
-void kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
+void *kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
 {
 	struct page *page;
 	unsigned long redzone_start;
@@ -564,7 +566,7 @@ void kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
 		quarantine_reduce();
 
 	if (unlikely(ptr == NULL))
-		return;
+		return NULL;
 
 	page = virt_to_page(ptr);
 	redzone_start = round_up((unsigned long)(ptr + size),
@@ -574,21 +576,23 @@ void kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
 	kasan_unpoison_shadow(ptr, size);
 	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
 		KASAN_PAGE_REDZONE);
+
+	return (void *)ptr;
 }
 
-void kasan_krealloc(const void *object, size_t size, gfp_t flags)
+void *kasan_krealloc(const void *object, size_t size, gfp_t flags)
 {
 	struct page *page;
 
 	if (unlikely(object == ZERO_SIZE_PTR))
-		return;
+		return ZERO_SIZE_PTR;
 
 	page = virt_to_head_page(object);
 
 	if (unlikely(!PageSlab(page)))
-		kasan_kmalloc_large(object, size, flags);
+		return kasan_kmalloc_large(object, size, flags);
 	else
-		kasan_kmalloc(page->slab_cache, object, size, flags);
+		return kasan_kmalloc(page->slab_cache, object, size, flags);
 }
 
 void kasan_poison_kfree(void *ptr, unsigned long ip)
diff --git a/mm/slab.c b/mm/slab.c
index aa76a70e087e..6fdca9ec2ea4 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -3551,7 +3551,7 @@ void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)
 {
 	void *ret = slab_alloc(cachep, flags, _RET_IP_);
 
-	kasan_slab_alloc(cachep, ret, flags);
+	ret = kasan_slab_alloc(cachep, ret, flags);
 	trace_kmem_cache_alloc(_RET_IP_, ret,
 			       cachep->object_size, cachep->size, flags);
 
@@ -3617,7 +3617,7 @@ kmem_cache_alloc_trace(struct kmem_cache *cachep, gfp_t flags, size_t size)
 
 	ret = slab_alloc(cachep, flags, _RET_IP_);
 
-	kasan_kmalloc(cachep, ret, size, flags);
+	ret = kasan_kmalloc(cachep, ret, size, flags);
 	trace_kmalloc(_RET_IP_, ret,
 		      size, cachep->size, flags);
 	return ret;
@@ -3641,7 +3641,7 @@ void *kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid)
 {
 	void *ret = slab_alloc_node(cachep, flags, nodeid, _RET_IP_);
 
-	kasan_slab_alloc(cachep, ret, flags);
+	ret = kasan_slab_alloc(cachep, ret, flags);
 	trace_kmem_cache_alloc_node(_RET_IP_, ret,
 				    cachep->object_size, cachep->size,
 				    flags, nodeid);
@@ -3660,7 +3660,7 @@ void *kmem_cache_alloc_node_trace(struct kmem_cache *cachep,
 
 	ret = slab_alloc_node(cachep, flags, nodeid, _RET_IP_);
 
-	kasan_kmalloc(cachep, ret, size, flags);
+	ret = kasan_kmalloc(cachep, ret, size, flags);
 	trace_kmalloc_node(_RET_IP_, ret,
 			   size, cachep->size,
 			   flags, nodeid);
@@ -3679,7 +3679,7 @@ __do_kmalloc_node(size_t size, gfp_t flags, int node, unsigned long caller)
 	if (unlikely(ZERO_OR_NULL_PTR(cachep)))
 		return cachep;
 	ret = kmem_cache_alloc_node_trace(cachep, flags, node, size);
-	kasan_kmalloc(cachep, ret, size, flags);
+	ret = kasan_kmalloc(cachep, ret, size, flags);
 
 	return ret;
 }
@@ -3715,7 +3715,7 @@ static __always_inline void *__do_kmalloc(size_t size, gfp_t flags,
 		return cachep;
 	ret = slab_alloc(cachep, flags, caller);
 
-	kasan_kmalloc(cachep, ret, size, flags);
+	ret = kasan_kmalloc(cachep, ret, size, flags);
 	trace_kmalloc(caller, ret,
 		      size, cachep->size, flags);
 
diff --git a/mm/slab.h b/mm/slab.h
index 68bdf498da3b..15ef6a0d9c16 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -441,7 +441,7 @@ static inline void slab_post_alloc_hook(struct kmem_cache *s, gfp_t flags,
 
 		kmemleak_alloc_recursive(object, s->object_size, 1,
 					 s->flags, flags);
-		kasan_slab_alloc(s, object, flags);
+		p[i] = kasan_slab_alloc(s, object, flags);
 	}
 
 	if (memcg_kmem_enabled())
diff --git a/mm/slab_common.c b/mm/slab_common.c
index 890b1f04a03a..c279b52c7565 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -1179,7 +1179,7 @@ void *kmalloc_order(size_t size, gfp_t flags, unsigned int order)
 	page = alloc_pages(flags, order);
 	ret = page ? page_address(page) : NULL;
 	kmemleak_alloc(ret, size, 1, flags);
-	kasan_kmalloc_large(ret, size, flags);
+	ret = kasan_kmalloc_large(ret, size, flags);
 	return ret;
 }
 EXPORT_SYMBOL(kmalloc_order);
@@ -1457,7 +1457,7 @@ static __always_inline void *__do_krealloc(const void *p, size_t new_size,
 		ks = ksize(p);
 
 	if (ks >= new_size) {
-		kasan_krealloc((void *)p, new_size, flags);
+		p = kasan_krealloc((void *)p, new_size, flags);
 		return (void *)p;
 	}
 
diff --git a/mm/slub.c b/mm/slub.c
index a3b8467c14af..a60887938c19 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1336,10 +1336,10 @@ static inline void dec_slabs_node(struct kmem_cache *s, int node,
  * Hooks for other subsystems that check memory allocations. In a typical
  * production configuration these hooks all should produce no code at all.
  */
-static inline void kmalloc_large_node_hook(void *ptr, size_t size, gfp_t flags)
+static inline void *kmalloc_large_node_hook(void *ptr, size_t size, gfp_t flags)
 {
 	kmemleak_alloc(ptr, size, 1, flags);
-	kasan_kmalloc_large(ptr, size, flags);
+	return kasan_kmalloc_large(ptr, size, flags);
 }
 
 static __always_inline void kfree_hook(void *x)
@@ -2732,7 +2732,7 @@ void *kmem_cache_alloc_trace(struct kmem_cache *s, gfp_t gfpflags, size_t size)
 {
 	void *ret = slab_alloc(s, gfpflags, _RET_IP_);
 	trace_kmalloc(_RET_IP_, ret, size, s->size, gfpflags);
-	kasan_kmalloc(s, ret, size, gfpflags);
+	ret = kasan_kmalloc(s, ret, size, gfpflags);
 	return ret;
 }
 EXPORT_SYMBOL(kmem_cache_alloc_trace);
@@ -2760,7 +2760,7 @@ void *kmem_cache_alloc_node_trace(struct kmem_cache *s,
 	trace_kmalloc_node(_RET_IP_, ret,
 			   size, s->size, gfpflags, node);
 
-	kasan_kmalloc(s, ret, size, gfpflags);
+	ret = kasan_kmalloc(s, ret, size, gfpflags);
 	return ret;
 }
 EXPORT_SYMBOL(kmem_cache_alloc_node_trace);
@@ -3750,7 +3750,7 @@ void *__kmalloc(size_t size, gfp_t flags)
 
 	trace_kmalloc(_RET_IP_, ret, size, s->size, flags);
 
-	kasan_kmalloc(s, ret, size, flags);
+	ret = kasan_kmalloc(s, ret, size, flags);
 
 	return ret;
 }
@@ -3767,8 +3767,7 @@ static void *kmalloc_large_node(size_t size, gfp_t flags, int node)
 	if (page)
 		ptr = page_address(page);
 
-	kmalloc_large_node_hook(ptr, size, flags);
-	return ptr;
+	return kmalloc_large_node_hook(ptr, size, flags);
 }
 
 void *__kmalloc_node(size_t size, gfp_t flags, int node)
@@ -3795,7 +3794,7 @@ void *__kmalloc_node(size_t size, gfp_t flags, int node)
 
 	trace_kmalloc_node(_RET_IP_, ret, size, s->size, flags, node);
 
-	kasan_kmalloc(s, ret, size, flags);
+	ret = kasan_kmalloc(s, ret, size, flags);
 
 	return ret;
 }
-- 
2.18.0.rc2.346.g013aa6912e-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v4 02/17] khwasan: move common kasan and khwasan code to common.c ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v4 02/17] khwasan: move common kasan and khwasan code to common.c
Date: Tue, 26 Jun 2018 13:15:12 +0000
Message-ID: <80100e6382d504bb14aa790c87588ef87ef33211.1530018818.git.andreyknvl () google ! com>
--------------------
KHWASAN will reuse a significant part of KASAN code, so move the common
parts to common.c without any functional changes.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/Makefile |   5 +-
 mm/kasan/common.c | 603 ++++++++++++++++++++++++++++++++++++++++++++++
 mm/kasan/kasan.c  | 567 +------------------------------------------
 mm/kasan/kasan.h  |   5 +
 4 files changed, 614 insertions(+), 566 deletions(-)
 create mode 100644 mm/kasan/common.c

diff --git a/mm/kasan/Makefile b/mm/kasan/Makefile
index 3289db38bc87..a6df14bffb6b 100644
--- a/mm/kasan/Makefile
+++ b/mm/kasan/Makefile
@@ -1,11 +1,14 @@
 # SPDX-License-Identifier: GPL-2.0
 KASAN_SANITIZE := n
+UBSAN_SANITIZE_common.o := n
 UBSAN_SANITIZE_kasan.o := n
 KCOV_INSTRUMENT := n
 
 CFLAGS_REMOVE_kasan.o = -pg
 # Function splitter causes unnecessary splits in __asan_load1/__asan_store1
 # see: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=63533
+
+CFLAGS_common.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 CFLAGS_kasan.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 
-obj-y := kasan.o report.o kasan_init.o quarantine.o
+obj-y := common.o kasan.o report.o kasan_init.o quarantine.o
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
new file mode 100644
index 000000000000..e78ebeff1f4c
--- /dev/null
+++ b/mm/kasan/common.c
@@ -0,0 +1,603 @@
+/*
+ * This file contains common KASAN and KHWASAN code.
+ *
+ * Copyright (c) 2014 Samsung Electronics Co., Ltd.
+ * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
+ *
+ * Some code borrowed from https://github.com/xairy/kasan-prototype by
+ *        Andrey Konovalov <andreyknvl@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/export.h>
+#include <linux/interrupt.h>
+#include <linux/init.h>
+#include <linux/kasan.h>
+#include <linux/kernel.h>
+#include <linux/kmemleak.h>
+#include <linux/linkage.h>
+#include <linux/memblock.h>
+#include <linux/memory.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/sched/task_stack.h>
+#include <linux/slab.h>
+#include <linux/stacktrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/vmalloc.h>
+#include <linux/bug.h>
+
+#include "kasan.h"
+#include "../slab.h"
+
+static inline int in_irqentry_text(unsigned long ptr)
+{
+	return (ptr >= (unsigned long)&__irqentry_text_start &&
+		ptr < (unsigned long)&__irqentry_text_end) ||
+		(ptr >= (unsigned long)&__softirqentry_text_start &&
+		 ptr < (unsigned long)&__softirqentry_text_end);
+}
+
+static inline void filter_irq_stacks(struct stack_trace *trace)
+{
+	int i;
+
+	if (!trace->nr_entries)
+		return;
+	for (i = 0; i < trace->nr_entries; i++)
+		if (in_irqentry_text(trace->entries[i])) {
+			/* Include the irqentry function into the stack. */
+			trace->nr_entries = i + 1;
+			break;
+		}
+}
+
+static inline depot_stack_handle_t save_stack(gfp_t flags)
+{
+	unsigned long entries[KASAN_STACK_DEPTH];
+	struct stack_trace trace = {
+		.nr_entries = 0,
+		.entries = entries,
+		.max_entries = KASAN_STACK_DEPTH,
+		.skip = 0
+	};
+
+	save_stack_trace(&trace);
+	filter_irq_stacks(&trace);
+	if (trace.nr_entries != 0 &&
+	    trace.entries[trace.nr_entries-1] == ULONG_MAX)
+		trace.nr_entries--;
+
+	return depot_save_stack(&trace, flags);
+}
+
+static inline void set_track(struct kasan_track *track, gfp_t flags)
+{
+	track->pid = current->pid;
+	track->stack = save_stack(flags);
+}
+
+void kasan_enable_current(void)
+{
+	current->kasan_depth++;
+}
+
+void kasan_disable_current(void)
+{
+	current->kasan_depth--;
+}
+
+void kasan_check_read(const volatile void *p, unsigned int size)
+{
+	check_memory_region((unsigned long)p, size, false, _RET_IP_);
+}
+EXPORT_SYMBOL(kasan_check_read);
+
+void kasan_check_write(const volatile void *p, unsigned int size)
+{
+	check_memory_region((unsigned long)p, size, true, _RET_IP_);
+}
+EXPORT_SYMBOL(kasan_check_write);
+
+#undef memset
+void *memset(void *addr, int c, size_t len)
+{
+	check_memory_region((unsigned long)addr, len, true, _RET_IP_);
+
+	return __memset(addr, c, len);
+}
+
+#undef memmove
+void *memmove(void *dest, const void *src, size_t len)
+{
+	check_memory_region((unsigned long)src, len, false, _RET_IP_);
+	check_memory_region((unsigned long)dest, len, true, _RET_IP_);
+
+	return __memmove(dest, src, len);
+}
+
+#undef memcpy
+void *memcpy(void *dest, const void *src, size_t len)
+{
+	check_memory_region((unsigned long)src, len, false, _RET_IP_);
+	check_memory_region((unsigned long)dest, len, true, _RET_IP_);
+
+	return __memcpy(dest, src, len);
+}
+
+/*
+ * Poisons the shadow memory for 'size' bytes starting from 'addr'.
+ * Memory addresses should be aligned to KASAN_SHADOW_SCALE_SIZE.
+ */
+void kasan_poison_shadow(const void *address, size_t size, u8 value)
+{
+	void *shadow_start, *shadow_end;
+
+	shadow_start = kasan_mem_to_shadow(address);
+	shadow_end = kasan_mem_to_shadow(address + size);
+
+	__memset(shadow_start, value, shadow_end - shadow_start);
+}
+
+void kasan_unpoison_shadow(const void *address, size_t size)
+{
+	kasan_poison_shadow(address, size, 0);
+
+	if (size & KASAN_SHADOW_MASK) {
+		u8 *shadow = (u8 *)kasan_mem_to_shadow(address + size);
+		*shadow = size & KASAN_SHADOW_MASK;
+	}
+}
+
+static void __kasan_unpoison_stack(struct task_struct *task, const void *sp)
+{
+	void *base = task_stack_page(task);
+	size_t size = sp - base;
+
+	kasan_unpoison_shadow(base, size);
+}
+
+/* Unpoison the entire stack for a task. */
+void kasan_unpoison_task_stack(struct task_struct *task)
+{
+	__kasan_unpoison_stack(task, task_stack_page(task) + THREAD_SIZE);
+}
+
+/* Unpoison the stack for the current task beyond a watermark sp value. */
+asmlinkage void kasan_unpoison_task_stack_below(const void *watermark)
+{
+	/*
+	 * Calculate the task stack base address.  Avoid using 'current'
+	 * because this function is called by early resume code which hasn't
+	 * yet set up the percpu register (%gs).
+	 */
+	void *base = (void *)((unsigned long)watermark & ~(THREAD_SIZE - 1));
+
+	kasan_unpoison_shadow(base, watermark - base);
+}
+
+/*
+ * Clear all poison for the region between the current SP and a provided
+ * watermark value, as is sometimes required prior to hand-crafted asm function
+ * returns in the middle of functions.
+ */
+void kasan_unpoison_stack_above_sp_to(const void *watermark)
+{
+	const void *sp = __builtin_frame_address(0);
+	size_t size = watermark - sp;
+
+	if (WARN_ON(sp > watermark))
+		return;
+	kasan_unpoison_shadow(sp, size);
+}
+
+void kasan_alloc_pages(struct page *page, unsigned int order)
+{
+	if (likely(!PageHighMem(page)))
+		kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
+}
+
+void kasan_free_pages(struct page *page, unsigned int order)
+{
+	if (likely(!PageHighMem(page)))
+		kasan_poison_shadow(page_address(page),
+				PAGE_SIZE << order,
+				KASAN_FREE_PAGE);
+}
+
+/*
+ * Adaptive redzone policy taken from the userspace AddressSanitizer runtime.
+ * For larger allocations larger redzones are used.
+ */
+static inline unsigned int optimal_redzone(unsigned int object_size)
+{
+	if (IS_ENABLED(CONFIG_KASAN_HW))
+		return 0;
+
+	return
+		object_size <= 64        - 16   ? 16 :
+		object_size <= 128       - 32   ? 32 :
+		object_size <= 512       - 64   ? 64 :
+		object_size <= 4096      - 128  ? 128 :
+		object_size <= (1 << 14) - 256  ? 256 :
+		object_size <= (1 << 15) - 512  ? 512 :
+		object_size <= (1 << 16) - 1024 ? 1024 : 2048;
+}
+
+void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
+			slab_flags_t *flags)
+{
+	unsigned int orig_size = *size;
+	int redzone_adjust;
+
+	/* Add alloc meta. */
+	cache->kasan_info.alloc_meta_offset = *size;
+	*size += sizeof(struct kasan_alloc_meta);
+
+	/* Add free meta. */
+	if (cache->flags & SLAB_TYPESAFE_BY_RCU || cache->ctor ||
+	    cache->object_size < sizeof(struct kasan_free_meta)) {
+		cache->kasan_info.free_meta_offset = *size;
+		*size += sizeof(struct kasan_free_meta);
+	}
+	redzone_adjust = optimal_redzone(cache->object_size) -
+		(*size - cache->object_size);
+
+	if (redzone_adjust > 0)
+		*size += redzone_adjust;
+
+	*size = min_t(unsigned int, KMALLOC_MAX_SIZE,
+			max(*size, cache->object_size +
+					optimal_redzone(cache->object_size)));
+
+	/*
+	 * If the metadata doesn't fit, don't enable KASAN at all.
+	 */
+	if (*size <= cache->kasan_info.alloc_meta_offset ||
+			*size <= cache->kasan_info.free_meta_offset) {
+		cache->kasan_info.alloc_meta_offset = 0;
+		cache->kasan_info.free_meta_offset = 0;
+		*size = orig_size;
+		return;
+	}
+
+	*flags |= SLAB_KASAN;
+}
+
+size_t kasan_metadata_size(struct kmem_cache *cache)
+{
+	return (cache->kasan_info.alloc_meta_offset ?
+		sizeof(struct kasan_alloc_meta) : 0) +
+		(cache->kasan_info.free_meta_offset ?
+		sizeof(struct kasan_free_meta) : 0);
+}
+
+struct kasan_alloc_meta *get_alloc_info(struct kmem_cache *cache,
+					const void *object)
+{
+	BUILD_BUG_ON(sizeof(struct kasan_alloc_meta) > 32);
+	return (void *)object + cache->kasan_info.alloc_meta_offset;
+}
+
+struct kasan_free_meta *get_free_info(struct kmem_cache *cache,
+				      const void *object)
+{
+	BUILD_BUG_ON(sizeof(struct kasan_free_meta) > 32);
+	return (void *)object + cache->kasan_info.free_meta_offset;
+}
+
+void kasan_poison_slab(struct page *page)
+{
+	kasan_poison_shadow(page_address(page),
+			PAGE_SIZE << compound_order(page),
+			KASAN_KMALLOC_REDZONE);
+}
+
+void kasan_unpoison_object_data(struct kmem_cache *cache, void *object)
+{
+	kasan_unpoison_shadow(object, cache->object_size);
+}
+
+void kasan_poison_object_data(struct kmem_cache *cache, void *object)
+{
+	kasan_poison_shadow(object,
+			round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE),
+			KASAN_KMALLOC_REDZONE);
+}
+
+void kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
+{
+	struct kasan_alloc_meta *alloc_info;
+
+	if (!(cache->flags & SLAB_KASAN))
+		return;
+
+	alloc_info = get_alloc_info(cache, object);
+	__memset(alloc_info, 0, sizeof(*alloc_info));
+}
+
+void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t flags)
+{
+	return kasan_kmalloc(cache, object, cache->object_size, flags);
+}
+
+static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
+			      unsigned long ip, bool quarantine)
+{
+	s8 shadow_byte;
+	unsigned long rounded_up_size;
+
+	if (unlikely(nearest_obj(cache, virt_to_head_page(object), object) !=
+	    object)) {
+		kasan_report_invalid_free(object, ip);
+		return true;
+	}
+
+	/* RCU slabs could be legally used after free within the RCU period */
+	if (unlikely(cache->flags & SLAB_TYPESAFE_BY_RCU))
+		return false;
+
+	shadow_byte = READ_ONCE(*(s8 *)kasan_mem_to_shadow(object));
+	if (shadow_byte < 0 || shadow_byte >= KASAN_SHADOW_SCALE_SIZE) {
+		kasan_report_invalid_free(object, ip);
+		return true;
+	}
+
+	rounded_up_size = round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE);
+	kasan_poison_shadow(object, rounded_up_size, KASAN_KMALLOC_FREE);
+
+	if (!quarantine || unlikely(!(cache->flags & SLAB_KASAN)))
+		return false;
+
+	set_track(&get_alloc_info(cache, object)->free_track, GFP_NOWAIT);
+	quarantine_put(get_free_info(cache, object), cache);
+	return true;
+}
+
+bool kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
+{
+	return __kasan_slab_free(cache, object, ip, true);
+}
+
+void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
+		   gfp_t flags)
+{
+	unsigned long redzone_start;
+	unsigned long redzone_end;
+
+	if (gfpflags_allow_blocking(flags))
+		quarantine_reduce();
+
+	if (unlikely(object == NULL))
+		return NULL;
+
+	redzone_start = round_up((unsigned long)(object + size),
+				KASAN_SHADOW_SCALE_SIZE);
+	redzone_end = round_up((unsigned long)object + cache->object_size,
+				KASAN_SHADOW_SCALE_SIZE);
+
+	kasan_unpoison_shadow(object, size);
+	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
+		KASAN_KMALLOC_REDZONE);
+
+	if (cache->flags & SLAB_KASAN)
+		set_track(&get_alloc_info(cache, object)->alloc_track, flags);
+
+	return (void *)object;
+}
+EXPORT_SYMBOL(kasan_kmalloc);
+
+void *kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
+{
+	struct page *page;
+	unsigned long redzone_start;
+	unsigned long redzone_end;
+
+	if (gfpflags_allow_blocking(flags))
+		quarantine_reduce();
+
+	if (unlikely(ptr == NULL))
+		return NULL;
+
+	page = virt_to_page(ptr);
+	redzone_start = round_up((unsigned long)(ptr + size),
+				KASAN_SHADOW_SCALE_SIZE);
+	redzone_end = (unsigned long)ptr + (PAGE_SIZE << compound_order(page));
+
+	kasan_unpoison_shadow(ptr, size);
+	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
+		KASAN_PAGE_REDZONE);
+
+	return (void *)ptr;
+}
+
+void *kasan_krealloc(const void *object, size_t size, gfp_t flags)
+{
+	struct page *page;
+
+	if (unlikely(object == ZERO_SIZE_PTR))
+		return (void *)object;
+
+	page = virt_to_head_page(object);
+
+	if (unlikely(!PageSlab(page)))
+		return kasan_kmalloc_large(object, size, flags);
+	else
+		return kasan_kmalloc(page->slab_cache, object, size, flags);
+}
+
+void kasan_poison_kfree(void *ptr, unsigned long ip)
+{
+	struct page *page;
+
+	page = virt_to_head_page(ptr);
+
+	if (unlikely(!PageSlab(page))) {
+		if (ptr != page_address(page)) {
+			kasan_report_invalid_free(ptr, ip);
+			return;
+		}
+		kasan_poison_shadow(ptr, PAGE_SIZE << compound_order(page),
+				KASAN_FREE_PAGE);
+	} else {
+		__kasan_slab_free(page->slab_cache, ptr, ip, false);
+	}
+}
+
+void kasan_kfree_large(void *ptr, unsigned long ip)
+{
+	if (ptr != page_address(virt_to_head_page(ptr)))
+		kasan_report_invalid_free(ptr, ip);
+	/* The object will be poisoned by page_alloc. */
+}
+
+int kasan_module_alloc(void *addr, size_t size)
+{
+	void *ret;
+	size_t shadow_size;
+	unsigned long shadow_start;
+
+	shadow_start = (unsigned long)kasan_mem_to_shadow(addr);
+	shadow_size = round_up(size >> KASAN_SHADOW_SCALE_SHIFT,
+			PAGE_SIZE);
+
+	if (WARN_ON(!PAGE_ALIGNED(shadow_start)))
+		return -EINVAL;
+
+	ret = __vmalloc_node_range(shadow_size, 1, shadow_start,
+			shadow_start + shadow_size,
+			GFP_KERNEL | __GFP_ZERO,
+			PAGE_KERNEL, VM_NO_GUARD, NUMA_NO_NODE,
+			__builtin_return_address(0));
+
+	if (ret) {
+		find_vm_area(addr)->flags |= VM_KASAN;
+		kmemleak_ignore(ret);
+		return 0;
+	}
+
+	return -ENOMEM;
+}
+
+void kasan_free_shadow(const struct vm_struct *vm)
+{
+	if (vm->flags & VM_KASAN)
+		vfree(kasan_mem_to_shadow(vm->addr));
+}
+
+#ifdef CONFIG_MEMORY_HOTPLUG
+static bool shadow_mapped(unsigned long addr)
+{
+	pgd_t *pgd = pgd_offset_k(addr);
+	p4d_t *p4d;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte;
+
+	if (pgd_none(*pgd))
+		return false;
+	p4d = p4d_offset(pgd, addr);
+	if (p4d_none(*p4d))
+		return false;
+	pud = pud_offset(p4d, addr);
+	if (pud_none(*pud))
+		return false;
+
+	/*
+	 * We can't use pud_large() or pud_huge(), the first one is
+	 * arch-specific, the last one depends on HUGETLB_PAGE.  So let's abuse
+	 * pud_bad(), if pud is bad then it's bad because it's huge.
+	 */
+	if (pud_bad(*pud))
+		return true;
+	pmd = pmd_offset(pud, addr);
+	if (pmd_none(*pmd))
+		return false;
+
+	if (pmd_bad(*pmd))
+		return true;
+	pte = pte_offset_kernel(pmd, addr);
+	return !pte_none(*pte);
+}
+
+static int __meminit kasan_mem_notifier(struct notifier_block *nb,
+			unsigned long action, void *data)
+{
+	struct memory_notify *mem_data = data;
+	unsigned long nr_shadow_pages, start_kaddr, shadow_start;
+	unsigned long shadow_end, shadow_size;
+
+	nr_shadow_pages = mem_data->nr_pages >> KASAN_SHADOW_SCALE_SHIFT;
+	start_kaddr = (unsigned long)pfn_to_kaddr(mem_data->start_pfn);
+	shadow_start = (unsigned long)kasan_mem_to_shadow((void *)start_kaddr);
+	shadow_size = nr_shadow_pages << PAGE_SHIFT;
+	shadow_end = shadow_start + shadow_size;
+
+	if (WARN_ON(mem_data->nr_pages % KASAN_SHADOW_SCALE_SIZE) ||
+		WARN_ON(start_kaddr % (KASAN_SHADOW_SCALE_SIZE << PAGE_SHIFT)))
+		return NOTIFY_BAD;
+
+	switch (action) {
+	case MEM_GOING_ONLINE: {
+		void *ret;
+
+		/*
+		 * If shadow is mapped already than it must have been mapped
+		 * during the boot. This could happen if we onlining previously
+		 * offlined memory.
+		 */
+		if (shadow_mapped(shadow_start))
+			return NOTIFY_OK;
+
+		ret = __vmalloc_node_range(shadow_size, PAGE_SIZE, shadow_start,
+					shadow_end, GFP_KERNEL,
+					PAGE_KERNEL, VM_NO_GUARD,
+					pfn_to_nid(mem_data->start_pfn),
+					__builtin_return_address(0));
+		if (!ret)
+			return NOTIFY_BAD;
+
+		kmemleak_ignore(ret);
+		return NOTIFY_OK;
+	}
+	case MEM_CANCEL_ONLINE:
+	case MEM_OFFLINE: {
+		struct vm_struct *vm;
+
+		/*
+		 * shadow_start was either mapped during boot by kasan_init()
+		 * or during memory online by __vmalloc_node_range().
+		 * In the latter case we can use vfree() to free shadow.
+		 * Non-NULL result of the find_vm_area() will tell us if
+		 * that was the second case.
+		 *
+		 * Currently it's not possible to free shadow mapped
+		 * during boot by kasan_init(). It's because the code
+		 * to do that hasn't been written yet. So we'll just
+		 * leak the memory.
+		 */
+		vm = find_vm_area((void *)shadow_start);
+		if (vm)
+			vfree((void *)shadow_start);
+	}
+	}
+
+	return NOTIFY_OK;
+}
+
+static int __init kasan_memhotplug_init(void)
+{
+	hotplug_memory_notifier(kasan_mem_notifier, 0);
+
+	return 0;
+}
+
+core_initcall(kasan_memhotplug_init);
+#endif
diff --git a/mm/kasan/kasan.c b/mm/kasan/kasan.c
index f04aa1e0ba48..44ec228de0a2 100644
--- a/mm/kasan/kasan.c
+++ b/mm/kasan/kasan.c
@@ -1,5 +1,5 @@
 /*
- * This file contains shadow memory manipulation code.
+ * This file contains core KASAN code.
  *
  * Copyright (c) 2014 Samsung Electronics Co., Ltd.
  * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
@@ -40,82 +40,6 @@
 #include "kasan.h"
 #include "../slab.h"
 
-void kasan_enable_current(void)
-{
-	current->kasan_depth++;
-}
-
-void kasan_disable_current(void)
-{
-	current->kasan_depth--;
-}
-
-/*
- * Poisons the shadow memory for 'size' bytes starting from 'addr'.
- * Memory addresses should be aligned to KASAN_SHADOW_SCALE_SIZE.
- */
-static void kasan_poison_shadow(const void *address, size_t size, u8 value)
-{
-	void *shadow_start, *shadow_end;
-
-	shadow_start = kasan_mem_to_shadow(address);
-	shadow_end = kasan_mem_to_shadow(address + size);
-
-	memset(shadow_start, value, shadow_end - shadow_start);
-}
-
-void kasan_unpoison_shadow(const void *address, size_t size)
-{
-	kasan_poison_shadow(address, size, 0);
-
-	if (size & KASAN_SHADOW_MASK) {
-		u8 *shadow = (u8 *)kasan_mem_to_shadow(address + size);
-		*shadow = size & KASAN_SHADOW_MASK;
-	}
-}
-
-static void __kasan_unpoison_stack(struct task_struct *task, const void *sp)
-{
-	void *base = task_stack_page(task);
-	size_t size = sp - base;
-
-	kasan_unpoison_shadow(base, size);
-}
-
-/* Unpoison the entire stack for a task. */
-void kasan_unpoison_task_stack(struct task_struct *task)
-{
-	__kasan_unpoison_stack(task, task_stack_page(task) + THREAD_SIZE);
-}
-
-/* Unpoison the stack for the current task beyond a watermark sp value. */
-asmlinkage void kasan_unpoison_task_stack_below(const void *watermark)
-{
-	/*
-	 * Calculate the task stack base address.  Avoid using 'current'
-	 * because this function is called by early resume code which hasn't
-	 * yet set up the percpu register (%gs).
-	 */
-	void *base = (void *)((unsigned long)watermark & ~(THREAD_SIZE - 1));
-
-	kasan_unpoison_shadow(base, watermark - base);
-}
-
-/*
- * Clear all poison for the region between the current SP and a provided
- * watermark value, as is sometimes required prior to hand-crafted asm function
- * returns in the middle of functions.
- */
-void kasan_unpoison_stack_above_sp_to(const void *watermark)
-{
-	const void *sp = __builtin_frame_address(0);
-	size_t size = watermark - sp;
-
-	if (WARN_ON(sp > watermark))
-		return;
-	kasan_unpoison_shadow(sp, size);
-}
-
 /*
  * All functions below always inlined so compiler could
  * perform better optimizations in each of __asan_loadX/__assn_storeX
@@ -260,121 +184,12 @@ static __always_inline void check_memory_region_inline(unsigned long addr,
 	kasan_report(addr, size, write, ret_ip);
 }
 
-static void check_memory_region(unsigned long addr,
-				size_t size, bool write,
+void check_memory_region(unsigned long addr, size_t size, bool write,
 				unsigned long ret_ip)
 {
 	check_memory_region_inline(addr, size, write, ret_ip);
 }
 
-void kasan_check_read(const volatile void *p, unsigned int size)
-{
-	check_memory_region((unsigned long)p, size, false, _RET_IP_);
-}
-EXPORT_SYMBOL(kasan_check_read);
-
-void kasan_check_write(const volatile void *p, unsigned int size)
-{
-	check_memory_region((unsigned long)p, size, true, _RET_IP_);
-}
-EXPORT_SYMBOL(kasan_check_write);
-
-#undef memset
-void *memset(void *addr, int c, size_t len)
-{
-	check_memory_region((unsigned long)addr, len, true, _RET_IP_);
-
-	return __memset(addr, c, len);
-}
-
-#undef memmove
-void *memmove(void *dest, const void *src, size_t len)
-{
-	check_memory_region((unsigned long)src, len, false, _RET_IP_);
-	check_memory_region((unsigned long)dest, len, true, _RET_IP_);
-
-	return __memmove(dest, src, len);
-}
-
-#undef memcpy
-void *memcpy(void *dest, const void *src, size_t len)
-{
-	check_memory_region((unsigned long)src, len, false, _RET_IP_);
-	check_memory_region((unsigned long)dest, len, true, _RET_IP_);
-
-	return __memcpy(dest, src, len);
-}
-
-void kasan_alloc_pages(struct page *page, unsigned int order)
-{
-	if (likely(!PageHighMem(page)))
-		kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
-}
-
-void kasan_free_pages(struct page *page, unsigned int order)
-{
-	if (likely(!PageHighMem(page)))
-		kasan_poison_shadow(page_address(page),
-				PAGE_SIZE << order,
-				KASAN_FREE_PAGE);
-}
-
-/*
- * Adaptive redzone policy taken from the userspace AddressSanitizer runtime.
- * For larger allocations larger redzones are used.
- */
-static unsigned int optimal_redzone(unsigned int object_size)
-{
-	return
-		object_size <= 64        - 16   ? 16 :
-		object_size <= 128       - 32   ? 32 :
-		object_size <= 512       - 64   ? 64 :
-		object_size <= 4096      - 128  ? 128 :
-		object_size <= (1 << 14) - 256  ? 256 :
-		object_size <= (1 << 15) - 512  ? 512 :
-		object_size <= (1 << 16) - 1024 ? 1024 : 2048;
-}
-
-void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
-			slab_flags_t *flags)
-{
-	unsigned int orig_size = *size;
-	int redzone_adjust;
-
-	/* Add alloc meta. */
-	cache->kasan_info.alloc_meta_offset = *size;
-	*size += sizeof(struct kasan_alloc_meta);
-
-	/* Add free meta. */
-	if (cache->flags & SLAB_TYPESAFE_BY_RCU || cache->ctor ||
-	    cache->object_size < sizeof(struct kasan_free_meta)) {
-		cache->kasan_info.free_meta_offset = *size;
-		*size += sizeof(struct kasan_free_meta);
-	}
-	redzone_adjust = optimal_redzone(cache->object_size) -
-		(*size - cache->object_size);
-
-	if (redzone_adjust > 0)
-		*size += redzone_adjust;
-
-	*size = min_t(unsigned int, KMALLOC_MAX_SIZE,
-			max(*size, cache->object_size +
-					optimal_redzone(cache->object_size)));
-
-	/*
-	 * If the metadata doesn't fit, don't enable KASAN at all.
-	 */
-	if (*size <= cache->kasan_info.alloc_meta_offset ||
-			*size <= cache->kasan_info.free_meta_offset) {
-		cache->kasan_info.alloc_meta_offset = 0;
-		cache->kasan_info.free_meta_offset = 0;
-		*size = orig_size;
-		return;
-	}
-
-	*flags |= SLAB_KASAN;
-}
-
 void kasan_cache_shrink(struct kmem_cache *cache)
 {
 	quarantine_remove_cache(cache);
@@ -386,274 +201,6 @@ void kasan_cache_shutdown(struct kmem_cache *cache)
 		quarantine_remove_cache(cache);
 }
 
-size_t kasan_metadata_size(struct kmem_cache *cache)
-{
-	return (cache->kasan_info.alloc_meta_offset ?
-		sizeof(struct kasan_alloc_meta) : 0) +
-		(cache->kasan_info.free_meta_offset ?
-		sizeof(struct kasan_free_meta) : 0);
-}
-
-void kasan_poison_slab(struct page *page)
-{
-	kasan_poison_shadow(page_address(page),
-			PAGE_SIZE << compound_order(page),
-			KASAN_KMALLOC_REDZONE);
-}
-
-void kasan_unpoison_object_data(struct kmem_cache *cache, void *object)
-{
-	kasan_unpoison_shadow(object, cache->object_size);
-}
-
-void kasan_poison_object_data(struct kmem_cache *cache, void *object)
-{
-	kasan_poison_shadow(object,
-			round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE),
-			KASAN_KMALLOC_REDZONE);
-}
-
-static inline int in_irqentry_text(unsigned long ptr)
-{
-	return (ptr >= (unsigned long)&__irqentry_text_start &&
-		ptr < (unsigned long)&__irqentry_text_end) ||
-		(ptr >= (unsigned long)&__softirqentry_text_start &&
-		 ptr < (unsigned long)&__softirqentry_text_end);
-}
-
-static inline void filter_irq_stacks(struct stack_trace *trace)
-{
-	int i;
-
-	if (!trace->nr_entries)
-		return;
-	for (i = 0; i < trace->nr_entries; i++)
-		if (in_irqentry_text(trace->entries[i])) {
-			/* Include the irqentry function into the stack. */
-			trace->nr_entries = i + 1;
-			break;
-		}
-}
-
-static inline depot_stack_handle_t save_stack(gfp_t flags)
-{
-	unsigned long entries[KASAN_STACK_DEPTH];
-	struct stack_trace trace = {
-		.nr_entries = 0,
-		.entries = entries,
-		.max_entries = KASAN_STACK_DEPTH,
-		.skip = 0
-	};
-
-	save_stack_trace(&trace);
-	filter_irq_stacks(&trace);
-	if (trace.nr_entries != 0 &&
-	    trace.entries[trace.nr_entries-1] == ULONG_MAX)
-		trace.nr_entries--;
-
-	return depot_save_stack(&trace, flags);
-}
-
-static inline void set_track(struct kasan_track *track, gfp_t flags)
-{
-	track->pid = current->pid;
-	track->stack = save_stack(flags);
-}
-
-struct kasan_alloc_meta *get_alloc_info(struct kmem_cache *cache,
-					const void *object)
-{
-	BUILD_BUG_ON(sizeof(struct kasan_alloc_meta) > 32);
-	return (void *)object + cache->kasan_info.alloc_meta_offset;
-}
-
-struct kasan_free_meta *get_free_info(struct kmem_cache *cache,
-				      const void *object)
-{
-	BUILD_BUG_ON(sizeof(struct kasan_free_meta) > 32);
-	return (void *)object + cache->kasan_info.free_meta_offset;
-}
-
-void kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
-{
-	struct kasan_alloc_meta *alloc_info;
-
-	if (!(cache->flags & SLAB_KASAN))
-		return;
-
-	alloc_info = get_alloc_info(cache, object);
-	__memset(alloc_info, 0, sizeof(*alloc_info));
-}
-
-void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t flags)
-{
-	return kasan_kmalloc(cache, object, cache->object_size, flags);
-}
-
-static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
-			      unsigned long ip, bool quarantine)
-{
-	s8 shadow_byte;
-	unsigned long rounded_up_size;
-
-	if (unlikely(nearest_obj(cache, virt_to_head_page(object), object) !=
-	    object)) {
-		kasan_report_invalid_free(object, ip);
-		return true;
-	}
-
-	/* RCU slabs could be legally used after free within the RCU period */
-	if (unlikely(cache->flags & SLAB_TYPESAFE_BY_RCU))
-		return false;
-
-	shadow_byte = READ_ONCE(*(s8 *)kasan_mem_to_shadow(object));
-	if (shadow_byte < 0 || shadow_byte >= KASAN_SHADOW_SCALE_SIZE) {
-		kasan_report_invalid_free(object, ip);
-		return true;
-	}
-
-	rounded_up_size = round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE);
-	kasan_poison_shadow(object, rounded_up_size, KASAN_KMALLOC_FREE);
-
-	if (!quarantine || unlikely(!(cache->flags & SLAB_KASAN)))
-		return false;
-
-	set_track(&get_alloc_info(cache, object)->free_track, GFP_NOWAIT);
-	quarantine_put(get_free_info(cache, object), cache);
-	return true;
-}
-
-bool kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
-{
-	return __kasan_slab_free(cache, object, ip, true);
-}
-
-void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
-		   gfp_t flags)
-{
-	unsigned long redzone_start;
-	unsigned long redzone_end;
-
-	if (gfpflags_allow_blocking(flags))
-		quarantine_reduce();
-
-	if (unlikely(object == NULL))
-		return NULL;
-
-	redzone_start = round_up((unsigned long)(object + size),
-				KASAN_SHADOW_SCALE_SIZE);
-	redzone_end = round_up((unsigned long)object + cache->object_size,
-				KASAN_SHADOW_SCALE_SIZE);
-
-	kasan_unpoison_shadow(object, size);
-	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
-		KASAN_KMALLOC_REDZONE);
-
-	if (cache->flags & SLAB_KASAN)
-		set_track(&get_alloc_info(cache, object)->alloc_track, flags);
-
-	return (void *)object;
-}
-EXPORT_SYMBOL(kasan_kmalloc);
-
-void *kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
-{
-	struct page *page;
-	unsigned long redzone_start;
-	unsigned long redzone_end;
-
-	if (gfpflags_allow_blocking(flags))
-		quarantine_reduce();
-
-	if (unlikely(ptr == NULL))
-		return NULL;
-
-	page = virt_to_page(ptr);
-	redzone_start = round_up((unsigned long)(ptr + size),
-				KASAN_SHADOW_SCALE_SIZE);
-	redzone_end = (unsigned long)ptr + (PAGE_SIZE << compound_order(page));
-
-	kasan_unpoison_shadow(ptr, size);
-	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
-		KASAN_PAGE_REDZONE);
-
-	return (void *)ptr;
-}
-
-void *kasan_krealloc(const void *object, size_t size, gfp_t flags)
-{
-	struct page *page;
-
-	if (unlikely(object == ZERO_SIZE_PTR))
-		return ZERO_SIZE_PTR;
-
-	page = virt_to_head_page(object);
-
-	if (unlikely(!PageSlab(page)))
-		return kasan_kmalloc_large(object, size, flags);
-	else
-		return kasan_kmalloc(page->slab_cache, object, size, flags);
-}
-
-void kasan_poison_kfree(void *ptr, unsigned long ip)
-{
-	struct page *page;
-
-	page = virt_to_head_page(ptr);
-
-	if (unlikely(!PageSlab(page))) {
-		if (ptr != page_address(page)) {
-			kasan_report_invalid_free(ptr, ip);
-			return;
-		}
-		kasan_poison_shadow(ptr, PAGE_SIZE << compound_order(page),
-				KASAN_FREE_PAGE);
-	} else {
-		__kasan_slab_free(page->slab_cache, ptr, ip, false);
-	}
-}
-
-void kasan_kfree_large(void *ptr, unsigned long ip)
-{
-	if (ptr != page_address(virt_to_head_page(ptr)))
-		kasan_report_invalid_free(ptr, ip);
-	/* The object will be poisoned by page_alloc. */
-}
-
-int kasan_module_alloc(void *addr, size_t size)
-{
-	void *ret;
-	size_t shadow_size;
-	unsigned long shadow_start;
-
-	shadow_start = (unsigned long)kasan_mem_to_shadow(addr);
-	shadow_size = round_up(size >> KASAN_SHADOW_SCALE_SHIFT,
-			PAGE_SIZE);
-
-	if (WARN_ON(!PAGE_ALIGNED(shadow_start)))
-		return -EINVAL;
-
-	ret = __vmalloc_node_range(shadow_size, 1, shadow_start,
-			shadow_start + shadow_size,
-			GFP_KERNEL | __GFP_ZERO,
-			PAGE_KERNEL, VM_NO_GUARD, NUMA_NO_NODE,
-			__builtin_return_address(0));
-
-	if (ret) {
-		find_vm_area(addr)->flags |= VM_KASAN;
-		kmemleak_ignore(ret);
-		return 0;
-	}
-
-	return -ENOMEM;
-}
-
-void kasan_free_shadow(const struct vm_struct *vm)
-{
-	if (vm->flags & VM_KASAN)
-		vfree(kasan_mem_to_shadow(vm->addr));
-}
-
 static void register_global(struct kasan_global *global)
 {
 	size_t aligned_size = round_up(global->size, KASAN_SHADOW_SCALE_SIZE);
@@ -794,113 +341,3 @@ DEFINE_ASAN_SET_SHADOW(f2);
 DEFINE_ASAN_SET_SHADOW(f3);
 DEFINE_ASAN_SET_SHADOW(f5);
 DEFINE_ASAN_SET_SHADOW(f8);
-
-#ifdef CONFIG_MEMORY_HOTPLUG
-static bool shadow_mapped(unsigned long addr)
-{
-	pgd_t *pgd = pgd_offset_k(addr);
-	p4d_t *p4d;
-	pud_t *pud;
-	pmd_t *pmd;
-	pte_t *pte;
-
-	if (pgd_none(*pgd))
-		return false;
-	p4d = p4d_offset(pgd, addr);
-	if (p4d_none(*p4d))
-		return false;
-	pud = pud_offset(p4d, addr);
-	if (pud_none(*pud))
-		return false;
-
-	/*
-	 * We can't use pud_large() or pud_huge(), the first one is
-	 * arch-specific, the last one depends on HUGETLB_PAGE.  So let's abuse
-	 * pud_bad(), if pud is bad then it's bad because it's huge.
-	 */
-	if (pud_bad(*pud))
-		return true;
-	pmd = pmd_offset(pud, addr);
-	if (pmd_none(*pmd))
-		return false;
-
-	if (pmd_bad(*pmd))
-		return true;
-	pte = pte_offset_kernel(pmd, addr);
-	return !pte_none(*pte);
-}
-
-static int __meminit kasan_mem_notifier(struct notifier_block *nb,
-			unsigned long action, void *data)
-{
-	struct memory_notify *mem_data = data;
-	unsigned long nr_shadow_pages, start_kaddr, shadow_start;
-	unsigned long shadow_end, shadow_size;
-
-	nr_shadow_pages = mem_data->nr_pages >> KASAN_SHADOW_SCALE_SHIFT;
-	start_kaddr = (unsigned long)pfn_to_kaddr(mem_data->start_pfn);
-	shadow_start = (unsigned long)kasan_mem_to_shadow((void *)start_kaddr);
-	shadow_size = nr_shadow_pages << PAGE_SHIFT;
-	shadow_end = shadow_start + shadow_size;
-
-	if (WARN_ON(mem_data->nr_pages % KASAN_SHADOW_SCALE_SIZE) ||
-		WARN_ON(start_kaddr % (KASAN_SHADOW_SCALE_SIZE << PAGE_SHIFT)))
-		return NOTIFY_BAD;
-
-	switch (action) {
-	case MEM_GOING_ONLINE: {
-		void *ret;
-
-		/*
-		 * If shadow is mapped already than it must have been mapped
-		 * during the boot. This could happen if we onlining previously
-		 * offlined memory.
-		 */
-		if (shadow_mapped(shadow_start))
-			return NOTIFY_OK;
-
-		ret = __vmalloc_node_range(shadow_size, PAGE_SIZE, shadow_start,
-					shadow_end, GFP_KERNEL,
-					PAGE_KERNEL, VM_NO_GUARD,
-					pfn_to_nid(mem_data->start_pfn),
-					__builtin_return_address(0));
-		if (!ret)
-			return NOTIFY_BAD;
-
-		kmemleak_ignore(ret);
-		return NOTIFY_OK;
-	}
-	case MEM_CANCEL_ONLINE:
-	case MEM_OFFLINE: {
-		struct vm_struct *vm;
-
-		/*
-		 * shadow_start was either mapped during boot by kasan_init()
-		 * or during memory online by __vmalloc_node_range().
-		 * In the latter case we can use vfree() to free shadow.
-		 * Non-NULL result of the find_vm_area() will tell us if
-		 * that was the second case.
-		 *
-		 * Currently it's not possible to free shadow mapped
-		 * during boot by kasan_init(). It's because the code
-		 * to do that hasn't been written yet. So we'll just
-		 * leak the memory.
-		 */
-		vm = find_vm_area((void *)shadow_start);
-		if (vm)
-			vfree((void *)shadow_start);
-	}
-	}
-
-	return NOTIFY_OK;
-}
-
-static int __init kasan_memhotplug_init(void)
-{
-	hotplug_memory_notifier(kasan_mem_notifier, 0);
-
-	return 0;
-}
-
-core_initcall(kasan_memhotplug_init);
-#endif
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index c12dcfde2ebd..659463800f10 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -105,6 +105,11 @@ static inline const void *kasan_shadow_to_mem(const void *shadow_addr)
 		<< KASAN_SHADOW_SCALE_SHIFT);
 }
 
+void kasan_poison_shadow(const void *address, size_t size, u8 value);
+
+void check_memory_region(unsigned long addr, size_t size, bool write,
+				unsigned long ret_ip);
+
 void kasan_report(unsigned long addr, size_t size,
 		bool is_write, unsigned long ip);
 void kasan_report_invalid_free(void *object, unsigned long ip);
-- 
2.18.0.rc2.346.g013aa6912e-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v4 03/17] khwasan: add CONFIG_KASAN_GENERIC and CONFIG_KASAN_HW ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v4 03/17] khwasan: add CONFIG_KASAN_GENERIC and CONFIG_KASAN_HW
Date: Tue, 26 Jun 2018 13:15:13 +0000
Message-ID: <026bef9435b6a5db7ada37b03b2109ad3f3fcdf2.1530018818.git.andreyknvl () google ! com>
--------------------
This commit splits the current CONFIG_KASAN config option into two:
1. CONFIG_KASAN_GENERIC, that enables the generic software-only KASAN
   version (the one that exists now);
2. CONFIG_KASAN_HW, that enables KHWASAN.

With CONFIG_KASAN_HW enabled, compiler options are changed to instrument
kernel files wiht -fsantize=hwaddress (except the ones for which
KASAN_SANITIZE := n is set).

Both CONFIG_KASAN_GENERIC and CONFIG_KASAN_HW support both
CONFIG_KASAN_INLINE and CONFIG_KASAN_OUTLINE instrumentation modes.

This commit also adds empty placeholder (for now) implementation of
KHWASAN specific hooks inserted by the compiler and adjusts common hooks
implementation to compile correctly with each of the config options.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/Kconfig             |  1 +
 include/linux/compiler-clang.h |  5 ++-
 include/linux/compiler-gcc.h   |  4 ++
 include/linux/compiler.h       |  3 +-
 include/linux/kasan.h          | 16 +++++--
 lib/Kconfig.kasan              | 76 ++++++++++++++++++++++++++--------
 mm/kasan/Makefile              |  6 ++-
 mm/kasan/kasan.h               |  3 +-
 mm/kasan/khwasan.c             | 75 +++++++++++++++++++++++++++++++++
 mm/slub.c                      |  2 +-
 scripts/Makefile.kasan         | 27 +++++++++++-
 11 files changed, 189 insertions(+), 29 deletions(-)
 create mode 100644 mm/kasan/khwasan.c

diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index 42c090cf0292..43d9de526d3a 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -92,6 +92,7 @@ config ARM64
 	select HAVE_ARCH_HUGE_VMAP
 	select HAVE_ARCH_JUMP_LABEL
 	select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
+	select HAVE_ARCH_KASAN_HW if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
 	select HAVE_ARCH_KGDB
 	select HAVE_ARCH_MMAP_RND_BITS
 	select HAVE_ARCH_MMAP_RND_COMPAT_BITS if COMPAT
diff --git a/include/linux/compiler-clang.h b/include/linux/compiler-clang.h
index 7087446c24c8..6ac1761ff102 100644
--- a/include/linux/compiler-clang.h
+++ b/include/linux/compiler-clang.h
@@ -21,13 +21,16 @@
 #define KASAN_ABI_VERSION 5
 
 /* emulate gcc's __SANITIZE_ADDRESS__ flag */
-#if __has_feature(address_sanitizer)
+#if __has_feature(address_sanitizer) || __has_feature(hwaddress_sanitizer)
 #define __SANITIZE_ADDRESS__
 #endif
 
 #undef __no_sanitize_address
 #define __no_sanitize_address __attribute__((no_sanitize("address")))
 
+#undef __no_sanitize_hwaddress
+#define __no_sanitize_hwaddress __attribute__((no_sanitize("hwaddress")))
+
 /* Clang doesn't have a way to turn it off per-function, yet. */
 #ifdef __noretpoline
 #undef __noretpoline
diff --git a/include/linux/compiler-gcc.h b/include/linux/compiler-gcc.h
index f1a7492a5cc8..24ca815de84b 100644
--- a/include/linux/compiler-gcc.h
+++ b/include/linux/compiler-gcc.h
@@ -338,6 +338,10 @@
 #define __no_sanitize_address
 #endif
 
+#if !defined(__no_sanitize_hwaddress)
+#define __no_sanitize_hwaddress	/* gcc doesn't support KHWASAN */
+#endif
+
 /*
  * A trick to suppress uninitialized variable warning without generating any
  * code
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 42506e4d1f53..6439fdd46b4e 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -195,7 +195,8 @@ void __read_once_size(const volatile void *p, void *res, int size)
  * 	https://gcc.gnu.org/bugzilla/show_bug.cgi?id=67368
  * '__maybe_unused' allows us to avoid defined-but-not-used warnings.
  */
-# define __no_kasan_or_inline __no_sanitize_address __maybe_unused
+# define __no_kasan_or_inline __no_sanitize_address __no_sanitize_hwaddress \
+			      __maybe_unused
 #else
 # define __no_kasan_or_inline __always_inline
 #endif
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index cbdc54543803..6608aa9b35ac 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -45,8 +45,6 @@ void kasan_free_pages(struct page *page, unsigned int order);
 
 void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 			slab_flags_t *flags);
-void kasan_cache_shrink(struct kmem_cache *cache);
-void kasan_cache_shutdown(struct kmem_cache *cache);
 
 void kasan_poison_slab(struct page *page);
 void kasan_unpoison_object_data(struct kmem_cache *cache, void *object);
@@ -94,8 +92,6 @@ static inline void kasan_free_pages(struct page *page, unsigned int order) {}
 static inline void kasan_cache_create(struct kmem_cache *cache,
 				      unsigned int *size,
 				      slab_flags_t *flags) {}
-static inline void kasan_cache_shrink(struct kmem_cache *cache) {}
-static inline void kasan_cache_shutdown(struct kmem_cache *cache) {}
 
 static inline void kasan_poison_slab(struct page *page) {}
 static inline void kasan_unpoison_object_data(struct kmem_cache *cache,
@@ -141,4 +137,16 @@ static inline size_t kasan_metadata_size(struct kmem_cache *cache) { return 0; }
 
 #endif /* CONFIG_KASAN */
 
+#ifdef CONFIG_KASAN_GENERIC
+
+void kasan_cache_shrink(struct kmem_cache *cache);
+void kasan_cache_shutdown(struct kmem_cache *cache);
+
+#else /* CONFIG_KASAN_GENERIC */
+
+static inline void kasan_cache_shrink(struct kmem_cache *cache) {}
+static inline void kasan_cache_shutdown(struct kmem_cache *cache) {}
+
+#endif /* CONFIG_KASAN_GENERIC */
+
 #endif /* LINUX_KASAN_H */
diff --git a/lib/Kconfig.kasan b/lib/Kconfig.kasan
index 3d35d062970d..baf2619b7ff4 100644
--- a/lib/Kconfig.kasan
+++ b/lib/Kconfig.kasan
@@ -1,33 +1,73 @@
 config HAVE_ARCH_KASAN
 	bool
 
+config HAVE_ARCH_KASAN_HW
+	bool
+
 if HAVE_ARCH_KASAN
 
 config KASAN
-	bool "KASan: runtime memory debugger"
+	bool "KASAN: runtime memory debugger"
+	help
+	  Enables KASAN (KernelAddressSANitizer) - runtime memory debugger,
+	  designed to find out-of-bounds accesses and use-after-free bugs.
+
+choice
+	prompt "KASAN mode"
+	depends on KASAN
+	default KASAN_GENERIC
+	help
+	  KASAN has two modes: KASAN (a classic version, similar to userspace
+	  ASan, enabled with CONFIG_KASAN_GENERIC) and KHWASAN (a version
+	  based on pointer tagging, only for arm64, similar to userspace
+	  HWASan, enabled with CONFIG_KASAN_HW).
+
+config KASAN_GENERIC
+	bool "KASAN: the generic mode"
 	depends on SLUB || (SLAB && !DEBUG_SLAB)
 	select CONSTRUCTORS
 	select STACKDEPOT
 	help
-	  Enables kernel address sanitizer - runtime memory debugger,
-	  designed to find out-of-bounds accesses and use-after-free bugs.
-	  This is strictly a debugging feature and it requires a gcc version
-	  of 4.9.2 or later. Detection of out of bounds accesses to stack or
-	  global variables requires gcc 5.0 or later.
-	  This feature consumes about 1/8 of available memory and brings about
-	  ~x3 performance slowdown.
+	  Enables the generic mode of KASAN.
+	  This is strictly a debugging feature and it requires a GCC version
+	  of 4.9.2 or later. Detection of out-of-bounds accesses to stack or
+	  global variables requires GCC 5.0 or later.
+	  This mode consumes about 1/8 of available memory at kernel start
+	  and introduces an overhead of ~x1.5 for the rest of the allocations.
+	  The performance slowdown is ~x3.
+	  For better error detection enable CONFIG_STACKTRACE.
+	  Currently CONFIG_KASAN_GENERIC doesn't work with CONFIG_DEBUG_SLAB
+	  (the resulting kernel does not boot).
+
+if HAVE_ARCH_KASAN_HW
+
+config KASAN_HW
+	bool "KHWASAN: the hardware assisted mode"
+	depends on SLUB || (SLAB && !DEBUG_SLAB)
+	select CONSTRUCTORS
+	select STACKDEPOT
+	help
+	  Enabled KHWASAN (KASAN mode based on pointer tagging).
+	  This mode requires Top Byte Ignore support by the CPU and therefore
+	  only supported for arm64.
+	  This feature requires clang revision 330044 or later.
+	  This mode consumes about 1/16 of available memory at kernel start
+	  and introduces an overhead of ~20% for the rest of the allocations.
 	  For better error detection enable CONFIG_STACKTRACE.
-	  Currently CONFIG_KASAN doesn't work with CONFIG_DEBUG_SLAB
+	  Currently CONFIG_KASAN_HW doesn't work with CONFIG_DEBUG_SLAB
 	  (the resulting kernel does not boot).
 
+endif
+
+endchoice
+
 config KASAN_EXTRA
-	bool "KAsan: extra checks"
-	depends on KASAN && DEBUG_KERNEL && !COMPILE_TEST
+	bool "KASAN: extra checks"
+	depends on KASAN_GENERIC && DEBUG_KERNEL && !COMPILE_TEST
 	help
-	  This enables further checks in the kernel address sanitizer, for now
-	  it only includes the address-use-after-scope check that can lead
-	  to excessive kernel stack usage, frame size warnings and longer
-	  compile time.
+	  This enables further checks in KASAN, for now it only includes the
+	  address-use-after-scope check that can lead to excessive kernel
+	  stack usage, frame size warnings and longer compile time.
 	  https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81715 has more
 
 
@@ -52,16 +92,16 @@ config KASAN_INLINE
 	  memory accesses. This is faster than outline (in some workloads
 	  it gives about x2 boost over outline instrumentation), but
 	  make kernel's .text size much bigger.
-	  This requires a gcc version of 5.0 or later.
+	  For CONFIG_KASAN_GENERIC this requires GCC 5.0 or later.
 
 endchoice
 
 config TEST_KASAN
-	tristate "Module for testing kasan for bug detection"
+	tristate "Module for testing KASAN for bug detection"
 	depends on m && KASAN
 	help
 	  This is a test module doing various nasty things like
 	  out of bounds accesses, use after free. It is useful for testing
-	  kernel debugging features like kernel address sanitizer.
+	  kernel debugging features like KASAN.
 
 endif
diff --git a/mm/kasan/Makefile b/mm/kasan/Makefile
index a6df14bffb6b..14955add96d3 100644
--- a/mm/kasan/Makefile
+++ b/mm/kasan/Makefile
@@ -2,6 +2,7 @@
 KASAN_SANITIZE := n
 UBSAN_SANITIZE_common.o := n
 UBSAN_SANITIZE_kasan.o := n
+UBSAN_SANITIZE_khwasan.o := n
 KCOV_INSTRUMENT := n
 
 CFLAGS_REMOVE_kasan.o = -pg
@@ -10,5 +11,8 @@ CFLAGS_REMOVE_kasan.o = -pg
 
 CFLAGS_common.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 CFLAGS_kasan.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
+CFLAGS_khwasan.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 
-obj-y := common.o kasan.o report.o kasan_init.o quarantine.o
+obj-$(CONFIG_KASAN) := common.o kasan_init.o report.o
+obj-$(CONFIG_KASAN_GENERIC) += kasan.o quarantine.o
+obj-$(CONFIG_KASAN_HW) += khwasan.o
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index 659463800f10..19b950eaccff 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -114,7 +114,8 @@ void kasan_report(unsigned long addr, size_t size,
 		bool is_write, unsigned long ip);
 void kasan_report_invalid_free(void *object, unsigned long ip);
 
-#if defined(CONFIG_SLAB) || defined(CONFIG_SLUB)
+#if defined(CONFIG_KASAN_GENERIC) && \
+	(defined(CONFIG_SLAB) || defined(CONFIG_SLUB))
 void quarantine_put(struct kasan_free_meta *info, struct kmem_cache *cache);
 void quarantine_reduce(void);
 void quarantine_remove_cache(struct kmem_cache *cache);
diff --git a/mm/kasan/khwasan.c b/mm/kasan/khwasan.c
new file mode 100644
index 000000000000..e2c3a7f7fd1f
--- /dev/null
+++ b/mm/kasan/khwasan.c
@@ -0,0 +1,75 @@
+/*
+ * This file contains core KHWASAN code.
+ *
+ * Copyright (c) 2018 Google, Inc.
+ * Author: Andrey Konovalov <andreyknvl@google.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+#define DISABLE_BRANCH_PROFILING
+
+#include <linux/export.h>
+#include <linux/interrupt.h>
+#include <linux/init.h>
+#include <linux/kasan.h>
+#include <linux/kernel.h>
+#include <linux/kmemleak.h>
+#include <linux/linkage.h>
+#include <linux/memblock.h>
+#include <linux/memory.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/printk.h>
+#include <linux/random.h>
+#include <linux/sched.h>
+#include <linux/sched/task_stack.h>
+#include <linux/slab.h>
+#include <linux/stacktrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/vmalloc.h>
+#include <linux/bug.h>
+
+#include "kasan.h"
+#include "../slab.h"
+
+void check_memory_region(unsigned long addr, size_t size, bool write,
+				unsigned long ret_ip)
+{
+}
+
+#define DEFINE_HWASAN_LOAD_STORE(size)					\
+	void __hwasan_load##size##_noabort(unsigned long addr)		\
+	{								\
+	}								\
+	EXPORT_SYMBOL(__hwasan_load##size##_noabort);			\
+	void __hwasan_store##size##_noabort(unsigned long addr)		\
+	{								\
+	}								\
+	EXPORT_SYMBOL(__hwasan_store##size##_noabort)
+
+DEFINE_HWASAN_LOAD_STORE(1);
+DEFINE_HWASAN_LOAD_STORE(2);
+DEFINE_HWASAN_LOAD_STORE(4);
+DEFINE_HWASAN_LOAD_STORE(8);
+DEFINE_HWASAN_LOAD_STORE(16);
+
+void __hwasan_loadN_noabort(unsigned long addr, unsigned long size)
+{
+}
+EXPORT_SYMBOL(__hwasan_loadN_noabort);
+
+void __hwasan_storeN_noabort(unsigned long addr, unsigned long size)
+{
+}
+EXPORT_SYMBOL(__hwasan_storeN_noabort);
+
+void __hwasan_tag_memory(unsigned long addr, u8 tag, unsigned long size)
+{
+}
+EXPORT_SYMBOL(__hwasan_tag_memory);
diff --git a/mm/slub.c b/mm/slub.c
index a60887938c19..5d1e1dee159b 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -2956,7 +2956,7 @@ static __always_inline void slab_free(struct kmem_cache *s, struct page *page,
 		do_slab_free(s, page, head, tail, cnt, addr);
 }
 
-#ifdef CONFIG_KASAN
+#ifdef CONFIG_KASAN_GENERIC
 void ___cache_free(struct kmem_cache *cache, void *x, unsigned long addr)
 {
 	do_slab_free(cache, virt_to_head_page(x), x, NULL, 1, addr);
diff --git a/scripts/Makefile.kasan b/scripts/Makefile.kasan
index 69552a39951d..49c6e056c697 100644
--- a/scripts/Makefile.kasan
+++ b/scripts/Makefile.kasan
@@ -1,5 +1,5 @@
 # SPDX-License-Identifier: GPL-2.0
-ifdef CONFIG_KASAN
+ifdef CONFIG_KASAN_GENERIC
 ifdef CONFIG_KASAN_INLINE
 	call_threshold := 10000
 else
@@ -42,6 +42,29 @@ ifdef CONFIG_KASAN_EXTRA
 CFLAGS_KASAN += $(call cc-option, -fsanitize-address-use-after-scope)
 endif
 
-CFLAGS_KASAN_NOSANITIZE := -fno-builtin
+endif
+
+ifdef CONFIG_KASAN_HW
+
+ifdef CONFIG_KASAN_INLINE
+    instrumentation_flags := -mllvm -hwasan-mapping-offset=$(KASAN_SHADOW_OFFSET)
+else
+    instrumentation_flags := -mllvm -hwasan-instrument-with-calls=1
+endif
 
+CFLAGS_KASAN := -fsanitize=kernel-hwaddress \
+		-mllvm -hwasan-instrument-stack=0 \
+		$(instrumentation_flags)
+
+ifeq ($(call cc-option, $(CFLAGS_KASAN) -Werror),)
+    ifneq ($(CONFIG_COMPILE_TEST),y)
+        $(warning Cannot use CONFIG_KASAN_HW: \
+            -fsanitize=hwaddress is not supported by compiler)
+    endif
+endif
+
+endif
+
+ifdef CONFIG_KASAN
+CFLAGS_KASAN_NOSANITIZE := -fno-builtin
 endif
-- 
2.18.0.rc2.346.g013aa6912e-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v4 05/17] khwasan: initialize shadow to 0xff ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v4 05/17] khwasan: initialize shadow to 0xff
Date: Tue, 26 Jun 2018 13:15:15 +0000
Message-ID: <4f11b60d5ace61e1a07ad4962ee11dab4f4ee48e.1530018818.git.andreyknvl () google ! com>
--------------------
A KHWASAN shadow memory cell contains a memory tag, that corresponds to
the tag in the top byte of the pointer, that points to that memory. The
native top byte value of kernel pointers is 0xff, so with KHWASAN we
need to initialize shadow memory to 0xff. This commit does that.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/mm/kasan_init.c | 16 ++++++++++++++--
 include/linux/kasan.h      |  8 ++++++++
 mm/kasan/common.c          |  3 ++-
 3 files changed, 24 insertions(+), 3 deletions(-)

diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c
index 12145874c02b..7a31e8ccbad2 100644
--- a/arch/arm64/mm/kasan_init.c
+++ b/arch/arm64/mm/kasan_init.c
@@ -44,6 +44,15 @@ static phys_addr_t __init kasan_alloc_zeroed_page(int node)
 	return __pa(p);
 }
 
+static phys_addr_t __init kasan_alloc_raw_page(int node)
+{
+	void *p = memblock_virt_alloc_try_nid_raw(PAGE_SIZE, PAGE_SIZE,
+						  __pa(MAX_DMA_ADDRESS),
+						  MEMBLOCK_ALLOC_ACCESSIBLE,
+						  node);
+	return __pa(p);
+}
+
 static pte_t *__init kasan_pte_offset(pmd_t *pmdp, unsigned long addr, int node,
 				      bool early)
 {
@@ -89,7 +98,9 @@ static void __init kasan_pte_populate(pmd_t *pmdp, unsigned long addr,
 
 	do {
 		phys_addr_t page_phys = early ? __pa_symbol(kasan_zero_page)
-					      : kasan_alloc_zeroed_page(node);
+					      : kasan_alloc_raw_page(node);
+		if (!early)
+			memset(__va(page_phys), KASAN_SHADOW_INIT, PAGE_SIZE);
 		next = addr + PAGE_SIZE;
 		set_pte(ptep, pfn_pte(__phys_to_pfn(page_phys), PAGE_KERNEL));
 	} while (ptep++, addr = next, addr != end && pte_none(READ_ONCE(*ptep)));
@@ -139,6 +150,7 @@ asmlinkage void __init kasan_early_init(void)
 		KASAN_SHADOW_END - (1UL << (64 - KASAN_SHADOW_SCALE_SHIFT)));
 	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_START, PGDIR_SIZE));
 	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PGDIR_SIZE));
+
 	kasan_pgd_populate(KASAN_SHADOW_START, KASAN_SHADOW_END, NUMA_NO_NODE,
 			   true);
 }
@@ -235,7 +247,7 @@ void __init kasan_init(void)
 		set_pte(&kasan_zero_pte[i],
 			pfn_pte(sym_to_pfn(kasan_zero_page), PAGE_KERNEL_RO));
 
-	memset(kasan_zero_page, 0, PAGE_SIZE);
+	memset(kasan_zero_page, KASAN_SHADOW_INIT, PAGE_SIZE);
 	cpu_replace_ttbr1(lm_alias(swapper_pg_dir));
 
 	/* At this point kasan is fully initialized. Enable error messages */
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index 6608aa9b35ac..336385baf926 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -139,6 +139,8 @@ static inline size_t kasan_metadata_size(struct kmem_cache *cache) { return 0; }
 
 #ifdef CONFIG_KASAN_GENERIC
 
+#define KASAN_SHADOW_INIT 0
+
 void kasan_cache_shrink(struct kmem_cache *cache);
 void kasan_cache_shutdown(struct kmem_cache *cache);
 
@@ -149,4 +151,10 @@ static inline void kasan_cache_shutdown(struct kmem_cache *cache) {}
 
 #endif /* CONFIG_KASAN_GENERIC */
 
+#ifdef CONFIG_KASAN_HW
+
+#define KASAN_SHADOW_INIT 0xFF
+
+#endif /* CONFIG_KASAN_HW */
+
 #endif /* LINUX_KASAN_H */
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index e78ebeff1f4c..656baa8984c7 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -473,11 +473,12 @@ int kasan_module_alloc(void *addr, size_t size)
 
 	ret = __vmalloc_node_range(shadow_size, 1, shadow_start,
 			shadow_start + shadow_size,
-			GFP_KERNEL | __GFP_ZERO,
+			GFP_KERNEL,
 			PAGE_KERNEL, VM_NO_GUARD, NUMA_NO_NODE,
 			__builtin_return_address(0));
 
 	if (ret) {
+		__memset(ret, KASAN_SHADOW_INIT, shadow_size);
 		find_vm_area(addr)->flags |= VM_KASAN;
 		kmemleak_ignore(ret);
 		return 0;
-- 
2.18.0.rc2.346.g013aa6912e-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v4 07/17] khwasan: add tag related helper functions ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v4 07/17] khwasan: add tag related helper functions
Date: Tue, 26 Jun 2018 13:15:17 +0000
Message-ID: <0aaff2112647cc4b3aaa1a2be127285dcb5af2c5.1530018818.git.andreyknvl () google ! com>
--------------------
This commit adds a few helper functions, that are meant to be used to
work with tags embedded in the top byte of kernel pointers: to set, to
get or to reset (set to 0xff) the top byte.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/mm/kasan_init.c |  2 ++
 include/linux/kasan.h      | 23 ++++++++++++++++
 mm/kasan/kasan.h           | 55 ++++++++++++++++++++++++++++++++++++++
 mm/kasan/khwasan.c         | 47 ++++++++++++++++++++++++++++++++
 4 files changed, 127 insertions(+)

diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c
index 7a31e8ccbad2..e7f37c0b7e14 100644
--- a/arch/arm64/mm/kasan_init.c
+++ b/arch/arm64/mm/kasan_init.c
@@ -250,6 +250,8 @@ void __init kasan_init(void)
 	memset(kasan_zero_page, KASAN_SHADOW_INIT, PAGE_SIZE);
 	cpu_replace_ttbr1(lm_alias(swapper_pg_dir));
 
+	khwasan_init();
+
 	/* At this point kasan is fully initialized. Enable error messages */
 	init_task.kasan_depth = 0;
 	pr_info("KernelAddressSanitizer initialized\n");
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index 336385baf926..d7624b879d86 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -155,6 +155,29 @@ static inline void kasan_cache_shutdown(struct kmem_cache *cache) {}
 
 #define KASAN_SHADOW_INIT 0xFF
 
+void khwasan_init(void);
+
+void *khwasan_set_tag(const void *addr, u8 tag);
+u8 khwasan_get_tag(const void *addr);
+void *khwasan_reset_tag(const void *ptr);
+
+#else /* CONFIG_KASAN_HW */
+
+static inline void khwasan_init(void) { }
+
+static inline void *khwasan_set_tag(const void *addr, u8 tag)
+{
+	return (void *)addr;
+}
+static inline u8 khwasan_get_tag(const void *addr)
+{
+	return 0xFF;
+}
+static inline void *khwasan_reset_tag(const void *ptr)
+{
+	return (void *)ptr;
+}
+
 #endif /* CONFIG_KASAN_HW */
 
 #endif /* LINUX_KASAN_H */
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index 19b950eaccff..a7cc27d96608 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -8,6 +8,10 @@
 #define KASAN_SHADOW_SCALE_SIZE (1UL << KASAN_SHADOW_SCALE_SHIFT)
 #define KASAN_SHADOW_MASK       (KASAN_SHADOW_SCALE_SIZE - 1)
 
+#define KHWASAN_TAG_KERNEL	0xFF /* native kernel pointers tag */
+#define KHWASAN_TAG_INVALID	0xFE /* inaccessible memory tag */
+#define KHWASAN_TAG_MAX		0xFD /* maximum value for random tags */
+
 #define KASAN_FREE_PAGE         0xFF  /* page was freed */
 #define KASAN_PAGE_REDZONE      0xFE  /* redzone for kmalloc_large allocations */
 #define KASAN_KMALLOC_REDZONE   0xFC  /* redzone inside slub object */
@@ -126,6 +130,57 @@ static inline void quarantine_reduce(void) { }
 static inline void quarantine_remove_cache(struct kmem_cache *cache) { }
 #endif
 
+#ifdef CONFIG_KASAN_HW
+
+#define KHWASAN_TAG_SHIFT 56
+#define KHWASAN_TAG_MASK (0xFFUL << KHWASAN_TAG_SHIFT)
+
+u8 random_tag(void);
+
+static inline void *set_tag(const void *addr, u8 tag)
+{
+	u64 a = (u64)addr;
+
+	a &= ~KHWASAN_TAG_MASK;
+	a |= ((u64)tag << KHWASAN_TAG_SHIFT);
+
+	return (void *)a;
+}
+
+static inline u8 get_tag(const void *addr)
+{
+	return (u8)((u64)addr >> KHWASAN_TAG_SHIFT);
+}
+
+static inline void *reset_tag(const void *addr)
+{
+	return set_tag(addr, KHWASAN_TAG_KERNEL);
+}
+
+#else /* CONFIG_KASAN_HW */
+
+static inline u8 random_tag(void)
+{
+	return 0;
+}
+
+static inline void *set_tag(const void *addr, u8 tag)
+{
+	return (void *)addr;
+}
+
+static inline u8 get_tag(const void *addr)
+{
+	return 0;
+}
+
+static inline void *reset_tag(const void *addr)
+{
+	return (void *)addr;
+}
+
+#endif /* CONFIG_KASAN_HW */
+
 /*
  * Exported functions for interfaces called from assembly or from generated
  * code. Declarations here to avoid warning about missing declarations.
diff --git a/mm/kasan/khwasan.c b/mm/kasan/khwasan.c
index e2c3a7f7fd1f..d34679b8f8c7 100644
--- a/mm/kasan/khwasan.c
+++ b/mm/kasan/khwasan.c
@@ -38,6 +38,53 @@
 #include "kasan.h"
 #include "../slab.h"
 
+static DEFINE_PER_CPU(u32, prng_state);
+
+void khwasan_init(void)
+{
+	int cpu;
+
+	for_each_possible_cpu(cpu)
+		per_cpu(prng_state, cpu) = get_random_u32();
+}
+
+/*
+ * If a preemption happens between this_cpu_read and this_cpu_write, the only
+ * side effect is that we'll give a few allocated in different contexts objects
+ * the same tag. Since KHWASAN is meant to be used a probabilistic bug-detection
+ * debug feature, this doesnât have significant negative impact.
+ *
+ * Ideally the tags use strong randomness to prevent any attempts to predict
+ * them during explicit exploit attempts. But strong randomness is expensive,
+ * and we did an intentional trade-off to use a PRNG. This non-atomic RMW
+ * sequence has in fact positive effect, since interrupts that randomly skew
+ * PRNG at unpredictable points do only good.
+ */
+u8 random_tag(void)
+{
+	u32 state = this_cpu_read(prng_state);
+
+	state = 1664525 * state + 1013904223;
+	this_cpu_write(prng_state, state);
+
+	return (u8)(state % (KHWASAN_TAG_MAX + 1));
+}
+
+void *khwasan_set_tag(const void *addr, u8 tag)
+{
+	return set_tag(addr, tag);
+}
+
+u8 khwasan_get_tag(const void *addr)
+{
+	return get_tag(addr);
+}
+
+void *khwasan_reset_tag(const void *addr)
+{
+	return reset_tag(addr);
+}
+
 void check_memory_region(unsigned long addr, size_t size, bool write,
 				unsigned long ret_ip)
 {
-- 
2.18.0.rc2.346.g013aa6912e-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v4 08/17] khwasan, arm64: fix up fault handling logic ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v4 08/17] khwasan, arm64: fix up fault handling logic
Date: Tue, 26 Jun 2018 13:15:18 +0000
Message-ID: <9f3e30f8144682c989bbf9fdac081501258a0b06.1530018818.git.andreyknvl () google ! com>
--------------------
show_pte in arm64 fault handling relies on the fact that the top byte of
a kernel pointer is 0xff, which isn't always the case with KHWASAN enabled.
Reset the top byte.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/mm/fault.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index b8eecc7b9531..b7b152783d54 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -32,6 +32,7 @@
 #include <linux/perf_event.h>
 #include <linux/preempt.h>
 #include <linux/hugetlb.h>
+#include <linux/kasan.h>
 
 #include <asm/bug.h>
 #include <asm/cmpxchg.h>
@@ -134,6 +135,8 @@ void show_pte(unsigned long addr)
 	pgd_t *pgdp;
 	pgd_t pgd;
 
+	addr = (unsigned long)khwasan_reset_tag((void *)addr);
+
 	if (addr < TASK_SIZE) {
 		/* TTBR0 */
 		mm = current->active_mm;
-- 
2.18.0.rc2.346.g013aa6912e-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v4 09/17] khwasan, arm64: enable top byte ignore for the kernel ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v4 09/17] khwasan, arm64: enable top byte ignore for the kernel
Date: Tue, 26 Jun 2018 13:15:19 +0000
Message-ID: <82571a6721547446b789f759f93bf7f3930541be.1530018818.git.andreyknvl () google ! com>
--------------------
KHWASAN uses the Top Byte Ignore feature of arm64 CPUs to store a pointer
tag in the top byte of each pointer. This commit enables the TCR_TBI1 bit,
which enables Top Byte Ignore for the kernel, when KHWASAN is used.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/pgtable-hwdef.h | 1 +
 arch/arm64/mm/proc.S                   | 8 +++++++-
 2 files changed, 8 insertions(+), 1 deletion(-)

diff --git a/arch/arm64/include/asm/pgtable-hwdef.h b/arch/arm64/include/asm/pgtable-hwdef.h
index fd208eac9f2a..483aceedad76 100644
--- a/arch/arm64/include/asm/pgtable-hwdef.h
+++ b/arch/arm64/include/asm/pgtable-hwdef.h
@@ -289,6 +289,7 @@
 #define TCR_A1			(UL(1) << 22)
 #define TCR_ASID16		(UL(1) << 36)
 #define TCR_TBI0		(UL(1) << 37)
+#define TCR_TBI1		(UL(1) << 38)
 #define TCR_HA			(UL(1) << 39)
 #define TCR_HD			(UL(1) << 40)
 #define TCR_NFD1		(UL(1) << 54)
diff --git a/arch/arm64/mm/proc.S b/arch/arm64/mm/proc.S
index 03646e6a2ef4..c5175e098d02 100644
--- a/arch/arm64/mm/proc.S
+++ b/arch/arm64/mm/proc.S
@@ -47,6 +47,12 @@
 /* PTWs cacheable, inner/outer WBWA */
 #define TCR_CACHE_FLAGS	TCR_IRGN_WBWA | TCR_ORGN_WBWA
 
+#ifdef CONFIG_KASAN_HW
+#define TCR_KASAN_FLAGS TCR_TBI1
+#else
+#define TCR_KASAN_FLAGS 0
+#endif
+
 #define MAIR(attr, mt)	((attr) << ((mt) * 8))
 
 /*
@@ -440,7 +446,7 @@ ENTRY(__cpu_setup)
 	 */
 	ldr	x10, =TCR_TxSZ(VA_BITS) | TCR_CACHE_FLAGS | TCR_SMP_FLAGS | \
 			TCR_TG_FLAGS | TCR_KASLR_FLAGS | TCR_ASID16 | \
-			TCR_TBI0 | TCR_A1
+			TCR_TBI0 | TCR_A1 | TCR_KASAN_FLAGS
 	tcr_set_idmap_t0sz	x10, x9
 
 	/*
-- 
2.18.0.rc2.346.g013aa6912e-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v4 11/17] khwasan: split out kasan_report.c from report.c ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v4 11/17] khwasan: split out kasan_report.c from report.c
Date: Tue, 26 Jun 2018 13:15:21 +0000
Message-ID: <2476c4c26e36d0ac8f28d87e1ec415a908a01b91.1530018818.git.andreyknvl () google ! com>
--------------------
This patch moves KASAN specific error reporting routines to kasan_report.c
without any functional changes, leaving common error reporting code in
report.c to be later reused by KHWASAN.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/Makefile         |   4 +-
 mm/kasan/kasan.h          |   7 ++
 mm/kasan/kasan_report.c   | 158 +++++++++++++++++++++++++
 mm/kasan/khwasan_report.c |  39 +++++++
 mm/kasan/report.c         | 234 +++++++++-----------------------------
 5 files changed, 257 insertions(+), 185 deletions(-)
 create mode 100644 mm/kasan/kasan_report.c
 create mode 100644 mm/kasan/khwasan_report.c

diff --git a/mm/kasan/Makefile b/mm/kasan/Makefile
index 14955add96d3..7ef536390365 100644
--- a/mm/kasan/Makefile
+++ b/mm/kasan/Makefile
@@ -14,5 +14,5 @@ CFLAGS_kasan.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 CFLAGS_khwasan.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 
 obj-$(CONFIG_KASAN) := common.o kasan_init.o report.o
-obj-$(CONFIG_KASAN_GENERIC) += kasan.o quarantine.o
-obj-$(CONFIG_KASAN_HW) += khwasan.o
+obj-$(CONFIG_KASAN_GENERIC) += kasan.o kasan_report.o quarantine.o
+obj-$(CONFIG_KASAN_HW) += khwasan.o khwasan_report.o
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index a7cc27d96608..82672473740c 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -109,11 +109,18 @@ static inline const void *kasan_shadow_to_mem(const void *shadow_addr)
 		<< KASAN_SHADOW_SCALE_SHIFT);
 }
 
+static inline bool addr_has_shadow(const void *addr)
+{
+	return (addr >= kasan_shadow_to_mem((void *)KASAN_SHADOW_START));
+}
+
 void kasan_poison_shadow(const void *address, size_t size, u8 value);
 
 void check_memory_region(unsigned long addr, size_t size, bool write,
 				unsigned long ret_ip);
 
+const char *get_bug_type(struct kasan_access_info *info);
+
 void kasan_report(unsigned long addr, size_t size,
 		bool is_write, unsigned long ip);
 void kasan_report_invalid_free(void *object, unsigned long ip);
diff --git a/mm/kasan/kasan_report.c b/mm/kasan/kasan_report.c
new file mode 100644
index 000000000000..2d8decbecbd5
--- /dev/null
+++ b/mm/kasan/kasan_report.c
@@ -0,0 +1,158 @@
+/*
+ * This file contains KASAN specific error reporting code.
+ *
+ * Copyright (c) 2014 Samsung Electronics Co., Ltd.
+ * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
+ *
+ * Some code borrowed from https://github.com/xairy/kasan-prototype by
+ *        Andrey Konovalov <andreyknvl@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/bitops.h>
+#include <linux/ftrace.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/stackdepot.h>
+#include <linux/stacktrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/kasan.h>
+#include <linux/module.h>
+
+#include <asm/sections.h>
+
+#include "kasan.h"
+#include "../slab.h"
+
+static const void *find_first_bad_addr(const void *addr, size_t size)
+{
+	u8 shadow_val = *(u8 *)kasan_mem_to_shadow(addr);
+	const void *first_bad_addr = addr;
+
+	while (!shadow_val && first_bad_addr < addr + size) {
+		first_bad_addr += KASAN_SHADOW_SCALE_SIZE;
+		shadow_val = *(u8 *)kasan_mem_to_shadow(first_bad_addr);
+	}
+	return first_bad_addr;
+}
+
+static const char *get_shadow_bug_type(struct kasan_access_info *info)
+{
+	const char *bug_type = "unknown-crash";
+	u8 *shadow_addr;
+
+	info->first_bad_addr = find_first_bad_addr(info->access_addr,
+						info->access_size);
+
+	shadow_addr = (u8 *)kasan_mem_to_shadow(info->first_bad_addr);
+
+	/*
+	 * If shadow byte value is in [0, KASAN_SHADOW_SCALE_SIZE) we can look
+	 * at the next shadow byte to determine the type of the bad access.
+	 */
+	if (*shadow_addr > 0 && *shadow_addr <= KASAN_SHADOW_SCALE_SIZE - 1)
+		shadow_addr++;
+
+	switch (*shadow_addr) {
+	case 0 ... KASAN_SHADOW_SCALE_SIZE - 1:
+		/*
+		 * In theory it's still possible to see these shadow values
+		 * due to a data race in the kernel code.
+		 */
+		bug_type = "out-of-bounds";
+		break;
+	case KASAN_PAGE_REDZONE:
+	case KASAN_KMALLOC_REDZONE:
+		bug_type = "slab-out-of-bounds";
+		break;
+	case KASAN_GLOBAL_REDZONE:
+		bug_type = "global-out-of-bounds";
+		break;
+	case KASAN_STACK_LEFT:
+	case KASAN_STACK_MID:
+	case KASAN_STACK_RIGHT:
+	case KASAN_STACK_PARTIAL:
+		bug_type = "stack-out-of-bounds";
+		break;
+	case KASAN_FREE_PAGE:
+	case KASAN_KMALLOC_FREE:
+		bug_type = "use-after-free";
+		break;
+	case KASAN_USE_AFTER_SCOPE:
+		bug_type = "use-after-scope";
+		break;
+	case KASAN_ALLOCA_LEFT:
+	case KASAN_ALLOCA_RIGHT:
+		bug_type = "alloca-out-of-bounds";
+		break;
+	}
+
+	return bug_type;
+}
+
+static const char *get_wild_bug_type(struct kasan_access_info *info)
+{
+	const char *bug_type = "unknown-crash";
+
+	if ((unsigned long)info->access_addr < PAGE_SIZE)
+		bug_type = "null-ptr-deref";
+	else if ((unsigned long)info->access_addr < TASK_SIZE)
+		bug_type = "user-memory-access";
+	else
+		bug_type = "wild-memory-access";
+
+	return bug_type;
+}
+
+const char *get_bug_type(struct kasan_access_info *info)
+{
+	if (addr_has_shadow(info->access_addr))
+		return get_shadow_bug_type(info);
+	return get_wild_bug_type(info);
+}
+
+#define DEFINE_ASAN_REPORT_LOAD(size)                     \
+void __asan_report_load##size##_noabort(unsigned long addr) \
+{                                                         \
+	kasan_report(addr, size, false, _RET_IP_);	  \
+}                                                         \
+EXPORT_SYMBOL(__asan_report_load##size##_noabort)
+
+#define DEFINE_ASAN_REPORT_STORE(size)                     \
+void __asan_report_store##size##_noabort(unsigned long addr) \
+{                                                          \
+	kasan_report(addr, size, true, _RET_IP_);	   \
+}                                                          \
+EXPORT_SYMBOL(__asan_report_store##size##_noabort)
+
+DEFINE_ASAN_REPORT_LOAD(1);
+DEFINE_ASAN_REPORT_LOAD(2);
+DEFINE_ASAN_REPORT_LOAD(4);
+DEFINE_ASAN_REPORT_LOAD(8);
+DEFINE_ASAN_REPORT_LOAD(16);
+DEFINE_ASAN_REPORT_STORE(1);
+DEFINE_ASAN_REPORT_STORE(2);
+DEFINE_ASAN_REPORT_STORE(4);
+DEFINE_ASAN_REPORT_STORE(8);
+DEFINE_ASAN_REPORT_STORE(16);
+
+void __asan_report_load_n_noabort(unsigned long addr, size_t size)
+{
+	kasan_report(addr, size, false, _RET_IP_);
+}
+EXPORT_SYMBOL(__asan_report_load_n_noabort);
+
+void __asan_report_store_n_noabort(unsigned long addr, size_t size)
+{
+	kasan_report(addr, size, true, _RET_IP_);
+}
+EXPORT_SYMBOL(__asan_report_store_n_noabort);
diff --git a/mm/kasan/khwasan_report.c b/mm/kasan/khwasan_report.c
new file mode 100644
index 000000000000..2edbc3c76be5
--- /dev/null
+++ b/mm/kasan/khwasan_report.c
@@ -0,0 +1,39 @@
+/*
+ * This file contains KHWASAN specific error reporting code.
+ *
+ * Copyright (c) 2014 Samsung Electronics Co., Ltd.
+ * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
+ *
+ * Some code borrowed from https://github.com/xairy/kasan-prototype by
+ *        Andrey Konovalov <andreyknvl@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/bitops.h>
+#include <linux/ftrace.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/stackdepot.h>
+#include <linux/stacktrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/kasan.h>
+#include <linux/module.h>
+
+#include <asm/sections.h>
+
+#include "kasan.h"
+#include "../slab.h"
+
+const char *get_bug_type(struct kasan_access_info *info)
+{
+	return "invalid-access";
+}
diff --git a/mm/kasan/report.c b/mm/kasan/report.c
index 5c169aa688fd..155247a6f8a8 100644
--- a/mm/kasan/report.c
+++ b/mm/kasan/report.c
@@ -1,5 +1,5 @@
 /*
- * This file contains error reporting code.
+ * This file contains common KASAN and KHWASAN error reporting code.
  *
  * Copyright (c) 2014 Samsung Electronics Co., Ltd.
  * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
@@ -39,103 +39,34 @@
 #define SHADOW_BYTES_PER_ROW (SHADOW_BLOCKS_PER_ROW * SHADOW_BYTES_PER_BLOCK)
 #define SHADOW_ROWS_AROUND_ADDR 2
 
-static const void *find_first_bad_addr(const void *addr, size_t size)
-{
-	u8 shadow_val = *(u8 *)kasan_mem_to_shadow(addr);
-	const void *first_bad_addr = addr;
-
-	while (!shadow_val && first_bad_addr < addr + size) {
-		first_bad_addr += KASAN_SHADOW_SCALE_SIZE;
-		shadow_val = *(u8 *)kasan_mem_to_shadow(first_bad_addr);
-	}
-	return first_bad_addr;
-}
+static unsigned long kasan_flags;
 
-static bool addr_has_shadow(struct kasan_access_info *info)
-{
-	return (info->access_addr >=
-		kasan_shadow_to_mem((void *)KASAN_SHADOW_START));
-}
+#define KASAN_BIT_REPORTED	0
+#define KASAN_BIT_MULTI_SHOT	1
 
-static const char *get_shadow_bug_type(struct kasan_access_info *info)
+bool kasan_save_enable_multi_shot(void)
 {
-	const char *bug_type = "unknown-crash";
-	u8 *shadow_addr;
-
-	info->first_bad_addr = find_first_bad_addr(info->access_addr,
-						info->access_size);
-
-	shadow_addr = (u8 *)kasan_mem_to_shadow(info->first_bad_addr);
-
-	/*
-	 * If shadow byte value is in [0, KASAN_SHADOW_SCALE_SIZE) we can look
-	 * at the next shadow byte to determine the type of the bad access.
-	 */
-	if (*shadow_addr > 0 && *shadow_addr <= KASAN_SHADOW_SCALE_SIZE - 1)
-		shadow_addr++;
-
-	switch (*shadow_addr) {
-	case 0 ... KASAN_SHADOW_SCALE_SIZE - 1:
-		/*
-		 * In theory it's still possible to see these shadow values
-		 * due to a data race in the kernel code.
-		 */
-		bug_type = "out-of-bounds";
-		break;
-	case KASAN_PAGE_REDZONE:
-	case KASAN_KMALLOC_REDZONE:
-		bug_type = "slab-out-of-bounds";
-		break;
-	case KASAN_GLOBAL_REDZONE:
-		bug_type = "global-out-of-bounds";
-		break;
-	case KASAN_STACK_LEFT:
-	case KASAN_STACK_MID:
-	case KASAN_STACK_RIGHT:
-	case KASAN_STACK_PARTIAL:
-		bug_type = "stack-out-of-bounds";
-		break;
-	case KASAN_FREE_PAGE:
-	case KASAN_KMALLOC_FREE:
-		bug_type = "use-after-free";
-		break;
-	case KASAN_USE_AFTER_SCOPE:
-		bug_type = "use-after-scope";
-		break;
-	case KASAN_ALLOCA_LEFT:
-	case KASAN_ALLOCA_RIGHT:
-		bug_type = "alloca-out-of-bounds";
-		break;
-	}
-
-	return bug_type;
+	return test_and_set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
 }
+EXPORT_SYMBOL_GPL(kasan_save_enable_multi_shot);
 
-static const char *get_wild_bug_type(struct kasan_access_info *info)
+void kasan_restore_multi_shot(bool enabled)
 {
-	const char *bug_type = "unknown-crash";
-
-	if ((unsigned long)info->access_addr < PAGE_SIZE)
-		bug_type = "null-ptr-deref";
-	else if ((unsigned long)info->access_addr < TASK_SIZE)
-		bug_type = "user-memory-access";
-	else
-		bug_type = "wild-memory-access";
-
-	return bug_type;
+	if (!enabled)
+		clear_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
 }
+EXPORT_SYMBOL_GPL(kasan_restore_multi_shot);
 
-static const char *get_bug_type(struct kasan_access_info *info)
+static int __init kasan_set_multi_shot(char *str)
 {
-	if (addr_has_shadow(info))
-		return get_shadow_bug_type(info);
-	return get_wild_bug_type(info);
+	set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
+	return 1;
 }
+__setup("kasan_multi_shot", kasan_set_multi_shot);
 
-static void print_error_description(struct kasan_access_info *info)
+static void print_error_description(struct kasan_access_info *info,
+					const char *bug_type)
 {
-	const char *bug_type = get_bug_type(info);
-
 	pr_err("BUG: KASAN: %s in %pS\n",
 		bug_type, (void *)info->ip);
 	pr_err("%s of size %zu at addr %px by task %s/%d\n",
@@ -143,25 +74,9 @@ static void print_error_description(struct kasan_access_info *info)
 		info->access_addr, current->comm, task_pid_nr(current));
 }
 
-static inline bool kernel_or_module_addr(const void *addr)
-{
-	if (addr >= (void *)_stext && addr < (void *)_end)
-		return true;
-	if (is_module_address((unsigned long)addr))
-		return true;
-	return false;
-}
-
-static inline bool init_task_stack_addr(const void *addr)
-{
-	return addr >= (void *)&init_thread_union.stack &&
-		(addr <= (void *)&init_thread_union.stack +
-			sizeof(init_thread_union.stack));
-}
-
 static DEFINE_SPINLOCK(report_lock);
 
-static void kasan_start_report(unsigned long *flags)
+static void start_report(unsigned long *flags)
 {
 	/*
 	 * Make sure we don't end up in loop.
@@ -171,7 +86,7 @@ static void kasan_start_report(unsigned long *flags)
 	pr_err("==================================================================\n");
 }
 
-static void kasan_end_report(unsigned long *flags)
+static void end_report(unsigned long *flags)
 {
 	pr_err("==================================================================\n");
 	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
@@ -249,6 +164,22 @@ static void describe_object(struct kmem_cache *cache, void *object,
 	describe_object_addr(cache, object, addr);
 }
 
+static inline bool kernel_or_module_addr(const void *addr)
+{
+	if (addr >= (void *)_stext && addr < (void *)_end)
+		return true;
+	if (is_module_address((unsigned long)addr))
+		return true;
+	return false;
+}
+
+static inline bool init_task_stack_addr(const void *addr)
+{
+	return addr >= (void *)&init_thread_union.stack &&
+		(addr <= (void *)&init_thread_union.stack +
+			sizeof(init_thread_union.stack));
+}
+
 static void print_address_description(void *addr)
 {
 	struct page *page = addr_to_page(addr);
@@ -326,29 +257,38 @@ static void print_shadow_for_address(const void *addr)
 	}
 }
 
+static bool report_enabled(void)
+{
+	if (current->kasan_depth)
+		return false;
+	if (test_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags))
+		return true;
+	return !test_and_set_bit(KASAN_BIT_REPORTED, &kasan_flags);
+}
+
 void kasan_report_invalid_free(void *object, unsigned long ip)
 {
 	unsigned long flags;
 
-	kasan_start_report(&flags);
+	start_report(&flags);
 	pr_err("BUG: KASAN: double-free or invalid-free in %pS\n", (void *)ip);
 	pr_err("\n");
 	print_address_description(object);
 	pr_err("\n");
 	print_shadow_for_address(object);
-	kasan_end_report(&flags);
+	end_report(&flags);
 }
 
 static void kasan_report_error(struct kasan_access_info *info)
 {
 	unsigned long flags;
 
-	kasan_start_report(&flags);
+	start_report(&flags);
 
-	print_error_description(info);
+	print_error_description(info, get_bug_type(info));
 	pr_err("\n");
 
-	if (!addr_has_shadow(info)) {
+	if (!addr_has_shadow(info->access_addr)) {
 		dump_stack();
 	} else {
 		print_address_description((void *)info->access_addr);
@@ -356,41 +296,7 @@ static void kasan_report_error(struct kasan_access_info *info)
 		print_shadow_for_address(info->first_bad_addr);
 	}
 
-	kasan_end_report(&flags);
-}
-
-static unsigned long kasan_flags;
-
-#define KASAN_BIT_REPORTED	0
-#define KASAN_BIT_MULTI_SHOT	1
-
-bool kasan_save_enable_multi_shot(void)
-{
-	return test_and_set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
-}
-EXPORT_SYMBOL_GPL(kasan_save_enable_multi_shot);
-
-void kasan_restore_multi_shot(bool enabled)
-{
-	if (!enabled)
-		clear_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
-}
-EXPORT_SYMBOL_GPL(kasan_restore_multi_shot);
-
-static int __init kasan_set_multi_shot(char *str)
-{
-	set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
-	return 1;
-}
-__setup("kasan_multi_shot", kasan_set_multi_shot);
-
-static inline bool kasan_report_enabled(void)
-{
-	if (current->kasan_depth)
-		return false;
-	if (test_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags))
-		return true;
-	return !test_and_set_bit(KASAN_BIT_REPORTED, &kasan_flags);
+	end_report(&flags);
 }
 
 void kasan_report(unsigned long addr, size_t size,
@@ -398,7 +304,7 @@ void kasan_report(unsigned long addr, size_t size,
 {
 	struct kasan_access_info info;
 
-	if (likely(!kasan_report_enabled()))
+	if (likely(!report_enabled()))
 		return;
 
 	disable_trace_on_warning();
@@ -411,41 +317,3 @@ void kasan_report(unsigned long addr, size_t size,
 
 	kasan_report_error(&info);
 }
-
-
-#define DEFINE_ASAN_REPORT_LOAD(size)                     \
-void __asan_report_load##size##_noabort(unsigned long addr) \
-{                                                         \
-	kasan_report(addr, size, false, _RET_IP_);	  \
-}                                                         \
-EXPORT_SYMBOL(__asan_report_load##size##_noabort)
-
-#define DEFINE_ASAN_REPORT_STORE(size)                     \
-void __asan_report_store##size##_noabort(unsigned long addr) \
-{                                                          \
-	kasan_report(addr, size, true, _RET_IP_);	   \
-}                                                          \
-EXPORT_SYMBOL(__asan_report_store##size##_noabort)
-
-DEFINE_ASAN_REPORT_LOAD(1);
-DEFINE_ASAN_REPORT_LOAD(2);
-DEFINE_ASAN_REPORT_LOAD(4);
-DEFINE_ASAN_REPORT_LOAD(8);
-DEFINE_ASAN_REPORT_LOAD(16);
-DEFINE_ASAN_REPORT_STORE(1);
-DEFINE_ASAN_REPORT_STORE(2);
-DEFINE_ASAN_REPORT_STORE(4);
-DEFINE_ASAN_REPORT_STORE(8);
-DEFINE_ASAN_REPORT_STORE(16);
-
-void __asan_report_load_n_noabort(unsigned long addr, size_t size)
-{
-	kasan_report(addr, size, false, _RET_IP_);
-}
-EXPORT_SYMBOL(__asan_report_load_n_noabort);
-
-void __asan_report_store_n_noabort(unsigned long addr, size_t size)
-{
-	kasan_report(addr, size, true, _RET_IP_);
-}
-EXPORT_SYMBOL(__asan_report_store_n_noabort);
-- 
2.18.0.rc2.346.g013aa6912e-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v4 13/17] khwasan: add hooks implementation ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 26 Jun 2018 13:15:23 +0000
Message-ID: <a2a93370d43ec85b02abaf8d007a15b464212221.1530018818.git.andreyknvl () google ! com>
--------------------
This commit adds KHWASAN specific hooks implementation and adjusts
common KASAN and KHWASAN ones.

1. When a new slab cache is created, KHWASAN rounds up the size of the
   objects in this cache to KASAN_SHADOW_SCALE_SIZE (== 16).

2. On each kmalloc KHWASAN generates a random tag, sets the shadow memory,
   that corresponds to this object to this tag, and embeds this tag value
   into the top byte of the returned pointer.

3. On each kfree KHWASAN poisons the shadow memory with a random tag to
   allow detection of use-after-free bugs.

The rest of the logic of the hook implementation is very much similar to
the one provided by KASAN. KHWASAN saves allocation and free stack metadata
to the slab object the same was KASAN does this.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/common.c  | 83 +++++++++++++++++++++++++++++++++++-----------
 mm/kasan/kasan.h   |  8 +++++
 mm/kasan/khwasan.c | 40 ++++++++++++++++++++++
 3 files changed, 111 insertions(+), 20 deletions(-)

diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 656baa8984c7..1e96ca050c75 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -140,6 +140,9 @@ void kasan_poison_shadow(const void *address, size_t size, u8 value)
 {
 	void *shadow_start, *shadow_end;
 
+	/* Perform shadow offset calculation based on untagged address */
+	address = reset_tag(address);
+
 	shadow_start = kasan_mem_to_shadow(address);
 	shadow_end = kasan_mem_to_shadow(address + size);
 
@@ -148,11 +151,20 @@ void kasan_poison_shadow(const void *address, size_t size, u8 value)
 
 void kasan_unpoison_shadow(const void *address, size_t size)
 {
-	kasan_poison_shadow(address, size, 0);
+	u8 tag = get_tag(address);
+
+	/* Perform shadow offset calculation based on untagged address */
+	address = reset_tag(address);
+
+	kasan_poison_shadow(address, size, tag);
 
 	if (size & KASAN_SHADOW_MASK) {
 		u8 *shadow = (u8 *)kasan_mem_to_shadow(address + size);
-		*shadow = size & KASAN_SHADOW_MASK;
+
+		if (IS_ENABLED(CONFIG_KASAN_HW))
+			*shadow = tag;
+		else
+			*shadow = size & KASAN_SHADOW_MASK;
 	}
 }
 
@@ -200,8 +212,9 @@ void kasan_unpoison_stack_above_sp_to(const void *watermark)
 
 void kasan_alloc_pages(struct page *page, unsigned int order)
 {
-	if (likely(!PageHighMem(page)))
-		kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
+	if (unlikely(PageHighMem(page)))
+		return;
+	kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
 }
 
 void kasan_free_pages(struct page *page, unsigned int order)
@@ -235,6 +248,7 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 			slab_flags_t *flags)
 {
 	unsigned int orig_size = *size;
+	unsigned int redzone_size = 0;
 	int redzone_adjust;
 
 	/* Add alloc meta. */
@@ -242,20 +256,20 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 	*size += sizeof(struct kasan_alloc_meta);
 
 	/* Add free meta. */
-	if (cache->flags & SLAB_TYPESAFE_BY_RCU || cache->ctor ||
-	    cache->object_size < sizeof(struct kasan_free_meta)) {
+	if (IS_ENABLED(CONFIG_KASAN_GENERIC) &&
+	    (cache->flags & SLAB_TYPESAFE_BY_RCU || cache->ctor ||
+	     cache->object_size < sizeof(struct kasan_free_meta))) {
 		cache->kasan_info.free_meta_offset = *size;
 		*size += sizeof(struct kasan_free_meta);
 	}
-	redzone_adjust = optimal_redzone(cache->object_size) -
-		(*size - cache->object_size);
 
+	redzone_size = optimal_redzone(cache->object_size);
+	redzone_adjust = redzone_size -	(*size - cache->object_size);
 	if (redzone_adjust > 0)
 		*size += redzone_adjust;
 
 	*size = min_t(unsigned int, KMALLOC_MAX_SIZE,
-			max(*size, cache->object_size +
-					optimal_redzone(cache->object_size)));
+			max(*size, cache->object_size + redzone_size));
 
 	/*
 	 * If the metadata doesn't fit, don't enable KASAN at all.
@@ -268,6 +282,8 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 		return;
 	}
 
+	cache->align = round_up(cache->align, KASAN_SHADOW_SCALE_SIZE);
+
 	*flags |= SLAB_KASAN;
 }
 
@@ -325,18 +341,41 @@ void kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
 
 void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t flags)
 {
-	return kasan_kmalloc(cache, object, cache->object_size, flags);
+	object = kasan_kmalloc(cache, object, cache->object_size, flags);
+	if (IS_ENABLED(CONFIG_KASAN_HW) && unlikely(cache->ctor)) {
+		/*
+		 * Cache constructor might use object's pointer value to
+		 * initialize some of its fields.
+		 */
+		cache->ctor(object);
+	}
+	return object;
+}
+
+static inline bool shadow_invalid(u8 tag, s8 shadow_byte)
+{
+        if (IS_ENABLED(CONFIG_KASAN_GENERIC))
+                return shadow_byte < 0 ||
+			shadow_byte >= KASAN_SHADOW_SCALE_SIZE;
+        else
+                return tag != (u8)shadow_byte;
 }
 
 static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
 			      unsigned long ip, bool quarantine)
 {
 	s8 shadow_byte;
+	u8 tag;
+	void *tagged_object;
 	unsigned long rounded_up_size;
 
+	tag = get_tag(object);
+	tagged_object = object;
+	object = reset_tag(object);
+
 	if (unlikely(nearest_obj(cache, virt_to_head_page(object), object) !=
 	    object)) {
-		kasan_report_invalid_free(object, ip);
+		kasan_report_invalid_free(tagged_object, ip);
 		return true;
 	}
 
@@ -345,20 +384,22 @@ static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
 		return false;
 
 	shadow_byte = READ_ONCE(*(s8 *)kasan_mem_to_shadow(object));
-	if (shadow_byte < 0 || shadow_byte >= KASAN_SHADOW_SCALE_SIZE) {
-		kasan_report_invalid_free(object, ip);
+	if (shadow_invalid(tag, shadow_byte)) {
+		kasan_report_invalid_free(tagged_object, ip);
 		return true;
 	}
 
 	rounded_up_size = round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE);
 	kasan_poison_shadow(object, rounded_up_size, KASAN_KMALLOC_FREE);
 
-	if (!quarantine || unlikely(!(cache->flags & SLAB_KASAN)))
+	if ((IS_ENABLED(CONFIG_KASAN_GENERIC) && !quarantine) ||
+			unlikely(!(cache->flags & SLAB_KASAN)))
 		return false;
 
 	set_track(&get_alloc_info(cache, object)->free_track, GFP_NOWAIT);
 	quarantine_put(get_free_info(cache, object), cache);
-	return true;
+
+	return IS_ENABLED(CONFIG_KASAN_GENERIC);
 }
 
 bool kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
@@ -371,6 +412,7 @@ void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 {
 	unsigned long redzone_start;
 	unsigned long redzone_end;
+	u8 tag;
 
 	if (gfpflags_allow_blocking(flags))
 		quarantine_reduce();
@@ -383,14 +425,15 @@ void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 	redzone_end = round_up((unsigned long)object + cache->object_size,
 				KASAN_SHADOW_SCALE_SIZE);
 
-	kasan_unpoison_shadow(object, size);
+	tag = random_tag();
+	kasan_unpoison_shadow(set_tag(object, tag), size);
 	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
 		KASAN_KMALLOC_REDZONE);
 
 	if (cache->flags & SLAB_KASAN)
 		set_track(&get_alloc_info(cache, object)->alloc_track, flags);
 
-	return (void *)object;
+	return set_tag(object, tag);
 }
 EXPORT_SYMBOL(kasan_kmalloc);
 
@@ -440,7 +483,7 @@ void kasan_poison_kfree(void *ptr, unsigned long ip)
 	page = virt_to_head_page(ptr);
 
 	if (unlikely(!PageSlab(page))) {
-		if (ptr != page_address(page)) {
+		if (reset_tag(ptr) != page_address(page)) {
 			kasan_report_invalid_free(ptr, ip);
 			return;
 		}
@@ -453,7 +496,7 @@ void kasan_poison_kfree(void *ptr, unsigned long ip)
 
 void kasan_kfree_large(void *ptr, unsigned long ip)
 {
-	if (ptr != page_address(virt_to_head_page(ptr)))
+	if (reset_tag(ptr) != page_address(virt_to_head_page(ptr)))
 		kasan_report_invalid_free(ptr, ip);
 	/* The object will be poisoned by page_alloc. */
 }
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index d60859d26be7..6f4f2ebf5f57 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -12,10 +12,18 @@
 #define KHWASAN_TAG_INVALID	0xFE /* inaccessible memory tag */
 #define KHWASAN_TAG_MAX		0xFD /* maximum value for random tags */
 
+#ifdef CONFIG_KASAN_GENERIC
 #define KASAN_FREE_PAGE         0xFF  /* page was freed */
 #define KASAN_PAGE_REDZONE      0xFE  /* redzone for kmalloc_large allocations */
 #define KASAN_KMALLOC_REDZONE   0xFC  /* redzone inside slub object */
 #define KASAN_KMALLOC_FREE      0xFB  /* object was freed (kmem_cache_free/kfree) */
+#else
+#define KASAN_FREE_PAGE         KHWASAN_TAG_INVALID
+#define KASAN_PAGE_REDZONE      KHWASAN_TAG_INVALID
+#define KASAN_KMALLOC_REDZONE   KHWASAN_TAG_INVALID
+#define KASAN_KMALLOC_FREE      KHWASAN_TAG_INVALID
+#endif
+
 #define KASAN_GLOBAL_REDZONE    0xFA  /* redzone for global variable */
 
 /*
diff --git a/mm/kasan/khwasan.c b/mm/kasan/khwasan.c
index d34679b8f8c7..fd1725022794 100644
--- a/mm/kasan/khwasan.c
+++ b/mm/kasan/khwasan.c
@@ -88,15 +88,52 @@ void *khwasan_reset_tag(const void *addr)
 void check_memory_region(unsigned long addr, size_t size, bool write,
 				unsigned long ret_ip)
 {
+	u8 tag;
+	u8 *shadow_first, *shadow_last, *shadow;
+	void *untagged_addr;
+
+	tag = get_tag((const void *)addr);
+
+	/* Ignore accesses for pointers tagged with 0xff (native kernel
+	 * pointer tag) to suppress false positives caused by kmap.
+	 *
+	 * Some kernel code was written to account for archs that don't keep
+	 * high memory mapped all the time, but rather map and unmap particular
+	 * pages when needed. Instead of storing a pointer to the kernel memory,
+	 * this code saves the address of the page structure and offset within
+	 * that page for later use. Those pages are then mapped and unmapped
+	 * with kmap/kunmap when necessary and virt_to_page is used to get the
+	 * virtual address of the page. For arm64 (that keeps the high memory
+	 * mapped all the time), kmap is turned into a page_address call.
+
+	 * The issue is that with use of the page_address + virt_to_page
+	 * sequence the top byte value of the original pointer gets lost (gets
+	 * set to KHWASAN_TAG_KERNEL (0xFF).
+	 */
+	if (tag == KHWASAN_TAG_KERNEL)
+		return;
+
+	untagged_addr = reset_tag((const void *)addr);
+	shadow_first = kasan_mem_to_shadow(untagged_addr);
+	shadow_last = kasan_mem_to_shadow(untagged_addr + size - 1);
+
+	for (shadow = shadow_first; shadow <= shadow_last; shadow++) {
+		if (*shadow != tag) {
+			kasan_report(addr, size, write, ret_ip);
+			return;
+		}
+	}
 }
 
 #define DEFINE_HWASAN_LOAD_STORE(size)					\
 	void __hwasan_load##size##_noabort(unsigned long addr)		\
 	{								\
+		check_memory_region(addr, size, false, _RET_IP_);	\
 	}								\
 	EXPORT_SYMBOL(__hwasan_load##size##_noabort);			\
 	void __hwasan_store##size##_noabort(unsigned long addr)		\
 	{								\
+		check_memory_region(addr, size, true, _RET_IP_);	\
 	}								\
 	EXPORT_SYMBOL(__hwasan_store##size##_noabort)
 
@@ -108,15 +145,18 @@ DEFINE_HWASAN_LOAD_STORE(16);
 
 void __hwasan_loadN_noabort(unsigned long addr, unsigned long size)
 {
+	check_memory_region(addr, size, false, _RET_IP_);
 }
 EXPORT_SYMBOL(__hwasan_loadN_noabort);
 
 void __hwasan_storeN_noabort(unsigned long addr, unsigned long size)
 {
+	check_memory_region(addr, size, true, _RET_IP_);
 }
 EXPORT_SYMBOL(__hwasan_storeN_noabort);
 
 void __hwasan_tag_memory(unsigned long addr, u8 tag, unsigned long size)
 {
+	kasan_poison_shadow((void *)addr, size, tag);
 }
 EXPORT_SYMBOL(__hwasan_tag_memory);
-- 
2.18.0.rc2.346.g013aa6912e-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Christopher Lameter <cl () linux ! com>
To: linux-sparse
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 15:38:13 +0000
Message-ID: <01000164f0fd5abc-df9ea911-9701-498c-adce-9f833e6df3ed-000000 () email ! amazonses ! com>
--------------------
On Tue, 31 Jul 2018, Dmitry Vyukov wrote:

> > Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
> > are few without constructors.
> > We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>
> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
> slabs can be useful without ctors or at least memset(0). Objects in
> such slabs need to be type-stable, but I can't understand how it's
> possible to establish type stability without a ctor... Are these bugs?
> Or I am missing something subtle? What would be a canonical usage of
> SLAB_TYPESAFE_BY_RCU slab without a ctor?

True that sounds fishy. Would someone post a list of SLAB_TYPESAFE_BY_RCU
slabs without ctors?

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Christopher Lameter <cl () linux ! com>
To: linux-mm
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 15:38:13 +0000
Message-ID: <01000164f0fd5abc-df9ea911-9701-498c-adce-9f833e6df3ed-000000 () email ! amazonses ! com>
--------------------
On Tue, 31 Jul 2018, Dmitry Vyukov wrote:

> > Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
> > are few without constructors.
> > We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>
> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
> slabs can be useful without ctors or at least memset(0). Objects in
> such slabs need to be type-stable, but I can't understand how it's
> possible to establish type stability without a ctor... Are these bugs?
> Or I am missing something subtle? What would be a canonical usage of
> SLAB_TYPESAFE_BY_RCU slab without a ctor?

True that sounds fishy. Would someone post a list of SLAB_TYPESAFE_BY_RCU
slabs without ctors?

================================================================================

From: Christopher Lameter <cl () linux ! com>
To: linux-kbuild
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 15:38:13 +0000
Message-ID: <01000164f0fd5abc-df9ea911-9701-498c-adce-9f833e6df3ed-000000 () email ! amazonses ! com>
--------------------
On Tue, 31 Jul 2018, Dmitry Vyukov wrote:

> > Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
> > are few without constructors.
> > We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>
> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
> slabs can be useful without ctors or at least memset(0). Objects in
> such slabs need to be type-stable, but I can't understand how it's
> possible to establish type stability without a ctor... Are these bugs?
> Or I am missing something subtle? What would be a canonical usage of
> SLAB_TYPESAFE_BY_RCU slab without a ctor?

True that sounds fishy. Would someone post a list of SLAB_TYPESAFE_BY_RCU
slabs without ctors?

--
To unsubscribe from this list: send the line "unsubscribe linux-kbuild" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Christopher Lameter <cl () linux ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 15:38:13 +0000
Message-ID: <01000164f0fd5abc-df9ea911-9701-498c-adce-9f833e6df3ed-000000 () email ! amazonses ! com>
--------------------
On Tue, 31 Jul 2018, Dmitry Vyukov wrote:

> > Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
> > are few without constructors.
> > We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>
> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
> slabs can be useful without ctors or at least memset(0). Objects in
> such slabs need to be type-stable, but I can't understand how it's
> possible to establish type stability without a ctor... Are these bugs?
> Or I am missing something subtle? What would be a canonical usage of
> SLAB_TYPESAFE_BY_RCU slab without a ctor?

True that sounds fishy. Would someone post a list of SLAB_TYPESAFE_BY_RCU
slabs without ctors?


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Christopher Lameter <cl () linux ! com>
To: linux-kernel
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 15:38:13 +0000
Message-ID: <01000164f0fd5abc-df9ea911-9701-498c-adce-9f833e6df3ed-000000 () email ! amazonses ! com>
--------------------
On Tue, 31 Jul 2018, Dmitry Vyukov wrote:

> > Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
> > are few without constructors.
> > We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>
> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
> slabs can be useful without ctors or at least memset(0). Objects in
> such slabs need to be type-stable, but I can't understand how it's
> possible to establish type stability without a ctor... Are these bugs?
> Or I am missing something subtle? What would be a canonical usage of
> SLAB_TYPESAFE_BY_RCU slab without a ctor?

True that sounds fishy. Would someone post a list of SLAB_TYPESAFE_BY_RCU
slabs without ctors?

================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-kernel
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:03:16 +0000
Message-ID: <CACT4Y+b6_d6b5Q4oScXuoXhsnkJPG+UZ1QxqPPGuM4_aU3n4mA () mail ! gmail ! com>
--------------------
On Tue, Jul 31, 2018 at 5:38 PM, Christopher Lameter <cl@linux.com> wrote:
> On Tue, 31 Jul 2018, Dmitry Vyukov wrote:
>
>> > Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>> > are few without constructors.
>> > We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>>
>> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
>> slabs can be useful without ctors or at least memset(0). Objects in
>> such slabs need to be type-stable, but I can't understand how it's
>> possible to establish type stability without a ctor... Are these bugs?
>> Or I am missing something subtle? What would be a canonical usage of
>> SLAB_TYPESAFE_BY_RCU slab without a ctor?
>
> True that sounds fishy. Would someone post a list of SLAB_TYPESAFE_BY_RCU
> slabs without ctors?

https://elixir.bootlin.com/linux/latest/source/fs/jbd2/journal.c#L2395
https://elixir.bootlin.com/linux/latest/source/fs/kernfs/mount.c#L415
https://elixir.bootlin.com/linux/latest/source/net/netfilter/nf_conntrack_core.c#L2065
https://elixir.bootlin.com/linux/latest/source/drivers/gpu/drm/i915/i915_gem.c#L5501
https://elixir.bootlin.com/linux/latest/source/drivers/gpu/drm/i915/selftests/mock_gem_device.c#L212
https://elixir.bootlin.com/linux/latest/source/drivers/staging/lustre/lustre/ldlm/ldlm_lockd.c#L1131

Also these in proto structs:
https://elixir.bootlin.com/linux/latest/source/net/dccp/ipv4.c#L959
https://elixir.bootlin.com/linux/latest/source/net/dccp/ipv6.c#L1048
https://elixir.bootlin.com/linux/latest/source/net/ipv4/tcp_ipv4.c#L2461
https://elixir.bootlin.com/linux/latest/source/net/ipv6/tcp_ipv6.c#L1980
https://elixir.bootlin.com/linux/latest/source/net/llc/af_llc.c#L145
https://elixir.bootlin.com/linux/latest/source/net/smc/af_smc.c#L105

They later created in net/core/sock.c without ctor.
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-doc
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:03:16 +0000
Message-ID: <CACT4Y+b6_d6b5Q4oScXuoXhsnkJPG+UZ1QxqPPGuM4_aU3n4mA () mail ! gmail ! com>
--------------------
On Tue, Jul 31, 2018 at 5:38 PM, Christopher Lameter <cl@linux.com> wrote:
> On Tue, 31 Jul 2018, Dmitry Vyukov wrote:
>
>> > Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>> > are few without constructors.
>> > We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>>
>> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
>> slabs can be useful without ctors or at least memset(0). Objects in
>> such slabs need to be type-stable, but I can't understand how it's
>> possible to establish type stability without a ctor... Are these bugs?
>> Or I am missing something subtle? What would be a canonical usage of
>> SLAB_TYPESAFE_BY_RCU slab without a ctor?
>
> True that sounds fishy. Would someone post a list of SLAB_TYPESAFE_BY_RCU
> slabs without ctors?

https://elixir.bootlin.com/linux/latest/source/fs/jbd2/journal.c#L2395
https://elixir.bootlin.com/linux/latest/source/fs/kernfs/mount.c#L415
https://elixir.bootlin.com/linux/latest/source/net/netfilter/nf_conntrack_core.c#L2065
https://elixir.bootlin.com/linux/latest/source/drivers/gpu/drm/i915/i915_gem.c#L5501
https://elixir.bootlin.com/linux/latest/source/drivers/gpu/drm/i915/selftests/mock_gem_device.c#L212
https://elixir.bootlin.com/linux/latest/source/drivers/staging/lustre/lustre/ldlm/ldlm_lockd.c#L1131

Also these in proto structs:
https://elixir.bootlin.com/linux/latest/source/net/dccp/ipv4.c#L959
https://elixir.bootlin.com/linux/latest/source/net/dccp/ipv6.c#L1048
https://elixir.bootlin.com/linux/latest/source/net/ipv4/tcp_ipv4.c#L2461
https://elixir.bootlin.com/linux/latest/source/net/ipv6/tcp_ipv6.c#L1980
https://elixir.bootlin.com/linux/latest/source/net/llc/af_llc.c#L145
https://elixir.bootlin.com/linux/latest/source/net/smc/af_smc.c#L105

They later created in net/core/sock.c without ctor.
--
To unsubscribe from this list: send the line "unsubscribe linux-doc" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-sparse
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:03:16 +0000
Message-ID: <CACT4Y+b6_d6b5Q4oScXuoXhsnkJPG+UZ1QxqPPGuM4_aU3n4mA () mail ! gmail ! com>
--------------------
On Tue, Jul 31, 2018 at 5:38 PM, Christopher Lameter <cl@linux.com> wrote:
> On Tue, 31 Jul 2018, Dmitry Vyukov wrote:
>
>> > Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>> > are few without constructors.
>> > We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>>
>> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
>> slabs can be useful without ctors or at least memset(0). Objects in
>> such slabs need to be type-stable, but I can't understand how it's
>> possible to establish type stability without a ctor... Are these bugs?
>> Or I am missing something subtle? What would be a canonical usage of
>> SLAB_TYPESAFE_BY_RCU slab without a ctor?
>
> True that sounds fishy. Would someone post a list of SLAB_TYPESAFE_BY_RCU
> slabs without ctors?

https://elixir.bootlin.com/linux/latest/source/fs/jbd2/journal.c#L2395
https://elixir.bootlin.com/linux/latest/source/fs/kernfs/mount.c#L415
https://elixir.bootlin.com/linux/latest/source/net/netfilter/nf_conntrack_core.c#L2065
https://elixir.bootlin.com/linux/latest/source/drivers/gpu/drm/i915/i915_gem.c#L5501
https://elixir.bootlin.com/linux/latest/source/drivers/gpu/drm/i915/selftests/mock_gem_device.c#L212
https://elixir.bootlin.com/linux/latest/source/drivers/staging/lustre/lustre/ldlm/ldlm_lockd.c#L1131

Also these in proto structs:
https://elixir.bootlin.com/linux/latest/source/net/dccp/ipv4.c#L959
https://elixir.bootlin.com/linux/latest/source/net/dccp/ipv6.c#L1048
https://elixir.bootlin.com/linux/latest/source/net/ipv4/tcp_ipv4.c#L2461
https://elixir.bootlin.com/linux/latest/source/net/ipv6/tcp_ipv6.c#L1980
https://elixir.bootlin.com/linux/latest/source/net/llc/af_llc.c#L145
https://elixir.bootlin.com/linux/latest/source/net/smc/af_smc.c#L105

They later created in net/core/sock.c without ctor.
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-mm
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:03:16 +0000
Message-ID: <CACT4Y+b6_d6b5Q4oScXuoXhsnkJPG+UZ1QxqPPGuM4_aU3n4mA () mail ! gmail ! com>
--------------------
On Tue, Jul 31, 2018 at 5:38 PM, Christopher Lameter <cl@linux.com> wrote:
> On Tue, 31 Jul 2018, Dmitry Vyukov wrote:
>
>> > Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>> > are few without constructors.
>> > We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>>
>> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
>> slabs can be useful without ctors or at least memset(0). Objects in
>> such slabs need to be type-stable, but I can't understand how it's
>> possible to establish type stability without a ctor... Are these bugs?
>> Or I am missing something subtle? What would be a canonical usage of
>> SLAB_TYPESAFE_BY_RCU slab without a ctor?
>
> True that sounds fishy. Would someone post a list of SLAB_TYPESAFE_BY_RCU
> slabs without ctors?

https://elixir.bootlin.com/linux/latest/source/fs/jbd2/journal.c#L2395
https://elixir.bootlin.com/linux/latest/source/fs/kernfs/mount.c#L415
https://elixir.bootlin.com/linux/latest/source/net/netfilter/nf_conntrack_core.c#L2065
https://elixir.bootlin.com/linux/latest/source/drivers/gpu/drm/i915/i915_gem.c#L5501
https://elixir.bootlin.com/linux/latest/source/drivers/gpu/drm/i915/selftests/mock_gem_device.c#L212
https://elixir.bootlin.com/linux/latest/source/drivers/staging/lustre/lustre/ldlm/ldlm_lockd.c#L1131

Also these in proto structs:
https://elixir.bootlin.com/linux/latest/source/net/dccp/ipv4.c#L959
https://elixir.bootlin.com/linux/latest/source/net/dccp/ipv6.c#L1048
https://elixir.bootlin.com/linux/latest/source/net/ipv4/tcp_ipv4.c#L2461
https://elixir.bootlin.com/linux/latest/source/net/ipv6/tcp_ipv6.c#L1980
https://elixir.bootlin.com/linux/latest/source/net/llc/af_llc.c#L145
https://elixir.bootlin.com/linux/latest/source/net/smc/af_smc.c#L105

They later created in net/core/sock.c without ctor.

================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:04:24 +0000
Message-ID: <b6b58786-85c9-e831-5571-58b5580c84ba () virtuozzo ! com>
--------------------


On 07/31/2018 06:03 PM, Dmitry Vyukov wrote:
> On Tue, Jul 31, 2018 at 4:50 PM, Andrey Ryabinin
> <aryabinin@virtuozzo.com> wrote:
>>
>>
>> On 07/31/2018 04:05 PM, Andrey Konovalov wrote:
>>> On Wed, Jul 25, 2018 at 3:44 PM, Vincenzo Frascino@Foss
>>> <vincenzo.frascino@arm.com> wrote:
>>>> On 06/26/2018 02:15 PM, Andrey Konovalov wrote:
>>>>
>>>>> @@ -325,18 +341,41 @@ void kasan_init_slab_obj(struct kmem_cache *cache,
>>>>> const void *object)
>>>>>     void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t
>>>>> flags)
>>>>>   {
>>>>> -       return kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>> +       object = kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>> +       if (IS_ENABLED(CONFIG_KASAN_HW) && unlikely(cache->ctor)) {
>>>>> +               /*
>>>>> +                * Cache constructor might use object's pointer value to
>>>>> +                * initialize some of its fields.
>>>>> +                */
>>>>> +               cache->ctor(object);
>>>>>
>>>> This seams breaking the kmem_cache_create() contract: "The @ctor is run when
>>>> new pages are allocated by the cache."
>>>> (https://elixir.bootlin.com/linux/v3.7/source/mm/slab_common.c#L83)
>>>>
>>>> Since there might be preexisting code relying on it, this could lead to
>>>> global side effects. Did you verify that this is not the case?
>>>>
>>>> Another concern is performance related if we consider this solution suitable
>>>> for "near-production", since with the current implementation you call the
>>>> ctor (where present) on an object multiple times and this ends up memsetting
>>>> and repopulating the memory every time (i.e. inode.c: inode_init_once). Do
>>>> you know what is the performance impact?
>>>
>>> We can assign tags to objects with constructors when a slab is
>>> allocated and call constructors once as usual. The downside is that
>>> such object would always have the same tag when it is reallocated, so
>>> we won't catch use-after-frees.
>>
>> Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>> are few without constructors.
>> We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
> 
> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
> slabs can be useful without ctors or at least memset(0). Objects in
> such slabs need to be type-stable, but I can't understand how it's
> possible to establish type stability without a ctor... Are these bugs?

Yeah, I puzzled by this too. However, I think it's hard but possible to make it work, at least in theory.
There must be an initializer, which consists of two parts:
a) initilize objects fields
b) expose object to the world (add it to list or something like that)

(a) part must somehow to be ok to race with another cpu which might already use the object.
(b) part must must use e.g. barriers to make sure that racy users will see previously inilized fields.
Racy users must have parring barrier of course.

But it sound fishy, and very easy to fuck up. I won't be surprised if every single one SLAB_TYPESAFE_BY_RCU user
without ->ctor is bogus. It certainly would be better to convert those to use ->ctor.

Such caches seems used by networking subsystem in proto_register():

		prot->slab = kmem_cache_create_usercopy(prot->name,
					prot->obj_size, 0,
					SLAB_HWCACHE_ALIGN | SLAB_ACCOUNT |
					prot->slab_flags,
					prot->useroffset, prot->usersize,
					NULL);

And certain protocols specify SLAB_TYPESAFE_BY_RCU in ->slab_flags, such as:
llc_proto, smc_proto, smc_proto6, tcp_prot, tcpv6_prot, dccp_v6_prot, dccp_v4_prot.


Also nf_conntrack_cachep, kernfs_node_cache, jbd2_journal_head_cache and i915_request cache.



_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-kbuild
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:04:24 +0000
Message-ID: <b6b58786-85c9-e831-5571-58b5580c84ba () virtuozzo ! com>
--------------------


On 07/31/2018 06:03 PM, Dmitry Vyukov wrote:
> On Tue, Jul 31, 2018 at 4:50 PM, Andrey Ryabinin
> <aryabinin@virtuozzo.com> wrote:
>>
>>
>> On 07/31/2018 04:05 PM, Andrey Konovalov wrote:
>>> On Wed, Jul 25, 2018 at 3:44 PM, Vincenzo Frascino@Foss
>>> <vincenzo.frascino@arm.com> wrote:
>>>> On 06/26/2018 02:15 PM, Andrey Konovalov wrote:
>>>>
>>>>> @@ -325,18 +341,41 @@ void kasan_init_slab_obj(struct kmem_cache *cache,
>>>>> const void *object)
>>>>>     void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t
>>>>> flags)
>>>>>   {
>>>>> -       return kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>> +       object = kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>> +       if (IS_ENABLED(CONFIG_KASAN_HW) && unlikely(cache->ctor)) {
>>>>> +               /*
>>>>> +                * Cache constructor might use object's pointer value to
>>>>> +                * initialize some of its fields.
>>>>> +                */
>>>>> +               cache->ctor(object);
>>>>>
>>>> This seams breaking the kmem_cache_create() contract: "The @ctor is run when
>>>> new pages are allocated by the cache."
>>>> (https://elixir.bootlin.com/linux/v3.7/source/mm/slab_common.c#L83)
>>>>
>>>> Since there might be preexisting code relying on it, this could lead to
>>>> global side effects. Did you verify that this is not the case?
>>>>
>>>> Another concern is performance related if we consider this solution suitable
>>>> for "near-production", since with the current implementation you call the
>>>> ctor (where present) on an object multiple times and this ends up memsetting
>>>> and repopulating the memory every time (i.e. inode.c: inode_init_once). Do
>>>> you know what is the performance impact?
>>>
>>> We can assign tags to objects with constructors when a slab is
>>> allocated and call constructors once as usual. The downside is that
>>> such object would always have the same tag when it is reallocated, so
>>> we won't catch use-after-frees.
>>
>> Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>> are few without constructors.
>> We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
> 
> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
> slabs can be useful without ctors or at least memset(0). Objects in
> such slabs need to be type-stable, but I can't understand how it's
> possible to establish type stability without a ctor... Are these bugs?

Yeah, I puzzled by this too. However, I think it's hard but possible to make it work, at least in theory.
There must be an initializer, which consists of two parts:
a) initilize objects fields
b) expose object to the world (add it to list or something like that)

(a) part must somehow to be ok to race with another cpu which might already use the object.
(b) part must must use e.g. barriers to make sure that racy users will see previously inilized fields.
Racy users must have parring barrier of course.

But it sound fishy, and very easy to fuck up. I won't be surprised if every single one SLAB_TYPESAFE_BY_RCU user
without ->ctor is bogus. It certainly would be better to convert those to use ->ctor.

Such caches seems used by networking subsystem in proto_register():

		prot->slab = kmem_cache_create_usercopy(prot->name,
					prot->obj_size, 0,
					SLAB_HWCACHE_ALIGN | SLAB_ACCOUNT |
					prot->slab_flags,
					prot->useroffset, prot->usersize,
					NULL);

And certain protocols specify SLAB_TYPESAFE_BY_RCU in ->slab_flags, such as:
llc_proto, smc_proto, smc_proto6, tcp_prot, tcpv6_prot, dccp_v6_prot, dccp_v4_prot.


Also nf_conntrack_cachep, kernfs_node_cache, jbd2_journal_head_cache and i915_request cache.


--
To unsubscribe from this list: send the line "unsubscribe linux-kbuild" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-kernel
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:04:24 +0000
Message-ID: <b6b58786-85c9-e831-5571-58b5580c84ba () virtuozzo ! com>
--------------------


On 07/31/2018 06:03 PM, Dmitry Vyukov wrote:
> On Tue, Jul 31, 2018 at 4:50 PM, Andrey Ryabinin
> <aryabinin@virtuozzo.com> wrote:
>>
>>
>> On 07/31/2018 04:05 PM, Andrey Konovalov wrote:
>>> On Wed, Jul 25, 2018 at 3:44 PM, Vincenzo Frascino@Foss
>>> <vincenzo.frascino@arm.com> wrote:
>>>> On 06/26/2018 02:15 PM, Andrey Konovalov wrote:
>>>>
>>>>> @@ -325,18 +341,41 @@ void kasan_init_slab_obj(struct kmem_cache *cache,
>>>>> const void *object)
>>>>>     void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t
>>>>> flags)
>>>>>   {
>>>>> -       return kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>> +       object = kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>> +       if (IS_ENABLED(CONFIG_KASAN_HW) && unlikely(cache->ctor)) {
>>>>> +               /*
>>>>> +                * Cache constructor might use object's pointer value to
>>>>> +                * initialize some of its fields.
>>>>> +                */
>>>>> +               cache->ctor(object);
>>>>>
>>>> This seams breaking the kmem_cache_create() contract: "The @ctor is run when
>>>> new pages are allocated by the cache."
>>>> (https://elixir.bootlin.com/linux/v3.7/source/mm/slab_common.c#L83)
>>>>
>>>> Since there might be preexisting code relying on it, this could lead to
>>>> global side effects. Did you verify that this is not the case?
>>>>
>>>> Another concern is performance related if we consider this solution suitable
>>>> for "near-production", since with the current implementation you call the
>>>> ctor (where present) on an object multiple times and this ends up memsetting
>>>> and repopulating the memory every time (i.e. inode.c: inode_init_once). Do
>>>> you know what is the performance impact?
>>>
>>> We can assign tags to objects with constructors when a slab is
>>> allocated and call constructors once as usual. The downside is that
>>> such object would always have the same tag when it is reallocated, so
>>> we won't catch use-after-frees.
>>
>> Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>> are few without constructors.
>> We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
> 
> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
> slabs can be useful without ctors or at least memset(0). Objects in
> such slabs need to be type-stable, but I can't understand how it's
> possible to establish type stability without a ctor... Are these bugs?

Yeah, I puzzled by this too. However, I think it's hard but possible to make it work, at least in theory.
There must be an initializer, which consists of two parts:
a) initilize objects fields
b) expose object to the world (add it to list or something like that)

(a) part must somehow to be ok to race with another cpu which might already use the object.
(b) part must must use e.g. barriers to make sure that racy users will see previously inilized fields.
Racy users must have parring barrier of course.

But it sound fishy, and very easy to fuck up. I won't be surprised if every single one SLAB_TYPESAFE_BY_RCU user
without ->ctor is bogus. It certainly would be better to convert those to use ->ctor.

Such caches seems used by networking subsystem in proto_register():

		prot->slab = kmem_cache_create_usercopy(prot->name,
					prot->obj_size, 0,
					SLAB_HWCACHE_ALIGN | SLAB_ACCOUNT |
					prot->slab_flags,
					prot->useroffset, prot->usersize,
					NULL);

And certain protocols specify SLAB_TYPESAFE_BY_RCU in ->slab_flags, such as:
llc_proto, smc_proto, smc_proto6, tcp_prot, tcpv6_prot, dccp_v6_prot, dccp_v4_prot.


Also nf_conntrack_cachep, kernfs_node_cache, jbd2_journal_head_cache and i915_request cache.


================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-doc
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:04:24 +0000
Message-ID: <b6b58786-85c9-e831-5571-58b5580c84ba () virtuozzo ! com>
--------------------


On 07/31/2018 06:03 PM, Dmitry Vyukov wrote:
> On Tue, Jul 31, 2018 at 4:50 PM, Andrey Ryabinin
> <aryabinin@virtuozzo.com> wrote:
>>
>>
>> On 07/31/2018 04:05 PM, Andrey Konovalov wrote:
>>> On Wed, Jul 25, 2018 at 3:44 PM, Vincenzo Frascino@Foss
>>> <vincenzo.frascino@arm.com> wrote:
>>>> On 06/26/2018 02:15 PM, Andrey Konovalov wrote:
>>>>
>>>>> @@ -325,18 +341,41 @@ void kasan_init_slab_obj(struct kmem_cache *cache,
>>>>> const void *object)
>>>>>     void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t
>>>>> flags)
>>>>>   {
>>>>> -       return kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>> +       object = kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>> +       if (IS_ENABLED(CONFIG_KASAN_HW) && unlikely(cache->ctor)) {
>>>>> +               /*
>>>>> +                * Cache constructor might use object's pointer value to
>>>>> +                * initialize some of its fields.
>>>>> +                */
>>>>> +               cache->ctor(object);
>>>>>
>>>> This seams breaking the kmem_cache_create() contract: "The @ctor is run when
>>>> new pages are allocated by the cache."
>>>> (https://elixir.bootlin.com/linux/v3.7/source/mm/slab_common.c#L83)
>>>>
>>>> Since there might be preexisting code relying on it, this could lead to
>>>> global side effects. Did you verify that this is not the case?
>>>>
>>>> Another concern is performance related if we consider this solution suitable
>>>> for "near-production", since with the current implementation you call the
>>>> ctor (where present) on an object multiple times and this ends up memsetting
>>>> and repopulating the memory every time (i.e. inode.c: inode_init_once). Do
>>>> you know what is the performance impact?
>>>
>>> We can assign tags to objects with constructors when a slab is
>>> allocated and call constructors once as usual. The downside is that
>>> such object would always have the same tag when it is reallocated, so
>>> we won't catch use-after-frees.
>>
>> Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>> are few without constructors.
>> We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
> 
> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
> slabs can be useful without ctors or at least memset(0). Objects in
> such slabs need to be type-stable, but I can't understand how it's
> possible to establish type stability without a ctor... Are these bugs?

Yeah, I puzzled by this too. However, I think it's hard but possible to make it work, at least in theory.
There must be an initializer, which consists of two parts:
a) initilize objects fields
b) expose object to the world (add it to list or something like that)

(a) part must somehow to be ok to race with another cpu which might already use the object.
(b) part must must use e.g. barriers to make sure that racy users will see previously inilized fields.
Racy users must have parring barrier of course.

But it sound fishy, and very easy to fuck up. I won't be surprised if every single one SLAB_TYPESAFE_BY_RCU user
without ->ctor is bogus. It certainly would be better to convert those to use ->ctor.

Such caches seems used by networking subsystem in proto_register():

		prot->slab = kmem_cache_create_usercopy(prot->name,
					prot->obj_size, 0,
					SLAB_HWCACHE_ALIGN | SLAB_ACCOUNT |
					prot->slab_flags,
					prot->useroffset, prot->usersize,
					NULL);

And certain protocols specify SLAB_TYPESAFE_BY_RCU in ->slab_flags, such as:
llc_proto, smc_proto, smc_proto6, tcp_prot, tcpv6_prot, dccp_v6_prot, dccp_v4_prot.


Also nf_conntrack_cachep, kernfs_node_cache, jbd2_journal_head_cache and i915_request cache.


--
To unsubscribe from this list: send the line "unsubscribe linux-doc" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-mm
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:04:24 +0000
Message-ID: <b6b58786-85c9-e831-5571-58b5580c84ba () virtuozzo ! com>
--------------------


On 07/31/2018 06:03 PM, Dmitry Vyukov wrote:
> On Tue, Jul 31, 2018 at 4:50 PM, Andrey Ryabinin
> <aryabinin@virtuozzo.com> wrote:
>>
>>
>> On 07/31/2018 04:05 PM, Andrey Konovalov wrote:
>>> On Wed, Jul 25, 2018 at 3:44 PM, Vincenzo Frascino@Foss
>>> <vincenzo.frascino@arm.com> wrote:
>>>> On 06/26/2018 02:15 PM, Andrey Konovalov wrote:
>>>>
>>>>> @@ -325,18 +341,41 @@ void kasan_init_slab_obj(struct kmem_cache *cache,
>>>>> const void *object)
>>>>>     void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t
>>>>> flags)
>>>>>   {
>>>>> -       return kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>> +       object = kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>> +       if (IS_ENABLED(CONFIG_KASAN_HW) && unlikely(cache->ctor)) {
>>>>> +               /*
>>>>> +                * Cache constructor might use object's pointer value to
>>>>> +                * initialize some of its fields.
>>>>> +                */
>>>>> +               cache->ctor(object);
>>>>>
>>>> This seams breaking the kmem_cache_create() contract: "The @ctor is run when
>>>> new pages are allocated by the cache."
>>>> (https://elixir.bootlin.com/linux/v3.7/source/mm/slab_common.c#L83)
>>>>
>>>> Since there might be preexisting code relying on it, this could lead to
>>>> global side effects. Did you verify that this is not the case?
>>>>
>>>> Another concern is performance related if we consider this solution suitable
>>>> for "near-production", since with the current implementation you call the
>>>> ctor (where present) on an object multiple times and this ends up memsetting
>>>> and repopulating the memory every time (i.e. inode.c: inode_init_once). Do
>>>> you know what is the performance impact?
>>>
>>> We can assign tags to objects with constructors when a slab is
>>> allocated and call constructors once as usual. The downside is that
>>> such object would always have the same tag when it is reallocated, so
>>> we won't catch use-after-frees.
>>
>> Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>> are few without constructors.
>> We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
> 
> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
> slabs can be useful without ctors or at least memset(0). Objects in
> such slabs need to be type-stable, but I can't understand how it's
> possible to establish type stability without a ctor... Are these bugs?

Yeah, I puzzled by this too. However, I think it's hard but possible to make it work, at least in theory.
There must be an initializer, which consists of two parts:
a) initilize objects fields
b) expose object to the world (add it to list or something like that)

(a) part must somehow to be ok to race with another cpu which might already use the object.
(b) part must must use e.g. barriers to make sure that racy users will see previously inilized fields.
Racy users must have parring barrier of course.

But it sound fishy, and very easy to fuck up. I won't be surprised if every single one SLAB_TYPESAFE_BY_RCU user
without ->ctor is bogus. It certainly would be better to convert those to use ->ctor.

Such caches seems used by networking subsystem in proto_register():

		prot->slab = kmem_cache_create_usercopy(prot->name,
					prot->obj_size, 0,
					SLAB_HWCACHE_ALIGN | SLAB_ACCOUNT |
					prot->slab_flags,
					prot->useroffset, prot->usersize,
					NULL);

And certain protocols specify SLAB_TYPESAFE_BY_RCU in ->slab_flags, such as:
llc_proto, smc_proto, smc_proto6, tcp_prot, tcpv6_prot, dccp_v6_prot, dccp_v4_prot.


Also nf_conntrack_cachep, kernfs_node_cache, jbd2_journal_head_cache and i915_request cache.


================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-doc
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:08:54 +0000
Message-ID: <CACT4Y+aUs0YMTaQx4x9QiQsMjFNZ5gUFNuMkG_hneDPTb3Nu=Q () mail ! gmail ! com>
--------------------
On Tue, Jul 31, 2018 at 6:04 PM, Andrey Ryabinin
<aryabinin@virtuozzo.com> wrote:
>>>>>> @@ -325,18 +341,41 @@ void kasan_init_slab_obj(struct kmem_cache *cache,
>>>>>> const void *object)
>>>>>>     void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t
>>>>>> flags)
>>>>>>   {
>>>>>> -       return kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>> +       object = kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>> +       if (IS_ENABLED(CONFIG_KASAN_HW) && unlikely(cache->ctor)) {
>>>>>> +               /*
>>>>>> +                * Cache constructor might use object's pointer value to
>>>>>> +                * initialize some of its fields.
>>>>>> +                */
>>>>>> +               cache->ctor(object);
>>>>>>
>>>>> This seams breaking the kmem_cache_create() contract: "The @ctor is run when
>>>>> new pages are allocated by the cache."
>>>>> (https://elixir.bootlin.com/linux/v3.7/source/mm/slab_common.c#L83)
>>>>>
>>>>> Since there might be preexisting code relying on it, this could lead to
>>>>> global side effects. Did you verify that this is not the case?
>>>>>
>>>>> Another concern is performance related if we consider this solution suitable
>>>>> for "near-production", since with the current implementation you call the
>>>>> ctor (where present) on an object multiple times and this ends up memsetting
>>>>> and repopulating the memory every time (i.e. inode.c: inode_init_once). Do
>>>>> you know what is the performance impact?
>>>>
>>>> We can assign tags to objects with constructors when a slab is
>>>> allocated and call constructors once as usual. The downside is that
>>>> such object would always have the same tag when it is reallocated, so
>>>> we won't catch use-after-frees.
>>>
>>> Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>>> are few without constructors.
>>> We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>>
>> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
>> slabs can be useful without ctors or at least memset(0). Objects in
>> such slabs need to be type-stable, but I can't understand how it's
>> possible to establish type stability without a ctor... Are these bugs?
>
> Yeah, I puzzled by this too. However, I think it's hard but possible to make it work, at least in theory.
> There must be an initializer, which consists of two parts:
> a) initilize objects fields
> b) expose object to the world (add it to list or something like that)
>
> (a) part must somehow to be ok to race with another cpu which might already use the object.
> (b) part must must use e.g. barriers to make sure that racy users will see previously inilized fields.
> Racy users must have parring barrier of course.
>
> But it sound fishy, and very easy to fuck up.


Agree on both fronts: theoretically possible but easy to fuck up. Even
if it works, complexity of the code should be brain damaging and there
are unlikely good reasons to just not be more explicit and use a ctor.


> I won't be surprised if every single one SLAB_TYPESAFE_BY_RCU user
> without ->ctor is bogus. It certainly would be better to convert those to use ->ctor.

I have another hypothesis: they are not bogus, just don't need
SLAB_TYPESAFE_BY_RCU :)


> Such caches seems used by networking subsystem in proto_register():
>
>                 prot->slab = kmem_cache_create_usercopy(prot->name,
>                                         prot->obj_size, 0,
>                                         SLAB_HWCACHE_ALIGN | SLAB_ACCOUNT |
>                                         prot->slab_flags,
>                                         prot->useroffset, prot->usersize,
>                                         NULL);
>
> And certain protocols specify SLAB_TYPESAFE_BY_RCU in ->slab_flags, such as:
> llc_proto, smc_proto, smc_proto6, tcp_prot, tcpv6_prot, dccp_v6_prot, dccp_v4_prot.
>
>
> Also nf_conntrack_cachep, kernfs_node_cache, jbd2_journal_head_cache and i915_request cache.
>
>
> --
> You received this message because you are subscribed to the Google Groups "kasan-dev" group.
> To unsubscribe from this group and stop receiving emails from it, send an email to kasan-dev+unsubscribe@googlegroups.com.
> To post to this group, send email to kasan-dev@googlegroups.com.
> To view this discussion on the web visit https://groups.google.com/d/msgid/kasan-dev/b6b58786-85c9-e831-5571-58b5580c84ba%40virtuozzo.com.
> For more options, visit https://groups.google.com/d/optout.
--
To unsubscribe from this list: send the line "unsubscribe linux-doc" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-kernel
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:08:54 +0000
Message-ID: <CACT4Y+aUs0YMTaQx4x9QiQsMjFNZ5gUFNuMkG_hneDPTb3Nu=Q () mail ! gmail ! com>
--------------------
On Tue, Jul 31, 2018 at 6:04 PM, Andrey Ryabinin
<aryabinin@virtuozzo.com> wrote:
>>>>>> @@ -325,18 +341,41 @@ void kasan_init_slab_obj(struct kmem_cache *cache,
>>>>>> const void *object)
>>>>>>     void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t
>>>>>> flags)
>>>>>>   {
>>>>>> -       return kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>> +       object = kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>> +       if (IS_ENABLED(CONFIG_KASAN_HW) && unlikely(cache->ctor)) {
>>>>>> +               /*
>>>>>> +                * Cache constructor might use object's pointer value to
>>>>>> +                * initialize some of its fields.
>>>>>> +                */
>>>>>> +               cache->ctor(object);
>>>>>>
>>>>> This seams breaking the kmem_cache_create() contract: "The @ctor is run when
>>>>> new pages are allocated by the cache."
>>>>> (https://elixir.bootlin.com/linux/v3.7/source/mm/slab_common.c#L83)
>>>>>
>>>>> Since there might be preexisting code relying on it, this could lead to
>>>>> global side effects. Did you verify that this is not the case?
>>>>>
>>>>> Another concern is performance related if we consider this solution suitable
>>>>> for "near-production", since with the current implementation you call the
>>>>> ctor (where present) on an object multiple times and this ends up memsetting
>>>>> and repopulating the memory every time (i.e. inode.c: inode_init_once). Do
>>>>> you know what is the performance impact?
>>>>
>>>> We can assign tags to objects with constructors when a slab is
>>>> allocated and call constructors once as usual. The downside is that
>>>> such object would always have the same tag when it is reallocated, so
>>>> we won't catch use-after-frees.
>>>
>>> Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>>> are few without constructors.
>>> We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>>
>> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
>> slabs can be useful without ctors or at least memset(0). Objects in
>> such slabs need to be type-stable, but I can't understand how it's
>> possible to establish type stability without a ctor... Are these bugs?
>
> Yeah, I puzzled by this too. However, I think it's hard but possible to make it work, at least in theory.
> There must be an initializer, which consists of two parts:
> a) initilize objects fields
> b) expose object to the world (add it to list or something like that)
>
> (a) part must somehow to be ok to race with another cpu which might already use the object.
> (b) part must must use e.g. barriers to make sure that racy users will see previously inilized fields.
> Racy users must have parring barrier of course.
>
> But it sound fishy, and very easy to fuck up.


Agree on both fronts: theoretically possible but easy to fuck up. Even
if it works, complexity of the code should be brain damaging and there
are unlikely good reasons to just not be more explicit and use a ctor.


> I won't be surprised if every single one SLAB_TYPESAFE_BY_RCU user
> without ->ctor is bogus. It certainly would be better to convert those to use ->ctor.

I have another hypothesis: they are not bogus, just don't need
SLAB_TYPESAFE_BY_RCU :)


> Such caches seems used by networking subsystem in proto_register():
>
>                 prot->slab = kmem_cache_create_usercopy(prot->name,
>                                         prot->obj_size, 0,
>                                         SLAB_HWCACHE_ALIGN | SLAB_ACCOUNT |
>                                         prot->slab_flags,
>                                         prot->useroffset, prot->usersize,
>                                         NULL);
>
> And certain protocols specify SLAB_TYPESAFE_BY_RCU in ->slab_flags, such as:
> llc_proto, smc_proto, smc_proto6, tcp_prot, tcpv6_prot, dccp_v6_prot, dccp_v4_prot.
>
>
> Also nf_conntrack_cachep, kernfs_node_cache, jbd2_journal_head_cache and i915_request cache.
>
>
> --
> You received this message because you are subscribed to the Google Groups "kasan-dev" group.
> To unsubscribe from this group and stop receiving emails from it, send an email to kasan-dev+unsubscribe@googlegroups.com.
> To post to this group, send email to kasan-dev@googlegroups.com.
> To view this discussion on the web visit https://groups.google.com/d/msgid/kasan-dev/b6b58786-85c9-e831-5571-58b5580c84ba%40virtuozzo.com.
> For more options, visit https://groups.google.com/d/optout.
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-sparse
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:08:54 +0000
Message-ID: <CACT4Y+aUs0YMTaQx4x9QiQsMjFNZ5gUFNuMkG_hneDPTb3Nu=Q () mail ! gmail ! com>
--------------------
On Tue, Jul 31, 2018 at 6:04 PM, Andrey Ryabinin
<aryabinin@virtuozzo.com> wrote:
>>>>>> @@ -325,18 +341,41 @@ void kasan_init_slab_obj(struct kmem_cache *cache,
>>>>>> const void *object)
>>>>>>     void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t
>>>>>> flags)
>>>>>>   {
>>>>>> -       return kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>> +       object = kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>> +       if (IS_ENABLED(CONFIG_KASAN_HW) && unlikely(cache->ctor)) {
>>>>>> +               /*
>>>>>> +                * Cache constructor might use object's pointer value to
>>>>>> +                * initialize some of its fields.
>>>>>> +                */
>>>>>> +               cache->ctor(object);
>>>>>>
>>>>> This seams breaking the kmem_cache_create() contract: "The @ctor is run when
>>>>> new pages are allocated by the cache."
>>>>> (https://elixir.bootlin.com/linux/v3.7/source/mm/slab_common.c#L83)
>>>>>
>>>>> Since there might be preexisting code relying on it, this could lead to
>>>>> global side effects. Did you verify that this is not the case?
>>>>>
>>>>> Another concern is performance related if we consider this solution suitable
>>>>> for "near-production", since with the current implementation you call the
>>>>> ctor (where present) on an object multiple times and this ends up memsetting
>>>>> and repopulating the memory every time (i.e. inode.c: inode_init_once). Do
>>>>> you know what is the performance impact?
>>>>
>>>> We can assign tags to objects with constructors when a slab is
>>>> allocated and call constructors once as usual. The downside is that
>>>> such object would always have the same tag when it is reallocated, so
>>>> we won't catch use-after-frees.
>>>
>>> Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>>> are few without constructors.
>>> We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>>
>> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
>> slabs can be useful without ctors or at least memset(0). Objects in
>> such slabs need to be type-stable, but I can't understand how it's
>> possible to establish type stability without a ctor... Are these bugs?
>
> Yeah, I puzzled by this too. However, I think it's hard but possible to make it work, at least in theory.
> There must be an initializer, which consists of two parts:
> a) initilize objects fields
> b) expose object to the world (add it to list or something like that)
>
> (a) part must somehow to be ok to race with another cpu which might already use the object.
> (b) part must must use e.g. barriers to make sure that racy users will see previously inilized fields.
> Racy users must have parring barrier of course.
>
> But it sound fishy, and very easy to fuck up.


Agree on both fronts: theoretically possible but easy to fuck up. Even
if it works, complexity of the code should be brain damaging and there
are unlikely good reasons to just not be more explicit and use a ctor.


> I won't be surprised if every single one SLAB_TYPESAFE_BY_RCU user
> without ->ctor is bogus. It certainly would be better to convert those to use ->ctor.

I have another hypothesis: they are not bogus, just don't need
SLAB_TYPESAFE_BY_RCU :)


> Such caches seems used by networking subsystem in proto_register():
>
>                 prot->slab = kmem_cache_create_usercopy(prot->name,
>                                         prot->obj_size, 0,
>                                         SLAB_HWCACHE_ALIGN | SLAB_ACCOUNT |
>                                         prot->slab_flags,
>                                         prot->useroffset, prot->usersize,
>                                         NULL);
>
> And certain protocols specify SLAB_TYPESAFE_BY_RCU in ->slab_flags, such as:
> llc_proto, smc_proto, smc_proto6, tcp_prot, tcpv6_prot, dccp_v6_prot, dccp_v4_prot.
>
>
> Also nf_conntrack_cachep, kernfs_node_cache, jbd2_journal_head_cache and i915_request cache.
>
>
> --
> You received this message because you are subscribed to the Google Groups "kasan-dev" group.
> To unsubscribe from this group and stop receiving emails from it, send an email to kasan-dev+unsubscribe@googlegroups.com.
> To post to this group, send email to kasan-dev@googlegroups.com.
> To view this discussion on the web visit https://groups.google.com/d/msgid/kasan-dev/b6b58786-85c9-e831-5571-58b5580c84ba%40virtuozzo.com.
> For more options, visit https://groups.google.com/d/optout.
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-sparse
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:18:48 +0000
Message-ID: <e73efbea-9f23-9d1f-bd1d-add0c0fe4710 () virtuozzo ! com>
--------------------


On 07/31/2018 07:08 PM, Dmitry Vyukov wrote:
> On Tue, Jul 31, 2018 at 6:04 PM, Andrey Ryabinin
> <aryabinin@virtuozzo.com> wrote:
>>>>>>> @@ -325,18 +341,41 @@ void kasan_init_slab_obj(struct kmem_cache *cache,
>>>>>>> const void *object)
>>>>>>>     void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t
>>>>>>> flags)
>>>>>>>   {
>>>>>>> -       return kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>>> +       object = kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>>> +       if (IS_ENABLED(CONFIG_KASAN_HW) && unlikely(cache->ctor)) {
>>>>>>> +               /*
>>>>>>> +                * Cache constructor might use object's pointer value to
>>>>>>> +                * initialize some of its fields.
>>>>>>> +                */
>>>>>>> +               cache->ctor(object);
>>>>>>>
>>>>>> This seams breaking the kmem_cache_create() contract: "The @ctor is run when
>>>>>> new pages are allocated by the cache."
>>>>>> (https://elixir.bootlin.com/linux/v3.7/source/mm/slab_common.c#L83)
>>>>>>
>>>>>> Since there might be preexisting code relying on it, this could lead to
>>>>>> global side effects. Did you verify that this is not the case?
>>>>>>
>>>>>> Another concern is performance related if we consider this solution suitable
>>>>>> for "near-production", since with the current implementation you call the
>>>>>> ctor (where present) on an object multiple times and this ends up memsetting
>>>>>> and repopulating the memory every time (i.e. inode.c: inode_init_once). Do
>>>>>> you know what is the performance impact?
>>>>>
>>>>> We can assign tags to objects with constructors when a slab is
>>>>> allocated and call constructors once as usual. The downside is that
>>>>> such object would always have the same tag when it is reallocated, so
>>>>> we won't catch use-after-frees.
>>>>
>>>> Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>>>> are few without constructors.
>>>> We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>>>
>>> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
>>> slabs can be useful without ctors or at least memset(0). Objects in
>>> such slabs need to be type-stable, but I can't understand how it's
>>> possible to establish type stability without a ctor... Are these bugs?
>>
>> Yeah, I puzzled by this too. However, I think it's hard but possible to make it work, at least in theory.
>> There must be an initializer, which consists of two parts:
>> a) initilize objects fields
>> b) expose object to the world (add it to list or something like that)
>>
>> (a) part must somehow to be ok to race with another cpu which might already use the object.
>> (b) part must must use e.g. barriers to make sure that racy users will see previously inilized fields.
>> Racy users must have parring barrier of course.
>>
>> But it sound fishy, and very easy to fuck up.
> 
> 
> Agree on both fronts: theoretically possible but easy to fuck up. Even
> if it works, complexity of the code should be brain damaging and there
> are unlikely good reasons to just not be more explicit and use a ctor.
> 
> 
>> I won't be surprised if every single one SLAB_TYPESAFE_BY_RCU user
>> without ->ctor is bogus. It certainly would be better to convert those to use ->ctor.
> 
> I have another hypothesis: they are not bogus, just don't need
> SLAB_TYPESAFE_BY_RCU :)
> 

I'd call this a bug too.
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-kernel
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:18:48 +0000
Message-ID: <e73efbea-9f23-9d1f-bd1d-add0c0fe4710 () virtuozzo ! com>
--------------------


On 07/31/2018 07:08 PM, Dmitry Vyukov wrote:
> On Tue, Jul 31, 2018 at 6:04 PM, Andrey Ryabinin
> <aryabinin@virtuozzo.com> wrote:
>>>>>>> @@ -325,18 +341,41 @@ void kasan_init_slab_obj(struct kmem_cache *cache,
>>>>>>> const void *object)
>>>>>>>     void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t
>>>>>>> flags)
>>>>>>>   {
>>>>>>> -       return kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>>> +       object = kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>>> +       if (IS_ENABLED(CONFIG_KASAN_HW) && unlikely(cache->ctor)) {
>>>>>>> +               /*
>>>>>>> +                * Cache constructor might use object's pointer value to
>>>>>>> +                * initialize some of its fields.
>>>>>>> +                */
>>>>>>> +               cache->ctor(object);
>>>>>>>
>>>>>> This seams breaking the kmem_cache_create() contract: "The @ctor is run when
>>>>>> new pages are allocated by the cache."
>>>>>> (https://elixir.bootlin.com/linux/v3.7/source/mm/slab_common.c#L83)
>>>>>>
>>>>>> Since there might be preexisting code relying on it, this could lead to
>>>>>> global side effects. Did you verify that this is not the case?
>>>>>>
>>>>>> Another concern is performance related if we consider this solution suitable
>>>>>> for "near-production", since with the current implementation you call the
>>>>>> ctor (where present) on an object multiple times and this ends up memsetting
>>>>>> and repopulating the memory every time (i.e. inode.c: inode_init_once). Do
>>>>>> you know what is the performance impact?
>>>>>
>>>>> We can assign tags to objects with constructors when a slab is
>>>>> allocated and call constructors once as usual. The downside is that
>>>>> such object would always have the same tag when it is reallocated, so
>>>>> we won't catch use-after-frees.
>>>>
>>>> Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>>>> are few without constructors.
>>>> We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>>>
>>> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
>>> slabs can be useful without ctors or at least memset(0). Objects in
>>> such slabs need to be type-stable, but I can't understand how it's
>>> possible to establish type stability without a ctor... Are these bugs?
>>
>> Yeah, I puzzled by this too. However, I think it's hard but possible to make it work, at least in theory.
>> There must be an initializer, which consists of two parts:
>> a) initilize objects fields
>> b) expose object to the world (add it to list or something like that)
>>
>> (a) part must somehow to be ok to race with another cpu which might already use the object.
>> (b) part must must use e.g. barriers to make sure that racy users will see previously inilized fields.
>> Racy users must have parring barrier of course.
>>
>> But it sound fishy, and very easy to fuck up.
> 
> 
> Agree on both fronts: theoretically possible but easy to fuck up. Even
> if it works, complexity of the code should be brain damaging and there
> are unlikely good reasons to just not be more explicit and use a ctor.
> 
> 
>> I won't be surprised if every single one SLAB_TYPESAFE_BY_RCU user
>> without ->ctor is bogus. It certainly would be better to convert those to use ->ctor.
> 
> I have another hypothesis: they are not bogus, just don't need
> SLAB_TYPESAFE_BY_RCU :)
> 

I'd call this a bug too.
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-kbuild
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:18:48 +0000
Message-ID: <e73efbea-9f23-9d1f-bd1d-add0c0fe4710 () virtuozzo ! com>
--------------------


On 07/31/2018 07:08 PM, Dmitry Vyukov wrote:
> On Tue, Jul 31, 2018 at 6:04 PM, Andrey Ryabinin
> <aryabinin@virtuozzo.com> wrote:
>>>>>>> @@ -325,18 +341,41 @@ void kasan_init_slab_obj(struct kmem_cache *cache,
>>>>>>> const void *object)
>>>>>>>     void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t
>>>>>>> flags)
>>>>>>>   {
>>>>>>> -       return kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>>> +       object = kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>>> +       if (IS_ENABLED(CONFIG_KASAN_HW) && unlikely(cache->ctor)) {
>>>>>>> +               /*
>>>>>>> +                * Cache constructor might use object's pointer value to
>>>>>>> +                * initialize some of its fields.
>>>>>>> +                */
>>>>>>> +               cache->ctor(object);
>>>>>>>
>>>>>> This seams breaking the kmem_cache_create() contract: "The @ctor is run when
>>>>>> new pages are allocated by the cache."
>>>>>> (https://elixir.bootlin.com/linux/v3.7/source/mm/slab_common.c#L83)
>>>>>>
>>>>>> Since there might be preexisting code relying on it, this could lead to
>>>>>> global side effects. Did you verify that this is not the case?
>>>>>>
>>>>>> Another concern is performance related if we consider this solution suitable
>>>>>> for "near-production", since with the current implementation you call the
>>>>>> ctor (where present) on an object multiple times and this ends up memsetting
>>>>>> and repopulating the memory every time (i.e. inode.c: inode_init_once). Do
>>>>>> you know what is the performance impact?
>>>>>
>>>>> We can assign tags to objects with constructors when a slab is
>>>>> allocated and call constructors once as usual. The downside is that
>>>>> such object would always have the same tag when it is reallocated, so
>>>>> we won't catch use-after-frees.
>>>>
>>>> Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>>>> are few without constructors.
>>>> We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>>>
>>> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
>>> slabs can be useful without ctors or at least memset(0). Objects in
>>> such slabs need to be type-stable, but I can't understand how it's
>>> possible to establish type stability without a ctor... Are these bugs?
>>
>> Yeah, I puzzled by this too. However, I think it's hard but possible to make it work, at least in theory.
>> There must be an initializer, which consists of two parts:
>> a) initilize objects fields
>> b) expose object to the world (add it to list or something like that)
>>
>> (a) part must somehow to be ok to race with another cpu which might already use the object.
>> (b) part must must use e.g. barriers to make sure that racy users will see previously inilized fields.
>> Racy users must have parring barrier of course.
>>
>> But it sound fishy, and very easy to fuck up.
> 
> 
> Agree on both fronts: theoretically possible but easy to fuck up. Even
> if it works, complexity of the code should be brain damaging and there
> are unlikely good reasons to just not be more explicit and use a ctor.
> 
> 
>> I won't be surprised if every single one SLAB_TYPESAFE_BY_RCU user
>> without ->ctor is bogus. It certainly would be better to convert those to use ->ctor.
> 
> I have another hypothesis: they are not bogus, just don't need
> SLAB_TYPESAFE_BY_RCU :)
> 

I'd call this a bug too.
--
To unsubscribe from this list: send the line "unsubscribe linux-kbuild" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-doc
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:18:48 +0000
Message-ID: <e73efbea-9f23-9d1f-bd1d-add0c0fe4710 () virtuozzo ! com>
--------------------


On 07/31/2018 07:08 PM, Dmitry Vyukov wrote:
> On Tue, Jul 31, 2018 at 6:04 PM, Andrey Ryabinin
> <aryabinin@virtuozzo.com> wrote:
>>>>>>> @@ -325,18 +341,41 @@ void kasan_init_slab_obj(struct kmem_cache *cache,
>>>>>>> const void *object)
>>>>>>>     void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t
>>>>>>> flags)
>>>>>>>   {
>>>>>>> -       return kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>>> +       object = kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>>> +       if (IS_ENABLED(CONFIG_KASAN_HW) && unlikely(cache->ctor)) {
>>>>>>> +               /*
>>>>>>> +                * Cache constructor might use object's pointer value to
>>>>>>> +                * initialize some of its fields.
>>>>>>> +                */
>>>>>>> +               cache->ctor(object);
>>>>>>>
>>>>>> This seams breaking the kmem_cache_create() contract: "The @ctor is run when
>>>>>> new pages are allocated by the cache."
>>>>>> (https://elixir.bootlin.com/linux/v3.7/source/mm/slab_common.c#L83)
>>>>>>
>>>>>> Since there might be preexisting code relying on it, this could lead to
>>>>>> global side effects. Did you verify that this is not the case?
>>>>>>
>>>>>> Another concern is performance related if we consider this solution suitable
>>>>>> for "near-production", since with the current implementation you call the
>>>>>> ctor (where present) on an object multiple times and this ends up memsetting
>>>>>> and repopulating the memory every time (i.e. inode.c: inode_init_once). Do
>>>>>> you know what is the performance impact?
>>>>>
>>>>> We can assign tags to objects with constructors when a slab is
>>>>> allocated and call constructors once as usual. The downside is that
>>>>> such object would always have the same tag when it is reallocated, so
>>>>> we won't catch use-after-frees.
>>>>
>>>> Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>>>> are few without constructors.
>>>> We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>>>
>>> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
>>> slabs can be useful without ctors or at least memset(0). Objects in
>>> such slabs need to be type-stable, but I can't understand how it's
>>> possible to establish type stability without a ctor... Are these bugs?
>>
>> Yeah, I puzzled by this too. However, I think it's hard but possible to make it work, at least in theory.
>> There must be an initializer, which consists of two parts:
>> a) initilize objects fields
>> b) expose object to the world (add it to list or something like that)
>>
>> (a) part must somehow to be ok to race with another cpu which might already use the object.
>> (b) part must must use e.g. barriers to make sure that racy users will see previously inilized fields.
>> Racy users must have parring barrier of course.
>>
>> But it sound fishy, and very easy to fuck up.
> 
> 
> Agree on both fronts: theoretically possible but easy to fuck up. Even
> if it works, complexity of the code should be brain damaging and there
> are unlikely good reasons to just not be more explicit and use a ctor.
> 
> 
>> I won't be surprised if every single one SLAB_TYPESAFE_BY_RCU user
>> without ->ctor is bogus. It certainly would be better to convert those to use ->ctor.
> 
> I have another hypothesis: they are not bogus, just don't need
> SLAB_TYPESAFE_BY_RCU :)
> 

I'd call this a bug too.
--
To unsubscribe from this list: send the line "unsubscribe linux-doc" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Andrey Ryabinin <aryabinin () virtuozzo ! com>
To: linux-mm
Subject: Re: [PATCH v4 13/17] khwasan: add hooks implementation
Date: Tue, 31 Jul 2018 16:18:48 +0000
Message-ID: <e73efbea-9f23-9d1f-bd1d-add0c0fe4710 () virtuozzo ! com>
--------------------


On 07/31/2018 07:08 PM, Dmitry Vyukov wrote:
> On Tue, Jul 31, 2018 at 6:04 PM, Andrey Ryabinin
> <aryabinin@virtuozzo.com> wrote:
>>>>>>> @@ -325,18 +341,41 @@ void kasan_init_slab_obj(struct kmem_cache *cache,
>>>>>>> const void *object)
>>>>>>>     void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t
>>>>>>> flags)
>>>>>>>   {
>>>>>>> -       return kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>>> +       object = kasan_kmalloc(cache, object, cache->object_size, flags);
>>>>>>> +       if (IS_ENABLED(CONFIG_KASAN_HW) && unlikely(cache->ctor)) {
>>>>>>> +               /*
>>>>>>> +                * Cache constructor might use object's pointer value to
>>>>>>> +                * initialize some of its fields.
>>>>>>> +                */
>>>>>>> +               cache->ctor(object);
>>>>>>>
>>>>>> This seams breaking the kmem_cache_create() contract: "The @ctor is run when
>>>>>> new pages are allocated by the cache."
>>>>>> (https://elixir.bootlin.com/linux/v3.7/source/mm/slab_common.c#L83)
>>>>>>
>>>>>> Since there might be preexisting code relying on it, this could lead to
>>>>>> global side effects. Did you verify that this is not the case?
>>>>>>
>>>>>> Another concern is performance related if we consider this solution suitable
>>>>>> for "near-production", since with the current implementation you call the
>>>>>> ctor (where present) on an object multiple times and this ends up memsetting
>>>>>> and repopulating the memory every time (i.e. inode.c: inode_init_once). Do
>>>>>> you know what is the performance impact?
>>>>>
>>>>> We can assign tags to objects with constructors when a slab is
>>>>> allocated and call constructors once as usual. The downside is that
>>>>> such object would always have the same tag when it is reallocated, so
>>>>> we won't catch use-after-frees.
>>>>
>>>> Actually you should do this for SLAB_TYPESAFE_BY_RCU slabs. Usually they are with ->ctors but there
>>>> are few without constructors.
>>>> We can't reinitialize or even retag them. The latter will definitely cause false-positive use-after-free reports.
>>>
>>> Somewhat offtopic, but I can't understand how SLAB_TYPESAFE_BY_RCU
>>> slabs can be useful without ctors or at least memset(0). Objects in
>>> such slabs need to be type-stable, but I can't understand how it's
>>> possible to establish type stability without a ctor... Are these bugs?
>>
>> Yeah, I puzzled by this too. However, I think it's hard but possible to make it work, at least in theory.
>> There must be an initializer, which consists of two parts:
>> a) initilize objects fields
>> b) expose object to the world (add it to list or something like that)
>>
>> (a) part must somehow to be ok to race with another cpu which might already use the object.
>> (b) part must must use e.g. barriers to make sure that racy users will see previously inilized fields.
>> Racy users must have parring barrier of course.
>>
>> But it sound fishy, and very easy to fuck up.
> 
> 
> Agree on both fronts: theoretically possible but easy to fuck up. Even
> if it works, complexity of the code should be brain damaging and there
> are unlikely good reasons to just not be more explicit and use a ctor.
> 
> 
>> I won't be surprised if every single one SLAB_TYPESAFE_BY_RCU user
>> without ->ctor is bogus. It certainly would be better to convert those to use ->ctor.
> 
> I have another hypothesis: they are not bogus, just don't need
> SLAB_TYPESAFE_BY_RCU :)
> 

I'd call this a bug too.

================================================================================


################################################################################

=== Thread: [PATCH v4 14/17] khwasan, arm64: add brk handler for inline instrumentation ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v4 14/17] khwasan, arm64: add brk handler for inline instrumentation
Date: Tue, 26 Jun 2018 13:15:24 +0000
Message-ID: <69fd53d114e5814020e5e265ae451a63b09c776e.1530018818.git.andreyknvl () google ! com>
--------------------
KHWASAN inline instrumentation mode (which embeds checks of shadow memory
into the generated code, instead of inserting a callback) generates a brk
instruction when a tag mismatch is detected.

This commit add a KHWASAN brk handler, that decodes the immediate value
passed to the brk instructions (to extract information about the memory
access that triggered the mismatch), reads the register values (x0 contains
the guilty address) and reports the bug.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/brk-imm.h |  2 +
 arch/arm64/kernel/traps.c        | 69 +++++++++++++++++++++++++++++++-
 2 files changed, 69 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
index ed693c5bcec0..e4a7013321dc 100644
--- a/arch/arm64/include/asm/brk-imm.h
+++ b/arch/arm64/include/asm/brk-imm.h
@@ -16,10 +16,12 @@
  * 0x400: for dynamic BRK instruction
  * 0x401: for compile time BRK instruction
  * 0x800: kernel-mode BUG() and WARN() traps
+ * 0x9xx: KHWASAN trap (allowed values 0x900 - 0x9ff)
  */
 #define FAULT_BRK_IMM			0x100
 #define KGDB_DYN_DBG_BRK_IMM		0x400
 #define KGDB_COMPILED_DBG_BRK_IMM	0x401
 #define BUG_BRK_IMM			0x800
+#define KHWASAN_BRK_IMM			0x900
 
 #endif
diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index d399d459397b..95152a4fd202 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -35,6 +35,7 @@
 #include <linux/sizes.h>
 #include <linux/syscalls.h>
 #include <linux/mm_types.h>
+#include <linux/kasan.h>
 
 #include <asm/atomic.h>
 #include <asm/bug.h>
@@ -269,10 +270,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
 	}
 }
 
-void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
 {
 	regs->pc += size;
+}
 
+void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+{
+	__arm64_skip_faulting_instruction(regs, size);
 	/*
 	 * If we were single stepping, we want to get the step exception after
 	 * we return from the trap.
@@ -791,7 +796,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
 	}
 
 	/* If thread survives, skip over the BUG instruction and continue: */
-	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 	return DBG_HOOK_HANDLED;
 }
 
@@ -801,6 +806,59 @@ static struct break_hook bug_break_hook = {
 	.fn = bug_handler,
 };
 
+#ifdef CONFIG_KASAN_HW
+
+#define KHWASAN_ESR_RECOVER	0x20
+#define KHWASAN_ESR_WRITE	0x10
+#define KHWASAN_ESR_SIZE_MASK	0x0f
+#define KHWASAN_ESR_SIZE(esr)	(1 << ((esr) & KHWASAN_ESR_SIZE_MASK))
+
+static int khwasan_handler(struct pt_regs *regs, unsigned int esr)
+{
+	bool recover = esr & KHWASAN_ESR_RECOVER;
+	bool write = esr & KHWASAN_ESR_WRITE;
+	size_t size = KHWASAN_ESR_SIZE(esr);
+	u64 addr = regs->regs[0];
+	u64 pc = regs->pc;
+
+	if (user_mode(regs))
+		return DBG_HOOK_ERROR;
+
+	kasan_report(addr, size, write, pc);
+
+	/*
+	 * The instrumentation allows to control whether we can proceed after
+	 * a crash was detected. This is done by passing the -recover flag to
+	 * the compiler. Disabling recovery allows to generate more compact
+	 * code.
+	 *
+	 * Unfortunately disabling recovery doesn't work for the kernel right
+	 * now. KHWASAN reporting is disabled in some contexts (for example when
+	 * the allocator accesses slab object metadata; same is true for KASAN;
+	 * this is controlled by current->kasan_depth). All these accesses are
+	 * detected by the tool, even though the reports for them are not
+	 * printed.
+	 *
+	 * This is something that might be fixed at some point in the future.
+	 */
+	if (!recover)
+		die("Oops - KHWASAN", regs, 0);
+
+	/* If thread survives, skip over the brk instruction and continue: */
+	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	return DBG_HOOK_HANDLED;
+}
+
+#define KHWASAN_ESR_VAL (0xf2000000 | KHWASAN_BRK_IMM)
+#define KHWASAN_ESR_MASK 0xffffff00
+
+static struct break_hook khwasan_break_hook = {
+	.esr_val = KHWASAN_ESR_VAL,
+	.esr_mask = KHWASAN_ESR_MASK,
+	.fn = khwasan_handler,
+};
+#endif
+
 /*
  * Initial handler for AArch64 BRK exceptions
  * This handler only used until debug_traps_init().
@@ -808,6 +866,10 @@ static struct break_hook bug_break_hook = {
 int __init early_brk64(unsigned long addr, unsigned int esr,
 		struct pt_regs *regs)
 {
+#ifdef CONFIG_KASAN_HW
+	if ((esr & KHWASAN_ESR_MASK) == KHWASAN_ESR_VAL)
+		return khwasan_handler(regs, esr) != DBG_HOOK_HANDLED;
+#endif
 	return bug_handler(regs, esr) != DBG_HOOK_HANDLED;
 }
 
@@ -815,4 +877,7 @@ int __init early_brk64(unsigned long addr, unsigned int esr,
 void __init trap_init(void)
 {
 	register_break_hook(&bug_break_hook);
+#ifdef CONFIG_KASAN_HW
+	register_break_hook(&khwasan_break_hook);
+#endif
 }
-- 
2.18.0.rc2.346.g013aa6912e-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v4 15/17] khwasan, mm, arm64: tag non slab memory allocated via pagealloc ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v4 15/17] khwasan, mm, arm64: tag non slab memory allocated via pagealloc
Date: Tue, 26 Jun 2018 13:15:25 +0000
Message-ID: <d2891451b51c6ba246f44383228ec13bc9206931.1530018818.git.andreyknvl () google ! com>
--------------------
KWHASAN doesn't check memory accesses through pointers tagged with 0xff.
When page_address is used to get pointer to memory that corresponds to
some page, the tag of the resulting pointer gets set to 0xff, even though
the allocated memory might have been tagged differently.

For slab pages it's impossible to recover the correct tag to return from
page_address, since the page might contain multiple slab objects tagged
with different values, and we can't know in advance which one of them is
going to get accessed. For non slab pages however, we can recover the tag
in page_address, since the whole page was marked with the same tag.

This patch adds tagging to non slab memory allocated with pagealloc. To
set the tag of the pointer returned from page_address, the tag gets stored
to page->flags when the memory gets allocated.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/memory.h   | 10 ++++++++++
 include/linux/mm.h                | 29 +++++++++++++++++++++++++++++
 include/linux/page-flags-layout.h | 10 ++++++++++
 mm/cma.c                          | 11 +++++++++++
 mm/kasan/common.c                 | 15 +++++++++++++--
 mm/page_alloc.c                   |  1 +
 6 files changed, 74 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index e9e054dfb1fc..3352a65b8312 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -305,7 +305,17 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define __virt_to_pgoff(kaddr)	(((u64)(kaddr) & ~PAGE_OFFSET) / PAGE_SIZE * sizeof(struct page))
 #define __page_to_voff(kaddr)	(((u64)(kaddr) & ~VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
 
+#ifndef CONFIG_KASAN_HW
 #define page_to_virt(page)	((void *)((__page_to_voff(page)) | PAGE_OFFSET))
+#else
+#define page_to_virt(page)	({					\
+	unsigned long __addr =						\
+		((__page_to_voff(page)) | PAGE_OFFSET);			\
+	__addr = KASAN_SET_TAG(__addr, page_kasan_tag(page));		\
+	((void *)__addr);						\
+})
+#endif
+
 #define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) | VMEMMAP_START))
 
 #define _virt_addr_valid(kaddr)	pfn_valid((((u64)(kaddr) & ~PAGE_OFFSET) \
diff --git a/include/linux/mm.h b/include/linux/mm.h
index a0fbb9ffe380..46afadf4f48c 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -784,6 +784,7 @@ int finish_mkwrite_fault(struct vm_fault *vmf);
 #define NODES_PGOFF		(SECTIONS_PGOFF - NODES_WIDTH)
 #define ZONES_PGOFF		(NODES_PGOFF - ZONES_WIDTH)
 #define LAST_CPUPID_PGOFF	(ZONES_PGOFF - LAST_CPUPID_WIDTH)
+#define KASAN_TAG_PGOFF		(LAST_CPUPID_PGOFF - KASAN_TAG_WIDTH)
 
 /*
  * Define the bit shifts to access each section.  For non-existent
@@ -794,6 +795,7 @@ int finish_mkwrite_fault(struct vm_fault *vmf);
 #define NODES_PGSHIFT		(NODES_PGOFF * (NODES_WIDTH != 0))
 #define ZONES_PGSHIFT		(ZONES_PGOFF * (ZONES_WIDTH != 0))
 #define LAST_CPUPID_PGSHIFT	(LAST_CPUPID_PGOFF * (LAST_CPUPID_WIDTH != 0))
+#define KASAN_TAG_PGSHIFT	(KASAN_TAG_PGOFF * (KASAN_TAG_WIDTH != 0))
 
 /* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */
 #ifdef NODE_NOT_IN_PAGE_FLAGS
@@ -816,6 +818,7 @@ int finish_mkwrite_fault(struct vm_fault *vmf);
 #define NODES_MASK		((1UL << NODES_WIDTH) - 1)
 #define SECTIONS_MASK		((1UL << SECTIONS_WIDTH) - 1)
 #define LAST_CPUPID_MASK	((1UL << LAST_CPUPID_SHIFT) - 1)
+#define KASAN_TAG_MASK		((1UL << KASAN_TAG_WIDTH) - 1)
 #define ZONEID_MASK		((1UL << ZONEID_SHIFT) - 1)
 
 static inline enum zone_type page_zonenum(const struct page *page)
@@ -1070,6 +1073,32 @@ static inline bool cpupid_match_pid(struct task_struct *task, int cpupid)
 }
 #endif /* CONFIG_NUMA_BALANCING */
 
+#ifdef CONFIG_KASAN_HW
+static inline u8 page_kasan_tag(const struct page *page)
+{
+	return (page->flags >> KASAN_TAG_PGSHIFT) & KASAN_TAG_MASK;
+}
+
+static inline void page_kasan_tag_set(struct page *page, u8 tag)
+{
+	page->flags &= ~(KASAN_TAG_MASK << KASAN_TAG_PGSHIFT);
+	page->flags |= (tag & KASAN_TAG_MASK) << KASAN_TAG_PGSHIFT;
+}
+
+static inline void page_kasan_tag_reset(struct page *page)
+{
+	page_kasan_tag_set(page, 0xff);
+}
+#else
+static inline u8 page_kasan_tag(const struct page *page)
+{
+	return 0xff;
+}
+
+static inline void page_kasan_tag_set(struct page *page, u8 tag) { }
+static inline void page_kasan_tag_reset(struct page *page) { }
+#endif
+
 static inline struct zone *page_zone(const struct page *page)
 {
 	return &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)];
diff --git a/include/linux/page-flags-layout.h b/include/linux/page-flags-layout.h
index 7ec86bf31ce4..8dbad17664c2 100644
--- a/include/linux/page-flags-layout.h
+++ b/include/linux/page-flags-layout.h
@@ -82,6 +82,16 @@
 #define LAST_CPUPID_WIDTH 0
 #endif
 
+#ifdef CONFIG_KASAN_HW
+#define KASAN_TAG_WIDTH 8
+#if SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH+LAST_CPUPID_WIDTH+KASAN_TAG_WIDTH \
+	> BITS_PER_LONG - NR_PAGEFLAGS
+#error "KASAN: not enough bits in page flags for tag"
+#endif
+#else
+#define KASAN_TAG_WIDTH 0
+#endif
+
 /*
  * We are going to use the flags for the page to node mapping if its in
  * there.  This includes the case where there is no node, so it is implicit.
diff --git a/mm/cma.c b/mm/cma.c
index 5809bbe360d7..fdad7ad0d9c4 100644
--- a/mm/cma.c
+++ b/mm/cma.c
@@ -407,6 +407,7 @@ struct page *cma_alloc(struct cma *cma, size_t count, unsigned int align,
 	unsigned long pfn = -1;
 	unsigned long start = 0;
 	unsigned long bitmap_maxno, bitmap_no, bitmap_count;
+	size_t i;
 	struct page *page = NULL;
 	int ret = -ENOMEM;
 
@@ -466,6 +467,16 @@ struct page *cma_alloc(struct cma *cma, size_t count, unsigned int align,
 
 	trace_cma_alloc(pfn, page, count, align);
 
+	/*
+	 * CMA can allocate multiple page blocks, which results in different
+	 * blocks being marked with different tags. Reset the tags to ignore
+	 * those page blocks.
+	 */
+	if (page) {
+		for (i = 0; i < count; i++)
+			page_kasan_tag_reset(page + i);
+	}
+
 	if (ret && !(gfp_mask & __GFP_NOWARN)) {
 		pr_err("%s: alloc failed, req-size: %zu pages, ret: %d\n",
 			__func__, count, ret);
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 1e96ca050c75..6cf7dec0b765 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -212,8 +212,15 @@ void kasan_unpoison_stack_above_sp_to(const void *watermark)
 
 void kasan_alloc_pages(struct page *page, unsigned int order)
 {
+	u8 tag;
+	unsigned long i;
+
 	if (unlikely(PageHighMem(page)))
 		return;
+
+	tag = random_tag();
+	for (i = 0; i < (1 << order); i++)
+		page_kasan_tag_set(page + i, tag);
 	kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
 }
 
@@ -311,6 +318,10 @@ struct kasan_free_meta *get_free_info(struct kmem_cache *cache,
 
 void kasan_poison_slab(struct page *page)
 {
+	unsigned long i;
+
+	for (i = 0; i < (1 << compound_order(page)); i++)
+		page_kasan_tag_reset(page + i);
 	kasan_poison_shadow(page_address(page),
 			PAGE_SIZE << compound_order(page),
 			KASAN_KMALLOC_REDZONE);
@@ -483,7 +494,7 @@ void kasan_poison_kfree(void *ptr, unsigned long ip)
 	page = virt_to_head_page(ptr);
 
 	if (unlikely(!PageSlab(page))) {
-		if (reset_tag(ptr) != page_address(page)) {
+		if (ptr != page_address(page)) {
 			kasan_report_invalid_free(ptr, ip);
 			return;
 		}
@@ -496,7 +507,7 @@ void kasan_poison_kfree(void *ptr, unsigned long ip)
 
 void kasan_kfree_large(void *ptr, unsigned long ip)
 {
-	if (reset_tag(ptr) != page_address(virt_to_head_page(ptr)))
+	if (ptr != page_address(virt_to_head_page(ptr)))
 		kasan_report_invalid_free(ptr, ip);
 	/* The object will be poisoned by page_alloc. */
 }
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 1521100f1e63..266e86323d73 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1176,6 +1176,7 @@ static void __meminit __init_single_page(struct page *page, unsigned long pfn,
 	init_page_count(page);
 	page_mapcount_reset(page);
 	page_cpupid_reset_last(page);
+	page_kasan_tag_reset(page);
 
 	INIT_LIST_HEAD(&page->lru);
 #ifdef WANT_PAGE_VIRTUAL
-- 
2.18.0.rc2.346.g013aa6912e-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v4 15/27] compiler: Option to default to hidden symbols ===

From: Thomas Garnier <thgarnie () google ! com>
To: linux-sparse
Subject: [PATCH v4 15/27] compiler: Option to default to hidden symbols
Date: Tue, 29 May 2018 22:15:16 +0000
Message-ID: <20180529221625.33541-16-thgarnie () google ! com>
--------------------
Provide an option to default visibility to hidden except for key
symbols. This option is disabled by default and will be used by x86_64
PIE support to remove errors between compilation units.

The default visibility is also enabled for external symbols that are
compared as they maybe equals (start/end of sections). In this case,
older versions of GCC will remove the comparison if the symbols are
hidden. This issue exists at least on gcc 4.9 and before.

Signed-off-by: Thomas Garnier <thgarnie@google.com>
---
 arch/x86/boot/boot.h                 |  2 +-
 arch/x86/include/asm/setup.h         |  2 +-
 arch/x86/kernel/cpu/microcode/core.c |  4 ++--
 drivers/base/firmware_loader/main.c  |  4 ++--
 include/asm-generic/sections.h       |  6 ++++++
 include/linux/compiler.h             |  7 +++++++
 init/Kconfig                         |  7 +++++++
 kernel/kallsyms.c                    | 16 ++++++++--------
 kernel/trace/trace.h                 |  4 ++--
 lib/dynamic_debug.c                  |  4 ++--
 10 files changed, 38 insertions(+), 18 deletions(-)

diff --git a/arch/x86/boot/boot.h b/arch/x86/boot/boot.h
index ef5a9cc66fb8..d726c35bdd96 100644
--- a/arch/x86/boot/boot.h
+++ b/arch/x86/boot/boot.h
@@ -193,7 +193,7 @@ static inline bool memcmp_gs(const void *s1, addr_t s2, size_t len)
 }
 
 /* Heap -- available for dynamic lists. */
-extern char _end[];
+extern char _end[] __default_visibility;
 extern char *HEAP;
 extern char *heap_end;
 #define RESET_HEAP() ((void *)( HEAP = _end ))
diff --git a/arch/x86/include/asm/setup.h b/arch/x86/include/asm/setup.h
index ae13bc974416..083a6e99b884 100644
--- a/arch/x86/include/asm/setup.h
+++ b/arch/x86/include/asm/setup.h
@@ -68,7 +68,7 @@ static inline void x86_ce4100_early_setup(void) { }
  * This is set up by the setup-routine at boot-time
  */
 extern struct boot_params boot_params;
-extern char _text[];
+extern char _text[] __default_visibility;
 
 static inline bool kaslr_enabled(void)
 {
diff --git a/arch/x86/kernel/cpu/microcode/core.c b/arch/x86/kernel/cpu/microcode/core.c
index 77e201301528..6a4f5d9d7eb6 100644
--- a/arch/x86/kernel/cpu/microcode/core.c
+++ b/arch/x86/kernel/cpu/microcode/core.c
@@ -149,8 +149,8 @@ static bool __init check_loader_disabled_bsp(void)
 	return *res;
 }
 
-extern struct builtin_fw __start_builtin_fw[];
-extern struct builtin_fw __end_builtin_fw[];
+extern struct builtin_fw __start_builtin_fw[] __default_visibility;
+extern struct builtin_fw __end_builtin_fw[] __default_visibility;
 
 bool get_builtin_firmware(struct cpio_data *cd, const char *name)
 {
diff --git a/drivers/base/firmware_loader/main.c b/drivers/base/firmware_loader/main.c
index 0943e7065e0e..2ffd019af2d4 100644
--- a/drivers/base/firmware_loader/main.c
+++ b/drivers/base/firmware_loader/main.c
@@ -94,8 +94,8 @@ static struct firmware_cache fw_cache;
 
 #ifdef CONFIG_FW_LOADER
 
-extern struct builtin_fw __start_builtin_fw[];
-extern struct builtin_fw __end_builtin_fw[];
+extern struct builtin_fw __start_builtin_fw[] __default_visibility;
+extern struct builtin_fw __end_builtin_fw[] __default_visibility;
 
 static void fw_copy_to_prealloc_buf(struct firmware *fw,
 				    void *buf, size_t size)
diff --git a/include/asm-generic/sections.h b/include/asm-generic/sections.h
index 849cd8eb5ca0..0a0e23405ddd 100644
--- a/include/asm-generic/sections.h
+++ b/include/asm-generic/sections.h
@@ -32,6 +32,9 @@
  *	__softirqentry_text_start, __softirqentry_text_end
  *	__start_opd, __end_opd
  */
+#ifdef CONFIG_DEFAULT_HIDDEN
+#pragma GCC visibility push(default)
+#endif
 extern char _text[], _stext[], _etext[];
 extern char _data[], _sdata[], _edata[];
 extern char __bss_start[], __bss_stop[];
@@ -49,6 +52,9 @@ extern char __start_once[], __end_once[];
 
 /* Start and end of .ctors section - used for constructor calls. */
 extern char __ctors_start[], __ctors_end[];
+#ifdef CONFIG_DEFAULT_HIDDEN
+#pragma GCC visibility pop
+#endif
 
 /* Start and end of .opd section - used for function descriptors. */
 extern char __start_opd[], __end_opd[];
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index ab4711c63601..a9ac84e37af9 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -278,6 +278,13 @@ unsigned long read_word_at_a_time(const void *addr)
 	__u.__val;					\
 })
 
+#ifdef CONFIG_DEFAULT_HIDDEN
+#pragma GCC visibility push(hidden)
+#define __default_visibility  __attribute__((visibility ("default")))
+#else
+#define __default_visibility
+#endif
+
 #endif /* __KERNEL__ */
 
 #endif /* __ASSEMBLY__ */
diff --git a/init/Kconfig b/init/Kconfig
index e4acab9f9fd1..f16247675f84 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1694,6 +1694,13 @@ config PROFILING
 config TRACEPOINTS
 	bool
 
+#
+# Default to hidden visibility for all symbols.
+# Useful for Position Independent Code to reduce global references.
+#
+config DEFAULT_HIDDEN
+	bool
+
 source "arch/Kconfig"
 
 endmenu		# General setup
diff --git a/kernel/kallsyms.c b/kernel/kallsyms.c
index a23e21ada81b..f4e58b7a6daf 100644
--- a/kernel/kallsyms.c
+++ b/kernel/kallsyms.c
@@ -29,24 +29,24 @@
  * These will be re-linked against their real values
  * during the second link stage.
  */
-extern const unsigned long kallsyms_addresses[] __weak;
-extern const int kallsyms_offsets[] __weak;
-extern const u8 kallsyms_names[] __weak;
+extern const unsigned long kallsyms_addresses[] __weak __default_visibility;
+extern const int kallsyms_offsets[] __weak __default_visibility;
+extern const u8 kallsyms_names[] __weak __default_visibility;
 
 /*
  * Tell the compiler that the count isn't in the small data section if the arch
  * has one (eg: FRV).
  */
 extern const unsigned long kallsyms_num_syms
-__attribute__((weak, section(".rodata")));
+__attribute__((weak, section(".rodata"))) __default_visibility;
 
 extern const unsigned long kallsyms_relative_base
-__attribute__((weak, section(".rodata")));
+__attribute__((weak, section(".rodata"))) __default_visibility;
 
-extern const u8 kallsyms_token_table[] __weak;
-extern const u16 kallsyms_token_index[] __weak;
+extern const u8 kallsyms_token_table[] __weak __default_visibility;
+extern const u16 kallsyms_token_index[] __weak __default_visibility;
 
-extern const unsigned long kallsyms_markers[] __weak;
+extern const unsigned long kallsyms_markers[] __weak __default_visibility;
 
 /*
  * Expand a compressed symbol data into the resulting uncompressed string,
diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6fb46a06c9dc..e659f452cf8c 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1746,8 +1746,8 @@ extern int trace_event_enable_disable(struct trace_event_file *file,
 				      int enable, int soft_disable);
 extern int tracing_alloc_snapshot(void);
 
-extern const char *__start___trace_bprintk_fmt[];
-extern const char *__stop___trace_bprintk_fmt[];
+extern const char *__start___trace_bprintk_fmt[] __default_visibility;
+extern const char *__stop___trace_bprintk_fmt[] __default_visibility;
 
 extern const char *__start___tracepoint_str[];
 extern const char *__stop___tracepoint_str[];
diff --git a/lib/dynamic_debug.c b/lib/dynamic_debug.c
index c7c96bc7654a..40b752b53627 100644
--- a/lib/dynamic_debug.c
+++ b/lib/dynamic_debug.c
@@ -37,8 +37,8 @@
 #include <linux/device.h>
 #include <linux/netdevice.h>
 
-extern struct _ddebug __start___verbose[];
-extern struct _ddebug __stop___verbose[];
+extern struct _ddebug __start___verbose[] __default_visibility;
+extern struct _ddebug __stop___verbose[] __default_visibility;
 
 struct ddebug_table {
 	struct list_head link;
-- 
2.17.0.921.gf22659ad46-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v4 16/17] khwasan: update kasan documentation ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v4 16/17] khwasan: update kasan documentation
Date: Tue, 26 Jun 2018 13:15:26 +0000
Message-ID: <7e68273c23ba268001aeb995bb93f977098f79db.1530018818.git.andreyknvl () google ! com>
--------------------
This patch updates KASAN documentation to reflect the addition of KHWASAN.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 Documentation/dev-tools/kasan.rst | 213 +++++++++++++++++-------------
 1 file changed, 123 insertions(+), 90 deletions(-)

diff --git a/Documentation/dev-tools/kasan.rst b/Documentation/dev-tools/kasan.rst
index aabc8738b3d8..842d95af74d3 100644
--- a/Documentation/dev-tools/kasan.rst
+++ b/Documentation/dev-tools/kasan.rst
@@ -8,11 +8,19 @@ KernelAddressSANitizer (KASAN) is a dynamic memory error detector. It provides
 a fast and comprehensive solution for finding use-after-free and out-of-bounds
 bugs.
 
-KASAN uses compile-time instrumentation for checking every memory access,
-therefore you will need a GCC version 4.9.2 or later. GCC 5.0 or later is
-required for detection of out-of-bounds accesses to stack or global variables.
+KASAN has two modes: classic KASAN (a classic version, similar to user space
+ASan) and KHWASAN (a version based on memory tagging, similar to user space
+HWASan).
 
-Currently KASAN is supported only for the x86_64 and arm64 architectures.
+KASAN uses compile-time instrumentation to insert validity checks before every
+memory access, and therefore requires a compiler version that supports that.
+For classic KASAN you need GCC version 4.9.2 or later. GCC 5.0 or later is
+required for detection of out-of-bounds accesses on stack and global variables.
+KHWASAN in turns is only supported in clang and requires revision 330044 or
+later.
+
+Currently classic KASAN is supported for the x86_64, arm64 and xtensa
+architectures, and KHWASAN is supported only for arm64.
 
 Usage
 -----
@@ -21,12 +29,14 @@ To enable KASAN configure kernel with::
 
 	  CONFIG_KASAN = y
 
-and choose between CONFIG_KASAN_OUTLINE and CONFIG_KASAN_INLINE. Outline and
-inline are compiler instrumentation types. The former produces smaller binary
-the latter is 1.1 - 2 times faster. Inline instrumentation requires a GCC
+and choose between CONFIG_KASAN_GENERIC (to enable classic KASAN) and
+CONFIG_KASAN_HW (to enabled KHWASAN). You also need to choose choose between
+CONFIG_KASAN_OUTLINE and CONFIG_KASAN_INLINE. Outline and inline are compiler
+instrumentation types. The former produces smaller binary while the latter is
+1.1 - 2 times faster. For classic KASAN inline instrumentation requires GCC
 version 5.0 or later.
 
-KASAN works with both SLUB and SLAB memory allocators.
+Both KASAN modes work with both SLUB and SLAB memory allocators.
 For better bug detection and nicer reporting, enable CONFIG_STACKTRACE.
 
 To disable instrumentation for specific files or directories, add a line
@@ -43,85 +53,80 @@ similar to the following to the respective kernel Makefile:
 Error reports
 ~~~~~~~~~~~~~
 
-A typical out of bounds access report looks like this::
+A typical out-of-bounds access classic KASAN report looks like this::
 
     ==================================================================
-    BUG: AddressSanitizer: out of bounds access in kmalloc_oob_right+0x65/0x75 [test_kasan] at addr ffff8800693bc5d3
-    Write of size 1 by task modprobe/1689
-    =============================================================================
-    BUG kmalloc-128 (Not tainted): kasan error
-    -----------------------------------------------------------------------------
-
-    Disabling lock debugging due to kernel taint
-    INFO: Allocated in kmalloc_oob_right+0x3d/0x75 [test_kasan] age=0 cpu=0 pid=1689
-     __slab_alloc+0x4b4/0x4f0
-     kmem_cache_alloc_trace+0x10b/0x190
-     kmalloc_oob_right+0x3d/0x75 [test_kasan]
-     init_module+0x9/0x47 [test_kasan]
-     do_one_initcall+0x99/0x200
-     load_module+0x2cb3/0x3b20
-     SyS_finit_module+0x76/0x80
-     system_call_fastpath+0x12/0x17
-    INFO: Slab 0xffffea0001a4ef00 objects=17 used=7 fp=0xffff8800693bd728 flags=0x100000000004080
-    INFO: Object 0xffff8800693bc558 @offset=1368 fp=0xffff8800693bc720
-
-    Bytes b4 ffff8800693bc548: 00 00 00 00 00 00 00 00 5a 5a 5a 5a 5a 5a 5a 5a  ........ZZZZZZZZ
-    Object ffff8800693bc558: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b  kkkkkkkkkkkkkkkk
-    Object ffff8800693bc568: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b  kkkkkkkkkkkkkkkk
-    Object ffff8800693bc578: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b  kkkkkkkkkkkkkkkk
-    Object ffff8800693bc588: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b  kkkkkkkkkkkkkkkk
-    Object ffff8800693bc598: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b  kkkkkkkkkkkkkkkk
-    Object ffff8800693bc5a8: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b  kkkkkkkkkkkkkkkk
-    Object ffff8800693bc5b8: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b  kkkkkkkkkkkkkkkk
-    Object ffff8800693bc5c8: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b a5  kkkkkkkkkkkkkkk.
-    Redzone ffff8800693bc5d8: cc cc cc cc cc cc cc cc                          ........
-    Padding ffff8800693bc718: 5a 5a 5a 5a 5a 5a 5a 5a                          ZZZZZZZZ
-    CPU: 0 PID: 1689 Comm: modprobe Tainted: G    B          3.18.0-rc1-mm1+ #98
-    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.7.5-0-ge51488c-20140602_164612-nilsson.home.kraxel.org 04/01/2014
-     ffff8800693bc000 0000000000000000 ffff8800693bc558 ffff88006923bb78
-     ffffffff81cc68ae 00000000000000f3 ffff88006d407600 ffff88006923bba8
-     ffffffff811fd848 ffff88006d407600 ffffea0001a4ef00 ffff8800693bc558
+    BUG: KASAN: slab-out-of-bounds in kmalloc_oob_right+0xa8/0xbc [test_kasan]
+    Write of size 1 at addr ffff8800696f3d3b by task insmod/2734
+    
+    CPU: 0 PID: 2734 Comm: insmod Not tainted 4.15.0+ #98
+    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.10.2-1 04/01/2014
     Call Trace:
-     [<ffffffff81cc68ae>] dump_stack+0x46/0x58
-     [<ffffffff811fd848>] print_trailer+0xf8/0x160
-     [<ffffffffa00026a7>] ? kmem_cache_oob+0xc3/0xc3 [test_kasan]
-     [<ffffffff811ff0f5>] object_err+0x35/0x40
-     [<ffffffffa0002065>] ? kmalloc_oob_right+0x65/0x75 [test_kasan]
-     [<ffffffff8120b9fa>] kasan_report_error+0x38a/0x3f0
-     [<ffffffff8120a79f>] ? kasan_poison_shadow+0x2f/0x40
-     [<ffffffff8120b344>] ? kasan_unpoison_shadow+0x14/0x40
-     [<ffffffff8120a79f>] ? kasan_poison_shadow+0x2f/0x40
-     [<ffffffffa00026a7>] ? kmem_cache_oob+0xc3/0xc3 [test_kasan]
-     [<ffffffff8120a995>] __asan_store1+0x75/0xb0
-     [<ffffffffa0002601>] ? kmem_cache_oob+0x1d/0xc3 [test_kasan]
-     [<ffffffffa0002065>] ? kmalloc_oob_right+0x65/0x75 [test_kasan]
-     [<ffffffffa0002065>] kmalloc_oob_right+0x65/0x75 [test_kasan]
-     [<ffffffffa00026b0>] init_module+0x9/0x47 [test_kasan]
-     [<ffffffff810002d9>] do_one_initcall+0x99/0x200
-     [<ffffffff811e4e5c>] ? __vunmap+0xec/0x160
-     [<ffffffff81114f63>] load_module+0x2cb3/0x3b20
-     [<ffffffff8110fd70>] ? m_show+0x240/0x240
-     [<ffffffff81115f06>] SyS_finit_module+0x76/0x80
-     [<ffffffff81cd3129>] system_call_fastpath+0x12/0x17
+     __dump_stack lib/dump_stack.c:17
+     dump_stack+0x83/0xbc lib/dump_stack.c:53
+     print_address_description+0x73/0x280 mm/kasan/report.c:254
+     kasan_report_error mm/kasan/report.c:352
+     kasan_report+0x10e/0x220 mm/kasan/report.c:410
+     __asan_report_store1_noabort+0x17/0x20 mm/kasan/report.c:505
+     kmalloc_oob_right+0xa8/0xbc [test_kasan] lib/test_kasan.c:42
+     kmalloc_tests_init+0x16/0x769 [test_kasan]
+     do_one_initcall+0x9e/0x240 init/main.c:832
+     do_init_module+0x1b6/0x542 kernel/module.c:3462
+     load_module+0x6042/0x9030 kernel/module.c:3786
+     SYSC_init_module+0x18f/0x1c0 kernel/module.c:3858
+     SyS_init_module+0x9/0x10 kernel/module.c:3841
+     do_syscall_64+0x198/0x480 arch/x86/entry/common.c:287
+     entry_SYSCALL_64_after_hwframe+0x21/0x86 arch/x86/entry/entry_64.S:251
+    RIP: 0033:0x7fdd79df99da
+    RSP: 002b:00007fff2229bdf8 EFLAGS: 00000202 ORIG_RAX: 00000000000000af
+    RAX: ffffffffffffffda RBX: 000055c408121190 RCX: 00007fdd79df99da
+    RDX: 00007fdd7a0b8f88 RSI: 0000000000055670 RDI: 00007fdd7a47e000
+    RBP: 000055c4081200b0 R08: 0000000000000003 R09: 0000000000000000
+    R10: 00007fdd79df5d0a R11: 0000000000000202 R12: 00007fdd7a0b8f88
+    R13: 000055c408120090 R14: 0000000000000000 R15: 0000000000000000
+    
+    Allocated by task 2734:
+     save_stack+0x43/0xd0 mm/kasan/common.c:176
+     set_track+0x20/0x30 mm/kasan/common.c:188
+     kasan_kmalloc+0x9a/0xc0 mm/kasan/kasan.c:372
+     kmem_cache_alloc_trace+0xcd/0x1a0 mm/slub.c:2761
+     kmalloc ./include/linux/slab.h:512
+     kmalloc_oob_right+0x56/0xbc [test_kasan] lib/test_kasan.c:36
+     kmalloc_tests_init+0x16/0x769 [test_kasan]
+     do_one_initcall+0x9e/0x240 init/main.c:832
+     do_init_module+0x1b6/0x542 kernel/module.c:3462
+     load_module+0x6042/0x9030 kernel/module.c:3786
+     SYSC_init_module+0x18f/0x1c0 kernel/module.c:3858
+     SyS_init_module+0x9/0x10 kernel/module.c:3841
+     do_syscall_64+0x198/0x480 arch/x86/entry/common.c:287
+     entry_SYSCALL_64_after_hwframe+0x21/0x86 arch/x86/entry/entry_64.S:251
+    
+    The buggy address belongs to the object at ffff8800696f3cc0
+     which belongs to the cache kmalloc-128 of size 128
+    The buggy address is located 123 bytes inside of
+     128-byte region [ffff8800696f3cc0, ffff8800696f3d40)
+    The buggy address belongs to the page:
+    page:ffffea0001a5bcc0 count:1 mapcount:0 mapping:          (null) index:0x0
+    flags: 0x100000000000100(slab)
+    raw: 0100000000000100 0000000000000000 0000000000000000 0000000180150015
+    raw: ffffea0001a8ce40 0000000300000003 ffff88006d001640 0000000000000000
+    page dumped because: kasan: bad access detected
+    
     Memory state around the buggy address:
-     ffff8800693bc300: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
-     ffff8800693bc380: fc fc 00 00 00 00 00 00 00 00 00 00 00 00 00 fc
-     ffff8800693bc400: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
-     ffff8800693bc480: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
-     ffff8800693bc500: fc fc fc fc fc fc fc fc fc fc fc 00 00 00 00 00
-    >ffff8800693bc580: 00 00 00 00 00 00 00 00 00 00 03 fc fc fc fc fc
-                                                 ^
-     ffff8800693bc600: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
-     ffff8800693bc680: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
-     ffff8800693bc700: fc fc fc fc fb fb fb fb fb fb fb fb fb fb fb fb
-     ffff8800693bc780: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
-     ffff8800693bc800: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
+     ffff8800696f3c00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 fc
+     ffff8800696f3c80: fc fc fc fc fc fc fc fc 00 00 00 00 00 00 00 00
+    >ffff8800696f3d00: 00 00 00 00 00 00 00 03 fc fc fc fc fc fc fc fc
+                                            ^
+     ffff8800696f3d80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 fc fc
+     ffff8800696f3e00: fc fc fc fc fc fc fc fc fb fb fb fb fb fb fb fb
     ==================================================================
 
-The header of the report discribe what kind of bug happened and what kind of
-access caused it. It's followed by the description of the accessed slub object
-(see 'SLUB Debug output' section in Documentation/vm/slub.rst for details) and
-the description of the accessed memory page.
+The header of the report provides a short summary of what kind of bug happened
+and what kind of access caused it. It's followed by a stack trace of the bad
+access, a stack trace of where the accessed memory was allocated (in case bad
+access happens on a slab object), and a stack trace of where the object was
+freed (in case of a use-after-free bug report). Next comes a description of
+the accessed slab object and information about the accessed memory page.
 
 In the last section the report shows memory state around the accessed address.
 Reading this part requires some understanding of how KASAN works.
@@ -138,18 +143,24 @@ inaccessible memory like redzones or freed memory (see mm/kasan/kasan.h).
 In the report above the arrows point to the shadow byte 03, which means that
 the accessed address is partially accessible.
 
+For KHWASAN this last report section shows the memory tags around the accessed
+address (see Implementation details section).
+
 
 Implementation details
 ----------------------
 
+Classic KASAN
+~~~~~~~~~~~~~
+
 From a high level, our approach to memory error detection is similar to that
 of kmemcheck: use shadow memory to record whether each byte of memory is safe
-to access, and use compile-time instrumentation to check shadow memory on each
-memory access.
+to access, and use compile-time instrumentation to insert checks of shadow
+memory on each memory access.
 
-AddressSanitizer dedicates 1/8 of kernel memory to its shadow memory
-(e.g. 16TB to cover 128TB on x86_64) and uses direct mapping with a scale and
-offset to translate a memory address to its corresponding shadow address.
+Classic KASAN dedicates 1/8th of kernel memory to its shadow memory (e.g. 16TB
+to cover 128TB on x86_64) and uses direct mapping with a scale and offset to
+translate a memory address to its corresponding shadow address.
 
 Here is the function which translates an address to its corresponding shadow
 address::
@@ -162,12 +173,34 @@ address::
 
 where ``KASAN_SHADOW_SCALE_SHIFT = 3``.
 
-Compile-time instrumentation used for checking memory accesses. Compiler inserts
-function calls (__asan_load*(addr), __asan_store*(addr)) before each memory
-access of size 1, 2, 4, 8 or 16. These functions check whether memory access is
-valid or not by checking corresponding shadow memory.
+Compile-time instrumentation is used to insert memory access checks. Compiler
+inserts function calls (__asan_load*(addr), __asan_store*(addr)) before each
+memory access of size 1, 2, 4, 8 or 16. These functions check whether memory
+access is valid or not by checking corresponding shadow memory.
 
 GCC 5.0 has possibility to perform inline instrumentation. Instead of making
 function calls GCC directly inserts the code to check the shadow memory.
 This option significantly enlarges kernel but it gives x1.1-x2 performance
 boost over outline instrumented kernel.
+
+KHWASAN
+~~~~~~~
+
+KHWASAN uses the Top Byte Ignore (TBI) feature of modern arm64 CPUs to store
+a pointer tag in the top byte of kernel pointers. KHWASAN also uses shadow
+memory to store memory tags associated with each 16-byte memory cell (therefore
+it dedicates 1/16th of the kernel memory for shadow memory).
+
+On each memory allocation KHWASAN generates a random tag, tags allocated memory
+with this tag, and embeds this tag into the returned pointer. KHWASAN uses
+compile-time instrumentation to insert checks before each memory access. These
+checks make sure that tag of the memory that is being accessed is equal to tag
+of the pointer that is used to access this memory. In case of a tag mismatch
+KHWASAN prints a bug report.
+
+KHWASAN also has two instrumentation modes (outline, that emits callbacks to
+check memory accesses; and inline, that performs the shadow memory checks
+inline). With outline instrumentation mode, a bug report is simply printed
+from the function that performs the access check. With inline instrumentation
+a brk instruction is emitted by the compiler, and a dedicated brk handler is
+used to print KHWASAN reports.
-- 
2.18.0.rc2.346.g013aa6912e-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH v4 2/4] fixup! add predefined macros for [u]int32_t ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v4 2/4] fixup! add predefined macros for [u]int32_t
Date: Mon, 17 Dec 2018 00:02:14 +0000
Message-ID: <20181217000216.8283-3-luc.vanoostenryck () gmail ! com>
--------------------
---
 lib.c | 8 --------
 1 file changed, 8 deletions(-)

diff --git a/lib.c b/lib.c
index 2a18b05e9..592fc414e 100644
--- a/lib.c
+++ b/lib.c
@@ -476,14 +476,6 @@ static void handle_arch_m64_finalize(void)
 		goto case_x86_64;
 	case ARCH_LP32:
 		/* default values */
-#if defined(__m68k__) \
-	|| ((defined(__sparc__) || defined(__sparc)) && !defined(__sparc_v9__)) \
-	|| defined(__mips__) || defined(__mips) \
-	|| defined(__powerpc__) || defined(__PPC__) || defined(PPC) \
-	|| defined(__riscv__) || defined(__riscv)
-		int32_ctype = &long_ctype;
-		uint32_ctype = &ulong_ctype;
-#endif
 		int64_ctype = &llong_ctype;
 		uint64_ctype = &ullong_ctype;
 		intmax_ctype = &llong_ctype;
-- 
2.20.0

================================================================================


################################################################################

=== Thread: [PATCH v4 4/4] add predefine_min() and use it for __{WCHAR,WINT}_MIN__ ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH v4 4/4] add predefine_min() and use it for __{WCHAR,WINT}_MIN__
Date: Mon, 17 Dec 2018 00:02:16 +0000
Message-ID: <20181217000216.8283-5-luc.vanoostenryck () gmail ! com>
--------------------
wchar_t & wint_t seems to be the only integer types needing
the _MIN__ macros.

Extend predefine_ctype() to handle these and use it to define
__WCHAR_MIN__ & __WINT_MIN__.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 lib.c | 19 +++++++++++++++++--
 1 file changed, 17 insertions(+), 2 deletions(-)

diff --git a/lib.c b/lib.c
index ed2f5e3dd..e4a65e937 100644
--- a/lib.c
+++ b/lib.c
@@ -1172,6 +1172,19 @@ static void predefined_max(const char *name, struct symbol *type)
 	predefine(buf, 1, "%#llx%s", max, suffix);
 }
 
+static void predefined_min(const char *name, struct symbol *type)
+{
+	const char *suffix = builtin_type_suffix(type);
+	char buf[32];
+
+	snprintf(buf, sizeof(buf), "__%s_MIN__", name);
+
+	if (is_signed_type(type))
+		predefine(buf, 1, "(-__%s_MAX__ - 1)", name);
+	else
+		predefine(buf, 1, "0%s", suffix);
+}
+
 static void predefined_type(const char *name, struct symbol *type)
 {
 	const char *typename = builtin_typename(type);
@@ -1188,6 +1201,8 @@ static void predefined_ctype(const char *name, struct symbol *type, int flags)
 	}
 	if (flags & PTYPE_MAX)
 		predefined_max(name, type);
+	if (flags & PTYPE_MIN)
+		predefined_min(name, type);
 	if (flags & PTYPE_TYPE)
 		predefined_type(name, type);
 	if (flags & PTYPE_WIDTH)
@@ -1242,8 +1257,8 @@ static void predefined_macros(void)
 	predefined_ctype("SHORT",     &short_ctype, PTYPE_SIZEOF);
 	predefined_ctype("SHRT",      &short_ctype, PTYPE_MAX|PTYPE_WIDTH);
 	predefined_ctype("SCHAR",     &schar_ctype, PTYPE_MAX|PTYPE_WIDTH);
-	predefined_ctype("WCHAR",      wchar_ctype, PTYPE_ALL_T|PTYPE_TYPE);
-	predefined_ctype("WINT",        wint_ctype, PTYPE_ALL_T|PTYPE_TYPE);
+	predefined_ctype("WCHAR",      wchar_ctype, PTYPE_ALL_T|PTYPE_MIN|PTYPE_TYPE);
+	predefined_ctype("WINT",        wint_ctype, PTYPE_ALL_T|PTYPE_MIN|PTYPE_TYPE);
 	predefined_ctype("CHAR16",   &ushort_ctype, PTYPE_TYPE);
 	predefined_ctype("CHAR32",     &uint_ctype, PTYPE_TYPE);
 
-- 
2.20.0

================================================================================


################################################################################

=== Thread: [PATCH v5 00/15] Compiler Attributes ===

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kbuild
Subject: [PATCH v5 00/15] Compiler Attributes
Date: Thu, 20 Sep 2018 17:22:46 +0000
Message-ID: <20180920172301.21868-1-miguel.ojeda.sandonis () gmail ! com>
--------------------
The Compiler Attributes series is an effort to disentangle
the include/linux/compiler*.h headers and bring them up to date.

The main idea behind the series is to use feature checking macros
(i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
which are compiler-agnostic (so they can be shared, reducing the size
of compiler-specific headers) and version-agnostic.

Other related improvements have been performed in the headers as well,
which on top of the use of __has_attribute it has amounted to a significant
simplification of these headers (e.g. GCC_VERSION is now only guarding 4
non-attribute macros).

This series should also help the efforts to support compiling the kernel
with clang and icc. A fair amount of documentation and comments have also
been added, clarified or removed; and the headers are now more readable,
which should help kernel developers in general.

The series was triggered due to the move to gcc >= 4.6. In turn, this series
has also triggered Sparse to gain the ability to recognize __has_attribute
on its own.

Finally, the nonstring variable attribute series has been applied on top
of this one.

You can also fetch it from:

  https://github.com/ojeda/linux/tree/compiler-attributes-v5

Enjoy!

Cheers,
Miguel

Cc: Andreas Dilger <adilger.kernel@dilger.ca>
Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
Cc: Michal Marek <michal.lkml@markovi.net>
Cc: Steven Rostedt <rostedt@goodmis.org>
Cc: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
Cc: Olof Johansson <olof@lxom.net>
Cc: Konstantin Ryabitsev <konstantin@linuxfoundation.org>
Cc: David S. Miller <davem@davemloft.net>
Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
Cc: Kees Cook <keescook@chromium.org>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: Ingo Molnar <mingo@kernel.org>
Cc: Paul Lawrence <paullawrence@google.com>
Cc: Sandipan Das <sandipan@linux.vnet.ibm.com>
Cc: Andrey Konovalov <andreyknvl@google.com>
Cc: David Woodhouse <dwmw2@infradead.org>
Cc: Will Deacon <will.deacon@arm.com>
Cc: Philippe Ombredanne <pombredanne@nexb.com>
Cc: Paul Burton <paul.burton@mips.com>
Cc: David Rientjes <rientjes@google.com>
Cc: Willy Tarreau <w@1wt.eu>
Cc: Martin Sebor <msebor@gmail.com>
Cc: Christopher Li <sparse@chrisli.org>
Cc: Jonathan Corbet <corbet@lwn.net>
Cc: Theodore Ts'o <tytso@mit.edu>
Cc: Geert Uytterhoeven <geert@linux-m68k.org>
Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Cc: Joe Perches <joe@perches.com>
Cc: Arnd Bergmann <arnd@arndb.de>
Cc: Dominique Martinet <asmadeus@codewreck.org>
Cc: Stefan Agner <stefan@agner.ch>
Cc: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
Cc: Nick Desaulniers <ndesaulniers@google.com>
Cc: Andrew Morton <akpm@linux-foundation.org>
Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: linux-doc@vger.kernel.org
Cc: linux-ext4@vger.kernel.org
Cc: linux-sparse@vger.kernel.org
Cc: linux-kbuild@vger.kernel.org

v4 -> v5

  * Removed the 2 __naked patches, since Greg just applied them to master;
    and rebased on top of that. The only change in the range-diff is in
    "Compiler Attributes: always use the extra-underscores syntax" due
    to the move of __naked to compiler_types.h.

  * Applied the nonstring series on top of this one (last 4 patches).
    Rationale:
      - the nonstring series would anyway be modified again by this one
        (so it has been rebased after the general cleanup).
      - ext4's "local" nonstring was merged
      - easier to manage (and easier to see the end result)

    The first 2 patches of the last 4 add the warning back at W=1 and
    add the __nonstring attribute. The other two serve as an example
    of usage and as a cleanup.

  * Cc lists removed from each commit; using a single list in the cover letter;
    and a new, revised list of Cc (wider audience).

Miguel Ojeda (15):
  Compiler Attributes: remove unused attributes
  Compiler Attributes: always use the extra-underscores syntax
  Compiler Attributes: remove unneeded tests
  Compiler Attributes: homogenize __must_be_array
  Compiler Attributes: remove unneeded sparse (__CHECKER__) tests
  Compiler Attributes: add missing SPDX ID in compiler_types.h
  Compiler Attributes: use feature checks instead of version checks
  Compiler Attributes: KENTRY used twice the "used" attribute
  Compiler Attributes: remove uses of __attribute__ from compiler.h
  Compiler Attributes: add Doc/process/programming-language.rst
  Compiler Attributes: add MAINTAINERS entry
  Compiler Attributes: add support for __nonstring (gcc >= 8)
  Compiler Attributes: enable -Wstringop-truncation on W=1 (gcc >= 8)
  Compiler Attributes: auxdisplay: panel: use __nonstring
  Compiler Attributes: ext4: remove local __nonstring definition

 Documentation/process/index.rst               |   1 +
 .../process/programming-language.rst          |  45 +++
 MAINTAINERS                                   |   5 +
 drivers/auxdisplay/panel.c                    |   7 +-
 fs/ext4/ext4.h                                |   9 -
 include/linux/compiler-clang.h                |   5 -
 include/linux/compiler-gcc.h                  |  70 +----
 include/linux/compiler-intel.h                |   9 -
 include/linux/compiler.h                      |  19 +-
 include/linux/compiler_attributes.h           | 258 ++++++++++++++++++
 include/linux/compiler_types.h                | 101 +------
 scripts/Makefile.extrawarn                    |   1 +
 12 files changed, 341 insertions(+), 189 deletions(-)
 create mode 100644 Documentation/process/programming-language.rst
 create mode 100644 include/linux/compiler_attributes.h

-- 
2.17.1

================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-ext4
Subject: [PATCH v5 00/15] Compiler Attributes
Date: Thu, 20 Sep 2018 17:22:46 +0000
Message-ID: <20180920172301.21868-1-miguel.ojeda.sandonis () gmail ! com>
--------------------
The Compiler Attributes series is an effort to disentangle
the include/linux/compiler*.h headers and bring them up to date.

The main idea behind the series is to use feature checking macros
(i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
which are compiler-agnostic (so they can be shared, reducing the size
of compiler-specific headers) and version-agnostic.

Other related improvements have been performed in the headers as well,
which on top of the use of __has_attribute it has amounted to a significant
simplification of these headers (e.g. GCC_VERSION is now only guarding 4
non-attribute macros).

This series should also help the efforts to support compiling the kernel
with clang and icc. A fair amount of documentation and comments have also
been added, clarified or removed; and the headers are now more readable,
which should help kernel developers in general.

The series was triggered due to the move to gcc >= 4.6. In turn, this series
has also triggered Sparse to gain the ability to recognize __has_attribute
on its own.

Finally, the nonstring variable attribute series has been applied on top
of this one.

You can also fetch it from:

  https://github.com/ojeda/linux/tree/compiler-attributes-v5

Enjoy!

Cheers,
Miguel

Cc: Andreas Dilger <adilger.kernel@dilger.ca>
Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
Cc: Michal Marek <michal.lkml@markovi.net>
Cc: Steven Rostedt <rostedt@goodmis.org>
Cc: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
Cc: Olof Johansson <olof@lxom.net>
Cc: Konstantin Ryabitsev <konstantin@linuxfoundation.org>
Cc: David S. Miller <davem@davemloft.net>
Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
Cc: Kees Cook <keescook@chromium.org>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: Ingo Molnar <mingo@kernel.org>
Cc: Paul Lawrence <paullawrence@google.com>
Cc: Sandipan Das <sandipan@linux.vnet.ibm.com>
Cc: Andrey Konovalov <andreyknvl@google.com>
Cc: David Woodhouse <dwmw2@infradead.org>
Cc: Will Deacon <will.deacon@arm.com>
Cc: Philippe Ombredanne <pombredanne@nexb.com>
Cc: Paul Burton <paul.burton@mips.com>
Cc: David Rientjes <rientjes@google.com>
Cc: Willy Tarreau <w@1wt.eu>
Cc: Martin Sebor <msebor@gmail.com>
Cc: Christopher Li <sparse@chrisli.org>
Cc: Jonathan Corbet <corbet@lwn.net>
Cc: Theodore Ts'o <tytso@mit.edu>
Cc: Geert Uytterhoeven <geert@linux-m68k.org>
Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Cc: Joe Perches <joe@perches.com>
Cc: Arnd Bergmann <arnd@arndb.de>
Cc: Dominique Martinet <asmadeus@codewreck.org>
Cc: Stefan Agner <stefan@agner.ch>
Cc: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
Cc: Nick Desaulniers <ndesaulniers@google.com>
Cc: Andrew Morton <akpm@linux-foundation.org>
Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: linux-doc@vger.kernel.org
Cc: linux-ext4@vger.kernel.org
Cc: linux-sparse@vger.kernel.org
Cc: linux-kbuild@vger.kernel.org

v4 -> v5

  * Removed the 2 __naked patches, since Greg just applied them to master;
    and rebased on top of that. The only change in the range-diff is in
    "Compiler Attributes: always use the extra-underscores syntax" due
    to the move of __naked to compiler_types.h.

  * Applied the nonstring series on top of this one (last 4 patches).
    Rationale:
      - the nonstring series would anyway be modified again by this one
        (so it has been rebased after the general cleanup).
      - ext4's "local" nonstring was merged
      - easier to manage (and easier to see the end result)

    The first 2 patches of the last 4 add the warning back at W=1 and
    add the __nonstring attribute. The other two serve as an example
    of usage and as a cleanup.

  * Cc lists removed from each commit; using a single list in the cover letter;
    and a new, revised list of Cc (wider audience).

Miguel Ojeda (15):
  Compiler Attributes: remove unused attributes
  Compiler Attributes: always use the extra-underscores syntax
  Compiler Attributes: remove unneeded tests
  Compiler Attributes: homogenize __must_be_array
  Compiler Attributes: remove unneeded sparse (__CHECKER__) tests
  Compiler Attributes: add missing SPDX ID in compiler_types.h
  Compiler Attributes: use feature checks instead of version checks
  Compiler Attributes: KENTRY used twice the "used" attribute
  Compiler Attributes: remove uses of __attribute__ from compiler.h
  Compiler Attributes: add Doc/process/programming-language.rst
  Compiler Attributes: add MAINTAINERS entry
  Compiler Attributes: add support for __nonstring (gcc >= 8)
  Compiler Attributes: enable -Wstringop-truncation on W=1 (gcc >= 8)
  Compiler Attributes: auxdisplay: panel: use __nonstring
  Compiler Attributes: ext4: remove local __nonstring definition

 Documentation/process/index.rst               |   1 +
 .../process/programming-language.rst          |  45 +++
 MAINTAINERS                                   |   5 +
 drivers/auxdisplay/panel.c                    |   7 +-
 fs/ext4/ext4.h                                |   9 -
 include/linux/compiler-clang.h                |   5 -
 include/linux/compiler-gcc.h                  |  70 +----
 include/linux/compiler-intel.h                |   9 -
 include/linux/compiler.h                      |  19 +-
 include/linux/compiler_attributes.h           | 258 ++++++++++++++++++
 include/linux/compiler_types.h                | 101 +------
 scripts/Makefile.extrawarn                    |   1 +
 12 files changed, 341 insertions(+), 189 deletions(-)
 create mode 100644 Documentation/process/programming-language.rst
 create mode 100644 include/linux/compiler_attributes.h

-- 
2.17.1

================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-sparse
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Thu, 20 Sep 2018 18:18:17 +0000
Message-ID: <CAKwvOdk2O+-FqXU1R8m=vSbZuMGJxY-_4AWd+tFpcTDKhWWFaw () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 10:23 AM Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
>
> The Compiler Attributes series is an effort to disentangle
> the include/linux/compiler*.h headers and bring them up to date.
>
> The main idea behind the series is to use feature checking macros
> (i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
> which are compiler-agnostic (so they can be shared, reducing the size
> of compiler-specific headers) and version-agnostic.
>
> Other related improvements have been performed in the headers as well,
> which on top of the use of __has_attribute it has amounted to a significant
> simplification of these headers (e.g. GCC_VERSION is now only guarding 4
> non-attribute macros).
>
> This series should also help the efforts to support compiling the kernel
> with clang and icc. A fair amount of documentation and comments have also
> been added, clarified or removed; and the headers are now more readable,
> which should help kernel developers in general.
>
> The series was triggered due to the move to gcc >= 4.6. In turn, this series
> has also triggered Sparse to gain the ability to recognize __has_attribute
> on its own.
>
> Finally, the nonstring variable attribute series has been applied on top
> of this one.
>
> You can also fetch it from:
>
>   https://github.com/ojeda/linux/tree/compiler-attributes-v5
>
> Enjoy!
>
> Cheers,
> Miguel

Thanks for this series.  I've reviewed the rest of the patches in the
series that didn't already have my reviewed by tags on them.  They
look good to go to me.

>
> Cc: Andreas Dilger <adilger.kernel@dilger.ca>
> Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
> Cc: Michal Marek <michal.lkml@markovi.net>
> Cc: Steven Rostedt <rostedt@goodmis.org>
> Cc: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
> Cc: Olof Johansson <olof@lxom.net>
> Cc: Konstantin Ryabitsev <konstantin@linuxfoundation.org>
> Cc: David S. Miller <davem@davemloft.net>
> Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
> Cc: Kees Cook <keescook@chromium.org>
> Cc: Thomas Gleixner <tglx@linutronix.de>
> Cc: Ingo Molnar <mingo@kernel.org>
> Cc: Paul Lawrence <paullawrence@google.com>
> Cc: Sandipan Das <sandipan@linux.vnet.ibm.com>
> Cc: Andrey Konovalov <andreyknvl@google.com>
> Cc: David Woodhouse <dwmw2@infradead.org>
> Cc: Will Deacon <will.deacon@arm.com>
> Cc: Philippe Ombredanne <pombredanne@nexb.com>
> Cc: Paul Burton <paul.burton@mips.com>
> Cc: David Rientjes <rientjes@google.com>
> Cc: Willy Tarreau <w@1wt.eu>
> Cc: Martin Sebor <msebor@gmail.com>
> Cc: Christopher Li <sparse@chrisli.org>
> Cc: Jonathan Corbet <corbet@lwn.net>
> Cc: Theodore Ts'o <tytso@mit.edu>
> Cc: Geert Uytterhoeven <geert@linux-m68k.org>
> Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
> Cc: Joe Perches <joe@perches.com>
> Cc: Arnd Bergmann <arnd@arndb.de>
> Cc: Dominique Martinet <asmadeus@codewreck.org>
> Cc: Stefan Agner <stefan@agner.ch>
> Cc: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> Cc: Nick Desaulniers <ndesaulniers@google.com>
> Cc: Andrew Morton <akpm@linux-foundation.org>
> Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
> Cc: Linus Torvalds <torvalds@linux-foundation.org>
> Cc: linux-doc@vger.kernel.org
> Cc: linux-ext4@vger.kernel.org
> Cc: linux-sparse@vger.kernel.org
> Cc: linux-kbuild@vger.kernel.org
>
> v4 -> v5
>
>   * Removed the 2 __naked patches, since Greg just applied them to master;
>     and rebased on top of that. The only change in the range-diff is in
>     "Compiler Attributes: always use the extra-underscores syntax" due
>     to the move of __naked to compiler_types.h.
>
>   * Applied the nonstring series on top of this one (last 4 patches).
>     Rationale:
>       - the nonstring series would anyway be modified again by this one
>         (so it has been rebased after the general cleanup).
>       - ext4's "local" nonstring was merged
>       - easier to manage (and easier to see the end result)
>
>     The first 2 patches of the last 4 add the warning back at W=1 and
>     add the __nonstring attribute. The other two serve as an example
>     of usage and as a cleanup.
>
>   * Cc lists removed from each commit; using a single list in the cover letter;
>     and a new, revised list of Cc (wider audience).
>
> Miguel Ojeda (15):
>   Compiler Attributes: remove unused attributes
>   Compiler Attributes: always use the extra-underscores syntax
>   Compiler Attributes: remove unneeded tests
>   Compiler Attributes: homogenize __must_be_array
>   Compiler Attributes: remove unneeded sparse (__CHECKER__) tests
>   Compiler Attributes: add missing SPDX ID in compiler_types.h
>   Compiler Attributes: use feature checks instead of version checks
>   Compiler Attributes: KENTRY used twice the "used" attribute
>   Compiler Attributes: remove uses of __attribute__ from compiler.h
>   Compiler Attributes: add Doc/process/programming-language.rst
>   Compiler Attributes: add MAINTAINERS entry
>   Compiler Attributes: add support for __nonstring (gcc >= 8)
>   Compiler Attributes: enable -Wstringop-truncation on W=1 (gcc >= 8)
>   Compiler Attributes: auxdisplay: panel: use __nonstring
>   Compiler Attributes: ext4: remove local __nonstring definition
>
>  Documentation/process/index.rst               |   1 +
>  .../process/programming-language.rst          |  45 +++
>  MAINTAINERS                                   |   5 +
>  drivers/auxdisplay/panel.c                    |   7 +-
>  fs/ext4/ext4.h                                |   9 -
>  include/linux/compiler-clang.h                |   5 -
>  include/linux/compiler-gcc.h                  |  70 +----
>  include/linux/compiler-intel.h                |   9 -
>  include/linux/compiler.h                      |  19 +-
>  include/linux/compiler_attributes.h           | 258 ++++++++++++++++++
>  include/linux/compiler_types.h                | 101 +------
>  scripts/Makefile.extrawarn                    |   1 +
>  12 files changed, 341 insertions(+), 189 deletions(-)
>  create mode 100644 Documentation/process/programming-language.rst
>  create mode 100644 include/linux/compiler_attributes.h
>
> --
> 2.17.1
>


-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-kernel
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Thu, 20 Sep 2018 18:18:17 +0000
Message-ID: <CAKwvOdk2O+-FqXU1R8m=vSbZuMGJxY-_4AWd+tFpcTDKhWWFaw () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 10:23 AM Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
>
> The Compiler Attributes series is an effort to disentangle
> the include/linux/compiler*.h headers and bring them up to date.
>
> The main idea behind the series is to use feature checking macros
> (i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
> which are compiler-agnostic (so they can be shared, reducing the size
> of compiler-specific headers) and version-agnostic.
>
> Other related improvements have been performed in the headers as well,
> which on top of the use of __has_attribute it has amounted to a significant
> simplification of these headers (e.g. GCC_VERSION is now only guarding 4
> non-attribute macros).
>
> This series should also help the efforts to support compiling the kernel
> with clang and icc. A fair amount of documentation and comments have also
> been added, clarified or removed; and the headers are now more readable,
> which should help kernel developers in general.
>
> The series was triggered due to the move to gcc >= 4.6. In turn, this series
> has also triggered Sparse to gain the ability to recognize __has_attribute
> on its own.
>
> Finally, the nonstring variable attribute series has been applied on top
> of this one.
>
> You can also fetch it from:
>
>   https://github.com/ojeda/linux/tree/compiler-attributes-v5
>
> Enjoy!
>
> Cheers,
> Miguel

Thanks for this series.  I've reviewed the rest of the patches in the
series that didn't already have my reviewed by tags on them.  They
look good to go to me.

>
> Cc: Andreas Dilger <adilger.kernel@dilger.ca>
> Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
> Cc: Michal Marek <michal.lkml@markovi.net>
> Cc: Steven Rostedt <rostedt@goodmis.org>
> Cc: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
> Cc: Olof Johansson <olof@lxom.net>
> Cc: Konstantin Ryabitsev <konstantin@linuxfoundation.org>
> Cc: David S. Miller <davem@davemloft.net>
> Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
> Cc: Kees Cook <keescook@chromium.org>
> Cc: Thomas Gleixner <tglx@linutronix.de>
> Cc: Ingo Molnar <mingo@kernel.org>
> Cc: Paul Lawrence <paullawrence@google.com>
> Cc: Sandipan Das <sandipan@linux.vnet.ibm.com>
> Cc: Andrey Konovalov <andreyknvl@google.com>
> Cc: David Woodhouse <dwmw2@infradead.org>
> Cc: Will Deacon <will.deacon@arm.com>
> Cc: Philippe Ombredanne <pombredanne@nexb.com>
> Cc: Paul Burton <paul.burton@mips.com>
> Cc: David Rientjes <rientjes@google.com>
> Cc: Willy Tarreau <w@1wt.eu>
> Cc: Martin Sebor <msebor@gmail.com>
> Cc: Christopher Li <sparse@chrisli.org>
> Cc: Jonathan Corbet <corbet@lwn.net>
> Cc: Theodore Ts'o <tytso@mit.edu>
> Cc: Geert Uytterhoeven <geert@linux-m68k.org>
> Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
> Cc: Joe Perches <joe@perches.com>
> Cc: Arnd Bergmann <arnd@arndb.de>
> Cc: Dominique Martinet <asmadeus@codewreck.org>
> Cc: Stefan Agner <stefan@agner.ch>
> Cc: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> Cc: Nick Desaulniers <ndesaulniers@google.com>
> Cc: Andrew Morton <akpm@linux-foundation.org>
> Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
> Cc: Linus Torvalds <torvalds@linux-foundation.org>
> Cc: linux-doc@vger.kernel.org
> Cc: linux-ext4@vger.kernel.org
> Cc: linux-sparse@vger.kernel.org
> Cc: linux-kbuild@vger.kernel.org
>
> v4 -> v5
>
>   * Removed the 2 __naked patches, since Greg just applied them to master;
>     and rebased on top of that. The only change in the range-diff is in
>     "Compiler Attributes: always use the extra-underscores syntax" due
>     to the move of __naked to compiler_types.h.
>
>   * Applied the nonstring series on top of this one (last 4 patches).
>     Rationale:
>       - the nonstring series would anyway be modified again by this one
>         (so it has been rebased after the general cleanup).
>       - ext4's "local" nonstring was merged
>       - easier to manage (and easier to see the end result)
>
>     The first 2 patches of the last 4 add the warning back at W=1 and
>     add the __nonstring attribute. The other two serve as an example
>     of usage and as a cleanup.
>
>   * Cc lists removed from each commit; using a single list in the cover letter;
>     and a new, revised list of Cc (wider audience).
>
> Miguel Ojeda (15):
>   Compiler Attributes: remove unused attributes
>   Compiler Attributes: always use the extra-underscores syntax
>   Compiler Attributes: remove unneeded tests
>   Compiler Attributes: homogenize __must_be_array
>   Compiler Attributes: remove unneeded sparse (__CHECKER__) tests
>   Compiler Attributes: add missing SPDX ID in compiler_types.h
>   Compiler Attributes: use feature checks instead of version checks
>   Compiler Attributes: KENTRY used twice the "used" attribute
>   Compiler Attributes: remove uses of __attribute__ from compiler.h
>   Compiler Attributes: add Doc/process/programming-language.rst
>   Compiler Attributes: add MAINTAINERS entry
>   Compiler Attributes: add support for __nonstring (gcc >= 8)
>   Compiler Attributes: enable -Wstringop-truncation on W=1 (gcc >= 8)
>   Compiler Attributes: auxdisplay: panel: use __nonstring
>   Compiler Attributes: ext4: remove local __nonstring definition
>
>  Documentation/process/index.rst               |   1 +
>  .../process/programming-language.rst          |  45 +++
>  MAINTAINERS                                   |   5 +
>  drivers/auxdisplay/panel.c                    |   7 +-
>  fs/ext4/ext4.h                                |   9 -
>  include/linux/compiler-clang.h                |   5 -
>  include/linux/compiler-gcc.h                  |  70 +----
>  include/linux/compiler-intel.h                |   9 -
>  include/linux/compiler.h                      |  19 +-
>  include/linux/compiler_attributes.h           | 258 ++++++++++++++++++
>  include/linux/compiler_types.h                | 101 +------
>  scripts/Makefile.extrawarn                    |   1 +
>  12 files changed, 341 insertions(+), 189 deletions(-)
>  create mode 100644 Documentation/process/programming-language.rst
>  create mode 100644 include/linux/compiler_attributes.h
>
> --
> 2.17.1
>


-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-kbuild
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Thu, 20 Sep 2018 18:18:17 +0000
Message-ID: <CAKwvOdk2O+-FqXU1R8m=vSbZuMGJxY-_4AWd+tFpcTDKhWWFaw () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 10:23 AM Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
>
> The Compiler Attributes series is an effort to disentangle
> the include/linux/compiler*.h headers and bring them up to date.
>
> The main idea behind the series is to use feature checking macros
> (i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
> which are compiler-agnostic (so they can be shared, reducing the size
> of compiler-specific headers) and version-agnostic.
>
> Other related improvements have been performed in the headers as well,
> which on top of the use of __has_attribute it has amounted to a significant
> simplification of these headers (e.g. GCC_VERSION is now only guarding 4
> non-attribute macros).
>
> This series should also help the efforts to support compiling the kernel
> with clang and icc. A fair amount of documentation and comments have also
> been added, clarified or removed; and the headers are now more readable,
> which should help kernel developers in general.
>
> The series was triggered due to the move to gcc >= 4.6. In turn, this series
> has also triggered Sparse to gain the ability to recognize __has_attribute
> on its own.
>
> Finally, the nonstring variable attribute series has been applied on top
> of this one.
>
> You can also fetch it from:
>
>   https://github.com/ojeda/linux/tree/compiler-attributes-v5
>
> Enjoy!
>
> Cheers,
> Miguel

Thanks for this series.  I've reviewed the rest of the patches in the
series that didn't already have my reviewed by tags on them.  They
look good to go to me.

>
> Cc: Andreas Dilger <adilger.kernel@dilger.ca>
> Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
> Cc: Michal Marek <michal.lkml@markovi.net>
> Cc: Steven Rostedt <rostedt@goodmis.org>
> Cc: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
> Cc: Olof Johansson <olof@lxom.net>
> Cc: Konstantin Ryabitsev <konstantin@linuxfoundation.org>
> Cc: David S. Miller <davem@davemloft.net>
> Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
> Cc: Kees Cook <keescook@chromium.org>
> Cc: Thomas Gleixner <tglx@linutronix.de>
> Cc: Ingo Molnar <mingo@kernel.org>
> Cc: Paul Lawrence <paullawrence@google.com>
> Cc: Sandipan Das <sandipan@linux.vnet.ibm.com>
> Cc: Andrey Konovalov <andreyknvl@google.com>
> Cc: David Woodhouse <dwmw2@infradead.org>
> Cc: Will Deacon <will.deacon@arm.com>
> Cc: Philippe Ombredanne <pombredanne@nexb.com>
> Cc: Paul Burton <paul.burton@mips.com>
> Cc: David Rientjes <rientjes@google.com>
> Cc: Willy Tarreau <w@1wt.eu>
> Cc: Martin Sebor <msebor@gmail.com>
> Cc: Christopher Li <sparse@chrisli.org>
> Cc: Jonathan Corbet <corbet@lwn.net>
> Cc: Theodore Ts'o <tytso@mit.edu>
> Cc: Geert Uytterhoeven <geert@linux-m68k.org>
> Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
> Cc: Joe Perches <joe@perches.com>
> Cc: Arnd Bergmann <arnd@arndb.de>
> Cc: Dominique Martinet <asmadeus@codewreck.org>
> Cc: Stefan Agner <stefan@agner.ch>
> Cc: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> Cc: Nick Desaulniers <ndesaulniers@google.com>
> Cc: Andrew Morton <akpm@linux-foundation.org>
> Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
> Cc: Linus Torvalds <torvalds@linux-foundation.org>
> Cc: linux-doc@vger.kernel.org
> Cc: linux-ext4@vger.kernel.org
> Cc: linux-sparse@vger.kernel.org
> Cc: linux-kbuild@vger.kernel.org
>
> v4 -> v5
>
>   * Removed the 2 __naked patches, since Greg just applied them to master;
>     and rebased on top of that. The only change in the range-diff is in
>     "Compiler Attributes: always use the extra-underscores syntax" due
>     to the move of __naked to compiler_types.h.
>
>   * Applied the nonstring series on top of this one (last 4 patches).
>     Rationale:
>       - the nonstring series would anyway be modified again by this one
>         (so it has been rebased after the general cleanup).
>       - ext4's "local" nonstring was merged
>       - easier to manage (and easier to see the end result)
>
>     The first 2 patches of the last 4 add the warning back at W=1 and
>     add the __nonstring attribute. The other two serve as an example
>     of usage and as a cleanup.
>
>   * Cc lists removed from each commit; using a single list in the cover letter;
>     and a new, revised list of Cc (wider audience).
>
> Miguel Ojeda (15):
>   Compiler Attributes: remove unused attributes
>   Compiler Attributes: always use the extra-underscores syntax
>   Compiler Attributes: remove unneeded tests
>   Compiler Attributes: homogenize __must_be_array
>   Compiler Attributes: remove unneeded sparse (__CHECKER__) tests
>   Compiler Attributes: add missing SPDX ID in compiler_types.h
>   Compiler Attributes: use feature checks instead of version checks
>   Compiler Attributes: KENTRY used twice the "used" attribute
>   Compiler Attributes: remove uses of __attribute__ from compiler.h
>   Compiler Attributes: add Doc/process/programming-language.rst
>   Compiler Attributes: add MAINTAINERS entry
>   Compiler Attributes: add support for __nonstring (gcc >= 8)
>   Compiler Attributes: enable -Wstringop-truncation on W=1 (gcc >= 8)
>   Compiler Attributes: auxdisplay: panel: use __nonstring
>   Compiler Attributes: ext4: remove local __nonstring definition
>
>  Documentation/process/index.rst               |   1 +
>  .../process/programming-language.rst          |  45 +++
>  MAINTAINERS                                   |   5 +
>  drivers/auxdisplay/panel.c                    |   7 +-
>  fs/ext4/ext4.h                                |   9 -
>  include/linux/compiler-clang.h                |   5 -
>  include/linux/compiler-gcc.h                  |  70 +----
>  include/linux/compiler-intel.h                |   9 -
>  include/linux/compiler.h                      |  19 +-
>  include/linux/compiler_attributes.h           | 258 ++++++++++++++++++
>  include/linux/compiler_types.h                | 101 +------
>  scripts/Makefile.extrawarn                    |   1 +
>  12 files changed, 341 insertions(+), 189 deletions(-)
>  create mode 100644 Documentation/process/programming-language.rst
>  create mode 100644 include/linux/compiler_attributes.h
>
> --
> 2.17.1
>


-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Sedat Dilek <sedat.dilek () gmail ! com>
To: linux-sparse
Subject: [PATCH v5 00/15] Compiler Attributes
Date: Mon, 24 Sep 2018 08:46:44 +0000
Message-ID: <CA+icZUW+N4V-bLEX0217dMYTrfzmgLZkbif8w3-zRQXnd7VYSQ () mail ! gmail ! com>
--------------------
[ Please CC me I am not subcribed to this ML ]

Quote from <https://lkml.org/lkml/2018/9/20/901>:

The Compiler Attributes series is an effort to disentangle
the include/linux/compiler*.h headers and bring them up to date.

The main idea behind the series is to use feature checking macros
(i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
which are compiler-agnostic (so they can be shared, reducing the size
of compiler-specific headers) and version-agnostic.

Other related improvements have been performed in the headers as well,
which on top of the use of __has_attribute it has amounted to a significant
simplification of these headers (e.g. GCC_VERSION is now only guarding 4
non-attribute macros).

This series should also help the efforts to support compiling the kernel
with clang and icc. A fair amount of documentation and comments have also
been added, clarified or removed; and the headers are now more readable,
which should help kernel developers in general.

The series was triggered due to the move to gcc >= 4.6. In turn, this series
has also triggered Sparse to gain the ability to recognize __has_attribute
on its own.

Finally, the nonstring variable attribute series has been applied on top
of this one.

You can also fetch it from:

  https://github.com/ojeda/linux/tree/compiler-attributes-v5

Enjoy!

Cheers,
Miguel

Cc: Andreas Dilger <adilger.kernel@dilger.ca>
Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
Cc: Michal Marek <michal.lkml@markovi.net>
Cc: Steven Rostedt <rostedt@goodmis.org>
Cc: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
Cc: Olof Johansson <olof@lxom.net>
Cc: Konstantin Ryabitsev <konstantin@linuxfoundation.org>
Cc: David S. Miller <davem@davemloft.net>
Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
Cc: Kees Cook <keescook@chromium.org>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: Ingo Molnar <mingo@kernel.org>
Cc: Paul Lawrence <paullawrence@google.com>
Cc: Sandipan Das <sandipan@linux.vnet.ibm.com>
Cc: Andrey Konovalov <andreyknvl@google.com>
Cc: David Woodhouse <dwmw2@infradead.org>
Cc: Will Deacon <will.deacon@arm.com>
Cc: Philippe Ombredanne <pombredanne@nexb.com>
Cc: Paul Burton <paul.burton@mips.com>
Cc: David Rientjes <rientjes@google.com>
Cc: Willy Tarreau <w@1wt.eu>
Cc: Martin Sebor <msebor@gmail.com>
Cc: Christopher Li <sparse@chrisli.org>
Cc: Jonathan Corbet <corbet@lwn.net>
Cc: Theodore Ts'o <tytso@mit.edu>
Cc: Geert Uytterhoeven <geert@linux-m68k.org>
Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Cc: Joe Perches <joe@perches.com>
Cc: Arnd Bergmann <arnd@arndb.de>
Cc: Dominique Martinet <asmadeus@codewreck.org>
Cc: Stefan Agner <stefan@agner.ch>
Cc: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
Cc: Nick Desaulniers <ndesaulniers@google.com>
Cc: Andrew Morton <akpm@linux-foundation.org>
Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: linux-doc@vger.kernel.org
Cc: linux-ext4@vger.kernel.org
Cc: linux-sparse@vger.kernel.org
Cc: linux-kbuild@vger.kernel.org

v4 -> v5

  * Removed the 2 __naked patches, since Greg just applied them to master;
    and rebased on top of that. The only change in the range-diff is in
    "Compiler Attributes: always use the extra-underscores syntax" due
    to the move of __naked to compiler_types.h.

  * Applied the nonstring series on top of this one (last 4 patches).
    Rationale:
      - the nonstring series would anyway be modified again by this one
        (so it has been rebased after the general cleanup).
      - ext4's "local" nonstring was merged
      - easier to manage (and easier to see the end result)

    The first 2 patches of the last 4 add the warning back at W=1 and
    add the __nonstring attribute. The other two serve as an example
    of usage and as a cleanup.

  * Cc lists removed from each commit; using a single list in the cover letter;
    and a new, revised list of Cc (wider audience).

Miguel Ojeda (15):
  Compiler Attributes: remove unused attributes
  Compiler Attributes: always use the extra-underscores syntax
  Compiler Attributes: remove unneeded tests
  Compiler Attributes: homogenize __must_be_array
  Compiler Attributes: remove unneeded sparse (__CHECKER__) tests
  Compiler Attributes: add missing SPDX ID in compiler_types.h
  Compiler Attributes: use feature checks instead of version checks
  Compiler Attributes: KENTRY used twice the "used" attribute
  Compiler Attributes: remove uses of __attribute__ from compiler.h
  Compiler Attributes: add Doc/process/programming-language.rst
  Compiler Attributes: add MAINTAINERS entry
  Compiler Attributes: add support for __nonstring (gcc >= 8)
  Compiler Attributes: enable -Wstringop-truncation on W=1 (gcc >= 8)
  Compiler Attributes: auxdisplay: panel: use __nonstring
  Compiler Attributes: ext4: remove local __nonstring definition

 Documentation/process/index.rst               |   1 +
 .../process/programming-language.rst          |  45 +++
 MAINTAINERS                                   |   5 +
 drivers/auxdisplay/panel.c                    |   7 +-
 fs/ext4/ext4.h                                |   9 -
 include/linux/compiler-clang.h                |   5 -
 include/linux/compiler-gcc.h                  |  70 +----
 include/linux/compiler-intel.h                |   9 -
 include/linux/compiler.h                      |  19 +-
 include/linux/compiler_attributes.h           | 258 ++++++++++++++++++
 include/linux/compiler_types.h                | 101 +------
 scripts/Makefile.extrawarn                    |   1 +
 12 files changed, 341 insertions(+), 189 deletions(-)
 create mode 100644 Documentation/process/programming-language.rst
 create mode 100644 include/linux/compiler_attributes.h

-- 
2.17.1

Tested-by: Sedat Dilek <sedat.dilek@gmail.com> (against Linux
v4.19-rc5 and built with LLVM/Clang v7)
================================================================================

From: Sedat Dilek <sedat.dilek () gmail ! com>
To: linux-kbuild
Subject: [PATCH v5 00/15] Compiler Attributes
Date: Mon, 24 Sep 2018 08:46:44 +0000
Message-ID: <CA+icZUW+N4V-bLEX0217dMYTrfzmgLZkbif8w3-zRQXnd7VYSQ () mail ! gmail ! com>
--------------------
[ Please CC me I am not subcribed to this ML ]

Quote from <https://lkml.org/lkml/2018/9/20/901>:

The Compiler Attributes series is an effort to disentangle
the include/linux/compiler*.h headers and bring them up to date.

The main idea behind the series is to use feature checking macros
(i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
which are compiler-agnostic (so they can be shared, reducing the size
of compiler-specific headers) and version-agnostic.

Other related improvements have been performed in the headers as well,
which on top of the use of __has_attribute it has amounted to a significant
simplification of these headers (e.g. GCC_VERSION is now only guarding 4
non-attribute macros).

This series should also help the efforts to support compiling the kernel
with clang and icc. A fair amount of documentation and comments have also
been added, clarified or removed; and the headers are now more readable,
which should help kernel developers in general.

The series was triggered due to the move to gcc >= 4.6. In turn, this series
has also triggered Sparse to gain the ability to recognize __has_attribute
on its own.

Finally, the nonstring variable attribute series has been applied on top
of this one.

You can also fetch it from:

  https://github.com/ojeda/linux/tree/compiler-attributes-v5

Enjoy!

Cheers,
Miguel

Cc: Andreas Dilger <adilger.kernel@dilger.ca>
Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
Cc: Michal Marek <michal.lkml@markovi.net>
Cc: Steven Rostedt <rostedt@goodmis.org>
Cc: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
Cc: Olof Johansson <olof@lxom.net>
Cc: Konstantin Ryabitsev <konstantin@linuxfoundation.org>
Cc: David S. Miller <davem@davemloft.net>
Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
Cc: Kees Cook <keescook@chromium.org>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: Ingo Molnar <mingo@kernel.org>
Cc: Paul Lawrence <paullawrence@google.com>
Cc: Sandipan Das <sandipan@linux.vnet.ibm.com>
Cc: Andrey Konovalov <andreyknvl@google.com>
Cc: David Woodhouse <dwmw2@infradead.org>
Cc: Will Deacon <will.deacon@arm.com>
Cc: Philippe Ombredanne <pombredanne@nexb.com>
Cc: Paul Burton <paul.burton@mips.com>
Cc: David Rientjes <rientjes@google.com>
Cc: Willy Tarreau <w@1wt.eu>
Cc: Martin Sebor <msebor@gmail.com>
Cc: Christopher Li <sparse@chrisli.org>
Cc: Jonathan Corbet <corbet@lwn.net>
Cc: Theodore Ts'o <tytso@mit.edu>
Cc: Geert Uytterhoeven <geert@linux-m68k.org>
Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Cc: Joe Perches <joe@perches.com>
Cc: Arnd Bergmann <arnd@arndb.de>
Cc: Dominique Martinet <asmadeus@codewreck.org>
Cc: Stefan Agner <stefan@agner.ch>
Cc: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
Cc: Nick Desaulniers <ndesaulniers@google.com>
Cc: Andrew Morton <akpm@linux-foundation.org>
Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: linux-doc@vger.kernel.org
Cc: linux-ext4@vger.kernel.org
Cc: linux-sparse@vger.kernel.org
Cc: linux-kbuild@vger.kernel.org

v4 -> v5

  * Removed the 2 __naked patches, since Greg just applied them to master;
    and rebased on top of that. The only change in the range-diff is in
    "Compiler Attributes: always use the extra-underscores syntax" due
    to the move of __naked to compiler_types.h.

  * Applied the nonstring series on top of this one (last 4 patches).
    Rationale:
      - the nonstring series would anyway be modified again by this one
        (so it has been rebased after the general cleanup).
      - ext4's "local" nonstring was merged
      - easier to manage (and easier to see the end result)

    The first 2 patches of the last 4 add the warning back at W=1 and
    add the __nonstring attribute. The other two serve as an example
    of usage and as a cleanup.

  * Cc lists removed from each commit; using a single list in the cover letter;
    and a new, revised list of Cc (wider audience).

Miguel Ojeda (15):
  Compiler Attributes: remove unused attributes
  Compiler Attributes: always use the extra-underscores syntax
  Compiler Attributes: remove unneeded tests
  Compiler Attributes: homogenize __must_be_array
  Compiler Attributes: remove unneeded sparse (__CHECKER__) tests
  Compiler Attributes: add missing SPDX ID in compiler_types.h
  Compiler Attributes: use feature checks instead of version checks
  Compiler Attributes: KENTRY used twice the "used" attribute
  Compiler Attributes: remove uses of __attribute__ from compiler.h
  Compiler Attributes: add Doc/process/programming-language.rst
  Compiler Attributes: add MAINTAINERS entry
  Compiler Attributes: add support for __nonstring (gcc >= 8)
  Compiler Attributes: enable -Wstringop-truncation on W=1 (gcc >= 8)
  Compiler Attributes: auxdisplay: panel: use __nonstring
  Compiler Attributes: ext4: remove local __nonstring definition

 Documentation/process/index.rst               |   1 +
 .../process/programming-language.rst          |  45 +++
 MAINTAINERS                                   |   5 +
 drivers/auxdisplay/panel.c                    |   7 +-
 fs/ext4/ext4.h                                |   9 -
 include/linux/compiler-clang.h                |   5 -
 include/linux/compiler-gcc.h                  |  70 +----
 include/linux/compiler-intel.h                |   9 -
 include/linux/compiler.h                      |  19 +-
 include/linux/compiler_attributes.h           | 258 ++++++++++++++++++
 include/linux/compiler_types.h                | 101 +------
 scripts/Makefile.extrawarn                    |   1 +
 12 files changed, 341 insertions(+), 189 deletions(-)
 create mode 100644 Documentation/process/programming-language.rst
 create mode 100644 include/linux/compiler_attributes.h

-- 
2.17.1

Tested-by: Sedat Dilek <sedat.dilek@gmail.com> (against Linux
v4.19-rc5 and built with LLVM/Clang v7)
================================================================================

From: Sedat Dilek <sedat.dilek () gmail ! com>
To: linux-doc
Subject: [PATCH v5 00/15] Compiler Attributes
Date: Mon, 24 Sep 2018 08:46:44 +0000
Message-ID: <CA+icZUW+N4V-bLEX0217dMYTrfzmgLZkbif8w3-zRQXnd7VYSQ () mail ! gmail ! com>
--------------------
[ Please CC me I am not subcribed to this ML ]

Quote from <https://lkml.org/lkml/2018/9/20/901>:

The Compiler Attributes series is an effort to disentangle
the include/linux/compiler*.h headers and bring them up to date.

The main idea behind the series is to use feature checking macros
(i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
which are compiler-agnostic (so they can be shared, reducing the size
of compiler-specific headers) and version-agnostic.

Other related improvements have been performed in the headers as well,
which on top of the use of __has_attribute it has amounted to a significant
simplification of these headers (e.g. GCC_VERSION is now only guarding 4
non-attribute macros).

This series should also help the efforts to support compiling the kernel
with clang and icc. A fair amount of documentation and comments have also
been added, clarified or removed; and the headers are now more readable,
which should help kernel developers in general.

The series was triggered due to the move to gcc >= 4.6. In turn, this series
has also triggered Sparse to gain the ability to recognize __has_attribute
on its own.

Finally, the nonstring variable attribute series has been applied on top
of this one.

You can also fetch it from:

  https://github.com/ojeda/linux/tree/compiler-attributes-v5

Enjoy!

Cheers,
Miguel

Cc: Andreas Dilger <adilger.kernel@dilger.ca>
Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
Cc: Michal Marek <michal.lkml@markovi.net>
Cc: Steven Rostedt <rostedt@goodmis.org>
Cc: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
Cc: Olof Johansson <olof@lxom.net>
Cc: Konstantin Ryabitsev <konstantin@linuxfoundation.org>
Cc: David S. Miller <davem@davemloft.net>
Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
Cc: Kees Cook <keescook@chromium.org>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: Ingo Molnar <mingo@kernel.org>
Cc: Paul Lawrence <paullawrence@google.com>
Cc: Sandipan Das <sandipan@linux.vnet.ibm.com>
Cc: Andrey Konovalov <andreyknvl@google.com>
Cc: David Woodhouse <dwmw2@infradead.org>
Cc: Will Deacon <will.deacon@arm.com>
Cc: Philippe Ombredanne <pombredanne@nexb.com>
Cc: Paul Burton <paul.burton@mips.com>
Cc: David Rientjes <rientjes@google.com>
Cc: Willy Tarreau <w@1wt.eu>
Cc: Martin Sebor <msebor@gmail.com>
Cc: Christopher Li <sparse@chrisli.org>
Cc: Jonathan Corbet <corbet@lwn.net>
Cc: Theodore Ts'o <tytso@mit.edu>
Cc: Geert Uytterhoeven <geert@linux-m68k.org>
Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Cc: Joe Perches <joe@perches.com>
Cc: Arnd Bergmann <arnd@arndb.de>
Cc: Dominique Martinet <asmadeus@codewreck.org>
Cc: Stefan Agner <stefan@agner.ch>
Cc: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
Cc: Nick Desaulniers <ndesaulniers@google.com>
Cc: Andrew Morton <akpm@linux-foundation.org>
Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: linux-doc@vger.kernel.org
Cc: linux-ext4@vger.kernel.org
Cc: linux-sparse@vger.kernel.org
Cc: linux-kbuild@vger.kernel.org

v4 -> v5

  * Removed the 2 __naked patches, since Greg just applied them to master;
    and rebased on top of that. The only change in the range-diff is in
    "Compiler Attributes: always use the extra-underscores syntax" due
    to the move of __naked to compiler_types.h.

  * Applied the nonstring series on top of this one (last 4 patches).
    Rationale:
      - the nonstring series would anyway be modified again by this one
        (so it has been rebased after the general cleanup).
      - ext4's "local" nonstring was merged
      - easier to manage (and easier to see the end result)

    The first 2 patches of the last 4 add the warning back at W=1 and
    add the __nonstring attribute. The other two serve as an example
    of usage and as a cleanup.

  * Cc lists removed from each commit; using a single list in the cover letter;
    and a new, revised list of Cc (wider audience).

Miguel Ojeda (15):
  Compiler Attributes: remove unused attributes
  Compiler Attributes: always use the extra-underscores syntax
  Compiler Attributes: remove unneeded tests
  Compiler Attributes: homogenize __must_be_array
  Compiler Attributes: remove unneeded sparse (__CHECKER__) tests
  Compiler Attributes: add missing SPDX ID in compiler_types.h
  Compiler Attributes: use feature checks instead of version checks
  Compiler Attributes: KENTRY used twice the "used" attribute
  Compiler Attributes: remove uses of __attribute__ from compiler.h
  Compiler Attributes: add Doc/process/programming-language.rst
  Compiler Attributes: add MAINTAINERS entry
  Compiler Attributes: add support for __nonstring (gcc >= 8)
  Compiler Attributes: enable -Wstringop-truncation on W=1 (gcc >= 8)
  Compiler Attributes: auxdisplay: panel: use __nonstring
  Compiler Attributes: ext4: remove local __nonstring definition

 Documentation/process/index.rst               |   1 +
 .../process/programming-language.rst          |  45 +++
 MAINTAINERS                                   |   5 +
 drivers/auxdisplay/panel.c                    |   7 +-
 fs/ext4/ext4.h                                |   9 -
 include/linux/compiler-clang.h                |   5 -
 include/linux/compiler-gcc.h                  |  70 +----
 include/linux/compiler-intel.h                |   9 -
 include/linux/compiler.h                      |  19 +-
 include/linux/compiler_attributes.h           | 258 ++++++++++++++++++
 include/linux/compiler_types.h                | 101 +------
 scripts/Makefile.extrawarn                    |   1 +
 12 files changed, 341 insertions(+), 189 deletions(-)
 create mode 100644 Documentation/process/programming-language.rst
 create mode 100644 include/linux/compiler_attributes.h

-- 
2.17.1

Tested-by: Sedat Dilek <sedat.dilek@gmail.com> (against Linux
v4.19-rc5 and built with LLVM/Clang v7)
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Mon, 24 Sep 2018 14:36:10 +0000
Message-ID: <20180924143608.aiyehiibgc7isk7k () ltop ! local>
--------------------
On Thu, Sep 20, 2018 at 07:22:46PM +0200, Miguel Ojeda wrote:
> The Compiler Attributes series is an effort to disentangle
> the include/linux/compiler*.h headers and bring them up to date.
> 
> The main idea behind the series is to use feature checking macros
> (i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
> which are compiler-agnostic (so they can be shared, reducing the size
> of compiler-specific headers) and version-agnostic.
> 
> Other related improvements have been performed in the headers as well,
> which on top of the use of __has_attribute it has amounted to a significant
> simplification of these headers (e.g. GCC_VERSION is now only guarding 4
> non-attribute macros).
> 
> This series should also help the efforts to support compiling the kernel
> with clang and icc. A fair amount of documentation and comments have also
> been added, clarified or removed; and the headers are now more readable,
> which should help kernel developers in general.
> 
> The series was triggered due to the move to gcc >= 4.6. In turn, this series
> has also triggered Sparse to gain the ability to recognize __has_attribute
> on its own.
> 
> Finally, the nonstring variable attribute series has been applied on top
> of this one.

Hi Miguel,
Feel free to add my
  Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
on patches 11-15 (I think the others ones already have it).

-- Luc
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-kernel
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Mon, 24 Sep 2018 14:36:10 +0000
Message-ID: <20180924143608.aiyehiibgc7isk7k () ltop ! local>
--------------------
On Thu, Sep 20, 2018 at 07:22:46PM +0200, Miguel Ojeda wrote:
> The Compiler Attributes series is an effort to disentangle
> the include/linux/compiler*.h headers and bring them up to date.
> 
> The main idea behind the series is to use feature checking macros
> (i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
> which are compiler-agnostic (so they can be shared, reducing the size
> of compiler-specific headers) and version-agnostic.
> 
> Other related improvements have been performed in the headers as well,
> which on top of the use of __has_attribute it has amounted to a significant
> simplification of these headers (e.g. GCC_VERSION is now only guarding 4
> non-attribute macros).
> 
> This series should also help the efforts to support compiling the kernel
> with clang and icc. A fair amount of documentation and comments have also
> been added, clarified or removed; and the headers are now more readable,
> which should help kernel developers in general.
> 
> The series was triggered due to the move to gcc >= 4.6. In turn, this series
> has also triggered Sparse to gain the ability to recognize __has_attribute
> on its own.
> 
> Finally, the nonstring variable attribute series has been applied on top
> of this one.

Hi Miguel,
Feel free to add my
  Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
on patches 11-15 (I think the others ones already have it).

-- Luc
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-kbuild
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Mon, 24 Sep 2018 14:36:10 +0000
Message-ID: <20180924143608.aiyehiibgc7isk7k () ltop ! local>
--------------------
On Thu, Sep 20, 2018 at 07:22:46PM +0200, Miguel Ojeda wrote:
> The Compiler Attributes series is an effort to disentangle
> the include/linux/compiler*.h headers and bring them up to date.
> 
> The main idea behind the series is to use feature checking macros
> (i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
> which are compiler-agnostic (so they can be shared, reducing the size
> of compiler-specific headers) and version-agnostic.
> 
> Other related improvements have been performed in the headers as well,
> which on top of the use of __has_attribute it has amounted to a significant
> simplification of these headers (e.g. GCC_VERSION is now only guarding 4
> non-attribute macros).
> 
> This series should also help the efforts to support compiling the kernel
> with clang and icc. A fair amount of documentation and comments have also
> been added, clarified or removed; and the headers are now more readable,
> which should help kernel developers in general.
> 
> The series was triggered due to the move to gcc >= 4.6. In turn, this series
> has also triggered Sparse to gain the ability to recognize __has_attribute
> on its own.
> 
> Finally, the nonstring variable attribute series has been applied on top
> of this one.

Hi Miguel,
Feel free to add my
  Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
on patches 11-15 (I think the others ones already have it).

-- Luc
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kbuild
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Sun, 30 Sep 2018 11:11:11 +0000
Message-ID: <CANiq72mATUQeNA5uTSY2XBPwDi-S+K+H3LJJU5_UFcxUj_E8sw () mail ! gmail ! com>
--------------------
Hi Sedat,

On Mon, Sep 24, 2018 at 10:46 AM Sedat Dilek <sedat.dilek@gmail.com> wrote:
>
> [ Please CC me I am not subcribed to this ML ]
>
> Quote from <https://lkml.org/lkml/2018/9/20/901>:
>
> Tested-by: Sedat Dilek <sedat.dilek@gmail.com> (against Linux
> v4.19-rc5 and built with LLVM/Clang v7)

Thanks a lot for testing! Which configuration roughly (e.g.
allmodconfig)? Did you boot it & run it for a while?

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-doc
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Sun, 30 Sep 2018 11:11:11 +0000
Message-ID: <CANiq72mATUQeNA5uTSY2XBPwDi-S+K+H3LJJU5_UFcxUj_E8sw () mail ! gmail ! com>
--------------------
Hi Sedat,

On Mon, Sep 24, 2018 at 10:46 AM Sedat Dilek <sedat.dilek@gmail.com> wrote:
>
> [ Please CC me I am not subcribed to this ML ]
>
> Quote from <https://lkml.org/lkml/2018/9/20/901>:
>
> Tested-by: Sedat Dilek <sedat.dilek@gmail.com> (against Linux
> v4.19-rc5 and built with LLVM/Clang v7)

Thanks a lot for testing! Which configuration roughly (e.g.
allmodconfig)? Did you boot it & run it for a while?

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Sun, 30 Sep 2018 11:11:11 +0000
Message-ID: <CANiq72mATUQeNA5uTSY2XBPwDi-S+K+H3LJJU5_UFcxUj_E8sw () mail ! gmail ! com>
--------------------
Hi Sedat,

On Mon, Sep 24, 2018 at 10:46 AM Sedat Dilek <sedat.dilek@gmail.com> wrote:
>
> [ Please CC me I am not subcribed to this ML ]
>
> Quote from <https://lkml.org/lkml/2018/9/20/901>:
>
> Tested-by: Sedat Dilek <sedat.dilek@gmail.com> (against Linux
> v4.19-rc5 and built with LLVM/Clang v7)

Thanks a lot for testing! Which configuration roughly (e.g.
allmodconfig)? Did you boot it & run it for a while?

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kbuild
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Sun, 30 Sep 2018 11:13:36 +0000
Message-ID: <CANiq72=Aom+D8LsvMDgD3Am9n3DH13CzZKuL3dfanyqFEGyNbA () mail ! gmail ! com>
--------------------
On Mon, Sep 24, 2018 at 4:36 PM Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
>
> On Thu, Sep 20, 2018 at 07:22:46PM +0200, Miguel Ojeda wrote:
> > The Compiler Attributes series is an effort to disentangle
> > the include/linux/compiler*.h headers and bring them up to date.
> >
> > The main idea behind the series is to use feature checking macros
> > (i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
> > which are compiler-agnostic (so they can be shared, reducing the size
> > of compiler-specific headers) and version-agnostic.
> >
> > Other related improvements have been performed in the headers as well,
> > which on top of the use of __has_attribute it has amounted to a significant
> > simplification of these headers (e.g. GCC_VERSION is now only guarding 4
> > non-attribute macros).
> >
> > This series should also help the efforts to support compiling the kernel
> > with clang and icc. A fair amount of documentation and comments have also
> > been added, clarified or removed; and the headers are now more readable,
> > which should help kernel developers in general.
> >
> > The series was triggered due to the move to gcc >= 4.6. In turn, this series
> > has also triggered Sparse to gain the ability to recognize __has_attribute
> > on its own.
> >
> > Finally, the nonstring variable attribute series has been applied on top
> > of this one.
>
> Hi Miguel,
> Feel free to add my
>   Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> on patches 11-15 (I think the others ones already have it).
>

Done for both Nick and you!

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-ext4
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Sun, 30 Sep 2018 11:13:36 +0000
Message-ID: <CANiq72=Aom+D8LsvMDgD3Am9n3DH13CzZKuL3dfanyqFEGyNbA () mail ! gmail ! com>
--------------------
On Mon, Sep 24, 2018 at 4:36 PM Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
>
> On Thu, Sep 20, 2018 at 07:22:46PM +0200, Miguel Ojeda wrote:
> > The Compiler Attributes series is an effort to disentangle
> > the include/linux/compiler*.h headers and bring them up to date.
> >
> > The main idea behind the series is to use feature checking macros
> > (i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
> > which are compiler-agnostic (so they can be shared, reducing the size
> > of compiler-specific headers) and version-agnostic.
> >
> > Other related improvements have been performed in the headers as well,
> > which on top of the use of __has_attribute it has amounted to a significant
> > simplification of these headers (e.g. GCC_VERSION is now only guarding 4
> > non-attribute macros).
> >
> > This series should also help the efforts to support compiling the kernel
> > with clang and icc. A fair amount of documentation and comments have also
> > been added, clarified or removed; and the headers are now more readable,
> > which should help kernel developers in general.
> >
> > The series was triggered due to the move to gcc >= 4.6. In turn, this series
> > has also triggered Sparse to gain the ability to recognize __has_attribute
> > on its own.
> >
> > Finally, the nonstring variable attribute series has been applied on top
> > of this one.
>
> Hi Miguel,
> Feel free to add my
>   Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> on patches 11-15 (I think the others ones already have it).
>

Done for both Nick and you!

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kernel
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Sun, 30 Sep 2018 11:13:36 +0000
Message-ID: <CANiq72=Aom+D8LsvMDgD3Am9n3DH13CzZKuL3dfanyqFEGyNbA () mail ! gmail ! com>
--------------------
On Mon, Sep 24, 2018 at 4:36 PM Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
>
> On Thu, Sep 20, 2018 at 07:22:46PM +0200, Miguel Ojeda wrote:
> > The Compiler Attributes series is an effort to disentangle
> > the include/linux/compiler*.h headers and bring them up to date.
> >
> > The main idea behind the series is to use feature checking macros
> > (i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
> > which are compiler-agnostic (so they can be shared, reducing the size
> > of compiler-specific headers) and version-agnostic.
> >
> > Other related improvements have been performed in the headers as well,
> > which on top of the use of __has_attribute it has amounted to a significant
> > simplification of these headers (e.g. GCC_VERSION is now only guarding 4
> > non-attribute macros).
> >
> > This series should also help the efforts to support compiling the kernel
> > with clang and icc. A fair amount of documentation and comments have also
> > been added, clarified or removed; and the headers are now more readable,
> > which should help kernel developers in general.
> >
> > The series was triggered due to the move to gcc >= 4.6. In turn, this series
> > has also triggered Sparse to gain the ability to recognize __has_attribute
> > on its own.
> >
> > Finally, the nonstring variable attribute series has been applied on top
> > of this one.
>
> Hi Miguel,
> Feel free to add my
>   Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> on patches 11-15 (I think the others ones already have it).
>

Done for both Nick and you!

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Sun, 30 Sep 2018 11:13:36 +0000
Message-ID: <CANiq72=Aom+D8LsvMDgD3Am9n3DH13CzZKuL3dfanyqFEGyNbA () mail ! gmail ! com>
--------------------
On Mon, Sep 24, 2018 at 4:36 PM Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
>
> On Thu, Sep 20, 2018 at 07:22:46PM +0200, Miguel Ojeda wrote:
> > The Compiler Attributes series is an effort to disentangle
> > the include/linux/compiler*.h headers and bring them up to date.
> >
> > The main idea behind the series is to use feature checking macros
> > (i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
> > which are compiler-agnostic (so they can be shared, reducing the size
> > of compiler-specific headers) and version-agnostic.
> >
> > Other related improvements have been performed in the headers as well,
> > which on top of the use of __has_attribute it has amounted to a significant
> > simplification of these headers (e.g. GCC_VERSION is now only guarding 4
> > non-attribute macros).
> >
> > This series should also help the efforts to support compiling the kernel
> > with clang and icc. A fair amount of documentation and comments have also
> > been added, clarified or removed; and the headers are now more readable,
> > which should help kernel developers in general.
> >
> > The series was triggered due to the move to gcc >= 4.6. In turn, this series
> > has also triggered Sparse to gain the ability to recognize __has_attribute
> > on its own.
> >
> > Finally, the nonstring variable attribute series has been applied on top
> > of this one.
>
> Hi Miguel,
> Feel free to add my
>   Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> on patches 11-15 (I think the others ones already have it).
>

Done for both Nick and you!

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-doc
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Sun, 30 Sep 2018 11:13:36 +0000
Message-ID: <CANiq72=Aom+D8LsvMDgD3Am9n3DH13CzZKuL3dfanyqFEGyNbA () mail ! gmail ! com>
--------------------
On Mon, Sep 24, 2018 at 4:36 PM Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
>
> On Thu, Sep 20, 2018 at 07:22:46PM +0200, Miguel Ojeda wrote:
> > The Compiler Attributes series is an effort to disentangle
> > the include/linux/compiler*.h headers and bring them up to date.
> >
> > The main idea behind the series is to use feature checking macros
> > (i.e. __has_attribute) instead of compiler version checks (e.g. GCC_VERSION),
> > which are compiler-agnostic (so they can be shared, reducing the size
> > of compiler-specific headers) and version-agnostic.
> >
> > Other related improvements have been performed in the headers as well,
> > which on top of the use of __has_attribute it has amounted to a significant
> > simplification of these headers (e.g. GCC_VERSION is now only guarding 4
> > non-attribute macros).
> >
> > This series should also help the efforts to support compiling the kernel
> > with clang and icc. A fair amount of documentation and comments have also
> > been added, clarified or removed; and the headers are now more readable,
> > which should help kernel developers in general.
> >
> > The series was triggered due to the move to gcc >= 4.6. In turn, this series
> > has also triggered Sparse to gain the ability to recognize __has_attribute
> > on its own.
> >
> > Finally, the nonstring variable attribute series has been applied on top
> > of this one.
>
> Hi Miguel,
> Feel free to add my
>   Reviewed-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> on patches 11-15 (I think the others ones already have it).
>

Done for both Nick and you!

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Sat, 06 Oct 2018 10:19:41 +0000
Message-ID: <CANiq72mw4gPMP+j9E19o3r9+fpn=Tq0nUDWuR=3HfYdPOgN81A () mail ! gmail ! com>
--------------------
On Sat, Oct 6, 2018 at 9:13 AM Sedat Dilek <sedat.dilek@gmail.com> wrote:
>
> Hi Miguel,
>
> In my testoings I am booting in QEMU and on bare metal.
> Both tests were successful.

Wow! I only wanted to confirm it booted and/or run for a while -- but
having more information is always good. Thanks a *lot*!

I will add a reference to your email in the commits.

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-doc
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Sat, 06 Oct 2018 10:19:41 +0000
Message-ID: <CANiq72mw4gPMP+j9E19o3r9+fpn=Tq0nUDWuR=3HfYdPOgN81A () mail ! gmail ! com>
--------------------
On Sat, Oct 6, 2018 at 9:13 AM Sedat Dilek <sedat.dilek@gmail.com> wrote:
>
> Hi Miguel,
>
> In my testoings I am booting in QEMU and on bare metal.
> Both tests were successful.

Wow! I only wanted to confirm it booted and/or run for a while -- but
having more information is always good. Thanks a *lot*!

I will add a reference to your email in the commits.

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kbuild
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Sat, 06 Oct 2018 10:19:41 +0000
Message-ID: <CANiq72mw4gPMP+j9E19o3r9+fpn=Tq0nUDWuR=3HfYdPOgN81A () mail ! gmail ! com>
--------------------
On Sat, Oct 6, 2018 at 9:13 AM Sedat Dilek <sedat.dilek@gmail.com> wrote:
>
> Hi Miguel,
>
> In my testoings I am booting in QEMU and on bare metal.
> Both tests were successful.

Wow! I only wanted to confirm it booted and/or run for a while -- but
having more information is always good. Thanks a *lot*!

I will add a reference to your email in the commits.

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-ext4
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Sat, 06 Oct 2018 10:19:41 +0000
Message-ID: <CANiq72mw4gPMP+j9E19o3r9+fpn=Tq0nUDWuR=3HfYdPOgN81A () mail ! gmail ! com>
--------------------
On Sat, Oct 6, 2018 at 9:13 AM Sedat Dilek <sedat.dilek@gmail.com> wrote:
>
> Hi Miguel,
>
> In my testoings I am booting in QEMU and on bare metal.
> Both tests were successful.

Wow! I only wanted to confirm it booted and/or run for a while -- but
having more information is always good. Thanks a *lot*!

I will add a reference to your email in the commits.

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kernel
Subject: Re: [PATCH v5 00/15] Compiler Attributes
Date: Sat, 06 Oct 2018 19:41:22 +0000
Message-ID: <CANiq72==PgCYLUf-N7S84jL87MezqVj63=saKxsNhc9U-2LhNQ () mail ! gmail ! com>
--------------------
--0000000000004b765005779493e0
Content-Type: text/plain; charset="UTF-8"

[CC'ing this message to linux-kernel so that we get it into the
archive at lore.kernel.org]

On Sat, Oct 6, 2018 at 9:13 AM Sedat Dilek <sedat.dilek@gmail.com> wrote:
>
> On Sun, Sep 30, 2018 at 1:11 PM Miguel Ojeda
> <miguel.ojeda.sandonis@gmail.com> wrote:
> >
> > Thanks a lot for testing! Which configuration roughly (e.g.
> > allmodconfig)? Did you boot it & run it for a while?
> >
>
> Hi Miguel,
>
> I am doing all my "ClangBuiltLinux" (short: cbl) experiments on
> Debian/testing AMD64 (which will be buster aka version 10).
>
> https://github.com/ClangBuiltLinux/linux
>
> I am using the kernel-config provided by the Debian Kernel Team -
> means be as close to my upstream.
>
> https://packages.debian.org/experimental/linux-image-4.19.0-rc4-amd64-unsigned
>
> Clang version 7 from Debian/unstable is the compiler I am using.
>
> https://packages.debian.org/clang-7
>
> Currently, some Kconfigs are known to be "BROKEN" (with Clang or
> independent of the used compiler).
> CONFIG_HARDENED_USERCOPY=n
> CONFIG_DRM_AMDGPU=n (built as module broken, not sure if this was
> fixed in meantime).
>
> https://github.com/ClangBuiltLinux/linux/issues/7
> https://lists.freedesktop.org/archives/amd-gfx/2018-July/024634.html
>
> In my testoings I am booting in QEMU and on bare metal.
> Both tests were successful.
>
> My last testings of your series v5 was on top of Linux v4.19-rc6.
> I have some more patches included (see below).
>
> My kernel-config and dmesg-log are attached.
>
> Hope this helps.
>
> Regards,
> - Sedat -
>
> P.S.: Some helpful informations
>
> [ VERSION ]
>
> $ cat /proc/version
> Linux version 4.19.0-rc6-1-amd64-cbl (sedat.dilek@gmail.com@iniza)
> (clang version 7.0.0-3 (tags/RELEASE_700/final)) #1 SMP 2018-10-01
>
> [ PATCH-SET ]
>
> $ git --no-pager log --no-merges --oneline v4.19-rc6..
> c1b160655219 (for-4.19/compiler-attributes-v5) Compiler Attributes:
> ext4: remove local __nonstring definition
> e33fe800b910 Compiler Attributes: auxdisplay: panel: use __nonstring
> 2d5e0d827ee0 Compiler Attributes: enable -Wstringop-truncation on W=1 (gcc >= 8)
> 1ab27d366e9f Compiler Attributes: add support for __nonstring (gcc >= 8)
> 58d53af4577c Compiler Attributes: add MAINTAINERS entry
> 8dbfe0eec5c1 Compiler Attributes: add Doc/process/programming-language.rst
> 5a886fa546eb Compiler Attributes: remove uses of __attribute__ from compiler.h
> 8414944bdb94 Compiler Attributes: KENTRY used twice the "used" attribute
> 16da1f71ccf1 Compiler Attributes: use feature checks instead of version checks
> efe932a226e2 Compiler Attributes: add missing SPDX ID in compiler_types.h
> f5add77ef51a Compiler Attributes: remove unneeded sparse (__CHECKER__) tests
> 5b1c25d038ac Compiler Attributes: homogenize __must_be_array
> 4bce3ce73e44 Compiler Attributes: remove unneeded tests
> 899d7604d8c2 Compiler Attributes: always use the extra-underscores syntax
> 704bd38d6793 Compiler Attributes: remove unused attributes
> ef8c4ed9db80 kbuild: allow to use GCC toolchain not in Clang search path
> 5a4630aadb9a ftrace: Build with CPPFLAGS to get -Qunused-arguments
> 9f9ff0dc3aed (for-4.19/x86-asm-goto-clang-fixes)
> include/linux/compiler*.h: add version detection to asm_volatile_goto
> cca6adeee1db kbuild: add --include-dir flag only for out-of-tree build
> bb5de5d28f73 kbuild: remove unneeded link_multi_deps
> 00d78ab2ba75 kbuild: remove dead code in cmd_files calculation in top Makefile
> 25815cf5ffec kbuild: hide most of targets when running config or mixed targets
> 0d91bf584fe5 kbuild: remove old check for CFLAGS use
> 487c7c7702ab kbuild: prefix Makefile.dtbinst path with $(srctree)
> unconditionally
> 36f546a1bdb5 kallsyms: remove left-over Blackfin code
> 80ffbaa5b1bd kallsyms: reduce size a little on 64-bit
> bac13b5b25c9 x86: Warn clang does not support asm-goto (v2)
> 1e98b02f5c59 (for-4.19/compiletime_assert-clang-fixes) compiler.h:
> give up __compiletime_assert_fallback()
>
> [ QEMU TESTING (run_qemu.sh) ]
> KPATH=$(pwd)
>
> qemu-system-x86_64 -enable-kvm -M pc -kernel $KPATH/bzImage -initrd
> $KPATH/initrd.img -m 512 -net none -serial stdio -append
> "root=/dev/ram0 console=ttyS0 hung_task_panic=1
> earlyprintk=ttyS0,115200"
>
> [ DEBIAN KERNEL VS. CBL KERNEL ]
>
> $ ./scripts/diffconfig /boot/config-4.19.0-rc4-amd64
> /boot/config-4.19.0-rc6-1-amd64-cbl
> -ALLOW_LOCKDOWN_LIFT_BY_SYSRQ n
> -ANDROID_BINDER_DEVICES "binder"
> -ANDROID_BINDER_IPC_SELFTEST n
> -CC_IS_GCC y
> -CHASH m
> -CHASH_SELFTEST n
> -CHASH_STATS n
> -CRYPTO_FIPS y
> -DEBUG_KERNEL_DC n
> -DRM_AMDGPU_CIK y
> -DRM_AMDGPU_GART_DEBUGFS n
> -DRM_AMDGPU_SI y
> -DRM_AMDGPU_USERPTR y
> -DRM_AMD_ACP y
> -DRM_AMD_DC y
> -DRM_AMD_DC_DCN1_0 y
> -DRM_SCHED m
> -HARDENED_USERCOPY_FALLBACK n
> -HARDENED_USERCOPY_PAGESPAN n
> -HSA_AMD m
> -LOCK_DOWN_IN_EFI_SECURE_BOOT y
> -LOCK_DOWN_KERNEL y
> -MODULE_SIG_ALL n
> -MODULE_SIG_FORCE n
> -MODULE_SIG_HASH "sha256"
> -MODULE_SIG_KEY ""
> -MODULE_SIG_SHA1 n
> -MODULE_SIG_SHA224 n
> -MODULE_SIG_SHA256 y
> -MODULE_SIG_SHA384 n
> -MODULE_SIG_SHA512 n
> -PLUGIN_HOSTCC ""
> -PM_GENERIC_DOMAINS y
> -PM_GENERIC_DOMAINS_SLEEP y
> -RTLWIFI_DEBUG_ST y
> -SECURITY_PERF_EVENTS_RESTRICT y
> -X86_X32_DISABLED y
>  ANDROID_BINDER_IPC m -> n
>  ASHMEM m -> n
>  BUILD_SALT "4.19.0-rc4-amd64" -> "4.19.0-rc6-1-amd64-cbl"
>  CLANG_VERSION 0 -> 70000
>  DRM_AMDGPU m -> n
>  GCC_VERSION 80200 -> 0
>  HARDENED_USERCOPY y -> n
>  MODULE_SIG y -> n
>  R8822BE m -> n
>  SYSTEM_TRUSTED_KEYS "debian/certs/test-signing-certs.pem" -> ""
>  TYPEC_TPS6598X m -> n
>  USB_COMMON m -> y
>  X86_X32 y -> n
> +CC_IS_CLANG y
> +FB_NVIDIA n
> +FB_RIVA n
>
> - EOT -

Cheers,
Miguel

--0000000000004b765005779493e0
Content-Type: text/plain; charset="US-ASCII"; name="dmesg_4.19.0-rc6-1-amd64-cbl.txt"
Content-Disposition: attachment; filename="dmesg_4.19.0-rc6-1-amd64-cbl.txt"
Content-Transfer-Encoding: base64
Content-ID: <f_jmxuftw61>
X-Attachment-Id: f_jmxuftw61
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--0000000000004b765005779493e0
Content-Type: application/octet-stream; name="config-4.19.0-rc6-1-amd64-cbl"
Content-Disposition: attachment; filename="config-4.19.0-rc6-1-amd64-cbl"
Content-Transfer-Encoding: base64
Content-ID: <f_jmxuftvo0>
X-Attachment-Id: f_jmxuftvo0
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--0000000000004b765005779493e0--
================================================================================


################################################################################

=== Thread: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8) ===

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kernel
Subject: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8)
Date: Thu, 20 Sep 2018 17:22:58 +0000
Message-ID: <20180920172301.21868-13-miguel.ojeda.sandonis () gmail ! com>
--------------------

================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-ext4
Subject: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8)
Date: Thu, 20 Sep 2018 17:22:58 +0000
Message-ID: <20180920172301.21868-13-miguel.ojeda.sandonis () gmail ! com>
--------------------

================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-doc
Subject: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8)
Date: Thu, 20 Sep 2018 17:22:58 +0000
Message-ID: <20180920172301.21868-13-miguel.ojeda.sandonis () gmail ! com>
--------------------

================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-sparse
Subject: Re: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8)
Date: Thu, 20 Sep 2018 18:07:53 +0000
Message-ID: <CAKwvOd=0GZi5M3ZRxps+Y1Cvbk=h-VOMM4+cBzHx2-LGO1cLSg () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 10:23 AM Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
>
> From the GCC manual:
>
>   nonstring
>
>     The nonstring variable attribute specifies that an object or member
>     declaration with type array of char, signed char, or unsigned char,
>     or pointer to such a type is intended to store character arrays that
>     do not necessarily contain a terminating NUL. This is useful in detecting
>     uses of such arrays or pointers with functions that expect NUL-terminated
>     strings, and to avoid warnings when such an array or pointer is used as
>     an argument to a bounded string manipulation function such as strncpy.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html
>
> This attribute can be used for documentation purposes (i.e. replacing
> comments), but it is most helpful when the following warnings are enabled:
>
>   -Wstringop-overflow
>
>     Warn for calls to string manipulation functions such as memcpy and
>     strcpy that are determined to overflow the destination buffer.
>
>     [...]
>
>   -Wstringop-truncation
>
>     Warn for calls to bounded string manipulation functions such as
>     strncat, strncpy, and stpncpy that may either truncate the copied
>     string or leave the destination unchanged.
>
>     [...]
>
>     In situations where a character array is intended to store a sequence
>     of bytes with no terminating NUL such an array may be annotated with
>     attribute nonstring to avoid this warning. Such arrays, however,
>     are not suitable arguments to functions that expect NUL-terminated
>     strings. To help detect accidental misuses of such arrays GCC issues
>     warnings unless it can prove that the use is safe.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html
>
> Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
> ---
>  include/linux/compiler_attributes.h | 14 ++++++++++++++
>  1 file changed, 14 insertions(+)
>
> diff --git a/include/linux/compiler_attributes.h b/include/linux/compiler_attributes.h
> index f0f9fc398440..6b28c1b7310c 100644
> --- a/include/linux/compiler_attributes.h
> +++ b/include/linux/compiler_attributes.h
> @@ -34,6 +34,7 @@
>  # define __GCC4_has_attribute___externally_visible__  1
>  # define __GCC4_has_attribute___noclone__             1
>  # define __GCC4_has_attribute___optimize__            1
> +# define __GCC4_has_attribute___nonstring__           0
>  # define __GCC4_has_attribute___no_sanitize_address__ (__GNUC_MINOR__ >= 8)
>  #endif
>
> @@ -181,6 +182,19 @@
>   */
>  #define   noinline                      __attribute__((__noinline__))
>
> +/*
> + * Optional: only supported since gcc >= 8
> + * Optional: not supported by clang
> + * Optional: not supported by icc
> + *
> + *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-nonstring-variable-attribute
> + */
> +#if __has_attribute(__nonstring__)
> +# define __nonstring                    __attribute__((__nonstring__))
> +#else
> +# define __nonstring
> +#endif
> +
>  /*
>   *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-noreturn-function-attribute
>   * clang: https://clang.llvm.org/docs/AttributeReference.html#noreturn
> --
> 2.17.1
>

Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>

-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-kbuild
Subject: Re: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8)
Date: Thu, 20 Sep 2018 18:07:53 +0000
Message-ID: <CAKwvOd=0GZi5M3ZRxps+Y1Cvbk=h-VOMM4+cBzHx2-LGO1cLSg () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 10:23 AM Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
>
> From the GCC manual:
>
>   nonstring
>
>     The nonstring variable attribute specifies that an object or member
>     declaration with type array of char, signed char, or unsigned char,
>     or pointer to such a type is intended to store character arrays that
>     do not necessarily contain a terminating NUL. This is useful in detecting
>     uses of such arrays or pointers with functions that expect NUL-terminated
>     strings, and to avoid warnings when such an array or pointer is used as
>     an argument to a bounded string manipulation function such as strncpy.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html
>
> This attribute can be used for documentation purposes (i.e. replacing
> comments), but it is most helpful when the following warnings are enabled:
>
>   -Wstringop-overflow
>
>     Warn for calls to string manipulation functions such as memcpy and
>     strcpy that are determined to overflow the destination buffer.
>
>     [...]
>
>   -Wstringop-truncation
>
>     Warn for calls to bounded string manipulation functions such as
>     strncat, strncpy, and stpncpy that may either truncate the copied
>     string or leave the destination unchanged.
>
>     [...]
>
>     In situations where a character array is intended to store a sequence
>     of bytes with no terminating NUL such an array may be annotated with
>     attribute nonstring to avoid this warning. Such arrays, however,
>     are not suitable arguments to functions that expect NUL-terminated
>     strings. To help detect accidental misuses of such arrays GCC issues
>     warnings unless it can prove that the use is safe.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html
>
> Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
> ---
>  include/linux/compiler_attributes.h | 14 ++++++++++++++
>  1 file changed, 14 insertions(+)
>
> diff --git a/include/linux/compiler_attributes.h b/include/linux/compiler_attributes.h
> index f0f9fc398440..6b28c1b7310c 100644
> --- a/include/linux/compiler_attributes.h
> +++ b/include/linux/compiler_attributes.h
> @@ -34,6 +34,7 @@
>  # define __GCC4_has_attribute___externally_visible__  1
>  # define __GCC4_has_attribute___noclone__             1
>  # define __GCC4_has_attribute___optimize__            1
> +# define __GCC4_has_attribute___nonstring__           0
>  # define __GCC4_has_attribute___no_sanitize_address__ (__GNUC_MINOR__ >= 8)
>  #endif
>
> @@ -181,6 +182,19 @@
>   */
>  #define   noinline                      __attribute__((__noinline__))
>
> +/*
> + * Optional: only supported since gcc >= 8
> + * Optional: not supported by clang
> + * Optional: not supported by icc
> + *
> + *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-nonstring-variable-attribute
> + */
> +#if __has_attribute(__nonstring__)
> +# define __nonstring                    __attribute__((__nonstring__))
> +#else
> +# define __nonstring
> +#endif
> +
>  /*
>   *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-noreturn-function-attribute
>   * clang: https://clang.llvm.org/docs/AttributeReference.html#noreturn
> --
> 2.17.1
>

Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>

-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-doc
Subject: Re: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8)
Date: Thu, 20 Sep 2018 18:07:53 +0000
Message-ID: <CAKwvOd=0GZi5M3ZRxps+Y1Cvbk=h-VOMM4+cBzHx2-LGO1cLSg () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 10:23 AM Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
>
> From the GCC manual:
>
>   nonstring
>
>     The nonstring variable attribute specifies that an object or member
>     declaration with type array of char, signed char, or unsigned char,
>     or pointer to such a type is intended to store character arrays that
>     do not necessarily contain a terminating NUL. This is useful in detecting
>     uses of such arrays or pointers with functions that expect NUL-terminated
>     strings, and to avoid warnings when such an array or pointer is used as
>     an argument to a bounded string manipulation function such as strncpy.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html
>
> This attribute can be used for documentation purposes (i.e. replacing
> comments), but it is most helpful when the following warnings are enabled:
>
>   -Wstringop-overflow
>
>     Warn for calls to string manipulation functions such as memcpy and
>     strcpy that are determined to overflow the destination buffer.
>
>     [...]
>
>   -Wstringop-truncation
>
>     Warn for calls to bounded string manipulation functions such as
>     strncat, strncpy, and stpncpy that may either truncate the copied
>     string or leave the destination unchanged.
>
>     [...]
>
>     In situations where a character array is intended to store a sequence
>     of bytes with no terminating NUL such an array may be annotated with
>     attribute nonstring to avoid this warning. Such arrays, however,
>     are not suitable arguments to functions that expect NUL-terminated
>     strings. To help detect accidental misuses of such arrays GCC issues
>     warnings unless it can prove that the use is safe.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html
>
> Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
> ---
>  include/linux/compiler_attributes.h | 14 ++++++++++++++
>  1 file changed, 14 insertions(+)
>
> diff --git a/include/linux/compiler_attributes.h b/include/linux/compiler_attributes.h
> index f0f9fc398440..6b28c1b7310c 100644
> --- a/include/linux/compiler_attributes.h
> +++ b/include/linux/compiler_attributes.h
> @@ -34,6 +34,7 @@
>  # define __GCC4_has_attribute___externally_visible__  1
>  # define __GCC4_has_attribute___noclone__             1
>  # define __GCC4_has_attribute___optimize__            1
> +# define __GCC4_has_attribute___nonstring__           0
>  # define __GCC4_has_attribute___no_sanitize_address__ (__GNUC_MINOR__ >= 8)
>  #endif
>
> @@ -181,6 +182,19 @@
>   */
>  #define   noinline                      __attribute__((__noinline__))
>
> +/*
> + * Optional: only supported since gcc >= 8
> + * Optional: not supported by clang
> + * Optional: not supported by icc
> + *
> + *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-nonstring-variable-attribute
> + */
> +#if __has_attribute(__nonstring__)
> +# define __nonstring                    __attribute__((__nonstring__))
> +#else
> +# define __nonstring
> +#endif
> +
>  /*
>   *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-noreturn-function-attribute
>   * clang: https://clang.llvm.org/docs/AttributeReference.html#noreturn
> --
> 2.17.1
>

Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>

-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Nick Desaulniers <ndesaulniers () google ! com>
To: linux-ext4
Subject: Re: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8)
Date: Thu, 20 Sep 2018 18:07:53 +0000
Message-ID: <CAKwvOd=0GZi5M3ZRxps+Y1Cvbk=h-VOMM4+cBzHx2-LGO1cLSg () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 10:23 AM Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
>
> From the GCC manual:
>
>   nonstring
>
>     The nonstring variable attribute specifies that an object or member
>     declaration with type array of char, signed char, or unsigned char,
>     or pointer to such a type is intended to store character arrays that
>     do not necessarily contain a terminating NUL. This is useful in detecting
>     uses of such arrays or pointers with functions that expect NUL-terminated
>     strings, and to avoid warnings when such an array or pointer is used as
>     an argument to a bounded string manipulation function such as strncpy.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html
>
> This attribute can be used for documentation purposes (i.e. replacing
> comments), but it is most helpful when the following warnings are enabled:
>
>   -Wstringop-overflow
>
>     Warn for calls to string manipulation functions such as memcpy and
>     strcpy that are determined to overflow the destination buffer.
>
>     [...]
>
>   -Wstringop-truncation
>
>     Warn for calls to bounded string manipulation functions such as
>     strncat, strncpy, and stpncpy that may either truncate the copied
>     string or leave the destination unchanged.
>
>     [...]
>
>     In situations where a character array is intended to store a sequence
>     of bytes with no terminating NUL such an array may be annotated with
>     attribute nonstring to avoid this warning. Such arrays, however,
>     are not suitable arguments to functions that expect NUL-terminated
>     strings. To help detect accidental misuses of such arrays GCC issues
>     warnings unless it can prove that the use is safe.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html
>
> Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
> ---
>  include/linux/compiler_attributes.h | 14 ++++++++++++++
>  1 file changed, 14 insertions(+)
>
> diff --git a/include/linux/compiler_attributes.h b/include/linux/compiler_attributes.h
> index f0f9fc398440..6b28c1b7310c 100644
> --- a/include/linux/compiler_attributes.h
> +++ b/include/linux/compiler_attributes.h
> @@ -34,6 +34,7 @@
>  # define __GCC4_has_attribute___externally_visible__  1
>  # define __GCC4_has_attribute___noclone__             1
>  # define __GCC4_has_attribute___optimize__            1
> +# define __GCC4_has_attribute___nonstring__           0
>  # define __GCC4_has_attribute___no_sanitize_address__ (__GNUC_MINOR__ >= 8)
>  #endif
>
> @@ -181,6 +182,19 @@
>   */
>  #define   noinline                      __attribute__((__noinline__))
>
> +/*
> + * Optional: only supported since gcc >= 8
> + * Optional: not supported by clang
> + * Optional: not supported by icc
> + *
> + *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html#index-nonstring-variable-attribute
> + */
> +#if __has_attribute(__nonstring__)
> +# define __nonstring                    __attribute__((__nonstring__))
> +#else
> +# define __nonstring
> +#endif
> +
>  /*
>   *   gcc: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#index-noreturn-function-attribute
>   * clang: https://clang.llvm.org/docs/AttributeReference.html#noreturn
> --
> 2.17.1
>

Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>

-- 
Thanks,
~Nick Desaulniers
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-doc
Subject: Re: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8)
Date: Thu, 20 Sep 2018 20:08:40 +0000
Message-ID: <CAGXu5jJEXFXnOVZrjoUyyuQ-=L=Lou3iLsA2Jjzci_McSgn2XA () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 10:22 AM, Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
> From the GCC manual:
>
>   nonstring
>
>     The nonstring variable attribute specifies that an object or member
>     declaration with type array of char, signed char, or unsigned char,
>     or pointer to such a type is intended to store character arrays that
>     do not necessarily contain a terminating NUL. This is useful in detecting
>     uses of such arrays or pointers with functions that expect NUL-terminated
>     strings, and to avoid warnings when such an array or pointer is used as
>     an argument to a bounded string manipulation function such as strncpy.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html
>
> This attribute can be used for documentation purposes (i.e. replacing
> comments), but it is most helpful when the following warnings are enabled:
>
>   -Wstringop-overflow
>
>     Warn for calls to string manipulation functions such as memcpy and
>     strcpy that are determined to overflow the destination buffer.
>
>     [...]
>
>   -Wstringop-truncation
>
>     Warn for calls to bounded string manipulation functions such as
>     strncat, strncpy, and stpncpy that may either truncate the copied
>     string or leave the destination unchanged.
>
>     [...]
>
>     In situations where a character array is intended to store a sequence
>     of bytes with no terminating NUL such an array may be annotated with
>     attribute nonstring to avoid this warning. Such arrays, however,
>     are not suitable arguments to functions that expect NUL-terminated
>     strings. To help detect accidental misuses of such arrays GCC issues
>     warnings unless it can prove that the use is safe.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html
>
> Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>

Reviewed-by: Kees Cook <keescook@chromium.org>

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-sparse
Subject: Re: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8)
Date: Thu, 20 Sep 2018 20:08:40 +0000
Message-ID: <CAGXu5jJEXFXnOVZrjoUyyuQ-=L=Lou3iLsA2Jjzci_McSgn2XA () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 10:22 AM, Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
> From the GCC manual:
>
>   nonstring
>
>     The nonstring variable attribute specifies that an object or member
>     declaration with type array of char, signed char, or unsigned char,
>     or pointer to such a type is intended to store character arrays that
>     do not necessarily contain a terminating NUL. This is useful in detecting
>     uses of such arrays or pointers with functions that expect NUL-terminated
>     strings, and to avoid warnings when such an array or pointer is used as
>     an argument to a bounded string manipulation function such as strncpy.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html
>
> This attribute can be used for documentation purposes (i.e. replacing
> comments), but it is most helpful when the following warnings are enabled:
>
>   -Wstringop-overflow
>
>     Warn for calls to string manipulation functions such as memcpy and
>     strcpy that are determined to overflow the destination buffer.
>
>     [...]
>
>   -Wstringop-truncation
>
>     Warn for calls to bounded string manipulation functions such as
>     strncat, strncpy, and stpncpy that may either truncate the copied
>     string or leave the destination unchanged.
>
>     [...]
>
>     In situations where a character array is intended to store a sequence
>     of bytes with no terminating NUL such an array may be annotated with
>     attribute nonstring to avoid this warning. Such arrays, however,
>     are not suitable arguments to functions that expect NUL-terminated
>     strings. To help detect accidental misuses of such arrays GCC issues
>     warnings unless it can prove that the use is safe.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html
>
> Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>

Reviewed-by: Kees Cook <keescook@chromium.org>

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-kernel
Subject: Re: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8)
Date: Thu, 20 Sep 2018 20:08:40 +0000
Message-ID: <CAGXu5jJEXFXnOVZrjoUyyuQ-=L=Lou3iLsA2Jjzci_McSgn2XA () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 10:22 AM, Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
> From the GCC manual:
>
>   nonstring
>
>     The nonstring variable attribute specifies that an object or member
>     declaration with type array of char, signed char, or unsigned char,
>     or pointer to such a type is intended to store character arrays that
>     do not necessarily contain a terminating NUL. This is useful in detecting
>     uses of such arrays or pointers with functions that expect NUL-terminated
>     strings, and to avoid warnings when such an array or pointer is used as
>     an argument to a bounded string manipulation function such as strncpy.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html
>
> This attribute can be used for documentation purposes (i.e. replacing
> comments), but it is most helpful when the following warnings are enabled:
>
>   -Wstringop-overflow
>
>     Warn for calls to string manipulation functions such as memcpy and
>     strcpy that are determined to overflow the destination buffer.
>
>     [...]
>
>   -Wstringop-truncation
>
>     Warn for calls to bounded string manipulation functions such as
>     strncat, strncpy, and stpncpy that may either truncate the copied
>     string or leave the destination unchanged.
>
>     [...]
>
>     In situations where a character array is intended to store a sequence
>     of bytes with no terminating NUL such an array may be annotated with
>     attribute nonstring to avoid this warning. Such arrays, however,
>     are not suitable arguments to functions that expect NUL-terminated
>     strings. To help detect accidental misuses of such arrays GCC issues
>     warnings unless it can prove that the use is safe.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html
>
> Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>

Reviewed-by: Kees Cook <keescook@chromium.org>

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Kees Cook <keescook () chromium ! org>
To: linux-ext4
Subject: Re: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8)
Date: Thu, 20 Sep 2018 20:08:40 +0000
Message-ID: <CAGXu5jJEXFXnOVZrjoUyyuQ-=L=Lou3iLsA2Jjzci_McSgn2XA () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 10:22 AM, Miguel Ojeda
<miguel.ojeda.sandonis@gmail.com> wrote:
> From the GCC manual:
>
>   nonstring
>
>     The nonstring variable attribute specifies that an object or member
>     declaration with type array of char, signed char, or unsigned char,
>     or pointer to such a type is intended to store character arrays that
>     do not necessarily contain a terminating NUL. This is useful in detecting
>     uses of such arrays or pointers with functions that expect NUL-terminated
>     strings, and to avoid warnings when such an array or pointer is used as
>     an argument to a bounded string manipulation function such as strncpy.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html
>
> This attribute can be used for documentation purposes (i.e. replacing
> comments), but it is most helpful when the following warnings are enabled:
>
>   -Wstringop-overflow
>
>     Warn for calls to string manipulation functions such as memcpy and
>     strcpy that are determined to overflow the destination buffer.
>
>     [...]
>
>   -Wstringop-truncation
>
>     Warn for calls to bounded string manipulation functions such as
>     strncat, strncpy, and stpncpy that may either truncate the copied
>     string or leave the destination unchanged.
>
>     [...]
>
>     In situations where a character array is intended to store a sequence
>     of bytes with no terminating NUL such an array may be annotated with
>     attribute nonstring to avoid this warning. Such arrays, however,
>     are not suitable arguments to functions that expect NUL-terminated
>     strings. To help detect accidental misuses of such arrays GCC issues
>     warnings unless it can prove that the use is safe.
>
>   https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html
>
> Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>

Reviewed-by: Kees Cook <keescook@chromium.org>

-Kees

-- 
Kees Cook
Pixel Security
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kbuild
Subject: Re: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8)
Date: Sun, 30 Sep 2018 11:16:15 +0000
Message-ID: <CANiq72nT12vjxjyPr2_nPFQroiiPbba7uoF0V-y_5+4yid=1iw () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 10:08 PM Kees Cook <keescook@chromium.org> wrote:
>
> Reviewed-by: Kees Cook <keescook@chromium.org>
>

Done! Thanks!

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-kernel
Subject: Re: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8)
Date: Sun, 30 Sep 2018 11:16:15 +0000
Message-ID: <CANiq72nT12vjxjyPr2_nPFQroiiPbba7uoF0V-y_5+4yid=1iw () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 10:08 PM Kees Cook <keescook@chromium.org> wrote:
>
> Reviewed-by: Kees Cook <keescook@chromium.org>
>

Done! Thanks!

Cheers,
Miguel
================================================================================

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-ext4
Subject: Re: [PATCH v5 12/15] Compiler Attributes: add support for __nonstring (gcc >= 8)
Date: Sun, 30 Sep 2018 11:16:15 +0000
Message-ID: <CANiq72nT12vjxjyPr2_nPFQroiiPbba7uoF0V-y_5+4yid=1iw () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 10:08 PM Kees Cook <keescook@chromium.org> wrote:
>
> Reviewed-by: Kees Cook <keescook@chromium.org>
>

Done! Thanks!

Cheers,
Miguel
================================================================================


################################################################################

=== Thread: [PATCH v5 13/15] Compiler Attributes: enable -Wstringop-truncation on W=1 (gcc >= 8) ===

From: Miguel Ojeda <miguel.ojeda.sandonis () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH v5 13/15] Compiler Attributes: enable -Wstringop-truncation on W=1 (gcc >= 8)
Date: Sun, 30 Sep 2018 11:17:33 +0000
Message-ID: <CANiq72nvZZAF445cBTiLCX-4EOLD6OsRo0TP8eXgwvBAzakgOA () mail ! gmail ! com>
--------------------
On Thu, Sep 20, 2018 at 9:56 PM Kees Cook <keescook@chromium.org> wrote:
>
> Yessss. :)
>
> Reviewed-by: Kees Cook <keescook@chromium.org>

Done! Thanks! :)

Cheers,
Miguel
================================================================================


################################################################################

=== Thread: [PATCH v6 09/18] khwasan, arm64: fix up fault handling logic ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v6 09/18] khwasan, arm64: fix up fault handling logic
Date: Wed, 29 Aug 2018 11:35:13 +0000
Message-ID: <4da4495dc20525db1654d3dd9d578b7965d98507.1535462971.git.andreyknvl () google ! com>
--------------------
show_pte in arm64 fault handling relies on the fact that the top byte of
a kernel pointer is 0xff, which isn't always the case with KHWASAN enabled.
Reset the top byte.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/mm/fault.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 50b30ff30de4..35feee78a9bd 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -32,6 +32,7 @@
 #include <linux/perf_event.h>
 #include <linux/preempt.h>
 #include <linux/hugetlb.h>
+#include <linux/kasan.h>
 
 #include <asm/bug.h>
 #include <asm/cmpxchg.h>
@@ -134,6 +135,8 @@ void show_pte(unsigned long addr)
 	pgd_t *pgdp;
 	pgd_t pgd;
 
+	addr = (unsigned long)khwasan_reset_tag((void *)addr);
+
 	if (addr < TASK_SIZE) {
 		/* TTBR0 */
 		mm = current->active_mm;
-- 
2.19.0.rc0.228.g281dcd1b4d0-goog

================================================================================


################################################################################

=== Thread: [PATCH v6 10/18] khwasan, arm64: enable top byte ignore for the kernel ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v6 10/18] khwasan, arm64: enable top byte ignore for the kernel
Date: Wed, 29 Aug 2018 11:35:14 +0000
Message-ID: <d34bd1004c8789729228d8bd38aaf08a65c94032.1535462971.git.andreyknvl () google ! com>
--------------------
KHWASAN uses the Top Byte Ignore feature of arm64 CPUs to store a pointer
tag in the top byte of each pointer. This commit enables the TCR_TBI1 bit,
which enables Top Byte Ignore for the kernel, when KHWASAN is used.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/pgtable-hwdef.h | 1 +
 arch/arm64/mm/proc.S                   | 8 +++++++-
 2 files changed, 8 insertions(+), 1 deletion(-)

diff --git a/arch/arm64/include/asm/pgtable-hwdef.h b/arch/arm64/include/asm/pgtable-hwdef.h
index fd208eac9f2a..483aceedad76 100644
--- a/arch/arm64/include/asm/pgtable-hwdef.h
+++ b/arch/arm64/include/asm/pgtable-hwdef.h
@@ -289,6 +289,7 @@
 #define TCR_A1			(UL(1) << 22)
 #define TCR_ASID16		(UL(1) << 36)
 #define TCR_TBI0		(UL(1) << 37)
+#define TCR_TBI1		(UL(1) << 38)
 #define TCR_HA			(UL(1) << 39)
 #define TCR_HD			(UL(1) << 40)
 #define TCR_NFD1		(UL(1) << 54)
diff --git a/arch/arm64/mm/proc.S b/arch/arm64/mm/proc.S
index 03646e6a2ef4..c5175e098d02 100644
--- a/arch/arm64/mm/proc.S
+++ b/arch/arm64/mm/proc.S
@@ -47,6 +47,12 @@
 /* PTWs cacheable, inner/outer WBWA */
 #define TCR_CACHE_FLAGS	TCR_IRGN_WBWA | TCR_ORGN_WBWA
 
+#ifdef CONFIG_KASAN_HW
+#define TCR_KASAN_FLAGS TCR_TBI1
+#else
+#define TCR_KASAN_FLAGS 0
+#endif
+
 #define MAIR(attr, mt)	((attr) << ((mt) * 8))
 
 /*
@@ -440,7 +446,7 @@ ENTRY(__cpu_setup)
 	 */
 	ldr	x10, =TCR_TxSZ(VA_BITS) | TCR_CACHE_FLAGS | TCR_SMP_FLAGS | \
 			TCR_TG_FLAGS | TCR_KASLR_FLAGS | TCR_ASID16 | \
-			TCR_TBI0 | TCR_A1
+			TCR_TBI0 | TCR_A1 | TCR_KASAN_FLAGS
 	tcr_set_idmap_t0sz	x10, x9
 
 	/*
-- 
2.19.0.rc0.228.g281dcd1b4d0-goog

================================================================================


################################################################################

=== Thread: [PATCH v6 11/18] khwasan, mm: perform untagged pointers comparison in krealloc ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v6 11/18] khwasan, mm: perform untagged pointers comparison in krealloc
Date: Wed, 29 Aug 2018 11:35:15 +0000
Message-ID: <a13a41e3ca65116eb5614c4dd396b23182e98fed.1535462971.git.andreyknvl () google ! com>
--------------------
The krealloc function checks where the same buffer was reused or a new one
allocated by comparing kernel pointers. KHWASAN changes memory tag on the
krealloc'ed chunk of memory and therefore also changes the pointer tag of
the returned pointer. Therefore we need to perform comparison on untagged
(with tags reset) pointers to check whether it's the same memory region or
not.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/slab_common.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/mm/slab_common.c b/mm/slab_common.c
index 3abfa0f86118..0d588dfebd7d 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -1513,7 +1513,7 @@ void *krealloc(const void *p, size_t new_size, gfp_t flags)
 	}
 
 	ret = __do_krealloc(p, new_size, flags);
-	if (ret && p != ret)
+	if (ret && khwasan_reset_tag(p) != khwasan_reset_tag(ret))
 		kfree(p);
 
 	return ret;
-- 
2.19.0.rc0.228.g281dcd1b4d0-goog

================================================================================


################################################################################

=== Thread: [PATCH v6 12/18] khwasan: split out kasan_report.c from report.c ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v6 12/18] khwasan: split out kasan_report.c from report.c
Date: Wed, 29 Aug 2018 11:35:16 +0000
Message-ID: <ecd557ae22f4f0108ffc5637f3ec9de3bf81c2fc.1535462971.git.andreyknvl () google ! com>
--------------------
This patch moves KASAN specific error reporting routines to kasan_report.c
without any functional changes, leaving common error reporting code in
report.c to be later reused by KHWASAN.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/Makefile         |   4 +-
 mm/kasan/kasan.h          |   7 ++
 mm/kasan/kasan_report.c   | 158 +++++++++++++++++++++++++
 mm/kasan/khwasan_report.c |  39 +++++++
 mm/kasan/report.c         | 234 +++++++++-----------------------------
 5 files changed, 257 insertions(+), 185 deletions(-)
 create mode 100644 mm/kasan/kasan_report.c
 create mode 100644 mm/kasan/khwasan_report.c

diff --git a/mm/kasan/Makefile b/mm/kasan/Makefile
index 14955add96d3..7ef536390365 100644
--- a/mm/kasan/Makefile
+++ b/mm/kasan/Makefile
@@ -14,5 +14,5 @@ CFLAGS_kasan.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 CFLAGS_khwasan.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 
 obj-$(CONFIG_KASAN) := common.o kasan_init.o report.o
-obj-$(CONFIG_KASAN_GENERIC) += kasan.o quarantine.o
-obj-$(CONFIG_KASAN_HW) += khwasan.o
+obj-$(CONFIG_KASAN_GENERIC) += kasan.o kasan_report.o quarantine.o
+obj-$(CONFIG_KASAN_HW) += khwasan.o khwasan_report.o
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index a7cc27d96608..82672473740c 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -109,11 +109,18 @@ static inline const void *kasan_shadow_to_mem(const void *shadow_addr)
 		<< KASAN_SHADOW_SCALE_SHIFT);
 }
 
+static inline bool addr_has_shadow(const void *addr)
+{
+	return (addr >= kasan_shadow_to_mem((void *)KASAN_SHADOW_START));
+}
+
 void kasan_poison_shadow(const void *address, size_t size, u8 value);
 
 void check_memory_region(unsigned long addr, size_t size, bool write,
 				unsigned long ret_ip);
 
+const char *get_bug_type(struct kasan_access_info *info);
+
 void kasan_report(unsigned long addr, size_t size,
 		bool is_write, unsigned long ip);
 void kasan_report_invalid_free(void *object, unsigned long ip);
diff --git a/mm/kasan/kasan_report.c b/mm/kasan/kasan_report.c
new file mode 100644
index 000000000000..2d8decbecbd5
--- /dev/null
+++ b/mm/kasan/kasan_report.c
@@ -0,0 +1,158 @@
+/*
+ * This file contains KASAN specific error reporting code.
+ *
+ * Copyright (c) 2014 Samsung Electronics Co., Ltd.
+ * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
+ *
+ * Some code borrowed from https://github.com/xairy/kasan-prototype by
+ *        Andrey Konovalov <andreyknvl@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/bitops.h>
+#include <linux/ftrace.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/stackdepot.h>
+#include <linux/stacktrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/kasan.h>
+#include <linux/module.h>
+
+#include <asm/sections.h>
+
+#include "kasan.h"
+#include "../slab.h"
+
+static const void *find_first_bad_addr(const void *addr, size_t size)
+{
+	u8 shadow_val = *(u8 *)kasan_mem_to_shadow(addr);
+	const void *first_bad_addr = addr;
+
+	while (!shadow_val && first_bad_addr < addr + size) {
+		first_bad_addr += KASAN_SHADOW_SCALE_SIZE;
+		shadow_val = *(u8 *)kasan_mem_to_shadow(first_bad_addr);
+	}
+	return first_bad_addr;
+}
+
+static const char *get_shadow_bug_type(struct kasan_access_info *info)
+{
+	const char *bug_type = "unknown-crash";
+	u8 *shadow_addr;
+
+	info->first_bad_addr = find_first_bad_addr(info->access_addr,
+						info->access_size);
+
+	shadow_addr = (u8 *)kasan_mem_to_shadow(info->first_bad_addr);
+
+	/*
+	 * If shadow byte value is in [0, KASAN_SHADOW_SCALE_SIZE) we can look
+	 * at the next shadow byte to determine the type of the bad access.
+	 */
+	if (*shadow_addr > 0 && *shadow_addr <= KASAN_SHADOW_SCALE_SIZE - 1)
+		shadow_addr++;
+
+	switch (*shadow_addr) {
+	case 0 ... KASAN_SHADOW_SCALE_SIZE - 1:
+		/*
+		 * In theory it's still possible to see these shadow values
+		 * due to a data race in the kernel code.
+		 */
+		bug_type = "out-of-bounds";
+		break;
+	case KASAN_PAGE_REDZONE:
+	case KASAN_KMALLOC_REDZONE:
+		bug_type = "slab-out-of-bounds";
+		break;
+	case KASAN_GLOBAL_REDZONE:
+		bug_type = "global-out-of-bounds";
+		break;
+	case KASAN_STACK_LEFT:
+	case KASAN_STACK_MID:
+	case KASAN_STACK_RIGHT:
+	case KASAN_STACK_PARTIAL:
+		bug_type = "stack-out-of-bounds";
+		break;
+	case KASAN_FREE_PAGE:
+	case KASAN_KMALLOC_FREE:
+		bug_type = "use-after-free";
+		break;
+	case KASAN_USE_AFTER_SCOPE:
+		bug_type = "use-after-scope";
+		break;
+	case KASAN_ALLOCA_LEFT:
+	case KASAN_ALLOCA_RIGHT:
+		bug_type = "alloca-out-of-bounds";
+		break;
+	}
+
+	return bug_type;
+}
+
+static const char *get_wild_bug_type(struct kasan_access_info *info)
+{
+	const char *bug_type = "unknown-crash";
+
+	if ((unsigned long)info->access_addr < PAGE_SIZE)
+		bug_type = "null-ptr-deref";
+	else if ((unsigned long)info->access_addr < TASK_SIZE)
+		bug_type = "user-memory-access";
+	else
+		bug_type = "wild-memory-access";
+
+	return bug_type;
+}
+
+const char *get_bug_type(struct kasan_access_info *info)
+{
+	if (addr_has_shadow(info->access_addr))
+		return get_shadow_bug_type(info);
+	return get_wild_bug_type(info);
+}
+
+#define DEFINE_ASAN_REPORT_LOAD(size)                     \
+void __asan_report_load##size##_noabort(unsigned long addr) \
+{                                                         \
+	kasan_report(addr, size, false, _RET_IP_);	  \
+}                                                         \
+EXPORT_SYMBOL(__asan_report_load##size##_noabort)
+
+#define DEFINE_ASAN_REPORT_STORE(size)                     \
+void __asan_report_store##size##_noabort(unsigned long addr) \
+{                                                          \
+	kasan_report(addr, size, true, _RET_IP_);	   \
+}                                                          \
+EXPORT_SYMBOL(__asan_report_store##size##_noabort)
+
+DEFINE_ASAN_REPORT_LOAD(1);
+DEFINE_ASAN_REPORT_LOAD(2);
+DEFINE_ASAN_REPORT_LOAD(4);
+DEFINE_ASAN_REPORT_LOAD(8);
+DEFINE_ASAN_REPORT_LOAD(16);
+DEFINE_ASAN_REPORT_STORE(1);
+DEFINE_ASAN_REPORT_STORE(2);
+DEFINE_ASAN_REPORT_STORE(4);
+DEFINE_ASAN_REPORT_STORE(8);
+DEFINE_ASAN_REPORT_STORE(16);
+
+void __asan_report_load_n_noabort(unsigned long addr, size_t size)
+{
+	kasan_report(addr, size, false, _RET_IP_);
+}
+EXPORT_SYMBOL(__asan_report_load_n_noabort);
+
+void __asan_report_store_n_noabort(unsigned long addr, size_t size)
+{
+	kasan_report(addr, size, true, _RET_IP_);
+}
+EXPORT_SYMBOL(__asan_report_store_n_noabort);
diff --git a/mm/kasan/khwasan_report.c b/mm/kasan/khwasan_report.c
new file mode 100644
index 000000000000..2edbc3c76be5
--- /dev/null
+++ b/mm/kasan/khwasan_report.c
@@ -0,0 +1,39 @@
+/*
+ * This file contains KHWASAN specific error reporting code.
+ *
+ * Copyright (c) 2014 Samsung Electronics Co., Ltd.
+ * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
+ *
+ * Some code borrowed from https://github.com/xairy/kasan-prototype by
+ *        Andrey Konovalov <andreyknvl@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/bitops.h>
+#include <linux/ftrace.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/stackdepot.h>
+#include <linux/stacktrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/kasan.h>
+#include <linux/module.h>
+
+#include <asm/sections.h>
+
+#include "kasan.h"
+#include "../slab.h"
+
+const char *get_bug_type(struct kasan_access_info *info)
+{
+	return "invalid-access";
+}
diff --git a/mm/kasan/report.c b/mm/kasan/report.c
index 5c169aa688fd..155247a6f8a8 100644
--- a/mm/kasan/report.c
+++ b/mm/kasan/report.c
@@ -1,5 +1,5 @@
 /*
- * This file contains error reporting code.
+ * This file contains common KASAN and KHWASAN error reporting code.
  *
  * Copyright (c) 2014 Samsung Electronics Co., Ltd.
  * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
@@ -39,103 +39,34 @@
 #define SHADOW_BYTES_PER_ROW (SHADOW_BLOCKS_PER_ROW * SHADOW_BYTES_PER_BLOCK)
 #define SHADOW_ROWS_AROUND_ADDR 2
 
-static const void *find_first_bad_addr(const void *addr, size_t size)
-{
-	u8 shadow_val = *(u8 *)kasan_mem_to_shadow(addr);
-	const void *first_bad_addr = addr;
-
-	while (!shadow_val && first_bad_addr < addr + size) {
-		first_bad_addr += KASAN_SHADOW_SCALE_SIZE;
-		shadow_val = *(u8 *)kasan_mem_to_shadow(first_bad_addr);
-	}
-	return first_bad_addr;
-}
+static unsigned long kasan_flags;
 
-static bool addr_has_shadow(struct kasan_access_info *info)
-{
-	return (info->access_addr >=
-		kasan_shadow_to_mem((void *)KASAN_SHADOW_START));
-}
+#define KASAN_BIT_REPORTED	0
+#define KASAN_BIT_MULTI_SHOT	1
 
-static const char *get_shadow_bug_type(struct kasan_access_info *info)
+bool kasan_save_enable_multi_shot(void)
 {
-	const char *bug_type = "unknown-crash";
-	u8 *shadow_addr;
-
-	info->first_bad_addr = find_first_bad_addr(info->access_addr,
-						info->access_size);
-
-	shadow_addr = (u8 *)kasan_mem_to_shadow(info->first_bad_addr);
-
-	/*
-	 * If shadow byte value is in [0, KASAN_SHADOW_SCALE_SIZE) we can look
-	 * at the next shadow byte to determine the type of the bad access.
-	 */
-	if (*shadow_addr > 0 && *shadow_addr <= KASAN_SHADOW_SCALE_SIZE - 1)
-		shadow_addr++;
-
-	switch (*shadow_addr) {
-	case 0 ... KASAN_SHADOW_SCALE_SIZE - 1:
-		/*
-		 * In theory it's still possible to see these shadow values
-		 * due to a data race in the kernel code.
-		 */
-		bug_type = "out-of-bounds";
-		break;
-	case KASAN_PAGE_REDZONE:
-	case KASAN_KMALLOC_REDZONE:
-		bug_type = "slab-out-of-bounds";
-		break;
-	case KASAN_GLOBAL_REDZONE:
-		bug_type = "global-out-of-bounds";
-		break;
-	case KASAN_STACK_LEFT:
-	case KASAN_STACK_MID:
-	case KASAN_STACK_RIGHT:
-	case KASAN_STACK_PARTIAL:
-		bug_type = "stack-out-of-bounds";
-		break;
-	case KASAN_FREE_PAGE:
-	case KASAN_KMALLOC_FREE:
-		bug_type = "use-after-free";
-		break;
-	case KASAN_USE_AFTER_SCOPE:
-		bug_type = "use-after-scope";
-		break;
-	case KASAN_ALLOCA_LEFT:
-	case KASAN_ALLOCA_RIGHT:
-		bug_type = "alloca-out-of-bounds";
-		break;
-	}
-
-	return bug_type;
+	return test_and_set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
 }
+EXPORT_SYMBOL_GPL(kasan_save_enable_multi_shot);
 
-static const char *get_wild_bug_type(struct kasan_access_info *info)
+void kasan_restore_multi_shot(bool enabled)
 {
-	const char *bug_type = "unknown-crash";
-
-	if ((unsigned long)info->access_addr < PAGE_SIZE)
-		bug_type = "null-ptr-deref";
-	else if ((unsigned long)info->access_addr < TASK_SIZE)
-		bug_type = "user-memory-access";
-	else
-		bug_type = "wild-memory-access";
-
-	return bug_type;
+	if (!enabled)
+		clear_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
 }
+EXPORT_SYMBOL_GPL(kasan_restore_multi_shot);
 
-static const char *get_bug_type(struct kasan_access_info *info)
+static int __init kasan_set_multi_shot(char *str)
 {
-	if (addr_has_shadow(info))
-		return get_shadow_bug_type(info);
-	return get_wild_bug_type(info);
+	set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
+	return 1;
 }
+__setup("kasan_multi_shot", kasan_set_multi_shot);
 
-static void print_error_description(struct kasan_access_info *info)
+static void print_error_description(struct kasan_access_info *info,
+					const char *bug_type)
 {
-	const char *bug_type = get_bug_type(info);
-
 	pr_err("BUG: KASAN: %s in %pS\n",
 		bug_type, (void *)info->ip);
 	pr_err("%s of size %zu at addr %px by task %s/%d\n",
@@ -143,25 +74,9 @@ static void print_error_description(struct kasan_access_info *info)
 		info->access_addr, current->comm, task_pid_nr(current));
 }
 
-static inline bool kernel_or_module_addr(const void *addr)
-{
-	if (addr >= (void *)_stext && addr < (void *)_end)
-		return true;
-	if (is_module_address((unsigned long)addr))
-		return true;
-	return false;
-}
-
-static inline bool init_task_stack_addr(const void *addr)
-{
-	return addr >= (void *)&init_thread_union.stack &&
-		(addr <= (void *)&init_thread_union.stack +
-			sizeof(init_thread_union.stack));
-}
-
 static DEFINE_SPINLOCK(report_lock);
 
-static void kasan_start_report(unsigned long *flags)
+static void start_report(unsigned long *flags)
 {
 	/*
 	 * Make sure we don't end up in loop.
@@ -171,7 +86,7 @@ static void kasan_start_report(unsigned long *flags)
 	pr_err("==================================================================\n");
 }
 
-static void kasan_end_report(unsigned long *flags)
+static void end_report(unsigned long *flags)
 {
 	pr_err("==================================================================\n");
 	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
@@ -249,6 +164,22 @@ static void describe_object(struct kmem_cache *cache, void *object,
 	describe_object_addr(cache, object, addr);
 }
 
+static inline bool kernel_or_module_addr(const void *addr)
+{
+	if (addr >= (void *)_stext && addr < (void *)_end)
+		return true;
+	if (is_module_address((unsigned long)addr))
+		return true;
+	return false;
+}
+
+static inline bool init_task_stack_addr(const void *addr)
+{
+	return addr >= (void *)&init_thread_union.stack &&
+		(addr <= (void *)&init_thread_union.stack +
+			sizeof(init_thread_union.stack));
+}
+
 static void print_address_description(void *addr)
 {
 	struct page *page = addr_to_page(addr);
@@ -326,29 +257,38 @@ static void print_shadow_for_address(const void *addr)
 	}
 }
 
+static bool report_enabled(void)
+{
+	if (current->kasan_depth)
+		return false;
+	if (test_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags))
+		return true;
+	return !test_and_set_bit(KASAN_BIT_REPORTED, &kasan_flags);
+}
+
 void kasan_report_invalid_free(void *object, unsigned long ip)
 {
 	unsigned long flags;
 
-	kasan_start_report(&flags);
+	start_report(&flags);
 	pr_err("BUG: KASAN: double-free or invalid-free in %pS\n", (void *)ip);
 	pr_err("\n");
 	print_address_description(object);
 	pr_err("\n");
 	print_shadow_for_address(object);
-	kasan_end_report(&flags);
+	end_report(&flags);
 }
 
 static void kasan_report_error(struct kasan_access_info *info)
 {
 	unsigned long flags;
 
-	kasan_start_report(&flags);
+	start_report(&flags);
 
-	print_error_description(info);
+	print_error_description(info, get_bug_type(info));
 	pr_err("\n");
 
-	if (!addr_has_shadow(info)) {
+	if (!addr_has_shadow(info->access_addr)) {
 		dump_stack();
 	} else {
 		print_address_description((void *)info->access_addr);
@@ -356,41 +296,7 @@ static void kasan_report_error(struct kasan_access_info *info)
 		print_shadow_for_address(info->first_bad_addr);
 	}
 
-	kasan_end_report(&flags);
-}
-
-static unsigned long kasan_flags;
-
-#define KASAN_BIT_REPORTED	0
-#define KASAN_BIT_MULTI_SHOT	1
-
-bool kasan_save_enable_multi_shot(void)
-{
-	return test_and_set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
-}
-EXPORT_SYMBOL_GPL(kasan_save_enable_multi_shot);
-
-void kasan_restore_multi_shot(bool enabled)
-{
-	if (!enabled)
-		clear_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
-}
-EXPORT_SYMBOL_GPL(kasan_restore_multi_shot);
-
-static int __init kasan_set_multi_shot(char *str)
-{
-	set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
-	return 1;
-}
-__setup("kasan_multi_shot", kasan_set_multi_shot);
-
-static inline bool kasan_report_enabled(void)
-{
-	if (current->kasan_depth)
-		return false;
-	if (test_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags))
-		return true;
-	return !test_and_set_bit(KASAN_BIT_REPORTED, &kasan_flags);
+	end_report(&flags);
 }
 
 void kasan_report(unsigned long addr, size_t size,
@@ -398,7 +304,7 @@ void kasan_report(unsigned long addr, size_t size,
 {
 	struct kasan_access_info info;
 
-	if (likely(!kasan_report_enabled()))
+	if (likely(!report_enabled()))
 		return;
 
 	disable_trace_on_warning();
@@ -411,41 +317,3 @@ void kasan_report(unsigned long addr, size_t size,
 
 	kasan_report_error(&info);
 }
-
-
-#define DEFINE_ASAN_REPORT_LOAD(size)                     \
-void __asan_report_load##size##_noabort(unsigned long addr) \
-{                                                         \
-	kasan_report(addr, size, false, _RET_IP_);	  \
-}                                                         \
-EXPORT_SYMBOL(__asan_report_load##size##_noabort)
-
-#define DEFINE_ASAN_REPORT_STORE(size)                     \
-void __asan_report_store##size##_noabort(unsigned long addr) \
-{                                                          \
-	kasan_report(addr, size, true, _RET_IP_);	   \
-}                                                          \
-EXPORT_SYMBOL(__asan_report_store##size##_noabort)
-
-DEFINE_ASAN_REPORT_LOAD(1);
-DEFINE_ASAN_REPORT_LOAD(2);
-DEFINE_ASAN_REPORT_LOAD(4);
-DEFINE_ASAN_REPORT_LOAD(8);
-DEFINE_ASAN_REPORT_LOAD(16);
-DEFINE_ASAN_REPORT_STORE(1);
-DEFINE_ASAN_REPORT_STORE(2);
-DEFINE_ASAN_REPORT_STORE(4);
-DEFINE_ASAN_REPORT_STORE(8);
-DEFINE_ASAN_REPORT_STORE(16);
-
-void __asan_report_load_n_noabort(unsigned long addr, size_t size)
-{
-	kasan_report(addr, size, false, _RET_IP_);
-}
-EXPORT_SYMBOL(__asan_report_load_n_noabort);
-
-void __asan_report_store_n_noabort(unsigned long addr, size_t size)
-{
-	kasan_report(addr, size, true, _RET_IP_);
-}
-EXPORT_SYMBOL(__asan_report_store_n_noabort);
-- 
2.19.0.rc0.228.g281dcd1b4d0-goog

================================================================================


################################################################################

=== Thread: [PATCH v6 14/18] khwasan: add hooks implementation ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v6 14/18] khwasan: add hooks implementation
Date: Wed, 29 Aug 2018 11:35:18 +0000
Message-ID: <4267d0903e0fdf9c261b91cf8a2bf0f71047a43c.1535462971.git.andreyknvl () google ! com>
--------------------
This commit adds KHWASAN specific hooks implementation and adjusts
common KASAN and KHWASAN ones.

1. When a new slab cache is created, KHWASAN rounds up the size of the
   objects in this cache to KASAN_SHADOW_SCALE_SIZE (== 16).

2. On each kmalloc KHWASAN generates a random tag, sets the shadow memory,
   that corresponds to this object to this tag, and embeds this tag value
   into the top byte of the returned pointer.

3. On each kfree KHWASAN poisons the shadow memory with a random tag to
   allow detection of use-after-free bugs.

The rest of the logic of the hook implementation is very much similar to
the one provided by KASAN. KHWASAN saves allocation and free stack metadata
to the slab object the same was KASAN does this.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/common.c  | 82 +++++++++++++++++++++++++++++++++++-----------
 mm/kasan/kasan.h   |  8 +++++
 mm/kasan/khwasan.c | 40 ++++++++++++++++++++++
 3 files changed, 111 insertions(+), 19 deletions(-)

diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index bed8e13c6e1d..938229b26f3a 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -140,6 +140,9 @@ void kasan_poison_shadow(const void *address, size_t size, u8 value)
 {
 	void *shadow_start, *shadow_end;
 
+	/* Perform shadow offset calculation based on untagged address */
+	address = reset_tag(address);
+
 	shadow_start = kasan_mem_to_shadow(address);
 	shadow_end = kasan_mem_to_shadow(address + size);
 
@@ -148,11 +151,20 @@ void kasan_poison_shadow(const void *address, size_t size, u8 value)
 
 void kasan_unpoison_shadow(const void *address, size_t size)
 {
-	kasan_poison_shadow(address, size, 0);
+	u8 tag = get_tag(address);
+
+	/* Perform shadow offset calculation based on untagged address */
+	address = reset_tag(address);
+
+	kasan_poison_shadow(address, size, tag);
 
 	if (size & KASAN_SHADOW_MASK) {
 		u8 *shadow = (u8 *)kasan_mem_to_shadow(address + size);
-		*shadow = size & KASAN_SHADOW_MASK;
+
+		if (IS_ENABLED(CONFIG_KASAN_HW))
+			*shadow = tag;
+		else
+			*shadow = size & KASAN_SHADOW_MASK;
 	}
 }
 
@@ -200,8 +212,9 @@ void kasan_unpoison_stack_above_sp_to(const void *watermark)
 
 void kasan_alloc_pages(struct page *page, unsigned int order)
 {
-	if (likely(!PageHighMem(page)))
-		kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
+	if (unlikely(PageHighMem(page)))
+		return;
+	kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
 }
 
 void kasan_free_pages(struct page *page, unsigned int order)
@@ -235,6 +248,7 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 			slab_flags_t *flags)
 {
 	unsigned int orig_size = *size;
+	unsigned int redzone_size = 0;
 	int redzone_adjust;
 
 	/* Add alloc meta. */
@@ -242,20 +256,20 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 	*size += sizeof(struct kasan_alloc_meta);
 
 	/* Add free meta. */
-	if (cache->flags & SLAB_TYPESAFE_BY_RCU || cache->ctor ||
-	    cache->object_size < sizeof(struct kasan_free_meta)) {
+	if (IS_ENABLED(CONFIG_KASAN_GENERIC) &&
+	    (cache->flags & SLAB_TYPESAFE_BY_RCU || cache->ctor ||
+	     cache->object_size < sizeof(struct kasan_free_meta))) {
 		cache->kasan_info.free_meta_offset = *size;
 		*size += sizeof(struct kasan_free_meta);
 	}
-	redzone_adjust = optimal_redzone(cache->object_size) -
-		(*size - cache->object_size);
 
+	redzone_size = optimal_redzone(cache->object_size);
+	redzone_adjust = redzone_size -	(*size - cache->object_size);
 	if (redzone_adjust > 0)
 		*size += redzone_adjust;
 
 	*size = min_t(unsigned int, KMALLOC_MAX_SIZE,
-			max(*size, cache->object_size +
-					optimal_redzone(cache->object_size)));
+			max(*size, cache->object_size + redzone_size));
 
 	/*
 	 * If the metadata doesn't fit, don't enable KASAN at all.
@@ -268,6 +282,8 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 		return;
 	}
 
+	cache->align = round_up(cache->align, KASAN_SHADOW_SCALE_SIZE);
+
 	*flags |= SLAB_KASAN;
 }
 
@@ -328,15 +344,30 @@ void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t flags)
 	return kasan_kmalloc(cache, object, cache->object_size, flags);
 }
 
+static inline bool shadow_invalid(u8 tag, s8 shadow_byte)
+{
+	if (IS_ENABLED(CONFIG_KASAN_GENERIC))
+		return shadow_byte < 0 ||
+			shadow_byte >= KASAN_SHADOW_SCALE_SIZE;
+	else
+		return tag != (u8)shadow_byte;
+}
+
 static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
 			      unsigned long ip, bool quarantine)
 {
 	s8 shadow_byte;
+	u8 tag;
+	void *tagged_object;
 	unsigned long rounded_up_size;
 
+	tag = get_tag(object);
+	tagged_object = object;
+	object = reset_tag(object);
+
 	if (unlikely(nearest_obj(cache, virt_to_head_page(object), object) !=
 	    object)) {
-		kasan_report_invalid_free(object, ip);
+		kasan_report_invalid_free(tagged_object, ip);
 		return true;
 	}
 
@@ -345,20 +376,22 @@ static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
 		return false;
 
 	shadow_byte = READ_ONCE(*(s8 *)kasan_mem_to_shadow(object));
-	if (shadow_byte < 0 || shadow_byte >= KASAN_SHADOW_SCALE_SIZE) {
-		kasan_report_invalid_free(object, ip);
+	if (shadow_invalid(tag, shadow_byte)) {
+		kasan_report_invalid_free(tagged_object, ip);
 		return true;
 	}
 
 	rounded_up_size = round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE);
 	kasan_poison_shadow(object, rounded_up_size, KASAN_KMALLOC_FREE);
 
-	if (!quarantine || unlikely(!(cache->flags & SLAB_KASAN)))
+	if ((IS_ENABLED(CONFIG_KASAN_GENERIC) && !quarantine) ||
+			unlikely(!(cache->flags & SLAB_KASAN)))
 		return false;
 
 	set_track(&get_alloc_info(cache, object)->free_track, GFP_NOWAIT);
 	quarantine_put(get_free_info(cache, object), cache);
-	return true;
+
+	return IS_ENABLED(CONFIG_KASAN_GENERIC);
 }
 
 bool kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
@@ -371,6 +404,7 @@ void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 {
 	unsigned long redzone_start;
 	unsigned long redzone_end;
+	u8 tag;
 
 	if (gfpflags_allow_blocking(flags))
 		quarantine_reduce();
@@ -383,14 +417,24 @@ void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 	redzone_end = round_up((unsigned long)object + cache->object_size,
 				KASAN_SHADOW_SCALE_SIZE);
 
-	kasan_unpoison_shadow(object, size);
+	/*
+	 * Objects with contructors and objects from SLAB_TYPESAFE_BY_RCU slabs
+	 * have tags preassigned and are already tagged.
+	 */
+	if (IS_ENABLED(CONFIG_KASAN_HW) &&
+			(cache->ctor || cache->flags & SLAB_TYPESAFE_BY_RCU))
+		tag = get_tag(object);
+	else
+		tag = random_tag();
+
+	kasan_unpoison_shadow(set_tag(object, tag), size);
 	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
 		KASAN_KMALLOC_REDZONE);
 
 	if (cache->flags & SLAB_KASAN)
 		set_track(&get_alloc_info(cache, object)->alloc_track, flags);
 
-	return (void *)object;
+	return set_tag(object, tag);
 }
 EXPORT_SYMBOL(kasan_kmalloc);
 
@@ -440,7 +484,7 @@ void kasan_poison_kfree(void *ptr, unsigned long ip)
 	page = virt_to_head_page(ptr);
 
 	if (unlikely(!PageSlab(page))) {
-		if (ptr != page_address(page)) {
+		if (reset_tag(ptr) != page_address(page)) {
 			kasan_report_invalid_free(ptr, ip);
 			return;
 		}
@@ -453,7 +497,7 @@ void kasan_poison_kfree(void *ptr, unsigned long ip)
 
 void kasan_kfree_large(void *ptr, unsigned long ip)
 {
-	if (ptr != page_address(virt_to_head_page(ptr)))
+	if (reset_tag(ptr) != page_address(virt_to_head_page(ptr)))
 		kasan_report_invalid_free(ptr, ip);
 	/* The object will be poisoned by page_alloc. */
 }
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index d60859d26be7..6f4f2ebf5f57 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -12,10 +12,18 @@
 #define KHWASAN_TAG_INVALID	0xFE /* inaccessible memory tag */
 #define KHWASAN_TAG_MAX		0xFD /* maximum value for random tags */
 
+#ifdef CONFIG_KASAN_GENERIC
 #define KASAN_FREE_PAGE         0xFF  /* page was freed */
 #define KASAN_PAGE_REDZONE      0xFE  /* redzone for kmalloc_large allocations */
 #define KASAN_KMALLOC_REDZONE   0xFC  /* redzone inside slub object */
 #define KASAN_KMALLOC_FREE      0xFB  /* object was freed (kmem_cache_free/kfree) */
+#else
+#define KASAN_FREE_PAGE         KHWASAN_TAG_INVALID
+#define KASAN_PAGE_REDZONE      KHWASAN_TAG_INVALID
+#define KASAN_KMALLOC_REDZONE   KHWASAN_TAG_INVALID
+#define KASAN_KMALLOC_FREE      KHWASAN_TAG_INVALID
+#endif
+
 #define KASAN_GLOBAL_REDZONE    0xFA  /* redzone for global variable */
 
 /*
diff --git a/mm/kasan/khwasan.c b/mm/kasan/khwasan.c
index 9d91bf3c8246..6b1309278e39 100644
--- a/mm/kasan/khwasan.c
+++ b/mm/kasan/khwasan.c
@@ -106,15 +106,52 @@ void *khwasan_preset_slab_tag(struct kmem_cache *cache, unsigned int idx,
 void check_memory_region(unsigned long addr, size_t size, bool write,
 				unsigned long ret_ip)
 {
+	u8 tag;
+	u8 *shadow_first, *shadow_last, *shadow;
+	void *untagged_addr;
+
+	tag = get_tag((const void *)addr);
+
+	/* Ignore accesses for pointers tagged with 0xff (native kernel
+	 * pointer tag) to suppress false positives caused by kmap.
+	 *
+	 * Some kernel code was written to account for archs that don't keep
+	 * high memory mapped all the time, but rather map and unmap particular
+	 * pages when needed. Instead of storing a pointer to the kernel memory,
+	 * this code saves the address of the page structure and offset within
+	 * that page for later use. Those pages are then mapped and unmapped
+	 * with kmap/kunmap when necessary and virt_to_page is used to get the
+	 * virtual address of the page. For arm64 (that keeps the high memory
+	 * mapped all the time), kmap is turned into a page_address call.
+
+	 * The issue is that with use of the page_address + virt_to_page
+	 * sequence the top byte value of the original pointer gets lost (gets
+	 * set to KHWASAN_TAG_KERNEL (0xFF).
+	 */
+	if (tag == KHWASAN_TAG_KERNEL)
+		return;
+
+	untagged_addr = reset_tag((const void *)addr);
+	shadow_first = kasan_mem_to_shadow(untagged_addr);
+	shadow_last = kasan_mem_to_shadow(untagged_addr + size - 1);
+
+	for (shadow = shadow_first; shadow <= shadow_last; shadow++) {
+		if (*shadow != tag) {
+			kasan_report(addr, size, write, ret_ip);
+			return;
+		}
+	}
 }
 
 #define DEFINE_HWASAN_LOAD_STORE(size)					\
 	void __hwasan_load##size##_noabort(unsigned long addr)		\
 	{								\
+		check_memory_region(addr, size, false, _RET_IP_);	\
 	}								\
 	EXPORT_SYMBOL(__hwasan_load##size##_noabort);			\
 	void __hwasan_store##size##_noabort(unsigned long addr)		\
 	{								\
+		check_memory_region(addr, size, true, _RET_IP_);	\
 	}								\
 	EXPORT_SYMBOL(__hwasan_store##size##_noabort)
 
@@ -126,15 +163,18 @@ DEFINE_HWASAN_LOAD_STORE(16);
 
 void __hwasan_loadN_noabort(unsigned long addr, unsigned long size)
 {
+	check_memory_region(addr, size, false, _RET_IP_);
 }
 EXPORT_SYMBOL(__hwasan_loadN_noabort);
 
 void __hwasan_storeN_noabort(unsigned long addr, unsigned long size)
 {
+	check_memory_region(addr, size, true, _RET_IP_);
 }
 EXPORT_SYMBOL(__hwasan_storeN_noabort);
 
 void __hwasan_tag_memory(unsigned long addr, u8 tag, unsigned long size)
 {
+	kasan_poison_shadow((void *)addr, size, tag);
 }
 EXPORT_SYMBOL(__hwasan_tag_memory);
-- 
2.19.0.rc0.228.g281dcd1b4d0-goog

================================================================================


################################################################################

=== Thread: [PATCH v6 15/18] khwasan, arm64: add brk handler for inline instrumentation ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v6 15/18] khwasan, arm64: add brk handler for inline instrumentation
Date: Wed, 29 Aug 2018 11:35:19 +0000
Message-ID: <f5e73b5ead3355932ad8b5fc96b141c3f5b8c16c.1535462971.git.andreyknvl () google ! com>
--------------------
KHWASAN inline instrumentation mode (which embeds checks of shadow memory
into the generated code, instead of inserting a callback) generates a brk
instruction when a tag mismatch is detected.

This commit add a KHWASAN brk handler, that decodes the immediate value
passed to the brk instructions (to extract information about the memory
access that triggered the mismatch), reads the register values (x0 contains
the guilty address) and reports the bug.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/brk-imm.h |  2 +
 arch/arm64/kernel/traps.c        | 69 +++++++++++++++++++++++++++++++-
 2 files changed, 69 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
index ed693c5bcec0..e4a7013321dc 100644
--- a/arch/arm64/include/asm/brk-imm.h
+++ b/arch/arm64/include/asm/brk-imm.h
@@ -16,10 +16,12 @@
  * 0x400: for dynamic BRK instruction
  * 0x401: for compile time BRK instruction
  * 0x800: kernel-mode BUG() and WARN() traps
+ * 0x9xx: KHWASAN trap (allowed values 0x900 - 0x9ff)
  */
 #define FAULT_BRK_IMM			0x100
 #define KGDB_DYN_DBG_BRK_IMM		0x400
 #define KGDB_COMPILED_DBG_BRK_IMM	0x401
 #define BUG_BRK_IMM			0x800
+#define KHWASAN_BRK_IMM			0x900
 
 #endif
diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 039e9ff379cc..fd70347d1ce7 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -35,6 +35,7 @@
 #include <linux/sizes.h>
 #include <linux/syscalls.h>
 #include <linux/mm_types.h>
+#include <linux/kasan.h>
 
 #include <asm/atomic.h>
 #include <asm/bug.h>
@@ -269,10 +270,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
 	}
 }
 
-void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
 {
 	regs->pc += size;
+}
 
+void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+{
+	__arm64_skip_faulting_instruction(regs, size);
 	/*
 	 * If we were single stepping, we want to get the step exception after
 	 * we return from the trap.
@@ -775,7 +780,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
 	}
 
 	/* If thread survives, skip over the BUG instruction and continue: */
-	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 	return DBG_HOOK_HANDLED;
 }
 
@@ -785,6 +790,59 @@ static struct break_hook bug_break_hook = {
 	.fn = bug_handler,
 };
 
+#ifdef CONFIG_KASAN_HW
+
+#define KHWASAN_ESR_RECOVER	0x20
+#define KHWASAN_ESR_WRITE	0x10
+#define KHWASAN_ESR_SIZE_MASK	0x0f
+#define KHWASAN_ESR_SIZE(esr)	(1 << ((esr) & KHWASAN_ESR_SIZE_MASK))
+
+static int khwasan_handler(struct pt_regs *regs, unsigned int esr)
+{
+	bool recover = esr & KHWASAN_ESR_RECOVER;
+	bool write = esr & KHWASAN_ESR_WRITE;
+	size_t size = KHWASAN_ESR_SIZE(esr);
+	u64 addr = regs->regs[0];
+	u64 pc = regs->pc;
+
+	if (user_mode(regs))
+		return DBG_HOOK_ERROR;
+
+	kasan_report(addr, size, write, pc);
+
+	/*
+	 * The instrumentation allows to control whether we can proceed after
+	 * a crash was detected. This is done by passing the -recover flag to
+	 * the compiler. Disabling recovery allows to generate more compact
+	 * code.
+	 *
+	 * Unfortunately disabling recovery doesn't work for the kernel right
+	 * now. KHWASAN reporting is disabled in some contexts (for example when
+	 * the allocator accesses slab object metadata; same is true for KASAN;
+	 * this is controlled by current->kasan_depth). All these accesses are
+	 * detected by the tool, even though the reports for them are not
+	 * printed.
+	 *
+	 * This is something that might be fixed at some point in the future.
+	 */
+	if (!recover)
+		die("Oops - KHWASAN", regs, 0);
+
+	/* If thread survives, skip over the brk instruction and continue: */
+	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	return DBG_HOOK_HANDLED;
+}
+
+#define KHWASAN_ESR_VAL (0xf2000000 | KHWASAN_BRK_IMM)
+#define KHWASAN_ESR_MASK 0xffffff00
+
+static struct break_hook khwasan_break_hook = {
+	.esr_val = KHWASAN_ESR_VAL,
+	.esr_mask = KHWASAN_ESR_MASK,
+	.fn = khwasan_handler,
+};
+#endif
+
 /*
  * Initial handler for AArch64 BRK exceptions
  * This handler only used until debug_traps_init().
@@ -792,6 +850,10 @@ static struct break_hook bug_break_hook = {
 int __init early_brk64(unsigned long addr, unsigned int esr,
 		struct pt_regs *regs)
 {
+#ifdef CONFIG_KASAN_HW
+	if ((esr & KHWASAN_ESR_MASK) == KHWASAN_ESR_VAL)
+		return khwasan_handler(regs, esr) != DBG_HOOK_HANDLED;
+#endif
 	return bug_handler(regs, esr) != DBG_HOOK_HANDLED;
 }
 
@@ -799,4 +861,7 @@ int __init early_brk64(unsigned long addr, unsigned int esr,
 void __init trap_init(void)
 {
 	register_break_hook(&bug_break_hook);
+#ifdef CONFIG_KASAN_HW
+	register_break_hook(&khwasan_break_hook);
+#endif
 }
-- 
2.19.0.rc0.228.g281dcd1b4d0-goog

================================================================================


################################################################################

=== Thread: [PATCH v6 16/18] khwasan, mm, arm64: tag non slab memory allocated via pagealloc ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v6 16/18] khwasan, mm, arm64: tag non slab memory allocated via pagealloc
Date: Wed, 29 Aug 2018 11:35:20 +0000
Message-ID: <db103bdc2109396af0c6007f1669ebbbb63b872b.1535462971.git.andreyknvl () google ! com>
--------------------
KWHASAN doesn't check memory accesses through pointers tagged with 0xff.
When page_address is used to get pointer to memory that corresponds to
some page, the tag of the resulting pointer gets set to 0xff, even though
the allocated memory might have been tagged differently.

For slab pages it's impossible to recover the correct tag to return from
page_address, since the page might contain multiple slab objects tagged
with different values, and we can't know in advance which one of them is
going to get accessed. For non slab pages however, we can recover the tag
in page_address, since the whole page was marked with the same tag.

This patch adds tagging to non slab memory allocated with pagealloc. To
set the tag of the pointer returned from page_address, the tag gets stored
to page->flags when the memory gets allocated.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/memory.h   | 10 ++++++++++
 include/linux/mm.h                | 29 +++++++++++++++++++++++++++++
 include/linux/page-flags-layout.h | 10 ++++++++++
 mm/cma.c                          | 11 +++++++++++
 mm/kasan/common.c                 | 17 +++++++++++++++--
 mm/page_alloc.c                   |  1 +
 6 files changed, 76 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index f5e2953b7009..ea7f928aba31 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -312,7 +312,17 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define __virt_to_pgoff(kaddr)	(((u64)(kaddr) & ~PAGE_OFFSET) / PAGE_SIZE * sizeof(struct page))
 #define __page_to_voff(kaddr)	(((u64)(kaddr) & ~VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
 
+#ifndef CONFIG_KASAN_HW
 #define page_to_virt(page)	((void *)((__page_to_voff(page)) | PAGE_OFFSET))
+#else
+#define page_to_virt(page)	({					\
+	unsigned long __addr =						\
+		((__page_to_voff(page)) | PAGE_OFFSET);			\
+	__addr = KASAN_SET_TAG(__addr, page_kasan_tag(page));		\
+	((void *)__addr);						\
+})
+#endif
+
 #define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) | VMEMMAP_START))
 
 #define _virt_addr_valid(kaddr)	pfn_valid((((u64)(kaddr) & ~PAGE_OFFSET) \
diff --git a/include/linux/mm.h b/include/linux/mm.h
index a61ebe8ad4ca..a1e7c590d925 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -804,6 +804,7 @@ vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf);
 #define NODES_PGOFF		(SECTIONS_PGOFF - NODES_WIDTH)
 #define ZONES_PGOFF		(NODES_PGOFF - ZONES_WIDTH)
 #define LAST_CPUPID_PGOFF	(ZONES_PGOFF - LAST_CPUPID_WIDTH)
+#define KASAN_TAG_PGOFF		(LAST_CPUPID_PGOFF - KASAN_TAG_WIDTH)
 
 /*
  * Define the bit shifts to access each section.  For non-existent
@@ -814,6 +815,7 @@ vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf);
 #define NODES_PGSHIFT		(NODES_PGOFF * (NODES_WIDTH != 0))
 #define ZONES_PGSHIFT		(ZONES_PGOFF * (ZONES_WIDTH != 0))
 #define LAST_CPUPID_PGSHIFT	(LAST_CPUPID_PGOFF * (LAST_CPUPID_WIDTH != 0))
+#define KASAN_TAG_PGSHIFT	(KASAN_TAG_PGOFF * (KASAN_TAG_WIDTH != 0))
 
 /* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */
 #ifdef NODE_NOT_IN_PAGE_FLAGS
@@ -836,6 +838,7 @@ vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf);
 #define NODES_MASK		((1UL << NODES_WIDTH) - 1)
 #define SECTIONS_MASK		((1UL << SECTIONS_WIDTH) - 1)
 #define LAST_CPUPID_MASK	((1UL << LAST_CPUPID_SHIFT) - 1)
+#define KASAN_TAG_MASK		((1UL << KASAN_TAG_WIDTH) - 1)
 #define ZONEID_MASK		((1UL << ZONEID_SHIFT) - 1)
 
 static inline enum zone_type page_zonenum(const struct page *page)
@@ -1081,6 +1084,32 @@ static inline bool cpupid_match_pid(struct task_struct *task, int cpupid)
 }
 #endif /* CONFIG_NUMA_BALANCING */
 
+#ifdef CONFIG_KASAN_HW
+static inline u8 page_kasan_tag(const struct page *page)
+{
+	return (page->flags >> KASAN_TAG_PGSHIFT) & KASAN_TAG_MASK;
+}
+
+static inline void page_kasan_tag_set(struct page *page, u8 tag)
+{
+	page->flags &= ~(KASAN_TAG_MASK << KASAN_TAG_PGSHIFT);
+	page->flags |= (tag & KASAN_TAG_MASK) << KASAN_TAG_PGSHIFT;
+}
+
+static inline void page_kasan_tag_reset(struct page *page)
+{
+	page_kasan_tag_set(page, 0xff);
+}
+#else
+static inline u8 page_kasan_tag(const struct page *page)
+{
+	return 0xff;
+}
+
+static inline void page_kasan_tag_set(struct page *page, u8 tag) { }
+static inline void page_kasan_tag_reset(struct page *page) { }
+#endif
+
 static inline struct zone *page_zone(const struct page *page)
 {
 	return &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)];
diff --git a/include/linux/page-flags-layout.h b/include/linux/page-flags-layout.h
index 7ec86bf31ce4..8dbad17664c2 100644
--- a/include/linux/page-flags-layout.h
+++ b/include/linux/page-flags-layout.h
@@ -82,6 +82,16 @@
 #define LAST_CPUPID_WIDTH 0
 #endif
 
+#ifdef CONFIG_KASAN_HW
+#define KASAN_TAG_WIDTH 8
+#if SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH+LAST_CPUPID_WIDTH+KASAN_TAG_WIDTH \
+	> BITS_PER_LONG - NR_PAGEFLAGS
+#error "KASAN: not enough bits in page flags for tag"
+#endif
+#else
+#define KASAN_TAG_WIDTH 0
+#endif
+
 /*
  * We are going to use the flags for the page to node mapping if its in
  * there.  This includes the case where there is no node, so it is implicit.
diff --git a/mm/cma.c b/mm/cma.c
index 4cb76121a3ab..c7b39dd3b4f6 100644
--- a/mm/cma.c
+++ b/mm/cma.c
@@ -407,6 +407,7 @@ struct page *cma_alloc(struct cma *cma, size_t count, unsigned int align,
 	unsigned long pfn = -1;
 	unsigned long start = 0;
 	unsigned long bitmap_maxno, bitmap_no, bitmap_count;
+	size_t i;
 	struct page *page = NULL;
 	int ret = -ENOMEM;
 
@@ -466,6 +467,16 @@ struct page *cma_alloc(struct cma *cma, size_t count, unsigned int align,
 
 	trace_cma_alloc(pfn, page, count, align);
 
+	/*
+	 * CMA can allocate multiple page blocks, which results in different
+	 * blocks being marked with different tags. Reset the tags to ignore
+	 * those page blocks.
+	 */
+	if (page) {
+		for (i = 0; i < count; i++)
+			page_kasan_tag_reset(page + i);
+	}
+
 	if (ret && !no_warn) {
 		pr_err("%s: alloc failed, req-size: %zu pages, ret: %d\n",
 			__func__, count, ret);
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 938229b26f3a..e5648f4218eb 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -212,8 +212,15 @@ void kasan_unpoison_stack_above_sp_to(const void *watermark)
 
 void kasan_alloc_pages(struct page *page, unsigned int order)
 {
+	u8 tag;
+	unsigned long i;
+
 	if (unlikely(PageHighMem(page)))
 		return;
+
+	tag = random_tag();
+	for (i = 0; i < (1 << order); i++)
+		page_kasan_tag_set(page + i, tag);
 	kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
 }
 
@@ -311,6 +318,12 @@ struct kasan_free_meta *get_free_info(struct kmem_cache *cache,
 
 void kasan_poison_slab(struct page *page)
 {
+	unsigned long i;
+
+	if (IS_ENABLED(CONFIG_SLAB))
+		page->s_mem = reset_tag(page->s_mem);
+	for (i = 0; i < (1 << compound_order(page)); i++)
+		page_kasan_tag_reset(page + i);
 	kasan_poison_shadow(page_address(page),
 			PAGE_SIZE << compound_order(page),
 			KASAN_KMALLOC_REDZONE);
@@ -484,7 +497,7 @@ void kasan_poison_kfree(void *ptr, unsigned long ip)
 	page = virt_to_head_page(ptr);
 
 	if (unlikely(!PageSlab(page))) {
-		if (reset_tag(ptr) != page_address(page)) {
+		if (ptr != page_address(page)) {
 			kasan_report_invalid_free(ptr, ip);
 			return;
 		}
@@ -497,7 +510,7 @@ void kasan_poison_kfree(void *ptr, unsigned long ip)
 
 void kasan_kfree_large(void *ptr, unsigned long ip)
 {
-	if (reset_tag(ptr) != page_address(virt_to_head_page(ptr)))
+	if (ptr != page_address(virt_to_head_page(ptr)))
 		kasan_report_invalid_free(ptr, ip);
 	/* The object will be poisoned by page_alloc. */
 }
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index e75865d58ba7..eb5627f89853 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1177,6 +1177,7 @@ static void __meminit __init_single_page(struct page *page, unsigned long pfn,
 	init_page_count(page);
 	page_mapcount_reset(page);
 	page_cpupid_reset_last(page);
+	page_kasan_tag_reset(page);
 
 	INIT_LIST_HEAD(&page->lru);
 #ifdef WANT_PAGE_VIRTUAL
-- 
2.19.0.rc0.228.g281dcd1b4d0-goog

================================================================================


################################################################################

=== Thread: [PATCH v6 18/18] kasan: add SPDX-License-Identifier mark to source files ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v6 18/18] kasan: add SPDX-License-Identifier mark to source files
Date: Wed, 29 Aug 2018 11:35:22 +0000
Message-ID: <878e719ff82dd0e43d9ede66856f2f76d072c417.1535462971.git.andreyknvl () google ! com>
--------------------
This patch adds a "SPDX-License-Identifier: GPL-2.0" mark to all source
files under mm/kasan.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/common.c         | 1 +
 mm/kasan/kasan.c          | 1 +
 mm/kasan/kasan_init.c     | 1 +
 mm/kasan/kasan_report.c   | 1 +
 mm/kasan/khwasan.c        | 1 +
 mm/kasan/khwasan_report.c | 1 +
 mm/kasan/quarantine.c     | 1 +
 mm/kasan/report.c         | 1 +
 8 files changed, 8 insertions(+)

diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index e5648f4218eb..f2576d93e74c 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains common KASAN and KHWASAN code.
  *
diff --git a/mm/kasan/kasan.c b/mm/kasan/kasan.c
index 44ec228de0a2..128a865c9e05 100644
--- a/mm/kasan/kasan.c
+++ b/mm/kasan/kasan.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains core KASAN code.
  *
diff --git a/mm/kasan/kasan_init.c b/mm/kasan/kasan_init.c
index 7a2a2f13f86f..b3c068ab2a85 100644
--- a/mm/kasan/kasan_init.c
+++ b/mm/kasan/kasan_init.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains some kasan initialization code.
  *
diff --git a/mm/kasan/kasan_report.c b/mm/kasan/kasan_report.c
index fdf2d77e3125..48da73f4ef7c 100644
--- a/mm/kasan/kasan_report.c
+++ b/mm/kasan/kasan_report.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains KASAN specific error reporting code.
  *
diff --git a/mm/kasan/khwasan.c b/mm/kasan/khwasan.c
index 6b1309278e39..934f80b2d22e 100644
--- a/mm/kasan/khwasan.c
+++ b/mm/kasan/khwasan.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains core KHWASAN code.
  *
diff --git a/mm/kasan/khwasan_report.c b/mm/kasan/khwasan_report.c
index 51238b404b08..4e193546d94e 100644
--- a/mm/kasan/khwasan_report.c
+++ b/mm/kasan/khwasan_report.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains KHWASAN specific error reporting code.
  *
diff --git a/mm/kasan/quarantine.c b/mm/kasan/quarantine.c
index 3a8ddf8baf7d..0e4dc1a22615 100644
--- a/mm/kasan/quarantine.c
+++ b/mm/kasan/quarantine.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * KASAN quarantine.
  *
diff --git a/mm/kasan/report.c b/mm/kasan/report.c
index e031c78f2e52..633b4b245798 100644
--- a/mm/kasan/report.c
+++ b/mm/kasan/report.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains common KASAN and KHWASAN error reporting code.
  *
-- 
2.19.0.rc0.228.g281dcd1b4d0-goog

================================================================================


################################################################################

=== Thread: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-arm-kernel
Subject: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Wed, 19 Sep 2018 18:54:48 +0000
Message-ID: <d74e710797323db0e43f047ea698fbc85060fc57.1537383101.git.andreyknvl () google ! com>
--------------------
An object constructor can initialize pointers within this objects based on
the address of the object. Since the object address might be tagged, we
need to assign a tag before calling constructor.

The implemented approach is to assign tags to objects with constructors
when a slab is allocated and call constructors once as usual. The
downside is that such object would always have the same tag when it is
reallocated, so we won't catch use-after-frees on it.

Also pressign tags for objects from SLAB_TYPESAFE_BY_RCU caches, since
they can be validy accessed after having been freed.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/slab.c |  2 +-
 mm/slub.c | 24 ++++++++++++++----------
 2 files changed, 15 insertions(+), 11 deletions(-)

diff --git a/mm/slab.c b/mm/slab.c
index 6fdca9ec2ea4..fe0ddf08aa2c 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -2574,7 +2574,7 @@ static void cache_init_objs(struct kmem_cache *cachep,
 
 	for (i = 0; i < cachep->num; i++) {
 		objp = index_to_obj(cachep, page, i);
-		kasan_init_slab_obj(cachep, objp);
+		objp = kasan_init_slab_obj(cachep, objp);
 
 		/* constructor could break poison info */
 		if (DEBUG == 0 && cachep->ctor) {
diff --git a/mm/slub.c b/mm/slub.c
index c4d5f4442ff1..75fc76e42a1e 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1413,16 +1413,17 @@ static inline bool slab_free_freelist_hook(struct kmem_cache *s,
 #endif
 }
 
-static void setup_object(struct kmem_cache *s, struct page *page,
+static void *setup_object(struct kmem_cache *s, struct page *page,
 				void *object)
 {
 	setup_object_debug(s, page, object);
-	kasan_init_slab_obj(s, object);
+	object = kasan_init_slab_obj(s, object);
 	if (unlikely(s->ctor)) {
 		kasan_unpoison_object_data(s, object);
 		s->ctor(object);
 		kasan_poison_object_data(s, object);
 	}
+	return object;
 }
 
 /*
@@ -1530,16 +1531,16 @@ static bool shuffle_freelist(struct kmem_cache *s, struct page *page)
 	/* First entry is used as the base of the freelist */
 	cur = next_freelist_entry(s, page, &pos, start, page_limit,
 				freelist_count);
+	cur = setup_object(s, page, cur);
 	page->freelist = cur;
 
 	for (idx = 1; idx < page->objects; idx++) {
-		setup_object(s, page, cur);
 		next = next_freelist_entry(s, page, &pos, start, page_limit,
 			freelist_count);
+		next = setup_object(s, page, next);
 		set_freepointer(s, cur, next);
 		cur = next;
 	}
-	setup_object(s, page, cur);
 	set_freepointer(s, cur, NULL);
 
 	return true;
@@ -1561,7 +1562,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	struct page *page;
 	struct kmem_cache_order_objects oo = s->oo;
 	gfp_t alloc_gfp;
-	void *start, *p;
+	void *start, *p, *next;
 	int idx, order;
 	bool shuffle;
 
@@ -1613,13 +1614,16 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 
 	if (!shuffle) {
 		for_each_object_idx(p, idx, s, start, page->objects) {
-			setup_object(s, page, p);
-			if (likely(idx < page->objects))
-				set_freepointer(s, p, p + s->size);
-			else
+			if (likely(idx < page->objects)) {
+				next = p + s->size;
+				next = setup_object(s, page, next);
+				set_freepointer(s, p, next);
+			} else
 				set_freepointer(s, p, NULL);
 		}
-		page->freelist = fixup_red_left(s, start);
+		start = fixup_red_left(s, start);
+		start = setup_object(s, page, start);
+		page->freelist = start;
 	}
 
 	page->inuse = page->objects;
-- 
2.19.0.397.gdd90340f6a-goog


_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kernel
Subject: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Wed, 19 Sep 2018 18:54:48 +0000
Message-ID: <d74e710797323db0e43f047ea698fbc85060fc57.1537383101.git.andreyknvl () google ! com>
--------------------
An object constructor can initialize pointers within this objects based on
the address of the object. Since the object address might be tagged, we
need to assign a tag before calling constructor.

The implemented approach is to assign tags to objects with constructors
when a slab is allocated and call constructors once as usual. The
downside is that such object would always have the same tag when it is
reallocated, so we won't catch use-after-frees on it.

Also pressign tags for objects from SLAB_TYPESAFE_BY_RCU caches, since
they can be validy accessed after having been freed.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/slab.c |  2 +-
 mm/slub.c | 24 ++++++++++++++----------
 2 files changed, 15 insertions(+), 11 deletions(-)

diff --git a/mm/slab.c b/mm/slab.c
index 6fdca9ec2ea4..fe0ddf08aa2c 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -2574,7 +2574,7 @@ static void cache_init_objs(struct kmem_cache *cachep,
 
 	for (i = 0; i < cachep->num; i++) {
 		objp = index_to_obj(cachep, page, i);
-		kasan_init_slab_obj(cachep, objp);
+		objp = kasan_init_slab_obj(cachep, objp);
 
 		/* constructor could break poison info */
 		if (DEBUG == 0 && cachep->ctor) {
diff --git a/mm/slub.c b/mm/slub.c
index c4d5f4442ff1..75fc76e42a1e 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1413,16 +1413,17 @@ static inline bool slab_free_freelist_hook(struct kmem_cache *s,
 #endif
 }
 
-static void setup_object(struct kmem_cache *s, struct page *page,
+static void *setup_object(struct kmem_cache *s, struct page *page,
 				void *object)
 {
 	setup_object_debug(s, page, object);
-	kasan_init_slab_obj(s, object);
+	object = kasan_init_slab_obj(s, object);
 	if (unlikely(s->ctor)) {
 		kasan_unpoison_object_data(s, object);
 		s->ctor(object);
 		kasan_poison_object_data(s, object);
 	}
+	return object;
 }
 
 /*
@@ -1530,16 +1531,16 @@ static bool shuffle_freelist(struct kmem_cache *s, struct page *page)
 	/* First entry is used as the base of the freelist */
 	cur = next_freelist_entry(s, page, &pos, start, page_limit,
 				freelist_count);
+	cur = setup_object(s, page, cur);
 	page->freelist = cur;
 
 	for (idx = 1; idx < page->objects; idx++) {
-		setup_object(s, page, cur);
 		next = next_freelist_entry(s, page, &pos, start, page_limit,
 			freelist_count);
+		next = setup_object(s, page, next);
 		set_freepointer(s, cur, next);
 		cur = next;
 	}
-	setup_object(s, page, cur);
 	set_freepointer(s, cur, NULL);
 
 	return true;
@@ -1561,7 +1562,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	struct page *page;
 	struct kmem_cache_order_objects oo = s->oo;
 	gfp_t alloc_gfp;
-	void *start, *p;
+	void *start, *p, *next;
 	int idx, order;
 	bool shuffle;
 
@@ -1613,13 +1614,16 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 
 	if (!shuffle) {
 		for_each_object_idx(p, idx, s, start, page->objects) {
-			setup_object(s, page, p);
-			if (likely(idx < page->objects))
-				set_freepointer(s, p, p + s->size);
-			else
+			if (likely(idx < page->objects)) {
+				next = p + s->size;
+				next = setup_object(s, page, next);
+				set_freepointer(s, p, next);
+			} else
 				set_freepointer(s, p, NULL);
 		}
-		page->freelist = fixup_red_left(s, start);
+		start = fixup_red_left(s, start);
+		start = setup_object(s, page, start);
+		page->freelist = start;
 	}
 
 	page->inuse = page->objects;
-- 
2.19.0.397.gdd90340f6a-goog

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kbuild
Subject: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Wed, 19 Sep 2018 18:54:48 +0000
Message-ID: <d74e710797323db0e43f047ea698fbc85060fc57.1537383101.git.andreyknvl () google ! com>
--------------------
An object constructor can initialize pointers within this objects based on
the address of the object. Since the object address might be tagged, we
need to assign a tag before calling constructor.

The implemented approach is to assign tags to objects with constructors
when a slab is allocated and call constructors once as usual. The
downside is that such object would always have the same tag when it is
reallocated, so we won't catch use-after-frees on it.

Also pressign tags for objects from SLAB_TYPESAFE_BY_RCU caches, since
they can be validy accessed after having been freed.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/slab.c |  2 +-
 mm/slub.c | 24 ++++++++++++++----------
 2 files changed, 15 insertions(+), 11 deletions(-)

diff --git a/mm/slab.c b/mm/slab.c
index 6fdca9ec2ea4..fe0ddf08aa2c 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -2574,7 +2574,7 @@ static void cache_init_objs(struct kmem_cache *cachep,
 
 	for (i = 0; i < cachep->num; i++) {
 		objp = index_to_obj(cachep, page, i);
-		kasan_init_slab_obj(cachep, objp);
+		objp = kasan_init_slab_obj(cachep, objp);
 
 		/* constructor could break poison info */
 		if (DEBUG == 0 && cachep->ctor) {
diff --git a/mm/slub.c b/mm/slub.c
index c4d5f4442ff1..75fc76e42a1e 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1413,16 +1413,17 @@ static inline bool slab_free_freelist_hook(struct kmem_cache *s,
 #endif
 }
 
-static void setup_object(struct kmem_cache *s, struct page *page,
+static void *setup_object(struct kmem_cache *s, struct page *page,
 				void *object)
 {
 	setup_object_debug(s, page, object);
-	kasan_init_slab_obj(s, object);
+	object = kasan_init_slab_obj(s, object);
 	if (unlikely(s->ctor)) {
 		kasan_unpoison_object_data(s, object);
 		s->ctor(object);
 		kasan_poison_object_data(s, object);
 	}
+	return object;
 }
 
 /*
@@ -1530,16 +1531,16 @@ static bool shuffle_freelist(struct kmem_cache *s, struct page *page)
 	/* First entry is used as the base of the freelist */
 	cur = next_freelist_entry(s, page, &pos, start, page_limit,
 				freelist_count);
+	cur = setup_object(s, page, cur);
 	page->freelist = cur;
 
 	for (idx = 1; idx < page->objects; idx++) {
-		setup_object(s, page, cur);
 		next = next_freelist_entry(s, page, &pos, start, page_limit,
 			freelist_count);
+		next = setup_object(s, page, next);
 		set_freepointer(s, cur, next);
 		cur = next;
 	}
-	setup_object(s, page, cur);
 	set_freepointer(s, cur, NULL);
 
 	return true;
@@ -1561,7 +1562,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	struct page *page;
 	struct kmem_cache_order_objects oo = s->oo;
 	gfp_t alloc_gfp;
-	void *start, *p;
+	void *start, *p, *next;
 	int idx, order;
 	bool shuffle;
 
@@ -1613,13 +1614,16 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 
 	if (!shuffle) {
 		for_each_object_idx(p, idx, s, start, page->objects) {
-			setup_object(s, page, p);
-			if (likely(idx < page->objects))
-				set_freepointer(s, p, p + s->size);
-			else
+			if (likely(idx < page->objects)) {
+				next = p + s->size;
+				next = setup_object(s, page, next);
+				set_freepointer(s, p, next);
+			} else
 				set_freepointer(s, p, NULL);
 		}
-		page->freelist = fixup_red_left(s, start);
+		start = fixup_red_left(s, start);
+		start = setup_object(s, page, start);
+		page->freelist = start;
 	}
 
 	page->inuse = page->objects;
-- 
2.19.0.397.gdd90340f6a-goog

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-mm
Subject: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Wed, 19 Sep 2018 18:54:48 +0000
Message-ID: <d74e710797323db0e43f047ea698fbc85060fc57.1537383101.git.andreyknvl () google ! com>
--------------------
An object constructor can initialize pointers within this objects based on
the address of the object. Since the object address might be tagged, we
need to assign a tag before calling constructor.

The implemented approach is to assign tags to objects with constructors
when a slab is allocated and call constructors once as usual. The
downside is that such object would always have the same tag when it is
reallocated, so we won't catch use-after-frees on it.

Also pressign tags for objects from SLAB_TYPESAFE_BY_RCU caches, since
they can be validy accessed after having been freed.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/slab.c |  2 +-
 mm/slub.c | 24 ++++++++++++++----------
 2 files changed, 15 insertions(+), 11 deletions(-)

diff --git a/mm/slab.c b/mm/slab.c
index 6fdca9ec2ea4..fe0ddf08aa2c 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -2574,7 +2574,7 @@ static void cache_init_objs(struct kmem_cache *cachep,
 
 	for (i = 0; i < cachep->num; i++) {
 		objp = index_to_obj(cachep, page, i);
-		kasan_init_slab_obj(cachep, objp);
+		objp = kasan_init_slab_obj(cachep, objp);
 
 		/* constructor could break poison info */
 		if (DEBUG == 0 && cachep->ctor) {
diff --git a/mm/slub.c b/mm/slub.c
index c4d5f4442ff1..75fc76e42a1e 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1413,16 +1413,17 @@ static inline bool slab_free_freelist_hook(struct kmem_cache *s,
 #endif
 }
 
-static void setup_object(struct kmem_cache *s, struct page *page,
+static void *setup_object(struct kmem_cache *s, struct page *page,
 				void *object)
 {
 	setup_object_debug(s, page, object);
-	kasan_init_slab_obj(s, object);
+	object = kasan_init_slab_obj(s, object);
 	if (unlikely(s->ctor)) {
 		kasan_unpoison_object_data(s, object);
 		s->ctor(object);
 		kasan_poison_object_data(s, object);
 	}
+	return object;
 }
 
 /*
@@ -1530,16 +1531,16 @@ static bool shuffle_freelist(struct kmem_cache *s, struct page *page)
 	/* First entry is used as the base of the freelist */
 	cur = next_freelist_entry(s, page, &pos, start, page_limit,
 				freelist_count);
+	cur = setup_object(s, page, cur);
 	page->freelist = cur;
 
 	for (idx = 1; idx < page->objects; idx++) {
-		setup_object(s, page, cur);
 		next = next_freelist_entry(s, page, &pos, start, page_limit,
 			freelist_count);
+		next = setup_object(s, page, next);
 		set_freepointer(s, cur, next);
 		cur = next;
 	}
-	setup_object(s, page, cur);
 	set_freepointer(s, cur, NULL);
 
 	return true;
@@ -1561,7 +1562,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	struct page *page;
 	struct kmem_cache_order_objects oo = s->oo;
 	gfp_t alloc_gfp;
-	void *start, *p;
+	void *start, *p, *next;
 	int idx, order;
 	bool shuffle;
 
@@ -1613,13 +1614,16 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 
 	if (!shuffle) {
 		for_each_object_idx(p, idx, s, start, page->objects) {
-			setup_object(s, page, p);
-			if (likely(idx < page->objects))
-				set_freepointer(s, p, p + s->size);
-			else
+			if (likely(idx < page->objects)) {
+				next = p + s->size;
+				next = setup_object(s, page, next);
+				set_freepointer(s, p, next);
+			} else
 				set_freepointer(s, p, NULL);
 		}
-		page->freelist = fixup_red_left(s, start);
+		start = fixup_red_left(s, start);
+		start = setup_object(s, page, start);
+		page->freelist = start;
 	}
 
 	page->inuse = page->objects;
-- 
2.19.0.397.gdd90340f6a-goog

================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-kbuild
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Fri, 21 Sep 2018 11:25:22 +0000
Message-ID: <CACT4Y+aoFSySFTd9FzA0xzRYQXSbs-wzX7B67hD3jTGAQEXBOA () mail ! gmail ! com>
--------------------
On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
> An object constructor can initialize pointers within this objects based on
> the address of the object. Since the object address might be tagged, we
> need to assign a tag before calling constructor.
>
> The implemented approach is to assign tags to objects with constructors
> when a slab is allocated and call constructors once as usual. The
> downside is that such object would always have the same tag when it is
> reallocated, so we won't catch use-after-frees on it.
>
> Also pressign tags for objects from SLAB_TYPESAFE_BY_RCU caches, since
> they can be validy accessed after having been freed.
>
> Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> ---
>  mm/slab.c |  2 +-
>  mm/slub.c | 24 ++++++++++++++----------
>  2 files changed, 15 insertions(+), 11 deletions(-)
>
> diff --git a/mm/slab.c b/mm/slab.c
> index 6fdca9ec2ea4..fe0ddf08aa2c 100644
> --- a/mm/slab.c
> +++ b/mm/slab.c
> @@ -2574,7 +2574,7 @@ static void cache_init_objs(struct kmem_cache *cachep,
>
>         for (i = 0; i < cachep->num; i++) {
>                 objp = index_to_obj(cachep, page, i);
> -               kasan_init_slab_obj(cachep, objp);
> +               objp = kasan_init_slab_obj(cachep, objp);
>
>                 /* constructor could break poison info */
>                 if (DEBUG == 0 && cachep->ctor) {
> diff --git a/mm/slub.c b/mm/slub.c
> index c4d5f4442ff1..75fc76e42a1e 100644
> --- a/mm/slub.c
> +++ b/mm/slub.c
> @@ -1413,16 +1413,17 @@ static inline bool slab_free_freelist_hook(struct kmem_cache *s,
>  #endif
>  }
>
> -static void setup_object(struct kmem_cache *s, struct page *page,
> +static void *setup_object(struct kmem_cache *s, struct page *page,
>                                 void *object)
>  {
>         setup_object_debug(s, page, object);
> -       kasan_init_slab_obj(s, object);
> +       object = kasan_init_slab_obj(s, object);
>         if (unlikely(s->ctor)) {
>                 kasan_unpoison_object_data(s, object);
>                 s->ctor(object);
>                 kasan_poison_object_data(s, object);
>         }
> +       return object;
>  }
>
>  /*
> @@ -1530,16 +1531,16 @@ static bool shuffle_freelist(struct kmem_cache *s, struct page *page)
>         /* First entry is used as the base of the freelist */
>         cur = next_freelist_entry(s, page, &pos, start, page_limit,
>                                 freelist_count);
> +       cur = setup_object(s, page, cur);
>         page->freelist = cur;
>
>         for (idx = 1; idx < page->objects; idx++) {
> -               setup_object(s, page, cur);
>                 next = next_freelist_entry(s, page, &pos, start, page_limit,
>                         freelist_count);
> +               next = setup_object(s, page, next);
>                 set_freepointer(s, cur, next);
>                 cur = next;
>         }
> -       setup_object(s, page, cur);
>         set_freepointer(s, cur, NULL);
>
>         return true;
> @@ -1561,7 +1562,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
>         struct page *page;
>         struct kmem_cache_order_objects oo = s->oo;
>         gfp_t alloc_gfp;
> -       void *start, *p;
> +       void *start, *p, *next;
>         int idx, order;
>         bool shuffle;
>
> @@ -1613,13 +1614,16 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
>
>         if (!shuffle) {
>                 for_each_object_idx(p, idx, s, start, page->objects) {
> -                       setup_object(s, page, p);
> -                       if (likely(idx < page->objects))
> -                               set_freepointer(s, p, p + s->size);
> -                       else
> +                       if (likely(idx < page->objects)) {
> +                               next = p + s->size;
> +                               next = setup_object(s, page, next);
> +                               set_freepointer(s, p, next);
> +                       } else
>                                 set_freepointer(s, p, NULL);
>                 }
> -               page->freelist = fixup_red_left(s, start);
> +               start = fixup_red_left(s, start);
> +               start = setup_object(s, page, start);
> +               page->freelist = start;
>         }

Just want to double-check that this is correct.
We now do an additional setup_object call after the loop, but we do 1
less in the loop. So total number of calls should be the same, right?
However, after the loop we call setup_object for the first object (?),
but inside of the loop we skip the call for the last object (?). Am I
missing something, or we call ctor twice for the last object and don't
call it for the first one?


>         page->inuse = page->objects;
> --
> 2.19.0.397.gdd90340f6a-goog
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-doc
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Fri, 21 Sep 2018 11:25:22 +0000
Message-ID: <CACT4Y+aoFSySFTd9FzA0xzRYQXSbs-wzX7B67hD3jTGAQEXBOA () mail ! gmail ! com>
--------------------
On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
> An object constructor can initialize pointers within this objects based on
> the address of the object. Since the object address might be tagged, we
> need to assign a tag before calling constructor.
>
> The implemented approach is to assign tags to objects with constructors
> when a slab is allocated and call constructors once as usual. The
> downside is that such object would always have the same tag when it is
> reallocated, so we won't catch use-after-frees on it.
>
> Also pressign tags for objects from SLAB_TYPESAFE_BY_RCU caches, since
> they can be validy accessed after having been freed.
>
> Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> ---
>  mm/slab.c |  2 +-
>  mm/slub.c | 24 ++++++++++++++----------
>  2 files changed, 15 insertions(+), 11 deletions(-)
>
> diff --git a/mm/slab.c b/mm/slab.c
> index 6fdca9ec2ea4..fe0ddf08aa2c 100644
> --- a/mm/slab.c
> +++ b/mm/slab.c
> @@ -2574,7 +2574,7 @@ static void cache_init_objs(struct kmem_cache *cachep,
>
>         for (i = 0; i < cachep->num; i++) {
>                 objp = index_to_obj(cachep, page, i);
> -               kasan_init_slab_obj(cachep, objp);
> +               objp = kasan_init_slab_obj(cachep, objp);
>
>                 /* constructor could break poison info */
>                 if (DEBUG == 0 && cachep->ctor) {
> diff --git a/mm/slub.c b/mm/slub.c
> index c4d5f4442ff1..75fc76e42a1e 100644
> --- a/mm/slub.c
> +++ b/mm/slub.c
> @@ -1413,16 +1413,17 @@ static inline bool slab_free_freelist_hook(struct kmem_cache *s,
>  #endif
>  }
>
> -static void setup_object(struct kmem_cache *s, struct page *page,
> +static void *setup_object(struct kmem_cache *s, struct page *page,
>                                 void *object)
>  {
>         setup_object_debug(s, page, object);
> -       kasan_init_slab_obj(s, object);
> +       object = kasan_init_slab_obj(s, object);
>         if (unlikely(s->ctor)) {
>                 kasan_unpoison_object_data(s, object);
>                 s->ctor(object);
>                 kasan_poison_object_data(s, object);
>         }
> +       return object;
>  }
>
>  /*
> @@ -1530,16 +1531,16 @@ static bool shuffle_freelist(struct kmem_cache *s, struct page *page)
>         /* First entry is used as the base of the freelist */
>         cur = next_freelist_entry(s, page, &pos, start, page_limit,
>                                 freelist_count);
> +       cur = setup_object(s, page, cur);
>         page->freelist = cur;
>
>         for (idx = 1; idx < page->objects; idx++) {
> -               setup_object(s, page, cur);
>                 next = next_freelist_entry(s, page, &pos, start, page_limit,
>                         freelist_count);
> +               next = setup_object(s, page, next);
>                 set_freepointer(s, cur, next);
>                 cur = next;
>         }
> -       setup_object(s, page, cur);
>         set_freepointer(s, cur, NULL);
>
>         return true;
> @@ -1561,7 +1562,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
>         struct page *page;
>         struct kmem_cache_order_objects oo = s->oo;
>         gfp_t alloc_gfp;
> -       void *start, *p;
> +       void *start, *p, *next;
>         int idx, order;
>         bool shuffle;
>
> @@ -1613,13 +1614,16 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
>
>         if (!shuffle) {
>                 for_each_object_idx(p, idx, s, start, page->objects) {
> -                       setup_object(s, page, p);
> -                       if (likely(idx < page->objects))
> -                               set_freepointer(s, p, p + s->size);
> -                       else
> +                       if (likely(idx < page->objects)) {
> +                               next = p + s->size;
> +                               next = setup_object(s, page, next);
> +                               set_freepointer(s, p, next);
> +                       } else
>                                 set_freepointer(s, p, NULL);
>                 }
> -               page->freelist = fixup_red_left(s, start);
> +               start = fixup_red_left(s, start);
> +               start = setup_object(s, page, start);
> +               page->freelist = start;
>         }

Just want to double-check that this is correct.
We now do an additional setup_object call after the loop, but we do 1
less in the loop. So total number of calls should be the same, right?
However, after the loop we call setup_object for the first object (?),
but inside of the loop we skip the call for the last object (?). Am I
missing something, or we call ctor twice for the last object and don't
call it for the first one?


>         page->inuse = page->objects;
> --
> 2.19.0.397.gdd90340f6a-goog
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Fri, 21 Sep 2018 11:25:22 +0000
Message-ID: <CACT4Y+aoFSySFTd9FzA0xzRYQXSbs-wzX7B67hD3jTGAQEXBOA () mail ! gmail ! com>
--------------------
On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
> An object constructor can initialize pointers within this objects based on
> the address of the object. Since the object address might be tagged, we
> need to assign a tag before calling constructor.
>
> The implemented approach is to assign tags to objects with constructors
> when a slab is allocated and call constructors once as usual. The
> downside is that such object would always have the same tag when it is
> reallocated, so we won't catch use-after-frees on it.
>
> Also pressign tags for objects from SLAB_TYPESAFE_BY_RCU caches, since
> they can be validy accessed after having been freed.
>
> Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> ---
>  mm/slab.c |  2 +-
>  mm/slub.c | 24 ++++++++++++++----------
>  2 files changed, 15 insertions(+), 11 deletions(-)
>
> diff --git a/mm/slab.c b/mm/slab.c
> index 6fdca9ec2ea4..fe0ddf08aa2c 100644
> --- a/mm/slab.c
> +++ b/mm/slab.c
> @@ -2574,7 +2574,7 @@ static void cache_init_objs(struct kmem_cache *cachep,
>
>         for (i = 0; i < cachep->num; i++) {
>                 objp = index_to_obj(cachep, page, i);
> -               kasan_init_slab_obj(cachep, objp);
> +               objp = kasan_init_slab_obj(cachep, objp);
>
>                 /* constructor could break poison info */
>                 if (DEBUG == 0 && cachep->ctor) {
> diff --git a/mm/slub.c b/mm/slub.c
> index c4d5f4442ff1..75fc76e42a1e 100644
> --- a/mm/slub.c
> +++ b/mm/slub.c
> @@ -1413,16 +1413,17 @@ static inline bool slab_free_freelist_hook(struct kmem_cache *s,
>  #endif
>  }
>
> -static void setup_object(struct kmem_cache *s, struct page *page,
> +static void *setup_object(struct kmem_cache *s, struct page *page,
>                                 void *object)
>  {
>         setup_object_debug(s, page, object);
> -       kasan_init_slab_obj(s, object);
> +       object = kasan_init_slab_obj(s, object);
>         if (unlikely(s->ctor)) {
>                 kasan_unpoison_object_data(s, object);
>                 s->ctor(object);
>                 kasan_poison_object_data(s, object);
>         }
> +       return object;
>  }
>
>  /*
> @@ -1530,16 +1531,16 @@ static bool shuffle_freelist(struct kmem_cache *s, struct page *page)
>         /* First entry is used as the base of the freelist */
>         cur = next_freelist_entry(s, page, &pos, start, page_limit,
>                                 freelist_count);
> +       cur = setup_object(s, page, cur);
>         page->freelist = cur;
>
>         for (idx = 1; idx < page->objects; idx++) {
> -               setup_object(s, page, cur);
>                 next = next_freelist_entry(s, page, &pos, start, page_limit,
>                         freelist_count);
> +               next = setup_object(s, page, next);
>                 set_freepointer(s, cur, next);
>                 cur = next;
>         }
> -       setup_object(s, page, cur);
>         set_freepointer(s, cur, NULL);
>
>         return true;
> @@ -1561,7 +1562,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
>         struct page *page;
>         struct kmem_cache_order_objects oo = s->oo;
>         gfp_t alloc_gfp;
> -       void *start, *p;
> +       void *start, *p, *next;
>         int idx, order;
>         bool shuffle;
>
> @@ -1613,13 +1614,16 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
>
>         if (!shuffle) {
>                 for_each_object_idx(p, idx, s, start, page->objects) {
> -                       setup_object(s, page, p);
> -                       if (likely(idx < page->objects))
> -                               set_freepointer(s, p, p + s->size);
> -                       else
> +                       if (likely(idx < page->objects)) {
> +                               next = p + s->size;
> +                               next = setup_object(s, page, next);
> +                               set_freepointer(s, p, next);
> +                       } else
>                                 set_freepointer(s, p, NULL);
>                 }
> -               page->freelist = fixup_red_left(s, start);
> +               start = fixup_red_left(s, start);
> +               start = setup_object(s, page, start);
> +               page->freelist = start;
>         }

Just want to double-check that this is correct.
We now do an additional setup_object call after the loop, but we do 1
less in the loop. So total number of calls should be the same, right?
However, after the loop we call setup_object for the first object (?),
but inside of the loop we skip the call for the last object (?). Am I
missing something, or we call ctor twice for the last object and don't
call it for the first one?


>         page->inuse = page->objects;
> --
> 2.19.0.397.gdd90340f6a-goog

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-mm
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Fri, 21 Sep 2018 11:25:22 +0000
Message-ID: <CACT4Y+aoFSySFTd9FzA0xzRYQXSbs-wzX7B67hD3jTGAQEXBOA () mail ! gmail ! com>
--------------------
On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
> An object constructor can initialize pointers within this objects based on
> the address of the object. Since the object address might be tagged, we
> need to assign a tag before calling constructor.
>
> The implemented approach is to assign tags to objects with constructors
> when a slab is allocated and call constructors once as usual. The
> downside is that such object would always have the same tag when it is
> reallocated, so we won't catch use-after-frees on it.
>
> Also pressign tags for objects from SLAB_TYPESAFE_BY_RCU caches, since
> they can be validy accessed after having been freed.
>
> Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> ---
>  mm/slab.c |  2 +-
>  mm/slub.c | 24 ++++++++++++++----------
>  2 files changed, 15 insertions(+), 11 deletions(-)
>
> diff --git a/mm/slab.c b/mm/slab.c
> index 6fdca9ec2ea4..fe0ddf08aa2c 100644
> --- a/mm/slab.c
> +++ b/mm/slab.c
> @@ -2574,7 +2574,7 @@ static void cache_init_objs(struct kmem_cache *cachep,
>
>         for (i = 0; i < cachep->num; i++) {
>                 objp = index_to_obj(cachep, page, i);
> -               kasan_init_slab_obj(cachep, objp);
> +               objp = kasan_init_slab_obj(cachep, objp);
>
>                 /* constructor could break poison info */
>                 if (DEBUG == 0 && cachep->ctor) {
> diff --git a/mm/slub.c b/mm/slub.c
> index c4d5f4442ff1..75fc76e42a1e 100644
> --- a/mm/slub.c
> +++ b/mm/slub.c
> @@ -1413,16 +1413,17 @@ static inline bool slab_free_freelist_hook(struct kmem_cache *s,
>  #endif
>  }
>
> -static void setup_object(struct kmem_cache *s, struct page *page,
> +static void *setup_object(struct kmem_cache *s, struct page *page,
>                                 void *object)
>  {
>         setup_object_debug(s, page, object);
> -       kasan_init_slab_obj(s, object);
> +       object = kasan_init_slab_obj(s, object);
>         if (unlikely(s->ctor)) {
>                 kasan_unpoison_object_data(s, object);
>                 s->ctor(object);
>                 kasan_poison_object_data(s, object);
>         }
> +       return object;
>  }
>
>  /*
> @@ -1530,16 +1531,16 @@ static bool shuffle_freelist(struct kmem_cache *s, struct page *page)
>         /* First entry is used as the base of the freelist */
>         cur = next_freelist_entry(s, page, &pos, start, page_limit,
>                                 freelist_count);
> +       cur = setup_object(s, page, cur);
>         page->freelist = cur;
>
>         for (idx = 1; idx < page->objects; idx++) {
> -               setup_object(s, page, cur);
>                 next = next_freelist_entry(s, page, &pos, start, page_limit,
>                         freelist_count);
> +               next = setup_object(s, page, next);
>                 set_freepointer(s, cur, next);
>                 cur = next;
>         }
> -       setup_object(s, page, cur);
>         set_freepointer(s, cur, NULL);
>
>         return true;
> @@ -1561,7 +1562,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
>         struct page *page;
>         struct kmem_cache_order_objects oo = s->oo;
>         gfp_t alloc_gfp;
> -       void *start, *p;
> +       void *start, *p, *next;
>         int idx, order;
>         bool shuffle;
>
> @@ -1613,13 +1614,16 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
>
>         if (!shuffle) {
>                 for_each_object_idx(p, idx, s, start, page->objects) {
> -                       setup_object(s, page, p);
> -                       if (likely(idx < page->objects))
> -                               set_freepointer(s, p, p + s->size);
> -                       else
> +                       if (likely(idx < page->objects)) {
> +                               next = p + s->size;
> +                               next = setup_object(s, page, next);
> +                               set_freepointer(s, p, next);
> +                       } else
>                                 set_freepointer(s, p, NULL);
>                 }
> -               page->freelist = fixup_red_left(s, start);
> +               start = fixup_red_left(s, start);
> +               start = setup_object(s, page, start);
> +               page->freelist = start;
>         }

Just want to double-check that this is correct.
We now do an additional setup_object call after the loop, but we do 1
less in the loop. So total number of calls should be the same, right?
However, after the loop we call setup_object for the first object (?),
but inside of the loop we skip the call for the last object (?). Am I
missing something, or we call ctor twice for the last object and don't
call it for the first one?


>         page->inuse = page->objects;
> --
> 2.19.0.397.gdd90340f6a-goog

================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-kernel
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Fri, 21 Sep 2018 11:25:22 +0000
Message-ID: <CACT4Y+aoFSySFTd9FzA0xzRYQXSbs-wzX7B67hD3jTGAQEXBOA () mail ! gmail ! com>
--------------------
On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
> An object constructor can initialize pointers within this objects based on
> the address of the object. Since the object address might be tagged, we
> need to assign a tag before calling constructor.
>
> The implemented approach is to assign tags to objects with constructors
> when a slab is allocated and call constructors once as usual. The
> downside is that such object would always have the same tag when it is
> reallocated, so we won't catch use-after-frees on it.
>
> Also pressign tags for objects from SLAB_TYPESAFE_BY_RCU caches, since
> they can be validy accessed after having been freed.
>
> Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
> ---
>  mm/slab.c |  2 +-
>  mm/slub.c | 24 ++++++++++++++----------
>  2 files changed, 15 insertions(+), 11 deletions(-)
>
> diff --git a/mm/slab.c b/mm/slab.c
> index 6fdca9ec2ea4..fe0ddf08aa2c 100644
> --- a/mm/slab.c
> +++ b/mm/slab.c
> @@ -2574,7 +2574,7 @@ static void cache_init_objs(struct kmem_cache *cachep,
>
>         for (i = 0; i < cachep->num; i++) {
>                 objp = index_to_obj(cachep, page, i);
> -               kasan_init_slab_obj(cachep, objp);
> +               objp = kasan_init_slab_obj(cachep, objp);
>
>                 /* constructor could break poison info */
>                 if (DEBUG == 0 && cachep->ctor) {
> diff --git a/mm/slub.c b/mm/slub.c
> index c4d5f4442ff1..75fc76e42a1e 100644
> --- a/mm/slub.c
> +++ b/mm/slub.c
> @@ -1413,16 +1413,17 @@ static inline bool slab_free_freelist_hook(struct kmem_cache *s,
>  #endif
>  }
>
> -static void setup_object(struct kmem_cache *s, struct page *page,
> +static void *setup_object(struct kmem_cache *s, struct page *page,
>                                 void *object)
>  {
>         setup_object_debug(s, page, object);
> -       kasan_init_slab_obj(s, object);
> +       object = kasan_init_slab_obj(s, object);
>         if (unlikely(s->ctor)) {
>                 kasan_unpoison_object_data(s, object);
>                 s->ctor(object);
>                 kasan_poison_object_data(s, object);
>         }
> +       return object;
>  }
>
>  /*
> @@ -1530,16 +1531,16 @@ static bool shuffle_freelist(struct kmem_cache *s, struct page *page)
>         /* First entry is used as the base of the freelist */
>         cur = next_freelist_entry(s, page, &pos, start, page_limit,
>                                 freelist_count);
> +       cur = setup_object(s, page, cur);
>         page->freelist = cur;
>
>         for (idx = 1; idx < page->objects; idx++) {
> -               setup_object(s, page, cur);
>                 next = next_freelist_entry(s, page, &pos, start, page_limit,
>                         freelist_count);
> +               next = setup_object(s, page, next);
>                 set_freepointer(s, cur, next);
>                 cur = next;
>         }
> -       setup_object(s, page, cur);
>         set_freepointer(s, cur, NULL);
>
>         return true;
> @@ -1561,7 +1562,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
>         struct page *page;
>         struct kmem_cache_order_objects oo = s->oo;
>         gfp_t alloc_gfp;
> -       void *start, *p;
> +       void *start, *p, *next;
>         int idx, order;
>         bool shuffle;
>
> @@ -1613,13 +1614,16 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
>
>         if (!shuffle) {
>                 for_each_object_idx(p, idx, s, start, page->objects) {
> -                       setup_object(s, page, p);
> -                       if (likely(idx < page->objects))
> -                               set_freepointer(s, p, p + s->size);
> -                       else
> +                       if (likely(idx < page->objects)) {
> +                               next = p + s->size;
> +                               next = setup_object(s, page, next);
> +                               set_freepointer(s, p, next);
> +                       } else
>                                 set_freepointer(s, p, NULL);
>                 }
> -               page->freelist = fixup_red_left(s, start);
> +               start = fixup_red_left(s, start);
> +               start = setup_object(s, page, start);
> +               page->freelist = start;
>         }

Just want to double-check that this is correct.
We now do an additional setup_object call after the loop, but we do 1
less in the loop. So total number of calls should be the same, right?
However, after the loop we call setup_object for the first object (?),
but inside of the loop we skip the call for the last object (?). Am I
missing something, or we call ctor twice for the last object and don't
call it for the first one?


>         page->inuse = page->objects;
> --
> 2.19.0.397.gdd90340f6a-goog
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-mm
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Fri, 21 Sep 2018 12:24:59 +0000
Message-ID: <CAAeHK+zBh0BiYq65QDxD-nxkHHF0QL6UQx8fs40K39R6XJJfzA () mail ! gmail ! com>
--------------------
On Fri, Sep 21, 2018 at 1:25 PM, Dmitry Vyukov <dvyukov@google.com> wrote:
> On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:

>>         if (!shuffle) {
>>                 for_each_object_idx(p, idx, s, start, page->objects) {
>> -                       setup_object(s, page, p);
>> -                       if (likely(idx < page->objects))
>> -                               set_freepointer(s, p, p + s->size);
>> -                       else
>> +                       if (likely(idx < page->objects)) {
>> +                               next = p + s->size;
>> +                               next = setup_object(s, page, next);
>> +                               set_freepointer(s, p, next);
>> +                       } else
>>                                 set_freepointer(s, p, NULL);
>>                 }
>> -               page->freelist = fixup_red_left(s, start);
>> +               start = fixup_red_left(s, start);
>> +               start = setup_object(s, page, start);
>> +               page->freelist = start;
>>         }
>
> Just want to double-check that this is correct.
> We now do an additional setup_object call after the loop, but we do 1
> less in the loop. So total number of calls should be the same, right?
> However, after the loop we call setup_object for the first object (?),
> but inside of the loop we skip the call for the last object (?). Am I
> missing something, or we call ctor twice for the last object and don't
> call it for the first one?

Inside the loop we call setup_object for the "next" object. So we
start iterating on the first one, but call setup_object for the
second. Then the loop moves on to the second one and calls
setup_object for the third. And so on. So the loop calls setup_object
for every object (including the last one) except for the first one.

The idea is that we want the freelist pointer that is stored in the
current object to have a tagged pointer to the next one, so we need to
assign a tag to the next object before storing the pointer in the
current one.

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kbuild
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Fri, 21 Sep 2018 12:24:59 +0000
Message-ID: <CAAeHK+zBh0BiYq65QDxD-nxkHHF0QL6UQx8fs40K39R6XJJfzA () mail ! gmail ! com>
--------------------
On Fri, Sep 21, 2018 at 1:25 PM, Dmitry Vyukov <dvyukov@google.com> wrote:
> On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:

>>         if (!shuffle) {
>>                 for_each_object_idx(p, idx, s, start, page->objects) {
>> -                       setup_object(s, page, p);
>> -                       if (likely(idx < page->objects))
>> -                               set_freepointer(s, p, p + s->size);
>> -                       else
>> +                       if (likely(idx < page->objects)) {
>> +                               next = p + s->size;
>> +                               next = setup_object(s, page, next);
>> +                               set_freepointer(s, p, next);
>> +                       } else
>>                                 set_freepointer(s, p, NULL);
>>                 }
>> -               page->freelist = fixup_red_left(s, start);
>> +               start = fixup_red_left(s, start);
>> +               start = setup_object(s, page, start);
>> +               page->freelist = start;
>>         }
>
> Just want to double-check that this is correct.
> We now do an additional setup_object call after the loop, but we do 1
> less in the loop. So total number of calls should be the same, right?
> However, after the loop we call setup_object for the first object (?),
> but inside of the loop we skip the call for the last object (?). Am I
> missing something, or we call ctor twice for the last object and don't
> call it for the first one?

Inside the loop we call setup_object for the "next" object. So we
start iterating on the first one, but call setup_object for the
second. Then the loop moves on to the second one and calls
setup_object for the third. And so on. So the loop calls setup_object
for every object (including the last one) except for the first one.

The idea is that we want the freelist pointer that is stored in the
current object to have a tagged pointer to the next one, so we need to
assign a tag to the next object before storing the pointer in the
current one.
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Fri, 21 Sep 2018 12:24:59 +0000
Message-ID: <CAAeHK+zBh0BiYq65QDxD-nxkHHF0QL6UQx8fs40K39R6XJJfzA () mail ! gmail ! com>
--------------------
On Fri, Sep 21, 2018 at 1:25 PM, Dmitry Vyukov <dvyukov@google.com> wrote:
> On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:

>>         if (!shuffle) {
>>                 for_each_object_idx(p, idx, s, start, page->objects) {
>> -                       setup_object(s, page, p);
>> -                       if (likely(idx < page->objects))
>> -                               set_freepointer(s, p, p + s->size);
>> -                       else
>> +                       if (likely(idx < page->objects)) {
>> +                               next = p + s->size;
>> +                               next = setup_object(s, page, next);
>> +                               set_freepointer(s, p, next);
>> +                       } else
>>                                 set_freepointer(s, p, NULL);
>>                 }
>> -               page->freelist = fixup_red_left(s, start);
>> +               start = fixup_red_left(s, start);
>> +               start = setup_object(s, page, start);
>> +               page->freelist = start;
>>         }
>
> Just want to double-check that this is correct.
> We now do an additional setup_object call after the loop, but we do 1
> less in the loop. So total number of calls should be the same, right?
> However, after the loop we call setup_object for the first object (?),
> but inside of the loop we skip the call for the last object (?). Am I
> missing something, or we call ctor twice for the last object and don't
> call it for the first one?

Inside the loop we call setup_object for the "next" object. So we
start iterating on the first one, but call setup_object for the
second. Then the loop moves on to the second one and calls
setup_object for the third. And so on. So the loop calls setup_object
for every object (including the last one) except for the first one.

The idea is that we want the freelist pointer that is stored in the
current object to have a tagged pointer to the next one, so we need to
assign a tag to the next object before storing the pointer in the
current one.
================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kernel
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Fri, 21 Sep 2018 12:24:59 +0000
Message-ID: <CAAeHK+zBh0BiYq65QDxD-nxkHHF0QL6UQx8fs40K39R6XJJfzA () mail ! gmail ! com>
--------------------
On Fri, Sep 21, 2018 at 1:25 PM, Dmitry Vyukov <dvyukov@google.com> wrote:
> On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:

>>         if (!shuffle) {
>>                 for_each_object_idx(p, idx, s, start, page->objects) {
>> -                       setup_object(s, page, p);
>> -                       if (likely(idx < page->objects))
>> -                               set_freepointer(s, p, p + s->size);
>> -                       else
>> +                       if (likely(idx < page->objects)) {
>> +                               next = p + s->size;
>> +                               next = setup_object(s, page, next);
>> +                               set_freepointer(s, p, next);
>> +                       } else
>>                                 set_freepointer(s, p, NULL);
>>                 }
>> -               page->freelist = fixup_red_left(s, start);
>> +               start = fixup_red_left(s, start);
>> +               start = setup_object(s, page, start);
>> +               page->freelist = start;
>>         }
>
> Just want to double-check that this is correct.
> We now do an additional setup_object call after the loop, but we do 1
> less in the loop. So total number of calls should be the same, right?
> However, after the loop we call setup_object for the first object (?),
> but inside of the loop we skip the call for the last object (?). Am I
> missing something, or we call ctor twice for the last object and don't
> call it for the first one?

Inside the loop we call setup_object for the "next" object. So we
start iterating on the first one, but call setup_object for the
second. Then the loop moves on to the second one and calls
setup_object for the third. And so on. So the loop calls setup_object
for every object (including the last one) except for the first one.

The idea is that we want the freelist pointer that is stored in the
current object to have a tagged pointer to the next one, so we need to
assign a tag to the next object before storing the pointer in the
current one.
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Mon, 24 Sep 2018 09:19:17 +0000
Message-ID: <CACT4Y+b6qe1S_4dAhYGPm0r1nFWW4Fn1rCNbSZW-7w7jMpcTtQ () mail ! gmail ! com>
--------------------
On Fri, Sep 21, 2018 at 2:24 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
> On Fri, Sep 21, 2018 at 1:25 PM, Dmitry Vyukov <dvyukov@google.com> wrote:
>> On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
>
>>>         if (!shuffle) {
>>>                 for_each_object_idx(p, idx, s, start, page->objects) {
>>> -                       setup_object(s, page, p);
>>> -                       if (likely(idx < page->objects))
>>> -                               set_freepointer(s, p, p + s->size);
>>> -                       else
>>> +                       if (likely(idx < page->objects)) {
>>> +                               next = p + s->size;
>>> +                               next = setup_object(s, page, next);
>>> +                               set_freepointer(s, p, next);
>>> +                       } else
>>>                                 set_freepointer(s, p, NULL);
>>>                 }
>>> -               page->freelist = fixup_red_left(s, start);
>>> +               start = fixup_red_left(s, start);
>>> +               start = setup_object(s, page, start);
>>> +               page->freelist = start;
>>>         }
>>
>> Just want to double-check that this is correct.
>> We now do an additional setup_object call after the loop, but we do 1
>> less in the loop. So total number of calls should be the same, right?
>> However, after the loop we call setup_object for the first object (?),
>> but inside of the loop we skip the call for the last object (?). Am I
>> missing something, or we call ctor twice for the last object and don't
>> call it for the first one?
>
> Inside the loop we call setup_object for the "next" object. So we
> start iterating on the first one, but call setup_object for the
> second. Then the loop moves on to the second one and calls
> setup_object for the third. And so on. So the loop calls setup_object
> for every object (including the last one) except for the first one.
>
> The idea is that we want the freelist pointer that is stored in the
> current object to have a tagged pointer to the next one, so we need to
> assign a tag to the next object before storing the pointer in the
> current one.

Ah, OK, then false alarm.

_______________________________________________
linux-arm-kernel mailing list
linux-arm-kernel@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-arm-kernel
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-mm
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Mon, 24 Sep 2018 09:19:17 +0000
Message-ID: <CACT4Y+b6qe1S_4dAhYGPm0r1nFWW4Fn1rCNbSZW-7w7jMpcTtQ () mail ! gmail ! com>
--------------------
On Fri, Sep 21, 2018 at 2:24 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
> On Fri, Sep 21, 2018 at 1:25 PM, Dmitry Vyukov <dvyukov@google.com> wrote:
>> On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
>
>>>         if (!shuffle) {
>>>                 for_each_object_idx(p, idx, s, start, page->objects) {
>>> -                       setup_object(s, page, p);
>>> -                       if (likely(idx < page->objects))
>>> -                               set_freepointer(s, p, p + s->size);
>>> -                       else
>>> +                       if (likely(idx < page->objects)) {
>>> +                               next = p + s->size;
>>> +                               next = setup_object(s, page, next);
>>> +                               set_freepointer(s, p, next);
>>> +                       } else
>>>                                 set_freepointer(s, p, NULL);
>>>                 }
>>> -               page->freelist = fixup_red_left(s, start);
>>> +               start = fixup_red_left(s, start);
>>> +               start = setup_object(s, page, start);
>>> +               page->freelist = start;
>>>         }
>>
>> Just want to double-check that this is correct.
>> We now do an additional setup_object call after the loop, but we do 1
>> less in the loop. So total number of calls should be the same, right?
>> However, after the loop we call setup_object for the first object (?),
>> but inside of the loop we skip the call for the last object (?). Am I
>> missing something, or we call ctor twice for the last object and don't
>> call it for the first one?
>
> Inside the loop we call setup_object for the "next" object. So we
> start iterating on the first one, but call setup_object for the
> second. Then the loop moves on to the second one and calls
> setup_object for the third. And so on. So the loop calls setup_object
> for every object (including the last one) except for the first one.
>
> The idea is that we want the freelist pointer that is stored in the
> current object to have a tagged pointer to the next one, so we need to
> assign a tag to the next object before storing the pointer in the
> current one.

Ah, OK, then false alarm.

================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-sparse
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Mon, 24 Sep 2018 09:19:17 +0000
Message-ID: <CACT4Y+b6qe1S_4dAhYGPm0r1nFWW4Fn1rCNbSZW-7w7jMpcTtQ () mail ! gmail ! com>
--------------------
On Fri, Sep 21, 2018 at 2:24 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
> On Fri, Sep 21, 2018 at 1:25 PM, Dmitry Vyukov <dvyukov@google.com> wrote:
>> On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
>
>>>         if (!shuffle) {
>>>                 for_each_object_idx(p, idx, s, start, page->objects) {
>>> -                       setup_object(s, page, p);
>>> -                       if (likely(idx < page->objects))
>>> -                               set_freepointer(s, p, p + s->size);
>>> -                       else
>>> +                       if (likely(idx < page->objects)) {
>>> +                               next = p + s->size;
>>> +                               next = setup_object(s, page, next);
>>> +                               set_freepointer(s, p, next);
>>> +                       } else
>>>                                 set_freepointer(s, p, NULL);
>>>                 }
>>> -               page->freelist = fixup_red_left(s, start);
>>> +               start = fixup_red_left(s, start);
>>> +               start = setup_object(s, page, start);
>>> +               page->freelist = start;
>>>         }
>>
>> Just want to double-check that this is correct.
>> We now do an additional setup_object call after the loop, but we do 1
>> less in the loop. So total number of calls should be the same, right?
>> However, after the loop we call setup_object for the first object (?),
>> but inside of the loop we skip the call for the last object (?). Am I
>> missing something, or we call ctor twice for the last object and don't
>> call it for the first one?
>
> Inside the loop we call setup_object for the "next" object. So we
> start iterating on the first one, but call setup_object for the
> second. Then the loop moves on to the second one and calls
> setup_object for the third. And so on. So the loop calls setup_object
> for every object (including the last one) except for the first one.
>
> The idea is that we want the freelist pointer that is stored in the
> current object to have a tagged pointer to the next one, so we need to
> assign a tag to the next object before storing the pointer in the
> current one.

Ah, OK, then false alarm.
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-kernel
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Mon, 24 Sep 2018 09:19:17 +0000
Message-ID: <CACT4Y+b6qe1S_4dAhYGPm0r1nFWW4Fn1rCNbSZW-7w7jMpcTtQ () mail ! gmail ! com>
--------------------
On Fri, Sep 21, 2018 at 2:24 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
> On Fri, Sep 21, 2018 at 1:25 PM, Dmitry Vyukov <dvyukov@google.com> wrote:
>> On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
>
>>>         if (!shuffle) {
>>>                 for_each_object_idx(p, idx, s, start, page->objects) {
>>> -                       setup_object(s, page, p);
>>> -                       if (likely(idx < page->objects))
>>> -                               set_freepointer(s, p, p + s->size);
>>> -                       else
>>> +                       if (likely(idx < page->objects)) {
>>> +                               next = p + s->size;
>>> +                               next = setup_object(s, page, next);
>>> +                               set_freepointer(s, p, next);
>>> +                       } else
>>>                                 set_freepointer(s, p, NULL);
>>>                 }
>>> -               page->freelist = fixup_red_left(s, start);
>>> +               start = fixup_red_left(s, start);
>>> +               start = setup_object(s, page, start);
>>> +               page->freelist = start;
>>>         }
>>
>> Just want to double-check that this is correct.
>> We now do an additional setup_object call after the loop, but we do 1
>> less in the loop. So total number of calls should be the same, right?
>> However, after the loop we call setup_object for the first object (?),
>> but inside of the loop we skip the call for the last object (?). Am I
>> missing something, or we call ctor twice for the last object and don't
>> call it for the first one?
>
> Inside the loop we call setup_object for the "next" object. So we
> start iterating on the first one, but call setup_object for the
> second. Then the loop moves on to the second one and calls
> setup_object for the third. And so on. So the loop calls setup_object
> for every object (including the last one) except for the first one.
>
> The idea is that we want the freelist pointer that is stored in the
> current object to have a tagged pointer to the next one, so we need to
> assign a tag to the next object before storing the pointer in the
> current one.

Ah, OK, then false alarm.
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-doc
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Mon, 24 Sep 2018 09:19:17 +0000
Message-ID: <CACT4Y+b6qe1S_4dAhYGPm0r1nFWW4Fn1rCNbSZW-7w7jMpcTtQ () mail ! gmail ! com>
--------------------
On Fri, Sep 21, 2018 at 2:24 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
> On Fri, Sep 21, 2018 at 1:25 PM, Dmitry Vyukov <dvyukov@google.com> wrote:
>> On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
>
>>>         if (!shuffle) {
>>>                 for_each_object_idx(p, idx, s, start, page->objects) {
>>> -                       setup_object(s, page, p);
>>> -                       if (likely(idx < page->objects))
>>> -                               set_freepointer(s, p, p + s->size);
>>> -                       else
>>> +                       if (likely(idx < page->objects)) {
>>> +                               next = p + s->size;
>>> +                               next = setup_object(s, page, next);
>>> +                               set_freepointer(s, p, next);
>>> +                       } else
>>>                                 set_freepointer(s, p, NULL);
>>>                 }
>>> -               page->freelist = fixup_red_left(s, start);
>>> +               start = fixup_red_left(s, start);
>>> +               start = setup_object(s, page, start);
>>> +               page->freelist = start;
>>>         }
>>
>> Just want to double-check that this is correct.
>> We now do an additional setup_object call after the loop, but we do 1
>> less in the loop. So total number of calls should be the same, right?
>> However, after the loop we call setup_object for the first object (?),
>> but inside of the loop we skip the call for the last object (?). Am I
>> missing something, or we call ctor twice for the last object and don't
>> call it for the first one?
>
> Inside the loop we call setup_object for the "next" object. So we
> start iterating on the first one, but call setup_object for the
> second. Then the loop moves on to the second one and calls
> setup_object for the third. And so on. So the loop calls setup_object
> for every object (including the last one) except for the first one.
>
> The idea is that we want the freelist pointer that is stored in the
> current object to have a tagged pointer to the next one, so we need to
> assign a tag to the next object before storing the pointer in the
> current one.

Ah, OK, then false alarm.
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-kbuild
Subject: Re: [PATCH v8 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Mon, 24 Sep 2018 09:19:17 +0000
Message-ID: <CACT4Y+b6qe1S_4dAhYGPm0r1nFWW4Fn1rCNbSZW-7w7jMpcTtQ () mail ! gmail ! com>
--------------------
On Fri, Sep 21, 2018 at 2:24 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
> On Fri, Sep 21, 2018 at 1:25 PM, Dmitry Vyukov <dvyukov@google.com> wrote:
>> On Wed, Sep 19, 2018 at 8:54 PM, Andrey Konovalov <andreyknvl@google.com> wrote:
>
>>>         if (!shuffle) {
>>>                 for_each_object_idx(p, idx, s, start, page->objects) {
>>> -                       setup_object(s, page, p);
>>> -                       if (likely(idx < page->objects))
>>> -                               set_freepointer(s, p, p + s->size);
>>> -                       else
>>> +                       if (likely(idx < page->objects)) {
>>> +                               next = p + s->size;
>>> +                               next = setup_object(s, page, next);
>>> +                               set_freepointer(s, p, next);
>>> +                       } else
>>>                                 set_freepointer(s, p, NULL);
>>>                 }
>>> -               page->freelist = fixup_red_left(s, start);
>>> +               start = fixup_red_left(s, start);
>>> +               start = setup_object(s, page, start);
>>> +               page->freelist = start;
>>>         }
>>
>> Just want to double-check that this is correct.
>> We now do an additional setup_object call after the loop, but we do 1
>> less in the loop. So total number of calls should be the same, right?
>> However, after the loop we call setup_object for the first object (?),
>> but inside of the loop we skip the call for the last object (?). Am I
>> missing something, or we call ctor twice for the last object and don't
>> call it for the first one?
>
> Inside the loop we call setup_object for the "next" object. So we
> start iterating on the first one, but call setup_object for the
> second. Then the loop moves on to the second one and calls
> setup_object for the third. And so on. So the loop calls setup_object
> for every object (including the last one) except for the first one.
>
> The idea is that we want the freelist pointer that is stored in the
> current object to have a tagged pointer to the next one, so we need to
> assign a tag to the next object before storing the pointer in the
> current one.

Ah, OK, then false alarm.
================================================================================


################################################################################

=== Thread: [PATCH v9 00/20] kasan: add software tag-based mode for arm64 ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-arm-kernel
Subject: [PATCH v9 00/20] kasan: add software tag-based mode for arm64
Date: Fri, 21 Sep 2018 15:13:22 +0000
Message-ID: <cover.1537542735.git.andreyknvl () google ! com>
--------------------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================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 00/20] kasan: add software tag-based mode for arm64
Date: Fri, 21 Sep 2018 15:13:22 +0000
Message-ID: <cover.1537542735.git.andreyknvl () google ! com>
--------------------
This patchset adds a new software tag-based mode to KASAN [1].
(Initially this mode was called KHWASAN, but it got renamed,
 see the naming rationale at the end of this section).

The plan is to implement HWASan [2] for the kernel with the incentive,
that it's going to have comparable to KASAN performance, but in the same
time consume much less memory, trading that off for somewhat imprecise
bug detection and being supported only for arm64.

The underlying ideas of the approach used by software tag-based KASAN are:

1. By using the Top Byte Ignore (TBI) arm64 CPU feature, we can store
   pointer tags in the top byte of each kernel pointer.

2. Using shadow memory, we can store memory tags for each chunk of kernel
   memory.

3. On each memory allocation, we can generate a random tag, embed it into
   the returned pointer and set the memory tags that correspond to this
   chunk of memory to the same value.

4. By using compiler instrumentation, before each memory access we can add
   a check that the pointer tag matches the tag of the memory that is being
   accessed.

5. On a tag mismatch we report an error.

With this patchset the existing KASAN mode gets renamed to generic KASAN,
with the word "generic" meaning that the implementation can be supported
by any architecture as it is purely software.

The new mode this patchset adds is called software tag-based KASAN. The
word "tag-based" refers to the fact that this mode uses tags embedded into
the top byte of kernel pointers and the TBI arm64 CPU feature that allows
to dereference such pointers. The word "software" here means that shadow
memory manipulation and tag checking on pointer dereference is done in
software. As it is the only tag-based implementation right now, "software
tag-based" KASAN is sometimes referred to as simply "tag-based" in this
patchset.

A potential expansion of this mode is a hardware tag-based mode, which would
use hardware memory tagging support (announced by Arm [3]) instead of
compiler instrumentation and manual shadow memory manipulation.

Same as generic KASAN, software tag-based KASAN is strictly a debugging
feature.

[1] https://www.kernel.org/doc/html/latest/dev-tools/kasan.html

[2] http://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.html

[3] https://community.arm.com/processors/b/blog/posts/arm-a-profile-architecture-2018-developments-armv85a


====== Rationale

On mobile devices generic KASAN's memory usage is significant problem. One
of the main reasons to have tag-based KASAN is to be able to perform a
similar set of checks as the generic one does, but with lower memory
requirements.

Comment from Vishwath Mohan <vishwath@google.com>:

I don't have data on-hand, but anecdotally both ASAN and KASAN have proven
problematic to enable for environments that don't tolerate the increased
memory pressure well. This includes,
(a) Low-memory form factors - Wear, TV, Things, lower-tier phones like Go,
(c) Connected components like Pixel's visual core [1].

These are both places I'd love to have a low(er) memory footprint option at
my disposal.

Comment from Evgenii Stepanov <eugenis@google.com>:

Looking at a live Android device under load, slab (according to
/proc/meminfo) + kernel stack take 8-10% available RAM (~350MB). KASAN's
overhead of 2x - 3x on top of it is not insignificant.

Not having this overhead enables near-production use - ex. running
KASAN/KHWASAN kernel on a personal, daily-use device to catch bugs that do
not reproduce in test configuration. These are the ones that often cost
the most engineering time to track down.

CPU overhead is bad, but generally tolerable. RAM is critical, in our
experience. Once it gets low enough, OOM-killer makes your life miserable.

[1] https://www.blog.google/products/pixel/pixel-visual-core-image-processing-and-machine-learning-pixel-2/


====== Technical details

Software tag-based KASAN mode is implemented in a very similar way to the
generic one. This patchset essentially does the following:

1. TCR_TBI1 is set to enable Top Byte Ignore.

2. Shadow memory is used (with a different scale, 1:16, so each shadow
   byte corresponds to 16 bytes of kernel memory) to store memory tags.

3. All slab objects are aligned to shadow scale, which is 16 bytes.

4. All pointers returned from the slab allocator are tagged with a random
   tag and the corresponding shadow memory is poisoned with the same value.

5. Compiler instrumentation is used to insert tag checks. Either by
   calling callbacks or by inlining them (CONFIG_KASAN_OUTLINE and
   CONFIG_KASAN_INLINE flags are reused).

6. When a tag mismatch is detected in callback instrumentation mode
   KASAN simply prints a bug report. In case of inline instrumentation,
   clang inserts a brk instruction, and KASAN has it's own brk handler,
   which reports the bug.

7. The memory in between slab objects is marked with a reserved tag, and
   acts as a redzone.

8. When a slab object is freed it's marked with a reserved tag.

Bug detection is imprecise for two reasons:

1. We won't catch some small out-of-bounds accesses, that fall into the
   same shadow cell, as the last byte of a slab object.

2. We only have 1 byte to store tags, which means we have a 1/256
   probability of a tag match for an incorrect access (actually even
   slightly less due to reserved tag values).

Despite that there's a particular type of bugs that tag-based KASAN can
detect compared to generic KASAN: use-after-free after the object has been
allocated by someone else.


====== Testing

Some kernel developers voiced a concern that changing the top byte of
kernel pointers may lead to subtle bugs that are difficult to discover.
To address this concern deliberate testing has been performed.

It doesn't seem feasible to do some kind of static checking to find
potential issues with pointer tagging, so a dynamic approach was taken.
All pointer comparisons/subtractions have been instrumented in an LLVM
compiler pass and a kernel module that would print a bug report whenever
two pointers with different tags are being compared/subtracted (ignoring
comparisons with NULL pointers and with pointers obtained by casting an
error code to a pointer type) has been used. Then the kernel has been
booted in QEMU and on an Odroid C2 board and syzkaller has been run.

This yielded the following results.

The two places that look interesting are:

is_vmalloc_addr in include/linux/mm.h
is_kernel_rodata in mm/util.c

Here we compare a pointer with some fixed untagged values to make sure
that the pointer lies in a particular part of the kernel address space.
Since tag-based KASAN doesn't add tags to pointers that belong to rodata
or vmalloc regions, this should work as is. To make sure debug checks to
those two functions that check that the result doesn't change whether
we operate on pointers with or without untagging has been added.

A few other cases that don't look that interesting:

Comparing pointers to achieve unique sorting order of pointee objects
(e.g. sorting locks addresses before performing a double lock):

tty_ldisc_lock_pair_timeout in drivers/tty/tty_ldisc.c
pipe_double_lock in fs/pipe.c
unix_state_double_lock in net/unix/af_unix.c
lock_two_nondirectories in fs/inode.c
mutex_lock_double in kernel/events/core.c

ep_cmp_ffd in fs/eventpoll.c
fsnotify_compare_groups fs/notify/mark.c

Nothing needs to be done here, since the tags embedded into pointers
don't change, so the sorting order would still be unique.

Checks that a pointer belongs to some particular allocation:

is_sibling_entry in lib/radix-tree.c
object_is_on_stack in include/linux/sched/task_stack.h

Nothing needs to be done here either, since two pointers can only belong
to the same allocation if they have the same tag.

Overall, since the kernel boots and works, there are no critical bugs.
As for the rest, the traditional kernel testing way (use until fails) is
the only one that looks feasible.

Another point here is that tag-based KASAN is available under a separate
config option that needs to be deliberately enabled. Even though it might
be used in a "near-production" environment to find bugs that are not found
during fuzzing or running tests, it is still a debug tool.


====== Benchmarks

The following numbers were collected on Odroid C2 board. Both generic and
tag-based KASAN were used in inline instrumentation mode.

Boot time [1]:
* ~1.7 sec for clean kernel
* ~5.0 sec for generic KASAN
* ~5.0 sec for tag-based KASAN

Network performance [2]:
* 8.33 Gbits/sec for clean kernel
* 3.17 Gbits/sec for generic KASAN
* 2.85 Gbits/sec for tag-based KASAN

Slab memory usage after boot [3]:
* ~40 kb for clean kernel
* ~105 kb (~260% overhead) for generic KASAN
* ~47 kb (~20% overhead) for tag-based KASAN

KASAN memory overhead consists of three main parts:
1. Increased slab memory usage due to redzones.
2. Shadow memory (the whole reserved once during boot).
3. Quaratine (grows gradually until some preset limit; the more the limit,
   the more the chance to detect a use-after-free).

Comparing tag-based vs generic KASAN for each of these points:
1. 20% vs 260% overhead.
2. 1/16th vs 1/8th of physical memory.
3. Tag-based KASAN doesn't require quarantine.

[1] Time before the ext4 driver is initialized.
[2] Measured as `iperf -s & iperf -c 127.0.0.1 -t 30`.
[3] Measured as `cat /proc/meminfo | grep Slab`.


====== Some notes

A few notes:

1. The patchset can be found here:
   https://github.com/xairy/kasan-prototype/tree/khwasan

2. Building requires a recent Clang version (7.0.0 or later).

3. Stack instrumentation is not supported yet and will be added later.


====== Changes

Changes in v9:
- Fixed kasan_init_slab_obj() hook when KASAN is disabled.
- Added assign_tag() function that preassigns tags for caches with
  constructors.
- Fixed KASAN_TAG_MASK redefinition in include/linux/mm.h vs
  mm/kasan/kasan.h.

Changes in v8:
- Rebased onto 7876320f (4.19-rc4).
- Renamed KHWASAN to software tag-based KASAN (see the top of the cover
  letter for details).
- Explicitly called tag-based KASAN a debug tool.
- Reused kasan_init_slab_obj() callback to preassign tags to caches
  without constructors, remove khwasan_preset_sl(u/a)b_tag().
- Moved move obj_to_index to include/linux/slab_def.h from mm/slab.c.
- Moved cache->s_mem untagging to alloc_slabmgmt() for SLAB.
- Fixed check_memory_region() to correctly handle user memory accesses and
  size == 0 case.
- Merged __no_sanitize_hwaddress into __no_sanitize_address.
- Defined KASAN_SET_TAG and KASAN_RESET_TAG macros for non KASAN builds to
  avoid duplication of __kimg_to_phys, _virt_addr_is_linear and
  page_to_virt macros.
- Fixed and simplified find_first_bad_addr for generic KASAN.
- Use non symbolized example KASAN report in documentation.
- Mention clang version requirements for both KASAN modes in the Kconfig
  options and in the documentation.
- Various small fixes.

Version v7 got accidentally skipped.

Changes in v6:
- Rebased onto 050cdc6c (4.19-rc1+).
- Added notes regarding patchset testing into the cover letter.

Changes in v5:
- Rebased onto 1ffaddd029 (4.18-rc8).
- Preassign tags for objects from caches with constructors and
  SLAB_TYPESAFE_BY_RCU caches.
- Fix SLAB allocator support by untagging page->s_mem in
  kasan_poison_slab().
- Performed dynamic testing to find potential places where pointer tagging
  might result in bugs [1].
- Clarified and fixed memory usage benchmarks in the cover letter.
- Added a rationale for having KHWASAN to the cover letter.

Changes in v4:
- Fixed SPDX comment style in mm/kasan/kasan.h.
- Fixed mm/kasan/kasan.h changes being included in a wrong patch.
- Swapped "khwasan, arm64: fix up fault handling logic" and "khwasan: add
  tag related helper functions" patches order.
- Rebased onto 6f0d349d (4.18-rc2+).

Changes in v3:
- Minor documentation fixes.
- Fixed CFLAGS variable name in KASAN makefile.
- Added a "SPDX-License-Identifier: GPL-2.0" line to all source files
  under mm/kasan.
- Rebased onto 81e97f013 (4.18-rc1+).

Changes in v2:
- Changed kmalloc_large_node_hook to return tagged pointer instead of
  using an output argument.
- Fix checking whether -fsanitize=hwaddress is supported by the compiler.
- Removed duplication of -fno-builtin for KASAN and KHWASAN.
- Removed {} block for one line for_each_possible_cpu loop.
- Made set_track() static inline as it is used only in common.c.
- Moved optimal_redzone() to common.c.
- Fixed using tagged pointer for shadow calculation in
  kasan_unpoison_shadow().
- Restored setting cache->align in kasan_cache_create(), which was
  accidentally lost.
- Simplified __kasan_slab_free(), kasan_alloc_pages() and kasan_kmalloc().
- Removed tagging from kasan_kmalloc_large().
- Added page_kasan_tag_reset() to kasan_poison_slab() and removed
  !PageSlab() check from page_to_virt.
- Reset pointer tag in _virt_addr_is_linear.
- Set page tag for each page when multiple pages are allocated or freed.
- Added a comment as to why we ignore cma allocated pages.

Changes in v1:
- Rebased onto 4.17-rc4.
- Updated benchmarking stats.
- Documented compiler version requirements, memory usage and slowdown.
- Dropped kvm patches, as clang + arm64 + kvm is completely broken [1].

Changes in RFC v3:
- Renamed CONFIG_KASAN_CLASSIC and CONFIG_KASAN_TAGS to
  CONFIG_KASAN_GENERIC and CONFIG_KASAN_HW respectively.
- Switch to -fsanitize=kernel-hwaddress instead of -fsanitize=hwaddress.
- Removed unnecessary excessive shadow initialization.
- Removed khwasan_enabled flag (itâs not needed since KHWASAN is
  initialized before any slab caches are used).
- Split out kasan_report.c and khwasan_report.c from report.c.
- Moved more common KASAN and KHWASAN functions to common.c.
- Added tagging to pagealloc.
- Rebased onto 4.17-rc1.
- Temporarily dropped patch that adds kvm support (arm64 + kvm + clang
  combo is broken right now [2]).

Changes in RFC v2:
- Removed explicit casts to u8 * for kasan_mem_to_shadow() calls.
- Introduced KASAN_TCR_FLAGS for setting the TCR_TBI1 flag.
- Added a comment regarding the non-atomic RMW sequence in
  khwasan_random_tag().
- Made all tag related functions accept const void *.
- Untagged pointers in __kimg_to_phys, which is used by virt_to_phys.
- Untagged pointers in show_ptr in fault handling logic.
- Untagged pointers passed to KVM.
- Added two reserved tag values: 0xFF and 0xFE.
- Used the reserved tag 0xFF to disable validity checking (to resolve the
  issue with pointer tag being lost after page_address + kmap usage).
- Used the reserved tag 0xFE to mark redzones and freed objects.
- Added mnemonics for esr manipulation in KHWASAN brk handler.
- Added a comment about the -recover flag.
- Some minor cleanups and fixes.
- Rebased onto 3215b9d5 (4.16-rc6+).
- Tested on real hardware (Odroid C2 board).
- Added better benchmarks.

[1] https://lkml.org/lkml/2018/7/18/765
[2] https://lkml.org/lkml/2018/4/19/775

Andrey Konovalov (20):
  kasan, mm: change hooks signatures
  kasan: move common generic and tag-based code to common.c
  kasan: rename source files to reflect the new naming scheme
  kasan: add CONFIG_KASAN_GENERIC and CONFIG_KASAN_SW_TAGS
  kasan, arm64: adjust shadow size for tag-based mode
  kasan: initialize shadow to 0xff for tag-based mode
  kasan, arm64: untag address in __kimg_to_phys and _virt_addr_is_linear
  kasan: add tag related helper functions
  kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
  mm: move obj_to_index to include/linux/slab_def.h
  kasan, arm64: fix up fault handling logic
  kasan, arm64: enable top byte ignore for the kernel
  kasan, mm: perform untagged pointers comparison in krealloc
  kasan: split out generic_report.c from report.c
  kasan: add bug reporting routines for tag-based mode
  kasan: add hooks implementation for tag-based mode
  kasan, arm64: add brk handler for inline instrumentation
  kasan, mm, arm64: tag non slab memory allocated via pagealloc
  kasan: update documentation
  kasan: add SPDX-License-Identifier mark to source files

 Documentation/dev-tools/kasan.rst      | 232 +++++----
 arch/arm64/Kconfig                     |   1 +
 arch/arm64/Makefile                    |   2 +-
 arch/arm64/include/asm/brk-imm.h       |   2 +
 arch/arm64/include/asm/memory.h        |  36 +-
 arch/arm64/include/asm/pgtable-hwdef.h |   1 +
 arch/arm64/kernel/traps.c              |  68 ++-
 arch/arm64/mm/fault.c                  |   3 +
 arch/arm64/mm/kasan_init.c             |  18 +-
 arch/arm64/mm/proc.S                   |   8 +-
 include/linux/compiler-clang.h         |   5 +-
 include/linux/kasan.h                  |  83 +++-
 include/linux/mm.h                     |  29 ++
 include/linux/page-flags-layout.h      |  10 +
 include/linux/slab_def.h               |  13 +
 lib/Kconfig.kasan                      |  87 +++-
 mm/cma.c                               |  11 +
 mm/kasan/Makefile                      |  15 +-
 mm/kasan/{kasan.c => common.c}         | 651 +++++++++----------------
 mm/kasan/generic.c                     | 344 +++++++++++++
 mm/kasan/generic_report.c              | 153 ++++++
 mm/kasan/{kasan_init.c => init.c}      |   1 +
 mm/kasan/kasan.h                       |  83 +++-
 mm/kasan/quarantine.c                  |   1 +
 mm/kasan/report.c                      | 272 +++--------
 mm/kasan/tags.c                        | 161 ++++++
 mm/kasan/tags_report.c                 |  58 +++
 mm/page_alloc.c                        |   1 +
 mm/slab.c                              |  29 +-
 mm/slab.h                              |   2 +-
 mm/slab_common.c                       |   6 +-
 mm/slub.c                              |  41 +-
 scripts/Makefile.kasan                 |  27 +-
 33 files changed, 1629 insertions(+), 825 deletions(-)
 rename mm/kasan/{kasan.c => common.c} (59%)
 create mode 100644 mm/kasan/generic.c
 create mode 100644 mm/kasan/generic_report.c
 rename mm/kasan/{kasan_init.c => init.c} (99%)
 create mode 100644 mm/kasan/tags.c
 create mode 100644 mm/kasan/tags_report.c

-- 
2.19.0.444.g18242da7ef-goog

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-kernel
Subject: [PATCH v9 00/20] kasan: add software tag-based mode for arm64
Date: Fri, 21 Sep 2018 15:13:22 +0000
Message-ID: <cover.1537542735.git.andreyknvl () google ! com>
--------------------
This patchset adds a new software tag-based mode to KASAN [1].
(Initially this mode was called KHWASAN, but it got renamed,
 see the naming rationale at the end of this section).

The plan is to implement HWASan [2] for the kernel with the incentive,
that it's going to have comparable to KASAN performance, but in the same
time consume much less memory, trading that off for somewhat imprecise
bug detection and being supported only for arm64.

The underlying ideas of the approach used by software tag-based KASAN are:

1. By using the Top Byte Ignore (TBI) arm64 CPU feature, we can store
   pointer tags in the top byte of each kernel pointer.

2. Using shadow memory, we can store memory tags for each chunk of kernel
   memory.

3. On each memory allocation, we can generate a random tag, embed it into
   the returned pointer and set the memory tags that correspond to this
   chunk of memory to the same value.

4. By using compiler instrumentation, before each memory access we can add
   a check that the pointer tag matches the tag of the memory that is being
   accessed.

5. On a tag mismatch we report an error.

With this patchset the existing KASAN mode gets renamed to generic KASAN,
with the word "generic" meaning that the implementation can be supported
by any architecture as it is purely software.

The new mode this patchset adds is called software tag-based KASAN. The
word "tag-based" refers to the fact that this mode uses tags embedded into
the top byte of kernel pointers and the TBI arm64 CPU feature that allows
to dereference such pointers. The word "software" here means that shadow
memory manipulation and tag checking on pointer dereference is done in
software. As it is the only tag-based implementation right now, "software
tag-based" KASAN is sometimes referred to as simply "tag-based" in this
patchset.

A potential expansion of this mode is a hardware tag-based mode, which would
use hardware memory tagging support (announced by Arm [3]) instead of
compiler instrumentation and manual shadow memory manipulation.

Same as generic KASAN, software tag-based KASAN is strictly a debugging
feature.

[1] https://www.kernel.org/doc/html/latest/dev-tools/kasan.html

[2] http://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.html

[3] https://community.arm.com/processors/b/blog/posts/arm-a-profile-architecture-2018-developments-armv85a


====== Rationale

On mobile devices generic KASAN's memory usage is significant problem. One
of the main reasons to have tag-based KASAN is to be able to perform a
similar set of checks as the generic one does, but with lower memory
requirements.

Comment from Vishwath Mohan <vishwath@google.com>:

I don't have data on-hand, but anecdotally both ASAN and KASAN have proven
problematic to enable for environments that don't tolerate the increased
memory pressure well. This includes,
(a) Low-memory form factors - Wear, TV, Things, lower-tier phones like Go,
(c) Connected components like Pixel's visual core [1].

These are both places I'd love to have a low(er) memory footprint option at
my disposal.

Comment from Evgenii Stepanov <eugenis@google.com>:

Looking at a live Android device under load, slab (according to
/proc/meminfo) + kernel stack take 8-10% available RAM (~350MB). KASAN's
overhead of 2x - 3x on top of it is not insignificant.

Not having this overhead enables near-production use - ex. running
KASAN/KHWASAN kernel on a personal, daily-use device to catch bugs that do
not reproduce in test configuration. These are the ones that often cost
the most engineering time to track down.

CPU overhead is bad, but generally tolerable. RAM is critical, in our
experience. Once it gets low enough, OOM-killer makes your life miserable.

[1] https://www.blog.google/products/pixel/pixel-visual-core-image-processing-and-machine-learning-pixel-2/


====== Technical details

Software tag-based KASAN mode is implemented in a very similar way to the
generic one. This patchset essentially does the following:

1. TCR_TBI1 is set to enable Top Byte Ignore.

2. Shadow memory is used (with a different scale, 1:16, so each shadow
   byte corresponds to 16 bytes of kernel memory) to store memory tags.

3. All slab objects are aligned to shadow scale, which is 16 bytes.

4. All pointers returned from the slab allocator are tagged with a random
   tag and the corresponding shadow memory is poisoned with the same value.

5. Compiler instrumentation is used to insert tag checks. Either by
   calling callbacks or by inlining them (CONFIG_KASAN_OUTLINE and
   CONFIG_KASAN_INLINE flags are reused).

6. When a tag mismatch is detected in callback instrumentation mode
   KASAN simply prints a bug report. In case of inline instrumentation,
   clang inserts a brk instruction, and KASAN has it's own brk handler,
   which reports the bug.

7. The memory in between slab objects is marked with a reserved tag, and
   acts as a redzone.

8. When a slab object is freed it's marked with a reserved tag.

Bug detection is imprecise for two reasons:

1. We won't catch some small out-of-bounds accesses, that fall into the
   same shadow cell, as the last byte of a slab object.

2. We only have 1 byte to store tags, which means we have a 1/256
   probability of a tag match for an incorrect access (actually even
   slightly less due to reserved tag values).

Despite that there's a particular type of bugs that tag-based KASAN can
detect compared to generic KASAN: use-after-free after the object has been
allocated by someone else.


====== Testing

Some kernel developers voiced a concern that changing the top byte of
kernel pointers may lead to subtle bugs that are difficult to discover.
To address this concern deliberate testing has been performed.

It doesn't seem feasible to do some kind of static checking to find
potential issues with pointer tagging, so a dynamic approach was taken.
All pointer comparisons/subtractions have been instrumented in an LLVM
compiler pass and a kernel module that would print a bug report whenever
two pointers with different tags are being compared/subtracted (ignoring
comparisons with NULL pointers and with pointers obtained by casting an
error code to a pointer type) has been used. Then the kernel has been
booted in QEMU and on an Odroid C2 board and syzkaller has been run.

This yielded the following results.

The two places that look interesting are:

is_vmalloc_addr in include/linux/mm.h
is_kernel_rodata in mm/util.c

Here we compare a pointer with some fixed untagged values to make sure
that the pointer lies in a particular part of the kernel address space.
Since tag-based KASAN doesn't add tags to pointers that belong to rodata
or vmalloc regions, this should work as is. To make sure debug checks to
those two functions that check that the result doesn't change whether
we operate on pointers with or without untagging has been added.

A few other cases that don't look that interesting:

Comparing pointers to achieve unique sorting order of pointee objects
(e.g. sorting locks addresses before performing a double lock):

tty_ldisc_lock_pair_timeout in drivers/tty/tty_ldisc.c
pipe_double_lock in fs/pipe.c
unix_state_double_lock in net/unix/af_unix.c
lock_two_nondirectories in fs/inode.c
mutex_lock_double in kernel/events/core.c

ep_cmp_ffd in fs/eventpoll.c
fsnotify_compare_groups fs/notify/mark.c

Nothing needs to be done here, since the tags embedded into pointers
don't change, so the sorting order would still be unique.

Checks that a pointer belongs to some particular allocation:

is_sibling_entry in lib/radix-tree.c
object_is_on_stack in include/linux/sched/task_stack.h

Nothing needs to be done here either, since two pointers can only belong
to the same allocation if they have the same tag.

Overall, since the kernel boots and works, there are no critical bugs.
As for the rest, the traditional kernel testing way (use until fails) is
the only one that looks feasible.

Another point here is that tag-based KASAN is available under a separate
config option that needs to be deliberately enabled. Even though it might
be used in a "near-production" environment to find bugs that are not found
during fuzzing or running tests, it is still a debug tool.


====== Benchmarks

The following numbers were collected on Odroid C2 board. Both generic and
tag-based KASAN were used in inline instrumentation mode.

Boot time [1]:
* ~1.7 sec for clean kernel
* ~5.0 sec for generic KASAN
* ~5.0 sec for tag-based KASAN

Network performance [2]:
* 8.33 Gbits/sec for clean kernel
* 3.17 Gbits/sec for generic KASAN
* 2.85 Gbits/sec for tag-based KASAN

Slab memory usage after boot [3]:
* ~40 kb for clean kernel
* ~105 kb (~260% overhead) for generic KASAN
* ~47 kb (~20% overhead) for tag-based KASAN

KASAN memory overhead consists of three main parts:
1. Increased slab memory usage due to redzones.
2. Shadow memory (the whole reserved once during boot).
3. Quaratine (grows gradually until some preset limit; the more the limit,
   the more the chance to detect a use-after-free).

Comparing tag-based vs generic KASAN for each of these points:
1. 20% vs 260% overhead.
2. 1/16th vs 1/8th of physical memory.
3. Tag-based KASAN doesn't require quarantine.

[1] Time before the ext4 driver is initialized.
[2] Measured as `iperf -s & iperf -c 127.0.0.1 -t 30`.
[3] Measured as `cat /proc/meminfo | grep Slab`.


====== Some notes

A few notes:

1. The patchset can be found here:
   https://github.com/xairy/kasan-prototype/tree/khwasan

2. Building requires a recent Clang version (7.0.0 or later).

3. Stack instrumentation is not supported yet and will be added later.


====== Changes

Changes in v9:
- Fixed kasan_init_slab_obj() hook when KASAN is disabled.
- Added assign_tag() function that preassigns tags for caches with
  constructors.
- Fixed KASAN_TAG_MASK redefinition in include/linux/mm.h vs
  mm/kasan/kasan.h.

Changes in v8:
- Rebased onto 7876320f (4.19-rc4).
- Renamed KHWASAN to software tag-based KASAN (see the top of the cover
  letter for details).
- Explicitly called tag-based KASAN a debug tool.
- Reused kasan_init_slab_obj() callback to preassign tags to caches
  without constructors, remove khwasan_preset_sl(u/a)b_tag().
- Moved move obj_to_index to include/linux/slab_def.h from mm/slab.c.
- Moved cache->s_mem untagging to alloc_slabmgmt() for SLAB.
- Fixed check_memory_region() to correctly handle user memory accesses and
  size == 0 case.
- Merged __no_sanitize_hwaddress into __no_sanitize_address.
- Defined KASAN_SET_TAG and KASAN_RESET_TAG macros for non KASAN builds to
  avoid duplication of __kimg_to_phys, _virt_addr_is_linear and
  page_to_virt macros.
- Fixed and simplified find_first_bad_addr for generic KASAN.
- Use non symbolized example KASAN report in documentation.
- Mention clang version requirements for both KASAN modes in the Kconfig
  options and in the documentation.
- Various small fixes.

Version v7 got accidentally skipped.

Changes in v6:
- Rebased onto 050cdc6c (4.19-rc1+).
- Added notes regarding patchset testing into the cover letter.

Changes in v5:
- Rebased onto 1ffaddd029 (4.18-rc8).
- Preassign tags for objects from caches with constructors and
  SLAB_TYPESAFE_BY_RCU caches.
- Fix SLAB allocator support by untagging page->s_mem in
  kasan_poison_slab().
- Performed dynamic testing to find potential places where pointer tagging
  might result in bugs [1].
- Clarified and fixed memory usage benchmarks in the cover letter.
- Added a rationale for having KHWASAN to the cover letter.

Changes in v4:
- Fixed SPDX comment style in mm/kasan/kasan.h.
- Fixed mm/kasan/kasan.h changes being included in a wrong patch.
- Swapped "khwasan, arm64: fix up fault handling logic" and "khwasan: add
  tag related helper functions" patches order.
- Rebased onto 6f0d349d (4.18-rc2+).

Changes in v3:
- Minor documentation fixes.
- Fixed CFLAGS variable name in KASAN makefile.
- Added a "SPDX-License-Identifier: GPL-2.0" line to all source files
  under mm/kasan.
- Rebased onto 81e97f013 (4.18-rc1+).

Changes in v2:
- Changed kmalloc_large_node_hook to return tagged pointer instead of
  using an output argument.
- Fix checking whether -fsanitize=hwaddress is supported by the compiler.
- Removed duplication of -fno-builtin for KASAN and KHWASAN.
- Removed {} block for one line for_each_possible_cpu loop.
- Made set_track() static inline as it is used only in common.c.
- Moved optimal_redzone() to common.c.
- Fixed using tagged pointer for shadow calculation in
  kasan_unpoison_shadow().
- Restored setting cache->align in kasan_cache_create(), which was
  accidentally lost.
- Simplified __kasan_slab_free(), kasan_alloc_pages() and kasan_kmalloc().
- Removed tagging from kasan_kmalloc_large().
- Added page_kasan_tag_reset() to kasan_poison_slab() and removed
  !PageSlab() check from page_to_virt.
- Reset pointer tag in _virt_addr_is_linear.
- Set page tag for each page when multiple pages are allocated or freed.
- Added a comment as to why we ignore cma allocated pages.

Changes in v1:
- Rebased onto 4.17-rc4.
- Updated benchmarking stats.
- Documented compiler version requirements, memory usage and slowdown.
- Dropped kvm patches, as clang + arm64 + kvm is completely broken [1].

Changes in RFC v3:
- Renamed CONFIG_KASAN_CLASSIC and CONFIG_KASAN_TAGS to
  CONFIG_KASAN_GENERIC and CONFIG_KASAN_HW respectively.
- Switch to -fsanitize=kernel-hwaddress instead of -fsanitize=hwaddress.
- Removed unnecessary excessive shadow initialization.
- Removed khwasan_enabled flag (itâs not needed since KHWASAN is
  initialized before any slab caches are used).
- Split out kasan_report.c and khwasan_report.c from report.c.
- Moved more common KASAN and KHWASAN functions to common.c.
- Added tagging to pagealloc.
- Rebased onto 4.17-rc1.
- Temporarily dropped patch that adds kvm support (arm64 + kvm + clang
  combo is broken right now [2]).

Changes in RFC v2:
- Removed explicit casts to u8 * for kasan_mem_to_shadow() calls.
- Introduced KASAN_TCR_FLAGS for setting the TCR_TBI1 flag.
- Added a comment regarding the non-atomic RMW sequence in
  khwasan_random_tag().
- Made all tag related functions accept const void *.
- Untagged pointers in __kimg_to_phys, which is used by virt_to_phys.
- Untagged pointers in show_ptr in fault handling logic.
- Untagged pointers passed to KVM.
- Added two reserved tag values: 0xFF and 0xFE.
- Used the reserved tag 0xFF to disable validity checking (to resolve the
  issue with pointer tag being lost after page_address + kmap usage).
- Used the reserved tag 0xFE to mark redzones and freed objects.
- Added mnemonics for esr manipulation in KHWASAN brk handler.
- Added a comment about the -recover flag.
- Some minor cleanups and fixes.
- Rebased onto 3215b9d5 (4.16-rc6+).
- Tested on real hardware (Odroid C2 board).
- Added better benchmarks.

[1] https://lkml.org/lkml/2018/7/18/765
[2] https://lkml.org/lkml/2018/4/19/775

Andrey Konovalov (20):
  kasan, mm: change hooks signatures
  kasan: move common generic and tag-based code to common.c
  kasan: rename source files to reflect the new naming scheme
  kasan: add CONFIG_KASAN_GENERIC and CONFIG_KASAN_SW_TAGS
  kasan, arm64: adjust shadow size for tag-based mode
  kasan: initialize shadow to 0xff for tag-based mode
  kasan, arm64: untag address in __kimg_to_phys and _virt_addr_is_linear
  kasan: add tag related helper functions
  kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
  mm: move obj_to_index to include/linux/slab_def.h
  kasan, arm64: fix up fault handling logic
  kasan, arm64: enable top byte ignore for the kernel
  kasan, mm: perform untagged pointers comparison in krealloc
  kasan: split out generic_report.c from report.c
  kasan: add bug reporting routines for tag-based mode
  kasan: add hooks implementation for tag-based mode
  kasan, arm64: add brk handler for inline instrumentation
  kasan, mm, arm64: tag non slab memory allocated via pagealloc
  kasan: update documentation
  kasan: add SPDX-License-Identifier mark to source files

 Documentation/dev-tools/kasan.rst      | 232 +++++----
 arch/arm64/Kconfig                     |   1 +
 arch/arm64/Makefile                    |   2 +-
 arch/arm64/include/asm/brk-imm.h       |   2 +
 arch/arm64/include/asm/memory.h        |  36 +-
 arch/arm64/include/asm/pgtable-hwdef.h |   1 +
 arch/arm64/kernel/traps.c              |  68 ++-
 arch/arm64/mm/fault.c                  |   3 +
 arch/arm64/mm/kasan_init.c             |  18 +-
 arch/arm64/mm/proc.S                   |   8 +-
 include/linux/compiler-clang.h         |   5 +-
 include/linux/kasan.h                  |  83 +++-
 include/linux/mm.h                     |  29 ++
 include/linux/page-flags-layout.h      |  10 +
 include/linux/slab_def.h               |  13 +
 lib/Kconfig.kasan                      |  87 +++-
 mm/cma.c                               |  11 +
 mm/kasan/Makefile                      |  15 +-
 mm/kasan/{kasan.c => common.c}         | 651 +++++++++----------------
 mm/kasan/generic.c                     | 344 +++++++++++++
 mm/kasan/generic_report.c              | 153 ++++++
 mm/kasan/{kasan_init.c => init.c}      |   1 +
 mm/kasan/kasan.h                       |  83 +++-
 mm/kasan/quarantine.c                  |   1 +
 mm/kasan/report.c                      | 272 +++--------
 mm/kasan/tags.c                        | 161 ++++++
 mm/kasan/tags_report.c                 |  58 +++
 mm/page_alloc.c                        |   1 +
 mm/slab.c                              |  29 +-
 mm/slab.h                              |   2 +-
 mm/slab_common.c                       |   6 +-
 mm/slub.c                              |  41 +-
 scripts/Makefile.kasan                 |  27 +-
 33 files changed, 1629 insertions(+), 825 deletions(-)
 rename mm/kasan/{kasan.c => common.c} (59%)
 create mode 100644 mm/kasan/generic.c
 create mode 100644 mm/kasan/generic_report.c
 rename mm/kasan/{kasan_init.c => init.c} (99%)
 create mode 100644 mm/kasan/tags.c
 create mode 100644 mm/kasan/tags_report.c

-- 
2.19.0.444.g18242da7ef-goog

================================================================================

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-mm
Subject: [PATCH v9 00/20] kasan: add software tag-based mode for arm64
Date: Fri, 21 Sep 2018 15:13:22 +0000
Message-ID: <cover.1537542735.git.andreyknvl () google ! com>
--------------------
This patchset adds a new software tag-based mode to KASAN [1].
(Initially this mode was called KHWASAN, but it got renamed,
 see the naming rationale at the end of this section).

The plan is to implement HWASan [2] for the kernel with the incentive,
that it's going to have comparable to KASAN performance, but in the same
time consume much less memory, trading that off for somewhat imprecise
bug detection and being supported only for arm64.

The underlying ideas of the approach used by software tag-based KASAN are:

1. By using the Top Byte Ignore (TBI) arm64 CPU feature, we can store
   pointer tags in the top byte of each kernel pointer.

2. Using shadow memory, we can store memory tags for each chunk of kernel
   memory.

3. On each memory allocation, we can generate a random tag, embed it into
   the returned pointer and set the memory tags that correspond to this
   chunk of memory to the same value.

4. By using compiler instrumentation, before each memory access we can add
   a check that the pointer tag matches the tag of the memory that is being
   accessed.

5. On a tag mismatch we report an error.

With this patchset the existing KASAN mode gets renamed to generic KASAN,
with the word "generic" meaning that the implementation can be supported
by any architecture as it is purely software.

The new mode this patchset adds is called software tag-based KASAN. The
word "tag-based" refers to the fact that this mode uses tags embedded into
the top byte of kernel pointers and the TBI arm64 CPU feature that allows
to dereference such pointers. The word "software" here means that shadow
memory manipulation and tag checking on pointer dereference is done in
software. As it is the only tag-based implementation right now, "software
tag-based" KASAN is sometimes referred to as simply "tag-based" in this
patchset.

A potential expansion of this mode is a hardware tag-based mode, which would
use hardware memory tagging support (announced by Arm [3]) instead of
compiler instrumentation and manual shadow memory manipulation.

Same as generic KASAN, software tag-based KASAN is strictly a debugging
feature.

[1] https://www.kernel.org/doc/html/latest/dev-tools/kasan.html

[2] http://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.html

[3] https://community.arm.com/processors/b/blog/posts/arm-a-profile-architecture-2018-developments-armv85a


====== Rationale

On mobile devices generic KASAN's memory usage is significant problem. One
of the main reasons to have tag-based KASAN is to be able to perform a
similar set of checks as the generic one does, but with lower memory
requirements.

Comment from Vishwath Mohan <vishwath@google.com>:

I don't have data on-hand, but anecdotally both ASAN and KASAN have proven
problematic to enable for environments that don't tolerate the increased
memory pressure well. This includes,
(a) Low-memory form factors - Wear, TV, Things, lower-tier phones like Go,
(c) Connected components like Pixel's visual core [1].

These are both places I'd love to have a low(er) memory footprint option at
my disposal.

Comment from Evgenii Stepanov <eugenis@google.com>:

Looking at a live Android device under load, slab (according to
/proc/meminfo) + kernel stack take 8-10% available RAM (~350MB). KASAN's
overhead of 2x - 3x on top of it is not insignificant.

Not having this overhead enables near-production use - ex. running
KASAN/KHWASAN kernel on a personal, daily-use device to catch bugs that do
not reproduce in test configuration. These are the ones that often cost
the most engineering time to track down.

CPU overhead is bad, but generally tolerable. RAM is critical, in our
experience. Once it gets low enough, OOM-killer makes your life miserable.

[1] https://www.blog.google/products/pixel/pixel-visual-core-image-processing-and-machine-learning-pixel-2/


====== Technical details

Software tag-based KASAN mode is implemented in a very similar way to the
generic one. This patchset essentially does the following:

1. TCR_TBI1 is set to enable Top Byte Ignore.

2. Shadow memory is used (with a different scale, 1:16, so each shadow
   byte corresponds to 16 bytes of kernel memory) to store memory tags.

3. All slab objects are aligned to shadow scale, which is 16 bytes.

4. All pointers returned from the slab allocator are tagged with a random
   tag and the corresponding shadow memory is poisoned with the same value.

5. Compiler instrumentation is used to insert tag checks. Either by
   calling callbacks or by inlining them (CONFIG_KASAN_OUTLINE and
   CONFIG_KASAN_INLINE flags are reused).

6. When a tag mismatch is detected in callback instrumentation mode
   KASAN simply prints a bug report. In case of inline instrumentation,
   clang inserts a brk instruction, and KASAN has it's own brk handler,
   which reports the bug.

7. The memory in between slab objects is marked with a reserved tag, and
   acts as a redzone.

8. When a slab object is freed it's marked with a reserved tag.

Bug detection is imprecise for two reasons:

1. We won't catch some small out-of-bounds accesses, that fall into the
   same shadow cell, as the last byte of a slab object.

2. We only have 1 byte to store tags, which means we have a 1/256
   probability of a tag match for an incorrect access (actually even
   slightly less due to reserved tag values).

Despite that there's a particular type of bugs that tag-based KASAN can
detect compared to generic KASAN: use-after-free after the object has been
allocated by someone else.


====== Testing

Some kernel developers voiced a concern that changing the top byte of
kernel pointers may lead to subtle bugs that are difficult to discover.
To address this concern deliberate testing has been performed.

It doesn't seem feasible to do some kind of static checking to find
potential issues with pointer tagging, so a dynamic approach was taken.
All pointer comparisons/subtractions have been instrumented in an LLVM
compiler pass and a kernel module that would print a bug report whenever
two pointers with different tags are being compared/subtracted (ignoring
comparisons with NULL pointers and with pointers obtained by casting an
error code to a pointer type) has been used. Then the kernel has been
booted in QEMU and on an Odroid C2 board and syzkaller has been run.

This yielded the following results.

The two places that look interesting are:

is_vmalloc_addr in include/linux/mm.h
is_kernel_rodata in mm/util.c

Here we compare a pointer with some fixed untagged values to make sure
that the pointer lies in a particular part of the kernel address space.
Since tag-based KASAN doesn't add tags to pointers that belong to rodata
or vmalloc regions, this should work as is. To make sure debug checks to
those two functions that check that the result doesn't change whether
we operate on pointers with or without untagging has been added.

A few other cases that don't look that interesting:

Comparing pointers to achieve unique sorting order of pointee objects
(e.g. sorting locks addresses before performing a double lock):

tty_ldisc_lock_pair_timeout in drivers/tty/tty_ldisc.c
pipe_double_lock in fs/pipe.c
unix_state_double_lock in net/unix/af_unix.c
lock_two_nondirectories in fs/inode.c
mutex_lock_double in kernel/events/core.c

ep_cmp_ffd in fs/eventpoll.c
fsnotify_compare_groups fs/notify/mark.c

Nothing needs to be done here, since the tags embedded into pointers
don't change, so the sorting order would still be unique.

Checks that a pointer belongs to some particular allocation:

is_sibling_entry in lib/radix-tree.c
object_is_on_stack in include/linux/sched/task_stack.h

Nothing needs to be done here either, since two pointers can only belong
to the same allocation if they have the same tag.

Overall, since the kernel boots and works, there are no critical bugs.
As for the rest, the traditional kernel testing way (use until fails) is
the only one that looks feasible.

Another point here is that tag-based KASAN is available under a separate
config option that needs to be deliberately enabled. Even though it might
be used in a "near-production" environment to find bugs that are not found
during fuzzing or running tests, it is still a debug tool.


====== Benchmarks

The following numbers were collected on Odroid C2 board. Both generic and
tag-based KASAN were used in inline instrumentation mode.

Boot time [1]:
* ~1.7 sec for clean kernel
* ~5.0 sec for generic KASAN
* ~5.0 sec for tag-based KASAN

Network performance [2]:
* 8.33 Gbits/sec for clean kernel
* 3.17 Gbits/sec for generic KASAN
* 2.85 Gbits/sec for tag-based KASAN

Slab memory usage after boot [3]:
* ~40 kb for clean kernel
* ~105 kb (~260% overhead) for generic KASAN
* ~47 kb (~20% overhead) for tag-based KASAN

KASAN memory overhead consists of three main parts:
1. Increased slab memory usage due to redzones.
2. Shadow memory (the whole reserved once during boot).
3. Quaratine (grows gradually until some preset limit; the more the limit,
   the more the chance to detect a use-after-free).

Comparing tag-based vs generic KASAN for each of these points:
1. 20% vs 260% overhead.
2. 1/16th vs 1/8th of physical memory.
3. Tag-based KASAN doesn't require quarantine.

[1] Time before the ext4 driver is initialized.
[2] Measured as `iperf -s & iperf -c 127.0.0.1 -t 30`.
[3] Measured as `cat /proc/meminfo | grep Slab`.


====== Some notes

A few notes:

1. The patchset can be found here:
   https://github.com/xairy/kasan-prototype/tree/khwasan

2. Building requires a recent Clang version (7.0.0 or later).

3. Stack instrumentation is not supported yet and will be added later.


====== Changes

Changes in v9:
- Fixed kasan_init_slab_obj() hook when KASAN is disabled.
- Added assign_tag() function that preassigns tags for caches with
  constructors.
- Fixed KASAN_TAG_MASK redefinition in include/linux/mm.h vs
  mm/kasan/kasan.h.

Changes in v8:
- Rebased onto 7876320f (4.19-rc4).
- Renamed KHWASAN to software tag-based KASAN (see the top of the cover
  letter for details).
- Explicitly called tag-based KASAN a debug tool.
- Reused kasan_init_slab_obj() callback to preassign tags to caches
  without constructors, remove khwasan_preset_sl(u/a)b_tag().
- Moved move obj_to_index to include/linux/slab_def.h from mm/slab.c.
- Moved cache->s_mem untagging to alloc_slabmgmt() for SLAB.
- Fixed check_memory_region() to correctly handle user memory accesses and
  size == 0 case.
- Merged __no_sanitize_hwaddress into __no_sanitize_address.
- Defined KASAN_SET_TAG and KASAN_RESET_TAG macros for non KASAN builds to
  avoid duplication of __kimg_to_phys, _virt_addr_is_linear and
  page_to_virt macros.
- Fixed and simplified find_first_bad_addr for generic KASAN.
- Use non symbolized example KASAN report in documentation.
- Mention clang version requirements for both KASAN modes in the Kconfig
  options and in the documentation.
- Various small fixes.

Version v7 got accidentally skipped.

Changes in v6:
- Rebased onto 050cdc6c (4.19-rc1+).
- Added notes regarding patchset testing into the cover letter.

Changes in v5:
- Rebased onto 1ffaddd029 (4.18-rc8).
- Preassign tags for objects from caches with constructors and
  SLAB_TYPESAFE_BY_RCU caches.
- Fix SLAB allocator support by untagging page->s_mem in
  kasan_poison_slab().
- Performed dynamic testing to find potential places where pointer tagging
  might result in bugs [1].
- Clarified and fixed memory usage benchmarks in the cover letter.
- Added a rationale for having KHWASAN to the cover letter.

Changes in v4:
- Fixed SPDX comment style in mm/kasan/kasan.h.
- Fixed mm/kasan/kasan.h changes being included in a wrong patch.
- Swapped "khwasan, arm64: fix up fault handling logic" and "khwasan: add
  tag related helper functions" patches order.
- Rebased onto 6f0d349d (4.18-rc2+).

Changes in v3:
- Minor documentation fixes.
- Fixed CFLAGS variable name in KASAN makefile.
- Added a "SPDX-License-Identifier: GPL-2.0" line to all source files
  under mm/kasan.
- Rebased onto 81e97f013 (4.18-rc1+).

Changes in v2:
- Changed kmalloc_large_node_hook to return tagged pointer instead of
  using an output argument.
- Fix checking whether -fsanitize=hwaddress is supported by the compiler.
- Removed duplication of -fno-builtin for KASAN and KHWASAN.
- Removed {} block for one line for_each_possible_cpu loop.
- Made set_track() static inline as it is used only in common.c.
- Moved optimal_redzone() to common.c.
- Fixed using tagged pointer for shadow calculation in
  kasan_unpoison_shadow().
- Restored setting cache->align in kasan_cache_create(), which was
  accidentally lost.
- Simplified __kasan_slab_free(), kasan_alloc_pages() and kasan_kmalloc().
- Removed tagging from kasan_kmalloc_large().
- Added page_kasan_tag_reset() to kasan_poison_slab() and removed
  !PageSlab() check from page_to_virt.
- Reset pointer tag in _virt_addr_is_linear.
- Set page tag for each page when multiple pages are allocated or freed.
- Added a comment as to why we ignore cma allocated pages.

Changes in v1:
- Rebased onto 4.17-rc4.
- Updated benchmarking stats.
- Documented compiler version requirements, memory usage and slowdown.
- Dropped kvm patches, as clang + arm64 + kvm is completely broken [1].

Changes in RFC v3:
- Renamed CONFIG_KASAN_CLASSIC and CONFIG_KASAN_TAGS to
  CONFIG_KASAN_GENERIC and CONFIG_KASAN_HW respectively.
- Switch to -fsanitize=kernel-hwaddress instead of -fsanitize=hwaddress.
- Removed unnecessary excessive shadow initialization.
- Removed khwasan_enabled flag (itâs not needed since KHWASAN is
  initialized before any slab caches are used).
- Split out kasan_report.c and khwasan_report.c from report.c.
- Moved more common KASAN and KHWASAN functions to common.c.
- Added tagging to pagealloc.
- Rebased onto 4.17-rc1.
- Temporarily dropped patch that adds kvm support (arm64 + kvm + clang
  combo is broken right now [2]).

Changes in RFC v2:
- Removed explicit casts to u8 * for kasan_mem_to_shadow() calls.
- Introduced KASAN_TCR_FLAGS for setting the TCR_TBI1 flag.
- Added a comment regarding the non-atomic RMW sequence in
  khwasan_random_tag().
- Made all tag related functions accept const void *.
- Untagged pointers in __kimg_to_phys, which is used by virt_to_phys.
- Untagged pointers in show_ptr in fault handling logic.
- Untagged pointers passed to KVM.
- Added two reserved tag values: 0xFF and 0xFE.
- Used the reserved tag 0xFF to disable validity checking (to resolve the
  issue with pointer tag being lost after page_address + kmap usage).
- Used the reserved tag 0xFE to mark redzones and freed objects.
- Added mnemonics for esr manipulation in KHWASAN brk handler.
- Added a comment about the -recover flag.
- Some minor cleanups and fixes.
- Rebased onto 3215b9d5 (4.16-rc6+).
- Tested on real hardware (Odroid C2 board).
- Added better benchmarks.

[1] https://lkml.org/lkml/2018/7/18/765
[2] https://lkml.org/lkml/2018/4/19/775

Andrey Konovalov (20):
  kasan, mm: change hooks signatures
  kasan: move common generic and tag-based code to common.c
  kasan: rename source files to reflect the new naming scheme
  kasan: add CONFIG_KASAN_GENERIC and CONFIG_KASAN_SW_TAGS
  kasan, arm64: adjust shadow size for tag-based mode
  kasan: initialize shadow to 0xff for tag-based mode
  kasan, arm64: untag address in __kimg_to_phys and _virt_addr_is_linear
  kasan: add tag related helper functions
  kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
  mm: move obj_to_index to include/linux/slab_def.h
  kasan, arm64: fix up fault handling logic
  kasan, arm64: enable top byte ignore for the kernel
  kasan, mm: perform untagged pointers comparison in krealloc
  kasan: split out generic_report.c from report.c
  kasan: add bug reporting routines for tag-based mode
  kasan: add hooks implementation for tag-based mode
  kasan, arm64: add brk handler for inline instrumentation
  kasan, mm, arm64: tag non slab memory allocated via pagealloc
  kasan: update documentation
  kasan: add SPDX-License-Identifier mark to source files

 Documentation/dev-tools/kasan.rst      | 232 +++++----
 arch/arm64/Kconfig                     |   1 +
 arch/arm64/Makefile                    |   2 +-
 arch/arm64/include/asm/brk-imm.h       |   2 +
 arch/arm64/include/asm/memory.h        |  36 +-
 arch/arm64/include/asm/pgtable-hwdef.h |   1 +
 arch/arm64/kernel/traps.c              |  68 ++-
 arch/arm64/mm/fault.c                  |   3 +
 arch/arm64/mm/kasan_init.c             |  18 +-
 arch/arm64/mm/proc.S                   |   8 +-
 include/linux/compiler-clang.h         |   5 +-
 include/linux/kasan.h                  |  83 +++-
 include/linux/mm.h                     |  29 ++
 include/linux/page-flags-layout.h      |  10 +
 include/linux/slab_def.h               |  13 +
 lib/Kconfig.kasan                      |  87 +++-
 mm/cma.c                               |  11 +
 mm/kasan/Makefile                      |  15 +-
 mm/kasan/{kasan.c => common.c}         | 651 +++++++++----------------
 mm/kasan/generic.c                     | 344 +++++++++++++
 mm/kasan/generic_report.c              | 153 ++++++
 mm/kasan/{kasan_init.c => init.c}      |   1 +
 mm/kasan/kasan.h                       |  83 +++-
 mm/kasan/quarantine.c                  |   1 +
 mm/kasan/report.c                      | 272 +++--------
 mm/kasan/tags.c                        | 161 ++++++
 mm/kasan/tags_report.c                 |  58 +++
 mm/page_alloc.c                        |   1 +
 mm/slab.c                              |  29 +-
 mm/slab.h                              |   2 +-
 mm/slab_common.c                       |   6 +-
 mm/slub.c                              |  41 +-
 scripts/Makefile.kasan                 |  27 +-
 33 files changed, 1629 insertions(+), 825 deletions(-)
 rename mm/kasan/{kasan.c => common.c} (59%)
 create mode 100644 mm/kasan/generic.c
 create mode 100644 mm/kasan/generic_report.c
 rename mm/kasan/{kasan_init.c => init.c} (99%)
 create mode 100644 mm/kasan/tags.c
 create mode 100644 mm/kasan/tags_report.c

-- 
2.19.0.444.g18242da7ef-goog

================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-doc
Subject: Re: [PATCH v9 00/20] kasan: add software tag-based mode for arm64
Date: Mon, 24 Sep 2018 15:14:05 +0000
Message-ID: <CACT4Y+bvH7WXEgbTWYtzVt93E=fqW69iaXWOs6tVxD=x0vdNGQ () mail ! gmail ! com>
--------------------
On Fri, Sep 21, 2018 at 5:13 PM, Andrey Konovalov <andreyknvl@google.com> w=
rote:
> This patchset adds a new software tag-based mode to KASAN [1].
> (Initially this mode was called KHWASAN, but it got renamed,
>  see the naming rationale at the end of this section).

Reviewed-by: Dmitry Vyukov <dvyukov@google.com>

> The plan is to implement HWASan [2] for the kernel with the incentive,
> that it's going to have comparable to KASAN performance, but in the same
> time consume much less memory, trading that off for somewhat imprecise
> bug detection and being supported only for arm64.
>
> The underlying ideas of the approach used by software tag-based KASAN are=
:
>
> 1. By using the Top Byte Ignore (TBI) arm64 CPU feature, we can store
>    pointer tags in the top byte of each kernel pointer.
>
> 2. Using shadow memory, we can store memory tags for each chunk of kernel
>    memory.
>
> 3. On each memory allocation, we can generate a random tag, embed it into
>    the returned pointer and set the memory tags that correspond to this
>    chunk of memory to the same value.
>
> 4. By using compiler instrumentation, before each memory access we can ad=
d
>    a check that the pointer tag matches the tag of the memory that is bei=
ng
>    accessed.
>
> 5. On a tag mismatch we report an error.
>
> With this patchset the existing KASAN mode gets renamed to generic KASAN,
> with the word "generic" meaning that the implementation can be supported
> by any architecture as it is purely software.
>
> The new mode this patchset adds is called software tag-based KASAN. The
> word "tag-based" refers to the fact that this mode uses tags embedded int=
o
> the top byte of kernel pointers and the TBI arm64 CPU feature that allows
> to dereference such pointers. The word "software" here means that shadow
> memory manipulation and tag checking on pointer dereference is done in
> software. As it is the only tag-based implementation right now, "software
> tag-based" KASAN is sometimes referred to as simply "tag-based" in this
> patchset.
>
> A potential expansion of this mode is a hardware tag-based mode, which wo=
uld
> use hardware memory tagging support (announced by Arm [3]) instead of
> compiler instrumentation and manual shadow memory manipulation.
>
> Same as generic KASAN, software tag-based KASAN is strictly a debugging
> feature.
>
> [1] https://www.kernel.org/doc/html/latest/dev-tools/kasan.html
>
> [2] http://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.htm=
l
>
> [3] https://community.arm.com/processors/b/blog/posts/arm-a-profile-archi=
tecture-2018-developments-armv85a
>
>
> =3D=3D=3D=3D=3D=3D Rationale
>
> On mobile devices generic KASAN's memory usage is significant problem. On=
e
> of the main reasons to have tag-based KASAN is to be able to perform a
> similar set of checks as the generic one does, but with lower memory
> requirements.
>
> Comment from Vishwath Mohan <vishwath@google.com>:
>
> I don't have data on-hand, but anecdotally both ASAN and KASAN have prove=
n
> problematic to enable for environments that don't tolerate the increased
> memory pressure well. This includes,
> (a) Low-memory form factors - Wear, TV, Things, lower-tier phones like Go=
,
> (c) Connected components like Pixel's visual core [1].
>
> These are both places I'd love to have a low(er) memory footprint option =
at
> my disposal.
>
> Comment from Evgenii Stepanov <eugenis@google.com>:
>
> Looking at a live Android device under load, slab (according to
> /proc/meminfo) + kernel stack take 8-10% available RAM (~350MB). KASAN's
> overhead of 2x - 3x on top of it is not insignificant.
>
> Not having this overhead enables near-production use - ex. running
> KASAN/KHWASAN kernel on a personal, daily-use device to catch bugs that d=
o
> not reproduce in test configuration. These are the ones that often cost
> the most engineering time to track down.
>
> CPU overhead is bad, but generally tolerable. RAM is critical, in our
> experience. Once it gets low enough, OOM-killer makes your life miserable=
.
>
> [1] https://www.blog.google/products/pixel/pixel-visual-core-image-proces=
sing-and-machine-learning-pixel-2/
>
>
> =3D=3D=3D=3D=3D=3D Technical details
>
> Software tag-based KASAN mode is implemented in a very similar way to the
> generic one. This patchset essentially does the following:
>
> 1. TCR_TBI1 is set to enable Top Byte Ignore.
>
> 2. Shadow memory is used (with a different scale, 1:16, so each shadow
>    byte corresponds to 16 bytes of kernel memory) to store memory tags.
>
> 3. All slab objects are aligned to shadow scale, which is 16 bytes.
>
> 4. All pointers returned from the slab allocator are tagged with a random
>    tag and the corresponding shadow memory is poisoned with the same valu=
e.
>
> 5. Compiler instrumentation is used to insert tag checks. Either by
>    calling callbacks or by inlining them (CONFIG_KASAN_OUTLINE and
>    CONFIG_KASAN_INLINE flags are reused).
>
> 6. When a tag mismatch is detected in callback instrumentation mode
>    KASAN simply prints a bug report. In case of inline instrumentation,
>    clang inserts a brk instruction, and KASAN has it's own brk handler,
>    which reports the bug.
>
> 7. The memory in between slab objects is marked with a reserved tag, and
>    acts as a redzone.
>
> 8. When a slab object is freed it's marked with a reserved tag.
>
> Bug detection is imprecise for two reasons:
>
> 1. We won't catch some small out-of-bounds accesses, that fall into the
>    same shadow cell, as the last byte of a slab object.
>
> 2. We only have 1 byte to store tags, which means we have a 1/256
>    probability of a tag match for an incorrect access (actually even
>    slightly less due to reserved tag values).
>
> Despite that there's a particular type of bugs that tag-based KASAN can
> detect compared to generic KASAN: use-after-free after the object has bee=
n
> allocated by someone else.
>
>
> =3D=3D=3D=3D=3D=3D Testing
>
> Some kernel developers voiced a concern that changing the top byte of
> kernel pointers may lead to subtle bugs that are difficult to discover.
> To address this concern deliberate testing has been performed.
>
> It doesn't seem feasible to do some kind of static checking to find
> potential issues with pointer tagging, so a dynamic approach was taken.
> All pointer comparisons/subtractions have been instrumented in an LLVM
> compiler pass and a kernel module that would print a bug report whenever
> two pointers with different tags are being compared/subtracted (ignoring
> comparisons with NULL pointers and with pointers obtained by casting an
> error code to a pointer type) has been used. Then the kernel has been
> booted in QEMU and on an Odroid C2 board and syzkaller has been run.
>
> This yielded the following results.
>
> The two places that look interesting are:
>
> is_vmalloc_addr in include/linux/mm.h
> is_kernel_rodata in mm/util.c
>
> Here we compare a pointer with some fixed untagged values to make sure
> that the pointer lies in a particular part of the kernel address space.
> Since tag-based KASAN doesn't add tags to pointers that belong to rodata
> or vmalloc regions, this should work as is. To make sure debug checks to
> those two functions that check that the result doesn't change whether
> we operate on pointers with or without untagging has been added.
>
> A few other cases that don't look that interesting:
>
> Comparing pointers to achieve unique sorting order of pointee objects
> (e.g. sorting locks addresses before performing a double lock):
>
> tty_ldisc_lock_pair_timeout in drivers/tty/tty_ldisc.c
> pipe_double_lock in fs/pipe.c
> unix_state_double_lock in net/unix/af_unix.c
> lock_two_nondirectories in fs/inode.c
> mutex_lock_double in kernel/events/core.c
>
> ep_cmp_ffd in fs/eventpoll.c
> fsnotify_compare_groups fs/notify/mark.c
>
> Nothing needs to be done here, since the tags embedded into pointers
> don't change, so the sorting order would still be unique.
>
> Checks that a pointer belongs to some particular allocation:
>
> is_sibling_entry in lib/radix-tree.c
> object_is_on_stack in include/linux/sched/task_stack.h
>
> Nothing needs to be done here either, since two pointers can only belong
> to the same allocation if they have the same tag.
>
> Overall, since the kernel boots and works, there are no critical bugs.
> As for the rest, the traditional kernel testing way (use until fails) is
> the only one that looks feasible.
>
> Another point here is that tag-based KASAN is available under a separate
> config option that needs to be deliberately enabled. Even though it might
> be used in a "near-production" environment to find bugs that are not foun=
d
> during fuzzing or running tests, it is still a debug tool.
>
>
> =3D=3D=3D=3D=3D=3D Benchmarks
>
> The following numbers were collected on Odroid C2 board. Both generic and
> tag-based KASAN were used in inline instrumentation mode.
>
> Boot time [1]:
> * ~1.7 sec for clean kernel
> * ~5.0 sec for generic KASAN
> * ~5.0 sec for tag-based KASAN
>
> Network performance [2]:
> * 8.33 Gbits/sec for clean kernel
> * 3.17 Gbits/sec for generic KASAN
> * 2.85 Gbits/sec for tag-based KASAN
>
> Slab memory usage after boot [3]:
> * ~40 kb for clean kernel
> * ~105 kb (~260% overhead) for generic KASAN
> * ~47 kb (~20% overhead) for tag-based KASAN
>
> KASAN memory overhead consists of three main parts:
> 1. Increased slab memory usage due to redzones.
> 2. Shadow memory (the whole reserved once during boot).
> 3. Quaratine (grows gradually until some preset limit; the more the limit=
,
>    the more the chance to detect a use-after-free).
>
> Comparing tag-based vs generic KASAN for each of these points:
> 1. 20% vs 260% overhead.
> 2. 1/16th vs 1/8th of physical memory.
> 3. Tag-based KASAN doesn't require quarantine.
>
> [1] Time before the ext4 driver is initialized.
> [2] Measured as `iperf -s & iperf -c 127.0.0.1 -t 30`.
> [3] Measured as `cat /proc/meminfo | grep Slab`.
>
>
> =3D=3D=3D=3D=3D=3D Some notes
>
> A few notes:
>
> 1. The patchset can be found here:
>    https://github.com/xairy/kasan-prototype/tree/khwasan
>
> 2. Building requires a recent Clang version (7.0.0 or later).
>
> 3. Stack instrumentation is not supported yet and will be added later.
>
>
> =3D=3D=3D=3D=3D=3D Changes
>
> Changes in v9:
> - Fixed kasan_init_slab_obj() hook when KASAN is disabled.
> - Added assign_tag() function that preassigns tags for caches with
>   constructors.
> - Fixed KASAN_TAG_MASK redefinition in include/linux/mm.h vs
>   mm/kasan/kasan.h.
>
> Changes in v8:
> - Rebased onto 7876320f (4.19-rc4).
> - Renamed KHWASAN to software tag-based KASAN (see the top of the cover
>   letter for details).
> - Explicitly called tag-based KASAN a debug tool.
> - Reused kasan_init_slab_obj() callback to preassign tags to caches
>   without constructors, remove khwasan_preset_sl(u/a)b_tag().
> - Moved move obj_to_index to include/linux/slab_def.h from mm/slab.c.
> - Moved cache->s_mem untagging to alloc_slabmgmt() for SLAB.
> - Fixed check_memory_region() to correctly handle user memory accesses an=
d
>   size =3D=3D 0 case.
> - Merged __no_sanitize_hwaddress into __no_sanitize_address.
> - Defined KASAN_SET_TAG and KASAN_RESET_TAG macros for non KASAN builds t=
o
>   avoid duplication of __kimg_to_phys, _virt_addr_is_linear and
>   page_to_virt macros.
> - Fixed and simplified find_first_bad_addr for generic KASAN.
> - Use non symbolized example KASAN report in documentation.
> - Mention clang version requirements for both KASAN modes in the Kconfig
>   options and in the documentation.
> - Various small fixes.
>
> Version v7 got accidentally skipped.
>
> Changes in v6:
> - Rebased onto 050cdc6c (4.19-rc1+).
> - Added notes regarding patchset testing into the cover letter.
>
> Changes in v5:
> - Rebased onto 1ffaddd029 (4.18-rc8).
> - Preassign tags for objects from caches with constructors and
>   SLAB_TYPESAFE_BY_RCU caches.
> - Fix SLAB allocator support by untagging page->s_mem in
>   kasan_poison_slab().
> - Performed dynamic testing to find potential places where pointer taggin=
g
>   might result in bugs [1].
> - Clarified and fixed memory usage benchmarks in the cover letter.
> - Added a rationale for having KHWASAN to the cover letter.
>
> Changes in v4:
> - Fixed SPDX comment style in mm/kasan/kasan.h.
> - Fixed mm/kasan/kasan.h changes being included in a wrong patch.
> - Swapped "khwasan, arm64: fix up fault handling logic" and "khwasan: add
>   tag related helper functions" patches order.
> - Rebased onto 6f0d349d (4.18-rc2+).
>
> Changes in v3:
> - Minor documentation fixes.
> - Fixed CFLAGS variable name in KASAN makefile.
> - Added a "SPDX-License-Identifier: GPL-2.0" line to all source files
>   under mm/kasan.
> - Rebased onto 81e97f013 (4.18-rc1+).
>
> Changes in v2:
> - Changed kmalloc_large_node_hook to return tagged pointer instead of
>   using an output argument.
> - Fix checking whether -fsanitize=3Dhwaddress is supported by the compile=
r.
> - Removed duplication of -fno-builtin for KASAN and KHWASAN.
> - Removed {} block for one line for_each_possible_cpu loop.
> - Made set_track() static inline as it is used only in common.c.
> - Moved optimal_redzone() to common.c.
> - Fixed using tagged pointer for shadow calculation in
>   kasan_unpoison_shadow().
> - Restored setting cache->align in kasan_cache_create(), which was
>   accidentally lost.
> - Simplified __kasan_slab_free(), kasan_alloc_pages() and kasan_kmalloc()=
.
> - Removed tagging from kasan_kmalloc_large().
> - Added page_kasan_tag_reset() to kasan_poison_slab() and removed
>   !PageSlab() check from page_to_virt.
> - Reset pointer tag in _virt_addr_is_linear.
> - Set page tag for each page when multiple pages are allocated or freed.
> - Added a comment as to why we ignore cma allocated pages.
>
> Changes in v1:
> - Rebased onto 4.17-rc4.
> - Updated benchmarking stats.
> - Documented compiler version requirements, memory usage and slowdown.
> - Dropped kvm patches, as clang + arm64 + kvm is completely broken [1].
>
> Changes in RFC v3:
> - Renamed CONFIG_KASAN_CLASSIC and CONFIG_KASAN_TAGS to
>   CONFIG_KASAN_GENERIC and CONFIG_KASAN_HW respectively.
> - Switch to -fsanitize=3Dkernel-hwaddress instead of -fsanitize=3Dhwaddre=
ss.
> - Removed unnecessary excessive shadow initialization.
> - Removed khwasan_enabled flag (it=E2=80=99s not needed since KHWASAN is
>   initialized before any slab caches are used).
> - Split out kasan_report.c and khwasan_report.c from report.c.
> - Moved more common KASAN and KHWASAN functions to common.c.
> - Added tagging to pagealloc.
> - Rebased onto 4.17-rc1.
> - Temporarily dropped patch that adds kvm support (arm64 + kvm + clang
>   combo is broken right now [2]).
>
> Changes in RFC v2:
> - Removed explicit casts to u8 * for kasan_mem_to_shadow() calls.
> - Introduced KASAN_TCR_FLAGS for setting the TCR_TBI1 flag.
> - Added a comment regarding the non-atomic RMW sequence in
>   khwasan_random_tag().
> - Made all tag related functions accept const void *.
> - Untagged pointers in __kimg_to_phys, which is used by virt_to_phys.
> - Untagged pointers in show_ptr in fault handling logic.
> - Untagged pointers passed to KVM.
> - Added two reserved tag values: 0xFF and 0xFE.
> - Used the reserved tag 0xFF to disable validity checking (to resolve the
>   issue with pointer tag being lost after page_address + kmap usage).
> - Used the reserved tag 0xFE to mark redzones and freed objects.
> - Added mnemonics for esr manipulation in KHWASAN brk handler.
> - Added a comment about the -recover flag.
> - Some minor cleanups and fixes.
> - Rebased onto 3215b9d5 (4.16-rc6+).
> - Tested on real hardware (Odroid C2 board).
> - Added better benchmarks.
>
> [1] https://lkml.org/lkml/2018/7/18/765
> [2] https://lkml.org/lkml/2018/4/19/775
>
> Andrey Konovalov (20):
>   kasan, mm: change hooks signatures
>   kasan: move common generic and tag-based code to common.c
>   kasan: rename source files to reflect the new naming scheme
>   kasan: add CONFIG_KASAN_GENERIC and CONFIG_KASAN_SW_TAGS
>   kasan, arm64: adjust shadow size for tag-based mode
>   kasan: initialize shadow to 0xff for tag-based mode
>   kasan, arm64: untag address in __kimg_to_phys and _virt_addr_is_linear
>   kasan: add tag related helper functions
>   kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
>   mm: move obj_to_index to include/linux/slab_def.h
>   kasan, arm64: fix up fault handling logic
>   kasan, arm64: enable top byte ignore for the kernel
>   kasan, mm: perform untagged pointers comparison in krealloc
>   kasan: split out generic_report.c from report.c
>   kasan: add bug reporting routines for tag-based mode
>   kasan: add hooks implementation for tag-based mode
>   kasan, arm64: add brk handler for inline instrumentation
>   kasan, mm, arm64: tag non slab memory allocated via pagealloc
>   kasan: update documentation
>   kasan: add SPDX-License-Identifier mark to source files
>
>  Documentation/dev-tools/kasan.rst      | 232 +++++----
>  arch/arm64/Kconfig                     |   1 +
>  arch/arm64/Makefile                    |   2 +-
>  arch/arm64/include/asm/brk-imm.h       |   2 +
>  arch/arm64/include/asm/memory.h        |  36 +-
>  arch/arm64/include/asm/pgtable-hwdef.h |   1 +
>  arch/arm64/kernel/traps.c              |  68 ++-
>  arch/arm64/mm/fault.c                  |   3 +
>  arch/arm64/mm/kasan_init.c             |  18 +-
>  arch/arm64/mm/proc.S                   |   8 +-
>  include/linux/compiler-clang.h         |   5 +-
>  include/linux/kasan.h                  |  83 +++-
>  include/linux/mm.h                     |  29 ++
>  include/linux/page-flags-layout.h      |  10 +
>  include/linux/slab_def.h               |  13 +
>  lib/Kconfig.kasan                      |  87 +++-
>  mm/cma.c                               |  11 +
>  mm/kasan/Makefile                      |  15 +-
>  mm/kasan/{kasan.c =3D> common.c}         | 651 +++++++++----------------
>  mm/kasan/generic.c                     | 344 +++++++++++++
>  mm/kasan/generic_report.c              | 153 ++++++
>  mm/kasan/{kasan_init.c =3D> init.c}      |   1 +
>  mm/kasan/kasan.h                       |  83 +++-
>  mm/kasan/quarantine.c                  |   1 +
>  mm/kasan/report.c                      | 272 +++--------
>  mm/kasan/tags.c                        | 161 ++++++
>  mm/kasan/tags_report.c                 |  58 +++
>  mm/page_alloc.c                        |   1 +
>  mm/slab.c                              |  29 +-
>  mm/slab.h                              |   2 +-
>  mm/slab_common.c                       |   6 +-
>  mm/slub.c                              |  41 +-
>  scripts/Makefile.kasan                 |  27 +-
>  33 files changed, 1629 insertions(+), 825 deletions(-)
>  rename mm/kasan/{kasan.c =3D> common.c} (59%)
>  create mode 100644 mm/kasan/generic.c
>  create mode 100644 mm/kasan/generic_report.c
>  rename mm/kasan/{kasan_init.c =3D> init.c} (99%)
>  create mode 100644 mm/kasan/tags.c
>  create mode 100644 mm/kasan/tags_report.c
>
> --
> 2.19.0.444.g18242da7ef-goog
>
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-mm
Subject: Re: [PATCH v9 00/20] kasan: add software tag-based mode for arm64
Date: Mon, 24 Sep 2018 15:14:05 +0000
Message-ID: <CACT4Y+bvH7WXEgbTWYtzVt93E=fqW69iaXWOs6tVxD=x0vdNGQ () mail ! gmail ! com>
--------------------
On Fri, Sep 21, 2018 at 5:13 PM, Andrey Konovalov <andreyknvl@google.com> w=
rote:
> This patchset adds a new software tag-based mode to KASAN [1].
> (Initially this mode was called KHWASAN, but it got renamed,
>  see the naming rationale at the end of this section).

Reviewed-by: Dmitry Vyukov <dvyukov@google.com>

> The plan is to implement HWASan [2] for the kernel with the incentive,
> that it's going to have comparable to KASAN performance, but in the same
> time consume much less memory, trading that off for somewhat imprecise
> bug detection and being supported only for arm64.
>
> The underlying ideas of the approach used by software tag-based KASAN are=
:
>
> 1. By using the Top Byte Ignore (TBI) arm64 CPU feature, we can store
>    pointer tags in the top byte of each kernel pointer.
>
> 2. Using shadow memory, we can store memory tags for each chunk of kernel
>    memory.
>
> 3. On each memory allocation, we can generate a random tag, embed it into
>    the returned pointer and set the memory tags that correspond to this
>    chunk of memory to the same value.
>
> 4. By using compiler instrumentation, before each memory access we can ad=
d
>    a check that the pointer tag matches the tag of the memory that is bei=
ng
>    accessed.
>
> 5. On a tag mismatch we report an error.
>
> With this patchset the existing KASAN mode gets renamed to generic KASAN,
> with the word "generic" meaning that the implementation can be supported
> by any architecture as it is purely software.
>
> The new mode this patchset adds is called software tag-based KASAN. The
> word "tag-based" refers to the fact that this mode uses tags embedded int=
o
> the top byte of kernel pointers and the TBI arm64 CPU feature that allows
> to dereference such pointers. The word "software" here means that shadow
> memory manipulation and tag checking on pointer dereference is done in
> software. As it is the only tag-based implementation right now, "software
> tag-based" KASAN is sometimes referred to as simply "tag-based" in this
> patchset.
>
> A potential expansion of this mode is a hardware tag-based mode, which wo=
uld
> use hardware memory tagging support (announced by Arm [3]) instead of
> compiler instrumentation and manual shadow memory manipulation.
>
> Same as generic KASAN, software tag-based KASAN is strictly a debugging
> feature.
>
> [1] https://www.kernel.org/doc/html/latest/dev-tools/kasan.html
>
> [2] http://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.htm=
l
>
> [3] https://community.arm.com/processors/b/blog/posts/arm-a-profile-archi=
tecture-2018-developments-armv85a
>
>
> =3D=3D=3D=3D=3D=3D Rationale
>
> On mobile devices generic KASAN's memory usage is significant problem. On=
e
> of the main reasons to have tag-based KASAN is to be able to perform a
> similar set of checks as the generic one does, but with lower memory
> requirements.
>
> Comment from Vishwath Mohan <vishwath@google.com>:
>
> I don't have data on-hand, but anecdotally both ASAN and KASAN have prove=
n
> problematic to enable for environments that don't tolerate the increased
> memory pressure well. This includes,
> (a) Low-memory form factors - Wear, TV, Things, lower-tier phones like Go=
,
> (c) Connected components like Pixel's visual core [1].
>
> These are both places I'd love to have a low(er) memory footprint option =
at
> my disposal.
>
> Comment from Evgenii Stepanov <eugenis@google.com>:
>
> Looking at a live Android device under load, slab (according to
> /proc/meminfo) + kernel stack take 8-10% available RAM (~350MB). KASAN's
> overhead of 2x - 3x on top of it is not insignificant.
>
> Not having this overhead enables near-production use - ex. running
> KASAN/KHWASAN kernel on a personal, daily-use device to catch bugs that d=
o
> not reproduce in test configuration. These are the ones that often cost
> the most engineering time to track down.
>
> CPU overhead is bad, but generally tolerable. RAM is critical, in our
> experience. Once it gets low enough, OOM-killer makes your life miserable=
.
>
> [1] https://www.blog.google/products/pixel/pixel-visual-core-image-proces=
sing-and-machine-learning-pixel-2/
>
>
> =3D=3D=3D=3D=3D=3D Technical details
>
> Software tag-based KASAN mode is implemented in a very similar way to the
> generic one. This patchset essentially does the following:
>
> 1. TCR_TBI1 is set to enable Top Byte Ignore.
>
> 2. Shadow memory is used (with a different scale, 1:16, so each shadow
>    byte corresponds to 16 bytes of kernel memory) to store memory tags.
>
> 3. All slab objects are aligned to shadow scale, which is 16 bytes.
>
> 4. All pointers returned from the slab allocator are tagged with a random
>    tag and the corresponding shadow memory is poisoned with the same valu=
e.
>
> 5. Compiler instrumentation is used to insert tag checks. Either by
>    calling callbacks or by inlining them (CONFIG_KASAN_OUTLINE and
>    CONFIG_KASAN_INLINE flags are reused).
>
> 6. When a tag mismatch is detected in callback instrumentation mode
>    KASAN simply prints a bug report. In case of inline instrumentation,
>    clang inserts a brk instruction, and KASAN has it's own brk handler,
>    which reports the bug.
>
> 7. The memory in between slab objects is marked with a reserved tag, and
>    acts as a redzone.
>
> 8. When a slab object is freed it's marked with a reserved tag.
>
> Bug detection is imprecise for two reasons:
>
> 1. We won't catch some small out-of-bounds accesses, that fall into the
>    same shadow cell, as the last byte of a slab object.
>
> 2. We only have 1 byte to store tags, which means we have a 1/256
>    probability of a tag match for an incorrect access (actually even
>    slightly less due to reserved tag values).
>
> Despite that there's a particular type of bugs that tag-based KASAN can
> detect compared to generic KASAN: use-after-free after the object has bee=
n
> allocated by someone else.
>
>
> =3D=3D=3D=3D=3D=3D Testing
>
> Some kernel developers voiced a concern that changing the top byte of
> kernel pointers may lead to subtle bugs that are difficult to discover.
> To address this concern deliberate testing has been performed.
>
> It doesn't seem feasible to do some kind of static checking to find
> potential issues with pointer tagging, so a dynamic approach was taken.
> All pointer comparisons/subtractions have been instrumented in an LLVM
> compiler pass and a kernel module that would print a bug report whenever
> two pointers with different tags are being compared/subtracted (ignoring
> comparisons with NULL pointers and with pointers obtained by casting an
> error code to a pointer type) has been used. Then the kernel has been
> booted in QEMU and on an Odroid C2 board and syzkaller has been run.
>
> This yielded the following results.
>
> The two places that look interesting are:
>
> is_vmalloc_addr in include/linux/mm.h
> is_kernel_rodata in mm/util.c
>
> Here we compare a pointer with some fixed untagged values to make sure
> that the pointer lies in a particular part of the kernel address space.
> Since tag-based KASAN doesn't add tags to pointers that belong to rodata
> or vmalloc regions, this should work as is. To make sure debug checks to
> those two functions that check that the result doesn't change whether
> we operate on pointers with or without untagging has been added.
>
> A few other cases that don't look that interesting:
>
> Comparing pointers to achieve unique sorting order of pointee objects
> (e.g. sorting locks addresses before performing a double lock):
>
> tty_ldisc_lock_pair_timeout in drivers/tty/tty_ldisc.c
> pipe_double_lock in fs/pipe.c
> unix_state_double_lock in net/unix/af_unix.c
> lock_two_nondirectories in fs/inode.c
> mutex_lock_double in kernel/events/core.c
>
> ep_cmp_ffd in fs/eventpoll.c
> fsnotify_compare_groups fs/notify/mark.c
>
> Nothing needs to be done here, since the tags embedded into pointers
> don't change, so the sorting order would still be unique.
>
> Checks that a pointer belongs to some particular allocation:
>
> is_sibling_entry in lib/radix-tree.c
> object_is_on_stack in include/linux/sched/task_stack.h
>
> Nothing needs to be done here either, since two pointers can only belong
> to the same allocation if they have the same tag.
>
> Overall, since the kernel boots and works, there are no critical bugs.
> As for the rest, the traditional kernel testing way (use until fails) is
> the only one that looks feasible.
>
> Another point here is that tag-based KASAN is available under a separate
> config option that needs to be deliberately enabled. Even though it might
> be used in a "near-production" environment to find bugs that are not foun=
d
> during fuzzing or running tests, it is still a debug tool.
>
>
> =3D=3D=3D=3D=3D=3D Benchmarks
>
> The following numbers were collected on Odroid C2 board. Both generic and
> tag-based KASAN were used in inline instrumentation mode.
>
> Boot time [1]:
> * ~1.7 sec for clean kernel
> * ~5.0 sec for generic KASAN
> * ~5.0 sec for tag-based KASAN
>
> Network performance [2]:
> * 8.33 Gbits/sec for clean kernel
> * 3.17 Gbits/sec for generic KASAN
> * 2.85 Gbits/sec for tag-based KASAN
>
> Slab memory usage after boot [3]:
> * ~40 kb for clean kernel
> * ~105 kb (~260% overhead) for generic KASAN
> * ~47 kb (~20% overhead) for tag-based KASAN
>
> KASAN memory overhead consists of three main parts:
> 1. Increased slab memory usage due to redzones.
> 2. Shadow memory (the whole reserved once during boot).
> 3. Quaratine (grows gradually until some preset limit; the more the limit=
,
>    the more the chance to detect a use-after-free).
>
> Comparing tag-based vs generic KASAN for each of these points:
> 1. 20% vs 260% overhead.
> 2. 1/16th vs 1/8th of physical memory.
> 3. Tag-based KASAN doesn't require quarantine.
>
> [1] Time before the ext4 driver is initialized.
> [2] Measured as `iperf -s & iperf -c 127.0.0.1 -t 30`.
> [3] Measured as `cat /proc/meminfo | grep Slab`.
>
>
> =3D=3D=3D=3D=3D=3D Some notes
>
> A few notes:
>
> 1. The patchset can be found here:
>    https://github.com/xairy/kasan-prototype/tree/khwasan
>
> 2. Building requires a recent Clang version (7.0.0 or later).
>
> 3. Stack instrumentation is not supported yet and will be added later.
>
>
> =3D=3D=3D=3D=3D=3D Changes
>
> Changes in v9:
> - Fixed kasan_init_slab_obj() hook when KASAN is disabled.
> - Added assign_tag() function that preassigns tags for caches with
>   constructors.
> - Fixed KASAN_TAG_MASK redefinition in include/linux/mm.h vs
>   mm/kasan/kasan.h.
>
> Changes in v8:
> - Rebased onto 7876320f (4.19-rc4).
> - Renamed KHWASAN to software tag-based KASAN (see the top of the cover
>   letter for details).
> - Explicitly called tag-based KASAN a debug tool.
> - Reused kasan_init_slab_obj() callback to preassign tags to caches
>   without constructors, remove khwasan_preset_sl(u/a)b_tag().
> - Moved move obj_to_index to include/linux/slab_def.h from mm/slab.c.
> - Moved cache->s_mem untagging to alloc_slabmgmt() for SLAB.
> - Fixed check_memory_region() to correctly handle user memory accesses an=
d
>   size =3D=3D 0 case.
> - Merged __no_sanitize_hwaddress into __no_sanitize_address.
> - Defined KASAN_SET_TAG and KASAN_RESET_TAG macros for non KASAN builds t=
o
>   avoid duplication of __kimg_to_phys, _virt_addr_is_linear and
>   page_to_virt macros.
> - Fixed and simplified find_first_bad_addr for generic KASAN.
> - Use non symbolized example KASAN report in documentation.
> - Mention clang version requirements for both KASAN modes in the Kconfig
>   options and in the documentation.
> - Various small fixes.
>
> Version v7 got accidentally skipped.
>
> Changes in v6:
> - Rebased onto 050cdc6c (4.19-rc1+).
> - Added notes regarding patchset testing into the cover letter.
>
> Changes in v5:
> - Rebased onto 1ffaddd029 (4.18-rc8).
> - Preassign tags for objects from caches with constructors and
>   SLAB_TYPESAFE_BY_RCU caches.
> - Fix SLAB allocator support by untagging page->s_mem in
>   kasan_poison_slab().
> - Performed dynamic testing to find potential places where pointer taggin=
g
>   might result in bugs [1].
> - Clarified and fixed memory usage benchmarks in the cover letter.
> - Added a rationale for having KHWASAN to the cover letter.
>
> Changes in v4:
> - Fixed SPDX comment style in mm/kasan/kasan.h.
> - Fixed mm/kasan/kasan.h changes being included in a wrong patch.
> - Swapped "khwasan, arm64: fix up fault handling logic" and "khwasan: add
>   tag related helper functions" patches order.
> - Rebased onto 6f0d349d (4.18-rc2+).
>
> Changes in v3:
> - Minor documentation fixes.
> - Fixed CFLAGS variable name in KASAN makefile.
> - Added a "SPDX-License-Identifier: GPL-2.0" line to all source files
>   under mm/kasan.
> - Rebased onto 81e97f013 (4.18-rc1+).
>
> Changes in v2:
> - Changed kmalloc_large_node_hook to return tagged pointer instead of
>   using an output argument.
> - Fix checking whether -fsanitize=3Dhwaddress is supported by the compile=
r.
> - Removed duplication of -fno-builtin for KASAN and KHWASAN.
> - Removed {} block for one line for_each_possible_cpu loop.
> - Made set_track() static inline as it is used only in common.c.
> - Moved optimal_redzone() to common.c.
> - Fixed using tagged pointer for shadow calculation in
>   kasan_unpoison_shadow().
> - Restored setting cache->align in kasan_cache_create(), which was
>   accidentally lost.
> - Simplified __kasan_slab_free(), kasan_alloc_pages() and kasan_kmalloc()=
.
> - Removed tagging from kasan_kmalloc_large().
> - Added page_kasan_tag_reset() to kasan_poison_slab() and removed
>   !PageSlab() check from page_to_virt.
> - Reset pointer tag in _virt_addr_is_linear.
> - Set page tag for each page when multiple pages are allocated or freed.
> - Added a comment as to why we ignore cma allocated pages.
>
> Changes in v1:
> - Rebased onto 4.17-rc4.
> - Updated benchmarking stats.
> - Documented compiler version requirements, memory usage and slowdown.
> - Dropped kvm patches, as clang + arm64 + kvm is completely broken [1].
>
> Changes in RFC v3:
> - Renamed CONFIG_KASAN_CLASSIC and CONFIG_KASAN_TAGS to
>   CONFIG_KASAN_GENERIC and CONFIG_KASAN_HW respectively.
> - Switch to -fsanitize=3Dkernel-hwaddress instead of -fsanitize=3Dhwaddre=
ss.
> - Removed unnecessary excessive shadow initialization.
> - Removed khwasan_enabled flag (it=E2=80=99s not needed since KHWASAN is
>   initialized before any slab caches are used).
> - Split out kasan_report.c and khwasan_report.c from report.c.
> - Moved more common KASAN and KHWASAN functions to common.c.
> - Added tagging to pagealloc.
> - Rebased onto 4.17-rc1.
> - Temporarily dropped patch that adds kvm support (arm64 + kvm + clang
>   combo is broken right now [2]).
>
> Changes in RFC v2:
> - Removed explicit casts to u8 * for kasan_mem_to_shadow() calls.
> - Introduced KASAN_TCR_FLAGS for setting the TCR_TBI1 flag.
> - Added a comment regarding the non-atomic RMW sequence in
>   khwasan_random_tag().
> - Made all tag related functions accept const void *.
> - Untagged pointers in __kimg_to_phys, which is used by virt_to_phys.
> - Untagged pointers in show_ptr in fault handling logic.
> - Untagged pointers passed to KVM.
> - Added two reserved tag values: 0xFF and 0xFE.
> - Used the reserved tag 0xFF to disable validity checking (to resolve the
>   issue with pointer tag being lost after page_address + kmap usage).
> - Used the reserved tag 0xFE to mark redzones and freed objects.
> - Added mnemonics for esr manipulation in KHWASAN brk handler.
> - Added a comment about the -recover flag.
> - Some minor cleanups and fixes.
> - Rebased onto 3215b9d5 (4.16-rc6+).
> - Tested on real hardware (Odroid C2 board).
> - Added better benchmarks.
>
> [1] https://lkml.org/lkml/2018/7/18/765
> [2] https://lkml.org/lkml/2018/4/19/775
>
> Andrey Konovalov (20):
>   kasan, mm: change hooks signatures
>   kasan: move common generic and tag-based code to common.c
>   kasan: rename source files to reflect the new naming scheme
>   kasan: add CONFIG_KASAN_GENERIC and CONFIG_KASAN_SW_TAGS
>   kasan, arm64: adjust shadow size for tag-based mode
>   kasan: initialize shadow to 0xff for tag-based mode
>   kasan, arm64: untag address in __kimg_to_phys and _virt_addr_is_linear
>   kasan: add tag related helper functions
>   kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
>   mm: move obj_to_index to include/linux/slab_def.h
>   kasan, arm64: fix up fault handling logic
>   kasan, arm64: enable top byte ignore for the kernel
>   kasan, mm: perform untagged pointers comparison in krealloc
>   kasan: split out generic_report.c from report.c
>   kasan: add bug reporting routines for tag-based mode
>   kasan: add hooks implementation for tag-based mode
>   kasan, arm64: add brk handler for inline instrumentation
>   kasan, mm, arm64: tag non slab memory allocated via pagealloc
>   kasan: update documentation
>   kasan: add SPDX-License-Identifier mark to source files
>
>  Documentation/dev-tools/kasan.rst      | 232 +++++----
>  arch/arm64/Kconfig                     |   1 +
>  arch/arm64/Makefile                    |   2 +-
>  arch/arm64/include/asm/brk-imm.h       |   2 +
>  arch/arm64/include/asm/memory.h        |  36 +-
>  arch/arm64/include/asm/pgtable-hwdef.h |   1 +
>  arch/arm64/kernel/traps.c              |  68 ++-
>  arch/arm64/mm/fault.c                  |   3 +
>  arch/arm64/mm/kasan_init.c             |  18 +-
>  arch/arm64/mm/proc.S                   |   8 +-
>  include/linux/compiler-clang.h         |   5 +-
>  include/linux/kasan.h                  |  83 +++-
>  include/linux/mm.h                     |  29 ++
>  include/linux/page-flags-layout.h      |  10 +
>  include/linux/slab_def.h               |  13 +
>  lib/Kconfig.kasan                      |  87 +++-
>  mm/cma.c                               |  11 +
>  mm/kasan/Makefile                      |  15 +-
>  mm/kasan/{kasan.c =3D> common.c}         | 651 +++++++++----------------
>  mm/kasan/generic.c                     | 344 +++++++++++++
>  mm/kasan/generic_report.c              | 153 ++++++
>  mm/kasan/{kasan_init.c =3D> init.c}      |   1 +
>  mm/kasan/kasan.h                       |  83 +++-
>  mm/kasan/quarantine.c                  |   1 +
>  mm/kasan/report.c                      | 272 +++--------
>  mm/kasan/tags.c                        | 161 ++++++
>  mm/kasan/tags_report.c                 |  58 +++
>  mm/page_alloc.c                        |   1 +
>  mm/slab.c                              |  29 +-
>  mm/slab.h                              |   2 +-
>  mm/slab_common.c                       |   6 +-
>  mm/slub.c                              |  41 +-
>  scripts/Makefile.kasan                 |  27 +-
>  33 files changed, 1629 insertions(+), 825 deletions(-)
>  rename mm/kasan/{kasan.c =3D> common.c} (59%)
>  create mode 100644 mm/kasan/generic.c
>  create mode 100644 mm/kasan/generic_report.c
>  rename mm/kasan/{kasan_init.c =3D> init.c} (99%)
>  create mode 100644 mm/kasan/tags.c
>  create mode 100644 mm/kasan/tags_report.c
>
> --
> 2.19.0.444.g18242da7ef-goog
>

================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-kernel
Subject: Re: [PATCH v9 00/20] kasan: add software tag-based mode for arm64
Date: Mon, 24 Sep 2018 15:14:05 +0000
Message-ID: <CACT4Y+bvH7WXEgbTWYtzVt93E=fqW69iaXWOs6tVxD=x0vdNGQ () mail ! gmail ! com>
--------------------
On Fri, Sep 21, 2018 at 5:13 PM, Andrey Konovalov <andreyknvl@google.com> w=
rote:
> This patchset adds a new software tag-based mode to KASAN [1].
> (Initially this mode was called KHWASAN, but it got renamed,
>  see the naming rationale at the end of this section).

Reviewed-by: Dmitry Vyukov <dvyukov@google.com>

> The plan is to implement HWASan [2] for the kernel with the incentive,
> that it's going to have comparable to KASAN performance, but in the same
> time consume much less memory, trading that off for somewhat imprecise
> bug detection and being supported only for arm64.
>
> The underlying ideas of the approach used by software tag-based KASAN are=
:
>
> 1. By using the Top Byte Ignore (TBI) arm64 CPU feature, we can store
>    pointer tags in the top byte of each kernel pointer.
>
> 2. Using shadow memory, we can store memory tags for each chunk of kernel
>    memory.
>
> 3. On each memory allocation, we can generate a random tag, embed it into
>    the returned pointer and set the memory tags that correspond to this
>    chunk of memory to the same value.
>
> 4. By using compiler instrumentation, before each memory access we can ad=
d
>    a check that the pointer tag matches the tag of the memory that is bei=
ng
>    accessed.
>
> 5. On a tag mismatch we report an error.
>
> With this patchset the existing KASAN mode gets renamed to generic KASAN,
> with the word "generic" meaning that the implementation can be supported
> by any architecture as it is purely software.
>
> The new mode this patchset adds is called software tag-based KASAN. The
> word "tag-based" refers to the fact that this mode uses tags embedded int=
o
> the top byte of kernel pointers and the TBI arm64 CPU feature that allows
> to dereference such pointers. The word "software" here means that shadow
> memory manipulation and tag checking on pointer dereference is done in
> software. As it is the only tag-based implementation right now, "software
> tag-based" KASAN is sometimes referred to as simply "tag-based" in this
> patchset.
>
> A potential expansion of this mode is a hardware tag-based mode, which wo=
uld
> use hardware memory tagging support (announced by Arm [3]) instead of
> compiler instrumentation and manual shadow memory manipulation.
>
> Same as generic KASAN, software tag-based KASAN is strictly a debugging
> feature.
>
> [1] https://www.kernel.org/doc/html/latest/dev-tools/kasan.html
>
> [2] http://clang.llvm.org/docs/HardwareAssistedAddressSanitizerDesign.htm=
l
>
> [3] https://community.arm.com/processors/b/blog/posts/arm-a-profile-archi=
tecture-2018-developments-armv85a
>
>
> =3D=3D=3D=3D=3D=3D Rationale
>
> On mobile devices generic KASAN's memory usage is significant problem. On=
e
> of the main reasons to have tag-based KASAN is to be able to perform a
> similar set of checks as the generic one does, but with lower memory
> requirements.
>
> Comment from Vishwath Mohan <vishwath@google.com>:
>
> I don't have data on-hand, but anecdotally both ASAN and KASAN have prove=
n
> problematic to enable for environments that don't tolerate the increased
> memory pressure well. This includes,
> (a) Low-memory form factors - Wear, TV, Things, lower-tier phones like Go=
,
> (c) Connected components like Pixel's visual core [1].
>
> These are both places I'd love to have a low(er) memory footprint option =
at
> my disposal.
>
> Comment from Evgenii Stepanov <eugenis@google.com>:
>
> Looking at a live Android device under load, slab (according to
> /proc/meminfo) + kernel stack take 8-10% available RAM (~350MB). KASAN's
> overhead of 2x - 3x on top of it is not insignificant.
>
> Not having this overhead enables near-production use - ex. running
> KASAN/KHWASAN kernel on a personal, daily-use device to catch bugs that d=
o
> not reproduce in test configuration. These are the ones that often cost
> the most engineering time to track down.
>
> CPU overhead is bad, but generally tolerable. RAM is critical, in our
> experience. Once it gets low enough, OOM-killer makes your life miserable=
.
>
> [1] https://www.blog.google/products/pixel/pixel-visual-core-image-proces=
sing-and-machine-learning-pixel-2/
>
>
> =3D=3D=3D=3D=3D=3D Technical details
>
> Software tag-based KASAN mode is implemented in a very similar way to the
> generic one. This patchset essentially does the following:
>
> 1. TCR_TBI1 is set to enable Top Byte Ignore.
>
> 2. Shadow memory is used (with a different scale, 1:16, so each shadow
>    byte corresponds to 16 bytes of kernel memory) to store memory tags.
>
> 3. All slab objects are aligned to shadow scale, which is 16 bytes.
>
> 4. All pointers returned from the slab allocator are tagged with a random
>    tag and the corresponding shadow memory is poisoned with the same valu=
e.
>
> 5. Compiler instrumentation is used to insert tag checks. Either by
>    calling callbacks or by inlining them (CONFIG_KASAN_OUTLINE and
>    CONFIG_KASAN_INLINE flags are reused).
>
> 6. When a tag mismatch is detected in callback instrumentation mode
>    KASAN simply prints a bug report. In case of inline instrumentation,
>    clang inserts a brk instruction, and KASAN has it's own brk handler,
>    which reports the bug.
>
> 7. The memory in between slab objects is marked with a reserved tag, and
>    acts as a redzone.
>
> 8. When a slab object is freed it's marked with a reserved tag.
>
> Bug detection is imprecise for two reasons:
>
> 1. We won't catch some small out-of-bounds accesses, that fall into the
>    same shadow cell, as the last byte of a slab object.
>
> 2. We only have 1 byte to store tags, which means we have a 1/256
>    probability of a tag match for an incorrect access (actually even
>    slightly less due to reserved tag values).
>
> Despite that there's a particular type of bugs that tag-based KASAN can
> detect compared to generic KASAN: use-after-free after the object has bee=
n
> allocated by someone else.
>
>
> =3D=3D=3D=3D=3D=3D Testing
>
> Some kernel developers voiced a concern that changing the top byte of
> kernel pointers may lead to subtle bugs that are difficult to discover.
> To address this concern deliberate testing has been performed.
>
> It doesn't seem feasible to do some kind of static checking to find
> potential issues with pointer tagging, so a dynamic approach was taken.
> All pointer comparisons/subtractions have been instrumented in an LLVM
> compiler pass and a kernel module that would print a bug report whenever
> two pointers with different tags are being compared/subtracted (ignoring
> comparisons with NULL pointers and with pointers obtained by casting an
> error code to a pointer type) has been used. Then the kernel has been
> booted in QEMU and on an Odroid C2 board and syzkaller has been run.
>
> This yielded the following results.
>
> The two places that look interesting are:
>
> is_vmalloc_addr in include/linux/mm.h
> is_kernel_rodata in mm/util.c
>
> Here we compare a pointer with some fixed untagged values to make sure
> that the pointer lies in a particular part of the kernel address space.
> Since tag-based KASAN doesn't add tags to pointers that belong to rodata
> or vmalloc regions, this should work as is. To make sure debug checks to
> those two functions that check that the result doesn't change whether
> we operate on pointers with or without untagging has been added.
>
> A few other cases that don't look that interesting:
>
> Comparing pointers to achieve unique sorting order of pointee objects
> (e.g. sorting locks addresses before performing a double lock):
>
> tty_ldisc_lock_pair_timeout in drivers/tty/tty_ldisc.c
> pipe_double_lock in fs/pipe.c
> unix_state_double_lock in net/unix/af_unix.c
> lock_two_nondirectories in fs/inode.c
> mutex_lock_double in kernel/events/core.c
>
> ep_cmp_ffd in fs/eventpoll.c
> fsnotify_compare_groups fs/notify/mark.c
>
> Nothing needs to be done here, since the tags embedded into pointers
> don't change, so the sorting order would still be unique.
>
> Checks that a pointer belongs to some particular allocation:
>
> is_sibling_entry in lib/radix-tree.c
> object_is_on_stack in include/linux/sched/task_stack.h
>
> Nothing needs to be done here either, since two pointers can only belong
> to the same allocation if they have the same tag.
>
> Overall, since the kernel boots and works, there are no critical bugs.
> As for the rest, the traditional kernel testing way (use until fails) is
> the only one that looks feasible.
>
> Another point here is that tag-based KASAN is available under a separate
> config option that needs to be deliberately enabled. Even though it might
> be used in a "near-production" environment to find bugs that are not foun=
d
> during fuzzing or running tests, it is still a debug tool.
>
>
> =3D=3D=3D=3D=3D=3D Benchmarks
>
> The following numbers were collected on Odroid C2 board. Both generic and
> tag-based KASAN were used in inline instrumentation mode.
>
> Boot time [1]:
> * ~1.7 sec for clean kernel
> * ~5.0 sec for generic KASAN
> * ~5.0 sec for tag-based KASAN
>
> Network performance [2]:
> * 8.33 Gbits/sec for clean kernel
> * 3.17 Gbits/sec for generic KASAN
> * 2.85 Gbits/sec for tag-based KASAN
>
> Slab memory usage after boot [3]:
> * ~40 kb for clean kernel
> * ~105 kb (~260% overhead) for generic KASAN
> * ~47 kb (~20% overhead) for tag-based KASAN
>
> KASAN memory overhead consists of three main parts:
> 1. Increased slab memory usage due to redzones.
> 2. Shadow memory (the whole reserved once during boot).
> 3. Quaratine (grows gradually until some preset limit; the more the limit=
,
>    the more the chance to detect a use-after-free).
>
> Comparing tag-based vs generic KASAN for each of these points:
> 1. 20% vs 260% overhead.
> 2. 1/16th vs 1/8th of physical memory.
> 3. Tag-based KASAN doesn't require quarantine.
>
> [1] Time before the ext4 driver is initialized.
> [2] Measured as `iperf -s & iperf -c 127.0.0.1 -t 30`.
> [3] Measured as `cat /proc/meminfo | grep Slab`.
>
>
> =3D=3D=3D=3D=3D=3D Some notes
>
> A few notes:
>
> 1. The patchset can be found here:
>    https://github.com/xairy/kasan-prototype/tree/khwasan
>
> 2. Building requires a recent Clang version (7.0.0 or later).
>
> 3. Stack instrumentation is not supported yet and will be added later.
>
>
> =3D=3D=3D=3D=3D=3D Changes
>
> Changes in v9:
> - Fixed kasan_init_slab_obj() hook when KASAN is disabled.
> - Added assign_tag() function that preassigns tags for caches with
>   constructors.
> - Fixed KASAN_TAG_MASK redefinition in include/linux/mm.h vs
>   mm/kasan/kasan.h.
>
> Changes in v8:
> - Rebased onto 7876320f (4.19-rc4).
> - Renamed KHWASAN to software tag-based KASAN (see the top of the cover
>   letter for details).
> - Explicitly called tag-based KASAN a debug tool.
> - Reused kasan_init_slab_obj() callback to preassign tags to caches
>   without constructors, remove khwasan_preset_sl(u/a)b_tag().
> - Moved move obj_to_index to include/linux/slab_def.h from mm/slab.c.
> - Moved cache->s_mem untagging to alloc_slabmgmt() for SLAB.
> - Fixed check_memory_region() to correctly handle user memory accesses an=
d
>   size =3D=3D 0 case.
> - Merged __no_sanitize_hwaddress into __no_sanitize_address.
> - Defined KASAN_SET_TAG and KASAN_RESET_TAG macros for non KASAN builds t=
o
>   avoid duplication of __kimg_to_phys, _virt_addr_is_linear and
>   page_to_virt macros.
> - Fixed and simplified find_first_bad_addr for generic KASAN.
> - Use non symbolized example KASAN report in documentation.
> - Mention clang version requirements for both KASAN modes in the Kconfig
>   options and in the documentation.
> - Various small fixes.
>
> Version v7 got accidentally skipped.
>
> Changes in v6:
> - Rebased onto 050cdc6c (4.19-rc1+).
> - Added notes regarding patchset testing into the cover letter.
>
> Changes in v5:
> - Rebased onto 1ffaddd029 (4.18-rc8).
> - Preassign tags for objects from caches with constructors and
>   SLAB_TYPESAFE_BY_RCU caches.
> - Fix SLAB allocator support by untagging page->s_mem in
>   kasan_poison_slab().
> - Performed dynamic testing to find potential places where pointer taggin=
g
>   might result in bugs [1].
> - Clarified and fixed memory usage benchmarks in the cover letter.
> - Added a rationale for having KHWASAN to the cover letter.
>
> Changes in v4:
> - Fixed SPDX comment style in mm/kasan/kasan.h.
> - Fixed mm/kasan/kasan.h changes being included in a wrong patch.
> - Swapped "khwasan, arm64: fix up fault handling logic" and "khwasan: add
>   tag related helper functions" patches order.
> - Rebased onto 6f0d349d (4.18-rc2+).
>
> Changes in v3:
> - Minor documentation fixes.
> - Fixed CFLAGS variable name in KASAN makefile.
> - Added a "SPDX-License-Identifier: GPL-2.0" line to all source files
>   under mm/kasan.
> - Rebased onto 81e97f013 (4.18-rc1+).
>
> Changes in v2:
> - Changed kmalloc_large_node_hook to return tagged pointer instead of
>   using an output argument.
> - Fix checking whether -fsanitize=3Dhwaddress is supported by the compile=
r.
> - Removed duplication of -fno-builtin for KASAN and KHWASAN.
> - Removed {} block for one line for_each_possible_cpu loop.
> - Made set_track() static inline as it is used only in common.c.
> - Moved optimal_redzone() to common.c.
> - Fixed using tagged pointer for shadow calculation in
>   kasan_unpoison_shadow().
> - Restored setting cache->align in kasan_cache_create(), which was
>   accidentally lost.
> - Simplified __kasan_slab_free(), kasan_alloc_pages() and kasan_kmalloc()=
.
> - Removed tagging from kasan_kmalloc_large().
> - Added page_kasan_tag_reset() to kasan_poison_slab() and removed
>   !PageSlab() check from page_to_virt.
> - Reset pointer tag in _virt_addr_is_linear.
> - Set page tag for each page when multiple pages are allocated or freed.
> - Added a comment as to why we ignore cma allocated pages.
>
> Changes in v1:
> - Rebased onto 4.17-rc4.
> - Updated benchmarking stats.
> - Documented compiler version requirements, memory usage and slowdown.
> - Dropped kvm patches, as clang + arm64 + kvm is completely broken [1].
>
> Changes in RFC v3:
> - Renamed CONFIG_KASAN_CLASSIC and CONFIG_KASAN_TAGS to
>   CONFIG_KASAN_GENERIC and CONFIG_KASAN_HW respectively.
> - Switch to -fsanitize=3Dkernel-hwaddress instead of -fsanitize=3Dhwaddre=
ss.
> - Removed unnecessary excessive shadow initialization.
> - Removed khwasan_enabled flag (it=E2=80=99s not needed since KHWASAN is
>   initialized before any slab caches are used).
> - Split out kasan_report.c and khwasan_report.c from report.c.
> - Moved more common KASAN and KHWASAN functions to common.c.
> - Added tagging to pagealloc.
> - Rebased onto 4.17-rc1.
> - Temporarily dropped patch that adds kvm support (arm64 + kvm + clang
>   combo is broken right now [2]).
>
> Changes in RFC v2:
> - Removed explicit casts to u8 * for kasan_mem_to_shadow() calls.
> - Introduced KASAN_TCR_FLAGS for setting the TCR_TBI1 flag.
> - Added a comment regarding the non-atomic RMW sequence in
>   khwasan_random_tag().
> - Made all tag related functions accept const void *.
> - Untagged pointers in __kimg_to_phys, which is used by virt_to_phys.
> - Untagged pointers in show_ptr in fault handling logic.
> - Untagged pointers passed to KVM.
> - Added two reserved tag values: 0xFF and 0xFE.
> - Used the reserved tag 0xFF to disable validity checking (to resolve the
>   issue with pointer tag being lost after page_address + kmap usage).
> - Used the reserved tag 0xFE to mark redzones and freed objects.
> - Added mnemonics for esr manipulation in KHWASAN brk handler.
> - Added a comment about the -recover flag.
> - Some minor cleanups and fixes.
> - Rebased onto 3215b9d5 (4.16-rc6+).
> - Tested on real hardware (Odroid C2 board).
> - Added better benchmarks.
>
> [1] https://lkml.org/lkml/2018/7/18/765
> [2] https://lkml.org/lkml/2018/4/19/775
>
> Andrey Konovalov (20):
>   kasan, mm: change hooks signatures
>   kasan: move common generic and tag-based code to common.c
>   kasan: rename source files to reflect the new naming scheme
>   kasan: add CONFIG_KASAN_GENERIC and CONFIG_KASAN_SW_TAGS
>   kasan, arm64: adjust shadow size for tag-based mode
>   kasan: initialize shadow to 0xff for tag-based mode
>   kasan, arm64: untag address in __kimg_to_phys and _virt_addr_is_linear
>   kasan: add tag related helper functions
>   kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
>   mm: move obj_to_index to include/linux/slab_def.h
>   kasan, arm64: fix up fault handling logic
>   kasan, arm64: enable top byte ignore for the kernel
>   kasan, mm: perform untagged pointers comparison in krealloc
>   kasan: split out generic_report.c from report.c
>   kasan: add bug reporting routines for tag-based mode
>   kasan: add hooks implementation for tag-based mode
>   kasan, arm64: add brk handler for inline instrumentation
>   kasan, mm, arm64: tag non slab memory allocated via pagealloc
>   kasan: update documentation
>   kasan: add SPDX-License-Identifier mark to source files
>
>  Documentation/dev-tools/kasan.rst      | 232 +++++----
>  arch/arm64/Kconfig                     |   1 +
>  arch/arm64/Makefile                    |   2 +-
>  arch/arm64/include/asm/brk-imm.h       |   2 +
>  arch/arm64/include/asm/memory.h        |  36 +-
>  arch/arm64/include/asm/pgtable-hwdef.h |   1 +
>  arch/arm64/kernel/traps.c              |  68 ++-
>  arch/arm64/mm/fault.c                  |   3 +
>  arch/arm64/mm/kasan_init.c             |  18 +-
>  arch/arm64/mm/proc.S                   |   8 +-
>  include/linux/compiler-clang.h         |   5 +-
>  include/linux/kasan.h                  |  83 +++-
>  include/linux/mm.h                     |  29 ++
>  include/linux/page-flags-layout.h      |  10 +
>  include/linux/slab_def.h               |  13 +
>  lib/Kconfig.kasan                      |  87 +++-
>  mm/cma.c                               |  11 +
>  mm/kasan/Makefile                      |  15 +-
>  mm/kasan/{kasan.c =3D> common.c}         | 651 +++++++++----------------
>  mm/kasan/generic.c                     | 344 +++++++++++++
>  mm/kasan/generic_report.c              | 153 ++++++
>  mm/kasan/{kasan_init.c =3D> init.c}      |   1 +
>  mm/kasan/kasan.h                       |  83 +++-
>  mm/kasan/quarantine.c                  |   1 +
>  mm/kasan/report.c                      | 272 +++--------
>  mm/kasan/tags.c                        | 161 ++++++
>  mm/kasan/tags_report.c                 |  58 +++
>  mm/page_alloc.c                        |   1 +
>  mm/slab.c                              |  29 +-
>  mm/slab.h                              |   2 +-
>  mm/slab_common.c                       |   6 +-
>  mm/slub.c                              |  41 +-
>  scripts/Makefile.kasan                 |  27 +-
>  33 files changed, 1629 insertions(+), 825 deletions(-)
>  rename mm/kasan/{kasan.c =3D> common.c} (59%)
>  create mode 100644 mm/kasan/generic.c
>  create mode 100644 mm/kasan/generic_report.c
>  rename mm/kasan/{kasan_init.c =3D> init.c} (99%)
>  create mode 100644 mm/kasan/tags.c
>  create mode 100644 mm/kasan/tags_report.c
>
> --
> 2.19.0.444.g18242da7ef-goog
>
================================================================================

From: Dmitry Vyukov <dvyukov () google ! com>
To: linux-arm-kernel
Subject: Re: [PATCH v9 00/20] kasan: add software tag-based mode for arm64
Date: Mon, 24 Sep 2018 15:14:05 +0000
Message-ID: <CACT4Y+bvH7WXEgbTWYtzVt93E=fqW69iaXWOs6tVxD=x0vdNGQ () mail ! gmail ! com>
--------------------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================================================================================


################################################################################

=== Thread: [PATCH v9 04/20] kasan: add CONFIG_KASAN_GENERIC and CONFIG_KASAN_SW_TAGS ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 04/20] kasan: add CONFIG_KASAN_GENERIC and CONFIG_KASAN_SW_TAGS
Date: Fri, 21 Sep 2018 15:13:26 +0000
Message-ID: <b2010da9f857532ee2dbe64c27a460cfaf28e562.1537542735.git.andreyknvl () google ! com>
--------------------
This commit splits the current CONFIG_KASAN config option into two:
1. CONFIG_KASAN_GENERIC, that enables the generic KASAN mode (the one
   that exists now);
2. CONFIG_KASAN_SW_TAGS, that enables the software tag-based KASAN mode.

The name CONFIG_KASAN_SW_TAGS is chosen as in the future we will have
another hardware tag-based KASAN mode, that will rely on hardware memory
tagging support in arm64.

With CONFIG_KASAN_SW_TAGS enabled, compiler options are changed to
instrument kernel files with -fsantize=kernel-hwaddress (except the ones
for which KASAN_SANITIZE := n is set).

Both CONFIG_KASAN_GENERIC and CONFIG_KASAN_SW_TAGS support both
CONFIG_KASAN_INLINE and CONFIG_KASAN_OUTLINE instrumentation modes.

This commit also adds empty placeholder (for now) implementation of
tag-based KASAN specific hooks inserted by the compiler and adjusts
common hooks implementation to compile correctly with each of the
config options.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/Kconfig             |  1 +
 include/linux/compiler-clang.h |  5 +-
 include/linux/kasan.h          | 16 +++++--
 lib/Kconfig.kasan              | 87 +++++++++++++++++++++++++++-------
 mm/kasan/Makefile              |  6 ++-
 mm/kasan/generic.c             |  2 +-
 mm/kasan/kasan.h               |  3 +-
 mm/kasan/tags.c                | 75 +++++++++++++++++++++++++++++
 mm/slub.c                      |  2 +-
 scripts/Makefile.kasan         | 27 ++++++++++-
 10 files changed, 194 insertions(+), 30 deletions(-)
 create mode 100644 mm/kasan/tags.c

diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index 1b1a0e95c751..287c32241b68 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -105,6 +105,7 @@ config ARM64
 	select HAVE_ARCH_HUGE_VMAP
 	select HAVE_ARCH_JUMP_LABEL
 	select HAVE_ARCH_KASAN if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
+	select HAVE_ARCH_KASAN_SW_TAGS if !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
 	select HAVE_ARCH_KGDB
 	select HAVE_ARCH_MMAP_RND_BITS
 	select HAVE_ARCH_MMAP_RND_COMPAT_BITS if COMPAT
diff --git a/include/linux/compiler-clang.h b/include/linux/compiler-clang.h
index b1ce500fe8b3..89ebe58259ba 100644
--- a/include/linux/compiler-clang.h
+++ b/include/linux/compiler-clang.h
@@ -17,11 +17,12 @@
 #define KASAN_ABI_VERSION 5
 
 /* emulate gcc's __SANITIZE_ADDRESS__ flag */
-#if __has_feature(address_sanitizer)
+#if __has_feature(address_sanitizer) || __has_feature(hwaddress_sanitizer)
 #define __SANITIZE_ADDRESS__
 #endif
 
-#define __no_sanitize_address __attribute__((no_sanitize("address")))
+#define __no_sanitize_address	\
+	__attribute__((no_sanitize("address", "hwaddress")))
 
 /*
  * Not all versions of clang implement the the type-generic versions
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index 52c86a568a4e..b66fdf5ea7ab 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -45,8 +45,6 @@ void kasan_free_pages(struct page *page, unsigned int order);
 
 void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 			slab_flags_t *flags);
-void kasan_cache_shrink(struct kmem_cache *cache);
-void kasan_cache_shutdown(struct kmem_cache *cache);
 
 void kasan_poison_slab(struct page *page);
 void kasan_unpoison_object_data(struct kmem_cache *cache, void *object);
@@ -97,8 +95,6 @@ static inline void kasan_free_pages(struct page *page, unsigned int order) {}
 static inline void kasan_cache_create(struct kmem_cache *cache,
 				      unsigned int *size,
 				      slab_flags_t *flags) {}
-static inline void kasan_cache_shrink(struct kmem_cache *cache) {}
-static inline void kasan_cache_shutdown(struct kmem_cache *cache) {}
 
 static inline void kasan_poison_slab(struct page *page) {}
 static inline void kasan_unpoison_object_data(struct kmem_cache *cache,
@@ -155,4 +151,16 @@ static inline size_t kasan_metadata_size(struct kmem_cache *cache) { return 0; }
 
 #endif /* CONFIG_KASAN */
 
+#ifdef CONFIG_KASAN_GENERIC
+
+void kasan_cache_shrink(struct kmem_cache *cache);
+void kasan_cache_shutdown(struct kmem_cache *cache);
+
+#else /* CONFIG_KASAN_GENERIC */
+
+static inline void kasan_cache_shrink(struct kmem_cache *cache) {}
+static inline void kasan_cache_shutdown(struct kmem_cache *cache) {}
+
+#endif /* CONFIG_KASAN_GENERIC */
+
 #endif /* LINUX_KASAN_H */
diff --git a/lib/Kconfig.kasan b/lib/Kconfig.kasan
index befb127507c0..181dfa7f4885 100644
--- a/lib/Kconfig.kasan
+++ b/lib/Kconfig.kasan
@@ -1,35 +1,86 @@
 config HAVE_ARCH_KASAN
 	bool
 
+config HAVE_ARCH_KASAN_SW_TAGS
+	bool
+
 if HAVE_ARCH_KASAN
 
 config KASAN
-	bool "KASan: runtime memory debugger"
+	bool "KASAN: runtime memory debugger"
+	help
+	  Enables KASAN (KernelAddressSANitizer) - runtime memory debugger,
+	  designed to find out-of-bounds accesses and use-after-free bugs.
+	  See Documentation/dev-tools/kasan.rst for details.
+
+choice
+	prompt "KASAN mode"
+	depends on KASAN
+	default KASAN_GENERIC
+	help
+	  KASAN has two modes: generic KASAN (similar to userspace ASan,
+	  x86_64/arm64/xtensa, enabled with CONFIG_KASAN_GENERIC) and
+	  software tag-based KASAN (a version based on software memory
+	  tagging, arm64 only, similar to userspace HWASan, enabled with
+	  CONFIG_KASAN_SW_TAGS).
+	  Both generic and tag-based KASAN are strictly debugging features.
+
+config KASAN_GENERIC
+	bool "Generic mode"
 	depends on (SLUB && SYSFS) || (SLAB && !DEBUG_SLAB)
 	select SLUB_DEBUG if SLUB
 	select CONSTRUCTORS
 	select STACKDEPOT
 	help
-	  Enables kernel address sanitizer - runtime memory debugger,
-	  designed to find out-of-bounds accesses and use-after-free bugs.
-	  This is strictly a debugging feature and it requires a gcc version
-	  of 4.9.2 or later. Detection of out of bounds accesses to stack or
-	  global variables requires gcc 5.0 or later.
-	  This feature consumes about 1/8 of available memory and brings about
-	  ~x3 performance slowdown.
+	  Enables generic KASAN mode.
+	  Supported in both GCC and Clang. With GCC it requires version 4.9.2
+	  or later for basic support and version 5.0 or later for detection of
+	  out-of-bounds accesses for stack and global variables and for inline
+	  instrumentation mode (CONFIG_KASAN_INLINE). With Clang it requires
+	  version 3.7.0 or later and it doesn't support detection of
+	  out-of-bounds accesses for global variables yet.
+	  This mode consumes about 1/8th of available memory at kernel start
+	  and introduces an overhead of ~x1.5 for the rest of the allocations.
+	  The performance slowdown is ~x3.
 	  For better error detection enable CONFIG_STACKTRACE.
-	  Currently CONFIG_KASAN doesn't work with CONFIG_DEBUG_SLAB
+	  Currently CONFIG_KASAN_GENERIC doesn't work with CONFIG_DEBUG_SLAB
 	  (the resulting kernel does not boot).
 
+if HAVE_ARCH_KASAN_SW_TAGS
+
+config KASAN_SW_TAGS
+	bool "Software tag-based mode"
+	depends on (SLUB && SYSFS) || (SLAB && !DEBUG_SLAB)
+	select SLUB_DEBUG if SLUB
+	select CONSTRUCTORS
+	select STACKDEPOT
+	help
+	  Enables software tag-based KASAN mode.
+	  This mode requires Top Byte Ignore support by the CPU and therefore
+	  is only supported for arm64.
+	  This mode requires Clang version 7.0.0 or later.
+	  This mode consumes about 1/16th of available memory at kernel start
+	  and introduces an overhead of ~20% for the rest of the allocations.
+	  This mode may potentially introduce problems relating to pointer
+	  casting and comparison, as it embeds tags into the top byte of each
+	  pointer.
+	  For better error detection enable CONFIG_STACKTRACE.
+	  Currently CONFIG_KASAN_SW_TAGS doesn't work with CONFIG_DEBUG_SLAB
+	  (the resulting kernel does not boot).
+
+endif
+
+endchoice
+
 config KASAN_EXTRA
-	bool "KAsan: extra checks"
-	depends on KASAN && DEBUG_KERNEL && !COMPILE_TEST
+	bool "KASAN: extra checks"
+	depends on KASAN_GENERIC && DEBUG_KERNEL && !COMPILE_TEST
 	help
-	  This enables further checks in the kernel address sanitizer, for now
-	  it only includes the address-use-after-scope check that can lead
-	  to excessive kernel stack usage, frame size warnings and longer
+	  This enables further checks in generic KASAN, for now it only
+	  includes the address-use-after-scope check that can lead to
+	  excessive kernel stack usage, frame size warnings and longer
 	  compile time.
-	  https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81715 has more
+	  See https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81715
 
 
 choice
@@ -53,16 +104,16 @@ config KASAN_INLINE
 	  memory accesses. This is faster than outline (in some workloads
 	  it gives about x2 boost over outline instrumentation), but
 	  make kernel's .text size much bigger.
-	  This requires a gcc version of 5.0 or later.
+	  For CONFIG_KASAN_GENERIC this requires GCC 5.0 or later.
 
 endchoice
 
 config TEST_KASAN
-	tristate "Module for testing kasan for bug detection"
+	tristate "Module for testing KASAN for bug detection"
 	depends on m && KASAN
 	help
 	  This is a test module doing various nasty things like
 	  out of bounds accesses, use after free. It is useful for testing
-	  kernel debugging features like kernel address sanitizer.
+	  kernel debugging features like KASAN.
 
 endif
diff --git a/mm/kasan/Makefile b/mm/kasan/Makefile
index d643530b24aa..68ba1822f003 100644
--- a/mm/kasan/Makefile
+++ b/mm/kasan/Makefile
@@ -2,6 +2,7 @@
 KASAN_SANITIZE := n
 UBSAN_SANITIZE_common.o := n
 UBSAN_SANITIZE_generic.o := n
+UBSAN_SANITIZE_tags.o := n
 KCOV_INSTRUMENT := n
 
 CFLAGS_REMOVE_generic.o = -pg
@@ -10,5 +11,8 @@ CFLAGS_REMOVE_generic.o = -pg
 
 CFLAGS_common.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 CFLAGS_generic.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
+CFLAGS_tags.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 
-obj-y := common.o generic.o report.o init.o quarantine.o
+obj-$(CONFIG_KASAN) := common.o init.o report.o
+obj-$(CONFIG_KASAN_GENERIC) += generic.o quarantine.o
+obj-$(CONFIG_KASAN_SW_TAGS) += tags.o
diff --git a/mm/kasan/generic.c b/mm/kasan/generic.c
index 44ec228de0a2..b8de6d33c55c 100644
--- a/mm/kasan/generic.c
+++ b/mm/kasan/generic.c
@@ -1,5 +1,5 @@
 /*
- * This file contains core KASAN code.
+ * This file contains core generic KASAN code.
  *
  * Copyright (c) 2014 Samsung Electronics Co., Ltd.
  * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index 659463800f10..19b950eaccff 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -114,7 +114,8 @@ void kasan_report(unsigned long addr, size_t size,
 		bool is_write, unsigned long ip);
 void kasan_report_invalid_free(void *object, unsigned long ip);
 
-#if defined(CONFIG_SLAB) || defined(CONFIG_SLUB)
+#if defined(CONFIG_KASAN_GENERIC) && \
+	(defined(CONFIG_SLAB) || defined(CONFIG_SLUB))
 void quarantine_put(struct kasan_free_meta *info, struct kmem_cache *cache);
 void quarantine_reduce(void);
 void quarantine_remove_cache(struct kmem_cache *cache);
diff --git a/mm/kasan/tags.c b/mm/kasan/tags.c
new file mode 100644
index 000000000000..04194923c543
--- /dev/null
+++ b/mm/kasan/tags.c
@@ -0,0 +1,75 @@
+/*
+ * This file contains core tag-based KASAN code.
+ *
+ * Copyright (c) 2018 Google, Inc.
+ * Author: Andrey Konovalov <andreyknvl@google.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+#define DISABLE_BRANCH_PROFILING
+
+#include <linux/export.h>
+#include <linux/interrupt.h>
+#include <linux/init.h>
+#include <linux/kasan.h>
+#include <linux/kernel.h>
+#include <linux/kmemleak.h>
+#include <linux/linkage.h>
+#include <linux/memblock.h>
+#include <linux/memory.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/printk.h>
+#include <linux/random.h>
+#include <linux/sched.h>
+#include <linux/sched/task_stack.h>
+#include <linux/slab.h>
+#include <linux/stacktrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/vmalloc.h>
+#include <linux/bug.h>
+
+#include "kasan.h"
+#include "../slab.h"
+
+void check_memory_region(unsigned long addr, size_t size, bool write,
+				unsigned long ret_ip)
+{
+}
+
+#define DEFINE_HWASAN_LOAD_STORE(size)					\
+	void __hwasan_load##size##_noabort(unsigned long addr)		\
+	{								\
+	}								\
+	EXPORT_SYMBOL(__hwasan_load##size##_noabort);			\
+	void __hwasan_store##size##_noabort(unsigned long addr)		\
+	{								\
+	}								\
+	EXPORT_SYMBOL(__hwasan_store##size##_noabort)
+
+DEFINE_HWASAN_LOAD_STORE(1);
+DEFINE_HWASAN_LOAD_STORE(2);
+DEFINE_HWASAN_LOAD_STORE(4);
+DEFINE_HWASAN_LOAD_STORE(8);
+DEFINE_HWASAN_LOAD_STORE(16);
+
+void __hwasan_loadN_noabort(unsigned long addr, unsigned long size)
+{
+}
+EXPORT_SYMBOL(__hwasan_loadN_noabort);
+
+void __hwasan_storeN_noabort(unsigned long addr, unsigned long size)
+{
+}
+EXPORT_SYMBOL(__hwasan_storeN_noabort);
+
+void __hwasan_tag_memory(unsigned long addr, u8 tag, unsigned long size)
+{
+}
+EXPORT_SYMBOL(__hwasan_tag_memory);
diff --git a/mm/slub.c b/mm/slub.c
index b2172284d421..c4d5f4442ff1 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -2954,7 +2954,7 @@ static __always_inline void slab_free(struct kmem_cache *s, struct page *page,
 		do_slab_free(s, page, head, tail, cnt, addr);
 }
 
-#ifdef CONFIG_KASAN
+#ifdef CONFIG_KASAN_GENERIC
 void ___cache_free(struct kmem_cache *cache, void *x, unsigned long addr)
 {
 	do_slab_free(cache, virt_to_head_page(x), x, NULL, 1, addr);
diff --git a/scripts/Makefile.kasan b/scripts/Makefile.kasan
index 69552a39951d..5bf3a808a282 100644
--- a/scripts/Makefile.kasan
+++ b/scripts/Makefile.kasan
@@ -1,5 +1,5 @@
 # SPDX-License-Identifier: GPL-2.0
-ifdef CONFIG_KASAN
+ifdef CONFIG_KASAN_GENERIC
 ifdef CONFIG_KASAN_INLINE
 	call_threshold := 10000
 else
@@ -42,6 +42,29 @@ ifdef CONFIG_KASAN_EXTRA
 CFLAGS_KASAN += $(call cc-option, -fsanitize-address-use-after-scope)
 endif
 
-CFLAGS_KASAN_NOSANITIZE := -fno-builtin
+endif
+
+ifdef CONFIG_KASAN_SW_TAGS
+
+ifdef CONFIG_KASAN_INLINE
+    instrumentation_flags := -mllvm -hwasan-mapping-offset=$(KASAN_SHADOW_OFFSET)
+else
+    instrumentation_flags := -mllvm -hwasan-instrument-with-calls=1
+endif
 
+CFLAGS_KASAN := -fsanitize=kernel-hwaddress \
+		-mllvm -hwasan-instrument-stack=0 \
+		$(instrumentation_flags)
+
+ifeq ($(call cc-option, $(CFLAGS_KASAN) -Werror),)
+    ifneq ($(CONFIG_COMPILE_TEST),y)
+        $(warning Cannot use CONFIG_KASAN_SW_TAGS: \
+            -fsanitize=hwaddress is not supported by compiler)
+    endif
+endif
+
+endif
+
+ifdef CONFIG_KASAN
+CFLAGS_KASAN_NOSANITIZE := -fno-builtin
 endif
-- 
2.19.0.444.g18242da7ef-goog

================================================================================


################################################################################

=== Thread: [PATCH v9 05/20] kasan, arm64: adjust shadow size for tag-based mode ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 05/20] kasan, arm64: adjust shadow size for tag-based mode
Date: Fri, 21 Sep 2018 15:13:27 +0000
Message-ID: <10cf432f0ffdb67fbd495acc61bdd9517af5b7b7.1537542735.git.andreyknvl () google ! com>
--------------------
Tag-based KASAN uses 1 shadow byte for 16 bytes of kernel memory, so it
requires 1/16th of the kernel virtual address space for the shadow memory.

This commit sets KASAN_SHADOW_SCALE_SHIFT to 4 when the tag-based KASAN
mode is enabled.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/Makefile             |  2 +-
 arch/arm64/include/asm/memory.h | 13 +++++++++----
 2 files changed, 10 insertions(+), 5 deletions(-)

diff --git a/arch/arm64/Makefile b/arch/arm64/Makefile
index 106039d25e2f..11f4750d8d41 100644
--- a/arch/arm64/Makefile
+++ b/arch/arm64/Makefile
@@ -94,7 +94,7 @@ endif
 # KASAN_SHADOW_OFFSET = VA_START + (1 << (VA_BITS - KASAN_SHADOW_SCALE_SHIFT))
 #				 - (1 << (64 - KASAN_SHADOW_SCALE_SHIFT))
 # in 32-bit arithmetic
-KASAN_SHADOW_SCALE_SHIFT := 3
+KASAN_SHADOW_SCALE_SHIFT := $(if $(CONFIG_KASAN_SW_TAGS), 4, 3)
 KASAN_SHADOW_OFFSET := $(shell printf "0x%08x00000000\n" $$(( \
 	(0xffffffff & (-1 << ($(CONFIG_ARM64_VA_BITS) - 32))) \
 	+ (1 << ($(CONFIG_ARM64_VA_BITS) - 32 - $(KASAN_SHADOW_SCALE_SHIFT))) \
diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index b96442960aea..0f1e024a951f 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -74,12 +74,17 @@
 #define KERNEL_END        _end
 
 /*
- * KASAN requires 1/8th of the kernel virtual address space for the shadow
- * region. KASAN can bloat the stack significantly, so double the (minimum)
- * stack size when KASAN is in use.
+ * Generic and tag-based KASAN require 1/8th and 1/16th of the kernel virtual
+ * address space for the shadow region respectively. They can bloat the stack
+ * significantly, so double the (minimum) stack size when they are in use.
  */
-#ifdef CONFIG_KASAN
+#ifdef CONFIG_KASAN_GENERIC
 #define KASAN_SHADOW_SCALE_SHIFT 3
+#endif
+#ifdef CONFIG_KASAN_SW_TAGS
+#define KASAN_SHADOW_SCALE_SHIFT 4
+#endif
+#ifdef CONFIG_KASAN
 #define KASAN_SHADOW_SIZE	(UL(1) << (VA_BITS - KASAN_SHADOW_SCALE_SHIFT))
 #define KASAN_THREAD_SHIFT	1
 #else
-- 
2.19.0.444.g18242da7ef-goog

================================================================================


################################################################################

=== Thread: [PATCH v9 06/20] kasan: initialize shadow to 0xff for tag-based mode ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 06/20] kasan: initialize shadow to 0xff for tag-based mode
Date: Fri, 21 Sep 2018 15:13:28 +0000
Message-ID: <4021682747e0fdc24ffffd5c22f006b74d4d4089.1537542735.git.andreyknvl () google ! com>
--------------------
A tag-based KASAN shadow memory cell contains a memory tag, that
corresponds to the tag in the top byte of the pointer, that points to that
memory. The native top byte value of kernel pointers is 0xff, so with
tag-based KASAN we need to initialize shadow memory to 0xff.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/mm/kasan_init.c | 16 ++++++++++++++--
 include/linux/kasan.h      |  8 ++++++++
 mm/kasan/common.c          |  3 ++-
 3 files changed, 24 insertions(+), 3 deletions(-)

diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c
index 12145874c02b..7a31e8ccbad2 100644
--- a/arch/arm64/mm/kasan_init.c
+++ b/arch/arm64/mm/kasan_init.c
@@ -44,6 +44,15 @@ static phys_addr_t __init kasan_alloc_zeroed_page(int node)
 	return __pa(p);
 }
 
+static phys_addr_t __init kasan_alloc_raw_page(int node)
+{
+	void *p = memblock_virt_alloc_try_nid_raw(PAGE_SIZE, PAGE_SIZE,
+						  __pa(MAX_DMA_ADDRESS),
+						  MEMBLOCK_ALLOC_ACCESSIBLE,
+						  node);
+	return __pa(p);
+}
+
 static pte_t *__init kasan_pte_offset(pmd_t *pmdp, unsigned long addr, int node,
 				      bool early)
 {
@@ -89,7 +98,9 @@ static void __init kasan_pte_populate(pmd_t *pmdp, unsigned long addr,
 
 	do {
 		phys_addr_t page_phys = early ? __pa_symbol(kasan_zero_page)
-					      : kasan_alloc_zeroed_page(node);
+					      : kasan_alloc_raw_page(node);
+		if (!early)
+			memset(__va(page_phys), KASAN_SHADOW_INIT, PAGE_SIZE);
 		next = addr + PAGE_SIZE;
 		set_pte(ptep, pfn_pte(__phys_to_pfn(page_phys), PAGE_KERNEL));
 	} while (ptep++, addr = next, addr != end && pte_none(READ_ONCE(*ptep)));
@@ -139,6 +150,7 @@ asmlinkage void __init kasan_early_init(void)
 		KASAN_SHADOW_END - (1UL << (64 - KASAN_SHADOW_SCALE_SHIFT)));
 	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_START, PGDIR_SIZE));
 	BUILD_BUG_ON(!IS_ALIGNED(KASAN_SHADOW_END, PGDIR_SIZE));
+
 	kasan_pgd_populate(KASAN_SHADOW_START, KASAN_SHADOW_END, NUMA_NO_NODE,
 			   true);
 }
@@ -235,7 +247,7 @@ void __init kasan_init(void)
 		set_pte(&kasan_zero_pte[i],
 			pfn_pte(sym_to_pfn(kasan_zero_page), PAGE_KERNEL_RO));
 
-	memset(kasan_zero_page, 0, PAGE_SIZE);
+	memset(kasan_zero_page, KASAN_SHADOW_INIT, PAGE_SIZE);
 	cpu_replace_ttbr1(lm_alias(swapper_pg_dir));
 
 	/* At this point kasan is fully initialized. Enable error messages */
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index b66fdf5ea7ab..7f6574c35c62 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -153,6 +153,8 @@ static inline size_t kasan_metadata_size(struct kmem_cache *cache) { return 0; }
 
 #ifdef CONFIG_KASAN_GENERIC
 
+#define KASAN_SHADOW_INIT 0
+
 void kasan_cache_shrink(struct kmem_cache *cache);
 void kasan_cache_shutdown(struct kmem_cache *cache);
 
@@ -163,4 +165,10 @@ static inline void kasan_cache_shutdown(struct kmem_cache *cache) {}
 
 #endif /* CONFIG_KASAN_GENERIC */
 
+#ifdef CONFIG_KASAN_SW_TAGS
+
+#define KASAN_SHADOW_INIT 0xFF
+
+#endif /* CONFIG_KASAN_SW_TAGS */
+
 #endif /* LINUX_KASAN_H */
diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 5f68c93734ba..7134e75447ff 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -473,11 +473,12 @@ int kasan_module_alloc(void *addr, size_t size)
 
 	ret = __vmalloc_node_range(shadow_size, 1, shadow_start,
 			shadow_start + shadow_size,
-			GFP_KERNEL | __GFP_ZERO,
+			GFP_KERNEL,
 			PAGE_KERNEL, VM_NO_GUARD, NUMA_NO_NODE,
 			__builtin_return_address(0));
 
 	if (ret) {
+		__memset(ret, KASAN_SHADOW_INIT, shadow_size);
 		find_vm_area(addr)->flags |= VM_KASAN;
 		kmemleak_ignore(ret);
 		return 0;
-- 
2.19.0.444.g18242da7ef-goog

================================================================================


################################################################################

=== Thread: [PATCH v9 07/20] kasan, arm64: untag address in __kimg_to_phys and _virt_addr_is_linear ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 07/20] kasan, arm64: untag address in __kimg_to_phys and _virt_addr_is_linear
Date: Fri, 21 Sep 2018 15:13:29 +0000
Message-ID: <1324a622035ee2a6fecd1e729a62af22542d9f3e.1537542735.git.andreyknvl () google ! com>
--------------------
__kimg_to_phys (which is used by virt_to_phys) and _virt_addr_is_linear
(which is used by virt_addr_valid) assume that the top byte of the address
is 0xff, which isn't always the case with tag-based KASAN.

This patch resets the tag in those macros.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/memory.h | 14 ++++++++++++--
 1 file changed, 12 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 0f1e024a951f..3226a0218b0b 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -92,6 +92,15 @@
 #define KASAN_THREAD_SHIFT	0
 #endif
 
+#ifdef CONFIG_KASAN_SW_TAGS
+#define KASAN_TAG_SHIFTED(tag)		((unsigned long)(tag) << 56)
+#define KASAN_SET_TAG(addr, tag)	(((addr) & ~KASAN_TAG_SHIFTED(0xff)) | \
+						KASAN_TAG_SHIFTED(tag))
+#define KASAN_RESET_TAG(addr)		KASAN_SET_TAG(addr, 0xff)
+#else
+#define KASAN_RESET_TAG(addr)		addr
+#endif
+
 #define MIN_THREAD_SHIFT	(14 + KASAN_THREAD_SHIFT)
 
 /*
@@ -232,7 +241,7 @@ static inline unsigned long kaslr_offset(void)
 #define __is_lm_address(addr)	(!!((addr) & BIT(VA_BITS - 1)))
 
 #define __lm_to_phys(addr)	(((addr) & ~PAGE_OFFSET) + PHYS_OFFSET)
-#define __kimg_to_phys(addr)	((addr) - kimage_voffset)
+#define __kimg_to_phys(addr)	(KASAN_RESET_TAG(addr) - kimage_voffset)
 
 #define __virt_to_phys_nodebug(x) ({					\
 	phys_addr_t __x = (phys_addr_t)(x);				\
@@ -308,7 +317,8 @@ static inline void *phys_to_virt(phys_addr_t x)
 #endif
 #endif
 
-#define _virt_addr_is_linear(kaddr)	(((u64)(kaddr)) >= PAGE_OFFSET)
+#define _virt_addr_is_linear(kaddr)	(KASAN_RESET_TAG((u64)(kaddr)) >= \
+						PAGE_OFFSET)
 #define virt_addr_valid(kaddr)		(_virt_addr_is_linear(kaddr) && \
 					 _virt_addr_valid(kaddr))
 
-- 
2.19.0.444.g18242da7ef-goog

================================================================================


################################################################################

=== Thread: [PATCH v9 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 09/20] kasan: preassign tags to objects with ctors or SLAB_TYPESAFE_BY_RCU
Date: Fri, 21 Sep 2018 15:13:31 +0000
Message-ID: <9ea379b38a763adeae0e43638a9769c96eea767f.1537542735.git.andreyknvl () google ! com>
--------------------
An object constructor can initialize pointers within this objects based on
the address of the object. Since the object address might be tagged, we
need to assign a tag before calling constructor.

The implemented approach is to assign tags to objects with constructors
when a slab is allocated and call constructors once as usual. The
downside is that such object would always have the same tag when it is
reallocated, so we won't catch use-after-frees on it.

Also pressign tags for objects from SLAB_TYPESAFE_BY_RCU caches, since
they can be validy accessed after having been freed.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/slab.c |  2 +-
 mm/slub.c | 24 ++++++++++++++----------
 2 files changed, 15 insertions(+), 11 deletions(-)

diff --git a/mm/slab.c b/mm/slab.c
index 6fdca9ec2ea4..fe0ddf08aa2c 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -2574,7 +2574,7 @@ static void cache_init_objs(struct kmem_cache *cachep,
 
 	for (i = 0; i < cachep->num; i++) {
 		objp = index_to_obj(cachep, page, i);
-		kasan_init_slab_obj(cachep, objp);
+		objp = kasan_init_slab_obj(cachep, objp);
 
 		/* constructor could break poison info */
 		if (DEBUG == 0 && cachep->ctor) {
diff --git a/mm/slub.c b/mm/slub.c
index c4d5f4442ff1..75fc76e42a1e 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1413,16 +1413,17 @@ static inline bool slab_free_freelist_hook(struct kmem_cache *s,
 #endif
 }
 
-static void setup_object(struct kmem_cache *s, struct page *page,
+static void *setup_object(struct kmem_cache *s, struct page *page,
 				void *object)
 {
 	setup_object_debug(s, page, object);
-	kasan_init_slab_obj(s, object);
+	object = kasan_init_slab_obj(s, object);
 	if (unlikely(s->ctor)) {
 		kasan_unpoison_object_data(s, object);
 		s->ctor(object);
 		kasan_poison_object_data(s, object);
 	}
+	return object;
 }
 
 /*
@@ -1530,16 +1531,16 @@ static bool shuffle_freelist(struct kmem_cache *s, struct page *page)
 	/* First entry is used as the base of the freelist */
 	cur = next_freelist_entry(s, page, &pos, start, page_limit,
 				freelist_count);
+	cur = setup_object(s, page, cur);
 	page->freelist = cur;
 
 	for (idx = 1; idx < page->objects; idx++) {
-		setup_object(s, page, cur);
 		next = next_freelist_entry(s, page, &pos, start, page_limit,
 			freelist_count);
+		next = setup_object(s, page, next);
 		set_freepointer(s, cur, next);
 		cur = next;
 	}
-	setup_object(s, page, cur);
 	set_freepointer(s, cur, NULL);
 
 	return true;
@@ -1561,7 +1562,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	struct page *page;
 	struct kmem_cache_order_objects oo = s->oo;
 	gfp_t alloc_gfp;
-	void *start, *p;
+	void *start, *p, *next;
 	int idx, order;
 	bool shuffle;
 
@@ -1613,13 +1614,16 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 
 	if (!shuffle) {
 		for_each_object_idx(p, idx, s, start, page->objects) {
-			setup_object(s, page, p);
-			if (likely(idx < page->objects))
-				set_freepointer(s, p, p + s->size);
-			else
+			if (likely(idx < page->objects)) {
+				next = p + s->size;
+				next = setup_object(s, page, next);
+				set_freepointer(s, p, next);
+			} else
 				set_freepointer(s, p, NULL);
 		}
-		page->freelist = fixup_red_left(s, start);
+		start = fixup_red_left(s, start);
+		start = setup_object(s, page, start);
+		page->freelist = start;
 	}
 
 	page->inuse = page->objects;
-- 
2.19.0.444.g18242da7ef-goog

================================================================================


################################################################################

=== Thread: [PATCH v9 10/20] mm: move obj_to_index to include/linux/slab_def.h ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 10/20] mm: move obj_to_index to include/linux/slab_def.h
Date: Fri, 21 Sep 2018 15:13:32 +0000
Message-ID: <9d62f917393456653c1d38c7173dc876cef03c93.1537542735.git.andreyknvl () google ! com>
--------------------
While with SLUB we can actually preassign tags for caches with contructors
and store them in pointers in the freelist, SLAB doesn't allow that since
the freelist is stored as an array of indexes, so there are no pointers to
store the tags.

Instead we compute the tag twice, once when a slab is created before
calling the constructor and then again each time when an object is
allocated with kmalloc. Tag is computed simply by taking the lowest byte of
the index that corresponds to the object. However in kasan_kmalloc we only
have access to the objects pointer, so we need a way to find out which
index this object corresponds to.

This patch moves obj_to_index from slab.c to include/linux/slab_def.h to
be reused by KASAN.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 include/linux/slab_def.h | 13 +++++++++++++
 mm/slab.c                | 13 -------------
 2 files changed, 13 insertions(+), 13 deletions(-)

diff --git a/include/linux/slab_def.h b/include/linux/slab_def.h
index 3485c58cfd1c..9a5eafb7145b 100644
--- a/include/linux/slab_def.h
+++ b/include/linux/slab_def.h
@@ -104,4 +104,17 @@ static inline void *nearest_obj(struct kmem_cache *cache, struct page *page,
 		return object;
 }
 
+/*
+ * We want to avoid an expensive divide : (offset / cache->size)
+ *   Using the fact that size is a constant for a particular cache,
+ *   we can replace (offset / cache->size) by
+ *   reciprocal_divide(offset, cache->reciprocal_buffer_size)
+ */
+static inline unsigned int obj_to_index(const struct kmem_cache *cache,
+					const struct page *page, void *obj)
+{
+	u32 offset = (obj - page->s_mem);
+	return reciprocal_divide(offset, cache->reciprocal_buffer_size);
+}
+
 #endif	/* _LINUX_SLAB_DEF_H */
diff --git a/mm/slab.c b/mm/slab.c
index fe0ddf08aa2c..6d8de7630944 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -406,19 +406,6 @@ static inline void *index_to_obj(struct kmem_cache *cache, struct page *page,
 	return page->s_mem + cache->size * idx;
 }
 
-/*
- * We want to avoid an expensive divide : (offset / cache->size)
- *   Using the fact that size is a constant for a particular cache,
- *   we can replace (offset / cache->size) by
- *   reciprocal_divide(offset, cache->reciprocal_buffer_size)
- */
-static inline unsigned int obj_to_index(const struct kmem_cache *cache,
-					const struct page *page, void *obj)
-{
-	u32 offset = (obj - page->s_mem);
-	return reciprocal_divide(offset, cache->reciprocal_buffer_size);
-}
-
 #define BOOT_CPUCACHE_ENTRIES	1
 /* internal cache of cache description objs */
 static struct kmem_cache kmem_cache_boot = {
-- 
2.19.0.444.g18242da7ef-goog

================================================================================


################################################################################

=== Thread: [PATCH v9 12/20] kasan, arm64: enable top byte ignore for the kernel ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 12/20] kasan, arm64: enable top byte ignore for the kernel
Date: Fri, 21 Sep 2018 15:13:34 +0000
Message-ID: <7edbd2d0b0c6f0b7a2b33e3d6da2f14b4b63b8ff.1537542735.git.andreyknvl () google ! com>
--------------------
Tag-based KASAN uses the Top Byte Ignore feature of arm64 CPUs to store a
pointer tag in the top byte of each pointer. This commit enables the
TCR_TBI1 bit, which enables Top Byte Ignore for the kernel, when tag-based
KASAN is used.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/pgtable-hwdef.h | 1 +
 arch/arm64/mm/proc.S                   | 8 +++++++-
 2 files changed, 8 insertions(+), 1 deletion(-)

diff --git a/arch/arm64/include/asm/pgtable-hwdef.h b/arch/arm64/include/asm/pgtable-hwdef.h
index fd208eac9f2a..483aceedad76 100644
--- a/arch/arm64/include/asm/pgtable-hwdef.h
+++ b/arch/arm64/include/asm/pgtable-hwdef.h
@@ -289,6 +289,7 @@
 #define TCR_A1			(UL(1) << 22)
 #define TCR_ASID16		(UL(1) << 36)
 #define TCR_TBI0		(UL(1) << 37)
+#define TCR_TBI1		(UL(1) << 38)
 #define TCR_HA			(UL(1) << 39)
 #define TCR_HD			(UL(1) << 40)
 #define TCR_NFD1		(UL(1) << 54)
diff --git a/arch/arm64/mm/proc.S b/arch/arm64/mm/proc.S
index 03646e6a2ef4..b2b44dbdb063 100644
--- a/arch/arm64/mm/proc.S
+++ b/arch/arm64/mm/proc.S
@@ -47,6 +47,12 @@
 /* PTWs cacheable, inner/outer WBWA */
 #define TCR_CACHE_FLAGS	TCR_IRGN_WBWA | TCR_ORGN_WBWA
 
+#ifdef CONFIG_KASAN_SW_TAGS
+#define TCR_KASAN_FLAGS TCR_TBI1
+#else
+#define TCR_KASAN_FLAGS 0
+#endif
+
 #define MAIR(attr, mt)	((attr) << ((mt) * 8))
 
 /*
@@ -440,7 +446,7 @@ ENTRY(__cpu_setup)
 	 */
 	ldr	x10, =TCR_TxSZ(VA_BITS) | TCR_CACHE_FLAGS | TCR_SMP_FLAGS | \
 			TCR_TG_FLAGS | TCR_KASLR_FLAGS | TCR_ASID16 | \
-			TCR_TBI0 | TCR_A1
+			TCR_TBI0 | TCR_A1 | TCR_KASAN_FLAGS
 	tcr_set_idmap_t0sz	x10, x9
 
 	/*
-- 
2.19.0.444.g18242da7ef-goog

================================================================================


################################################################################

=== Thread: [PATCH v9 13/20] kasan, mm: perform untagged pointers comparison in krealloc ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 13/20] kasan, mm: perform untagged pointers comparison in krealloc
Date: Fri, 21 Sep 2018 15:13:35 +0000
Message-ID: <77d6321d5117c8b8e2267a707b693a6eb67ea17c.1537542735.git.andreyknvl () google ! com>
--------------------
The krealloc function checks where the same buffer was reused or a new one
allocated by comparing kernel pointers. Tag-based KASAN changes memory tag
on the krealloc'ed chunk of memory and therefore also changes the pointer
tag of the returned pointer. Therefore we need to perform comparison on
untagged (with tags reset) pointers to check whether it's the same memory
region or not.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/slab_common.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/mm/slab_common.c b/mm/slab_common.c
index 3abfa0f86118..221c1be3f45f 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -1513,7 +1513,7 @@ void *krealloc(const void *p, size_t new_size, gfp_t flags)
 	}
 
 	ret = __do_krealloc(p, new_size, flags);
-	if (ret && p != ret)
+	if (ret && kasan_reset_tag(p) != kasan_reset_tag(ret))
 		kfree(p);
 
 	return ret;
-- 
2.19.0.444.g18242da7ef-goog

================================================================================


################################################################################

=== Thread: [PATCH v9 14/20] kasan: split out generic_report.c from report.c ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 14/20] kasan: split out generic_report.c from report.c
Date: Fri, 21 Sep 2018 15:13:36 +0000
Message-ID: <915ac24380a023d937f5d7fe466875d376153ae4.1537542735.git.andreyknvl () google ! com>
--------------------
This patch moves generic KASAN specific error reporting routines to
generic_report.c without any functional changes, leaving common error
reporting code in report.c to be later reused by tag-based KASAN.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/Makefile         |   4 +-
 mm/kasan/generic_report.c | 158 +++++++++++++++++++++++++
 mm/kasan/kasan.h          |   7 ++
 mm/kasan/report.c         | 234 +++++++++-----------------------------
 mm/kasan/tags_report.c    |  39 +++++++
 5 files changed, 257 insertions(+), 185 deletions(-)
 create mode 100644 mm/kasan/generic_report.c
 create mode 100644 mm/kasan/tags_report.c

diff --git a/mm/kasan/Makefile b/mm/kasan/Makefile
index 68ba1822f003..0a14fcff70ed 100644
--- a/mm/kasan/Makefile
+++ b/mm/kasan/Makefile
@@ -14,5 +14,5 @@ CFLAGS_generic.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 CFLAGS_tags.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 
 obj-$(CONFIG_KASAN) := common.o init.o report.o
-obj-$(CONFIG_KASAN_GENERIC) += generic.o quarantine.o
-obj-$(CONFIG_KASAN_SW_TAGS) += tags.o
+obj-$(CONFIG_KASAN_GENERIC) += generic.o generic_report.o quarantine.o
+obj-$(CONFIG_KASAN_SW_TAGS) += tags.o tags_report.o
diff --git a/mm/kasan/generic_report.c b/mm/kasan/generic_report.c
new file mode 100644
index 000000000000..5201d1770700
--- /dev/null
+++ b/mm/kasan/generic_report.c
@@ -0,0 +1,158 @@
+/*
+ * This file contains generic KASAN specific error reporting code.
+ *
+ * Copyright (c) 2014 Samsung Electronics Co., Ltd.
+ * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
+ *
+ * Some code borrowed from https://github.com/xairy/kasan-prototype by
+ *        Andrey Konovalov <andreyknvl@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/bitops.h>
+#include <linux/ftrace.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/stackdepot.h>
+#include <linux/stacktrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/kasan.h>
+#include <linux/module.h>
+
+#include <asm/sections.h>
+
+#include "kasan.h"
+#include "../slab.h"
+
+static const void *find_first_bad_addr(const void *addr, size_t size)
+{
+	u8 shadow_val = *(u8 *)kasan_mem_to_shadow(addr);
+	const void *first_bad_addr = addr;
+
+	while (!shadow_val && first_bad_addr < addr + size) {
+		first_bad_addr += KASAN_SHADOW_SCALE_SIZE;
+		shadow_val = *(u8 *)kasan_mem_to_shadow(first_bad_addr);
+	}
+	return first_bad_addr;
+}
+
+static const char *get_shadow_bug_type(struct kasan_access_info *info)
+{
+	const char *bug_type = "unknown-crash";
+	u8 *shadow_addr;
+
+	info->first_bad_addr = find_first_bad_addr(info->access_addr,
+						info->access_size);
+
+	shadow_addr = (u8 *)kasan_mem_to_shadow(info->first_bad_addr);
+
+	/*
+	 * If shadow byte value is in [0, KASAN_SHADOW_SCALE_SIZE) we can look
+	 * at the next shadow byte to determine the type of the bad access.
+	 */
+	if (*shadow_addr > 0 && *shadow_addr <= KASAN_SHADOW_SCALE_SIZE - 1)
+		shadow_addr++;
+
+	switch (*shadow_addr) {
+	case 0 ... KASAN_SHADOW_SCALE_SIZE - 1:
+		/*
+		 * In theory it's still possible to see these shadow values
+		 * due to a data race in the kernel code.
+		 */
+		bug_type = "out-of-bounds";
+		break;
+	case KASAN_PAGE_REDZONE:
+	case KASAN_KMALLOC_REDZONE:
+		bug_type = "slab-out-of-bounds";
+		break;
+	case KASAN_GLOBAL_REDZONE:
+		bug_type = "global-out-of-bounds";
+		break;
+	case KASAN_STACK_LEFT:
+	case KASAN_STACK_MID:
+	case KASAN_STACK_RIGHT:
+	case KASAN_STACK_PARTIAL:
+		bug_type = "stack-out-of-bounds";
+		break;
+	case KASAN_FREE_PAGE:
+	case KASAN_KMALLOC_FREE:
+		bug_type = "use-after-free";
+		break;
+	case KASAN_USE_AFTER_SCOPE:
+		bug_type = "use-after-scope";
+		break;
+	case KASAN_ALLOCA_LEFT:
+	case KASAN_ALLOCA_RIGHT:
+		bug_type = "alloca-out-of-bounds";
+		break;
+	}
+
+	return bug_type;
+}
+
+static const char *get_wild_bug_type(struct kasan_access_info *info)
+{
+	const char *bug_type = "unknown-crash";
+
+	if ((unsigned long)info->access_addr < PAGE_SIZE)
+		bug_type = "null-ptr-deref";
+	else if ((unsigned long)info->access_addr < TASK_SIZE)
+		bug_type = "user-memory-access";
+	else
+		bug_type = "wild-memory-access";
+
+	return bug_type;
+}
+
+const char *get_bug_type(struct kasan_access_info *info)
+{
+	if (addr_has_shadow(info->access_addr))
+		return get_shadow_bug_type(info);
+	return get_wild_bug_type(info);
+}
+
+#define DEFINE_ASAN_REPORT_LOAD(size)                     \
+void __asan_report_load##size##_noabort(unsigned long addr) \
+{                                                         \
+	kasan_report(addr, size, false, _RET_IP_);	  \
+}                                                         \
+EXPORT_SYMBOL(__asan_report_load##size##_noabort)
+
+#define DEFINE_ASAN_REPORT_STORE(size)                     \
+void __asan_report_store##size##_noabort(unsigned long addr) \
+{                                                          \
+	kasan_report(addr, size, true, _RET_IP_);	   \
+}                                                          \
+EXPORT_SYMBOL(__asan_report_store##size##_noabort)
+
+DEFINE_ASAN_REPORT_LOAD(1);
+DEFINE_ASAN_REPORT_LOAD(2);
+DEFINE_ASAN_REPORT_LOAD(4);
+DEFINE_ASAN_REPORT_LOAD(8);
+DEFINE_ASAN_REPORT_LOAD(16);
+DEFINE_ASAN_REPORT_STORE(1);
+DEFINE_ASAN_REPORT_STORE(2);
+DEFINE_ASAN_REPORT_STORE(4);
+DEFINE_ASAN_REPORT_STORE(8);
+DEFINE_ASAN_REPORT_STORE(16);
+
+void __asan_report_load_n_noabort(unsigned long addr, size_t size)
+{
+	kasan_report(addr, size, false, _RET_IP_);
+}
+EXPORT_SYMBOL(__asan_report_load_n_noabort);
+
+void __asan_report_store_n_noabort(unsigned long addr, size_t size)
+{
+	kasan_report(addr, size, true, _RET_IP_);
+}
+EXPORT_SYMBOL(__asan_report_store_n_noabort);
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index f16bee55b610..50adcab463f2 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -109,11 +109,18 @@ static inline const void *kasan_shadow_to_mem(const void *shadow_addr)
 		<< KASAN_SHADOW_SCALE_SHIFT);
 }
 
+static inline bool addr_has_shadow(const void *addr)
+{
+	return (addr >= kasan_shadow_to_mem((void *)KASAN_SHADOW_START));
+}
+
 void kasan_poison_shadow(const void *address, size_t size, u8 value);
 
 void check_memory_region(unsigned long addr, size_t size, bool write,
 				unsigned long ret_ip);
 
+const char *get_bug_type(struct kasan_access_info *info);
+
 void kasan_report(unsigned long addr, size_t size,
 		bool is_write, unsigned long ip);
 void kasan_report_invalid_free(void *object, unsigned long ip);
diff --git a/mm/kasan/report.c b/mm/kasan/report.c
index 5c169aa688fd..64a74f334c45 100644
--- a/mm/kasan/report.c
+++ b/mm/kasan/report.c
@@ -1,5 +1,5 @@
 /*
- * This file contains error reporting code.
+ * This file contains common generic and tag-based KASAN error reporting code.
  *
  * Copyright (c) 2014 Samsung Electronics Co., Ltd.
  * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
@@ -39,103 +39,34 @@
 #define SHADOW_BYTES_PER_ROW (SHADOW_BLOCKS_PER_ROW * SHADOW_BYTES_PER_BLOCK)
 #define SHADOW_ROWS_AROUND_ADDR 2
 
-static const void *find_first_bad_addr(const void *addr, size_t size)
-{
-	u8 shadow_val = *(u8 *)kasan_mem_to_shadow(addr);
-	const void *first_bad_addr = addr;
-
-	while (!shadow_val && first_bad_addr < addr + size) {
-		first_bad_addr += KASAN_SHADOW_SCALE_SIZE;
-		shadow_val = *(u8 *)kasan_mem_to_shadow(first_bad_addr);
-	}
-	return first_bad_addr;
-}
+static unsigned long kasan_flags;
 
-static bool addr_has_shadow(struct kasan_access_info *info)
-{
-	return (info->access_addr >=
-		kasan_shadow_to_mem((void *)KASAN_SHADOW_START));
-}
+#define KASAN_BIT_REPORTED	0
+#define KASAN_BIT_MULTI_SHOT	1
 
-static const char *get_shadow_bug_type(struct kasan_access_info *info)
+bool kasan_save_enable_multi_shot(void)
 {
-	const char *bug_type = "unknown-crash";
-	u8 *shadow_addr;
-
-	info->first_bad_addr = find_first_bad_addr(info->access_addr,
-						info->access_size);
-
-	shadow_addr = (u8 *)kasan_mem_to_shadow(info->first_bad_addr);
-
-	/*
-	 * If shadow byte value is in [0, KASAN_SHADOW_SCALE_SIZE) we can look
-	 * at the next shadow byte to determine the type of the bad access.
-	 */
-	if (*shadow_addr > 0 && *shadow_addr <= KASAN_SHADOW_SCALE_SIZE - 1)
-		shadow_addr++;
-
-	switch (*shadow_addr) {
-	case 0 ... KASAN_SHADOW_SCALE_SIZE - 1:
-		/*
-		 * In theory it's still possible to see these shadow values
-		 * due to a data race in the kernel code.
-		 */
-		bug_type = "out-of-bounds";
-		break;
-	case KASAN_PAGE_REDZONE:
-	case KASAN_KMALLOC_REDZONE:
-		bug_type = "slab-out-of-bounds";
-		break;
-	case KASAN_GLOBAL_REDZONE:
-		bug_type = "global-out-of-bounds";
-		break;
-	case KASAN_STACK_LEFT:
-	case KASAN_STACK_MID:
-	case KASAN_STACK_RIGHT:
-	case KASAN_STACK_PARTIAL:
-		bug_type = "stack-out-of-bounds";
-		break;
-	case KASAN_FREE_PAGE:
-	case KASAN_KMALLOC_FREE:
-		bug_type = "use-after-free";
-		break;
-	case KASAN_USE_AFTER_SCOPE:
-		bug_type = "use-after-scope";
-		break;
-	case KASAN_ALLOCA_LEFT:
-	case KASAN_ALLOCA_RIGHT:
-		bug_type = "alloca-out-of-bounds";
-		break;
-	}
-
-	return bug_type;
+	return test_and_set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
 }
+EXPORT_SYMBOL_GPL(kasan_save_enable_multi_shot);
 
-static const char *get_wild_bug_type(struct kasan_access_info *info)
+void kasan_restore_multi_shot(bool enabled)
 {
-	const char *bug_type = "unknown-crash";
-
-	if ((unsigned long)info->access_addr < PAGE_SIZE)
-		bug_type = "null-ptr-deref";
-	else if ((unsigned long)info->access_addr < TASK_SIZE)
-		bug_type = "user-memory-access";
-	else
-		bug_type = "wild-memory-access";
-
-	return bug_type;
+	if (!enabled)
+		clear_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
 }
+EXPORT_SYMBOL_GPL(kasan_restore_multi_shot);
 
-static const char *get_bug_type(struct kasan_access_info *info)
+static int __init kasan_set_multi_shot(char *str)
 {
-	if (addr_has_shadow(info))
-		return get_shadow_bug_type(info);
-	return get_wild_bug_type(info);
+	set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
+	return 1;
 }
+__setup("kasan_multi_shot", kasan_set_multi_shot);
 
-static void print_error_description(struct kasan_access_info *info)
+static void print_error_description(struct kasan_access_info *info,
+					const char *bug_type)
 {
-	const char *bug_type = get_bug_type(info);
-
 	pr_err("BUG: KASAN: %s in %pS\n",
 		bug_type, (void *)info->ip);
 	pr_err("%s of size %zu at addr %px by task %s/%d\n",
@@ -143,25 +74,9 @@ static void print_error_description(struct kasan_access_info *info)
 		info->access_addr, current->comm, task_pid_nr(current));
 }
 
-static inline bool kernel_or_module_addr(const void *addr)
-{
-	if (addr >= (void *)_stext && addr < (void *)_end)
-		return true;
-	if (is_module_address((unsigned long)addr))
-		return true;
-	return false;
-}
-
-static inline bool init_task_stack_addr(const void *addr)
-{
-	return addr >= (void *)&init_thread_union.stack &&
-		(addr <= (void *)&init_thread_union.stack +
-			sizeof(init_thread_union.stack));
-}
-
 static DEFINE_SPINLOCK(report_lock);
 
-static void kasan_start_report(unsigned long *flags)
+static void start_report(unsigned long *flags)
 {
 	/*
 	 * Make sure we don't end up in loop.
@@ -171,7 +86,7 @@ static void kasan_start_report(unsigned long *flags)
 	pr_err("==================================================================\n");
 }
 
-static void kasan_end_report(unsigned long *flags)
+static void end_report(unsigned long *flags)
 {
 	pr_err("==================================================================\n");
 	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
@@ -249,6 +164,22 @@ static void describe_object(struct kmem_cache *cache, void *object,
 	describe_object_addr(cache, object, addr);
 }
 
+static inline bool kernel_or_module_addr(const void *addr)
+{
+	if (addr >= (void *)_stext && addr < (void *)_end)
+		return true;
+	if (is_module_address((unsigned long)addr))
+		return true;
+	return false;
+}
+
+static inline bool init_task_stack_addr(const void *addr)
+{
+	return addr >= (void *)&init_thread_union.stack &&
+		(addr <= (void *)&init_thread_union.stack +
+			sizeof(init_thread_union.stack));
+}
+
 static void print_address_description(void *addr)
 {
 	struct page *page = addr_to_page(addr);
@@ -326,29 +257,38 @@ static void print_shadow_for_address(const void *addr)
 	}
 }
 
+static bool report_enabled(void)
+{
+	if (current->kasan_depth)
+		return false;
+	if (test_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags))
+		return true;
+	return !test_and_set_bit(KASAN_BIT_REPORTED, &kasan_flags);
+}
+
 void kasan_report_invalid_free(void *object, unsigned long ip)
 {
 	unsigned long flags;
 
-	kasan_start_report(&flags);
+	start_report(&flags);
 	pr_err("BUG: KASAN: double-free or invalid-free in %pS\n", (void *)ip);
 	pr_err("\n");
 	print_address_description(object);
 	pr_err("\n");
 	print_shadow_for_address(object);
-	kasan_end_report(&flags);
+	end_report(&flags);
 }
 
 static void kasan_report_error(struct kasan_access_info *info)
 {
 	unsigned long flags;
 
-	kasan_start_report(&flags);
+	start_report(&flags);
 
-	print_error_description(info);
+	print_error_description(info, get_bug_type(info));
 	pr_err("\n");
 
-	if (!addr_has_shadow(info)) {
+	if (!addr_has_shadow(info->access_addr)) {
 		dump_stack();
 	} else {
 		print_address_description((void *)info->access_addr);
@@ -356,41 +296,7 @@ static void kasan_report_error(struct kasan_access_info *info)
 		print_shadow_for_address(info->first_bad_addr);
 	}
 
-	kasan_end_report(&flags);
-}
-
-static unsigned long kasan_flags;
-
-#define KASAN_BIT_REPORTED	0
-#define KASAN_BIT_MULTI_SHOT	1
-
-bool kasan_save_enable_multi_shot(void)
-{
-	return test_and_set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
-}
-EXPORT_SYMBOL_GPL(kasan_save_enable_multi_shot);
-
-void kasan_restore_multi_shot(bool enabled)
-{
-	if (!enabled)
-		clear_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
-}
-EXPORT_SYMBOL_GPL(kasan_restore_multi_shot);
-
-static int __init kasan_set_multi_shot(char *str)
-{
-	set_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags);
-	return 1;
-}
-__setup("kasan_multi_shot", kasan_set_multi_shot);
-
-static inline bool kasan_report_enabled(void)
-{
-	if (current->kasan_depth)
-		return false;
-	if (test_bit(KASAN_BIT_MULTI_SHOT, &kasan_flags))
-		return true;
-	return !test_and_set_bit(KASAN_BIT_REPORTED, &kasan_flags);
+	end_report(&flags);
 }
 
 void kasan_report(unsigned long addr, size_t size,
@@ -398,7 +304,7 @@ void kasan_report(unsigned long addr, size_t size,
 {
 	struct kasan_access_info info;
 
-	if (likely(!kasan_report_enabled()))
+	if (likely(!report_enabled()))
 		return;
 
 	disable_trace_on_warning();
@@ -411,41 +317,3 @@ void kasan_report(unsigned long addr, size_t size,
 
 	kasan_report_error(&info);
 }
-
-
-#define DEFINE_ASAN_REPORT_LOAD(size)                     \
-void __asan_report_load##size##_noabort(unsigned long addr) \
-{                                                         \
-	kasan_report(addr, size, false, _RET_IP_);	  \
-}                                                         \
-EXPORT_SYMBOL(__asan_report_load##size##_noabort)
-
-#define DEFINE_ASAN_REPORT_STORE(size)                     \
-void __asan_report_store##size##_noabort(unsigned long addr) \
-{                                                          \
-	kasan_report(addr, size, true, _RET_IP_);	   \
-}                                                          \
-EXPORT_SYMBOL(__asan_report_store##size##_noabort)
-
-DEFINE_ASAN_REPORT_LOAD(1);
-DEFINE_ASAN_REPORT_LOAD(2);
-DEFINE_ASAN_REPORT_LOAD(4);
-DEFINE_ASAN_REPORT_LOAD(8);
-DEFINE_ASAN_REPORT_LOAD(16);
-DEFINE_ASAN_REPORT_STORE(1);
-DEFINE_ASAN_REPORT_STORE(2);
-DEFINE_ASAN_REPORT_STORE(4);
-DEFINE_ASAN_REPORT_STORE(8);
-DEFINE_ASAN_REPORT_STORE(16);
-
-void __asan_report_load_n_noabort(unsigned long addr, size_t size)
-{
-	kasan_report(addr, size, false, _RET_IP_);
-}
-EXPORT_SYMBOL(__asan_report_load_n_noabort);
-
-void __asan_report_store_n_noabort(unsigned long addr, size_t size)
-{
-	kasan_report(addr, size, true, _RET_IP_);
-}
-EXPORT_SYMBOL(__asan_report_store_n_noabort);
diff --git a/mm/kasan/tags_report.c b/mm/kasan/tags_report.c
new file mode 100644
index 000000000000..8af15e87d3bc
--- /dev/null
+++ b/mm/kasan/tags_report.c
@@ -0,0 +1,39 @@
+/*
+ * This file contains tag-based KASAN specific error reporting code.
+ *
+ * Copyright (c) 2014 Samsung Electronics Co., Ltd.
+ * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
+ *
+ * Some code borrowed from https://github.com/xairy/kasan-prototype by
+ *        Andrey Konovalov <andreyknvl@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/bitops.h>
+#include <linux/ftrace.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/stackdepot.h>
+#include <linux/stacktrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/kasan.h>
+#include <linux/module.h>
+
+#include <asm/sections.h>
+
+#include "kasan.h"
+#include "../slab.h"
+
+const char *get_bug_type(struct kasan_access_info *info)
+{
+	return "invalid-access";
+}
-- 
2.19.0.444.g18242da7ef-goog

================================================================================


################################################################################

=== Thread: [PATCH v9 15/20] kasan: add bug reporting routines for tag-based mode ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 15/20] kasan: add bug reporting routines for tag-based mode
Date: Fri, 21 Sep 2018 15:13:37 +0000
Message-ID: <e5c4d39e7bd10799f717d6c6653f871693576951.1537542735.git.andreyknvl () google ! com>
--------------------
This commit adds rountines, that print tag-based KASAN error reports.
Those are quite similar to generic KASAN, the difference is:

1. The way tag-based KASAN finds the first bad shadow cell (with a
   mismatching tag). Tag-based KASAN compares memory tags from the shadow
   memory to the pointer tag.

2. Tag-based KASAN reports all bugs with the "KASAN: invalid-access"
   header.

Also simplify generic KASAN find_first_bad_addr.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/generic_report.c | 16 ++++-------
 mm/kasan/kasan.h          |  5 ++++
 mm/kasan/report.c         | 57 +++++++++++++++++++++------------------
 mm/kasan/tags_report.c    | 18 +++++++++++++
 4 files changed, 59 insertions(+), 37 deletions(-)

diff --git a/mm/kasan/generic_report.c b/mm/kasan/generic_report.c
index 5201d1770700..a4604cceae59 100644
--- a/mm/kasan/generic_report.c
+++ b/mm/kasan/generic_report.c
@@ -33,16 +33,13 @@
 #include "kasan.h"
 #include "../slab.h"
 
-static const void *find_first_bad_addr(const void *addr, size_t size)
+void *find_first_bad_addr(void *addr, size_t size)
 {
-	u8 shadow_val = *(u8 *)kasan_mem_to_shadow(addr);
-	const void *first_bad_addr = addr;
+	void *p = addr;
 
-	while (!shadow_val && first_bad_addr < addr + size) {
-		first_bad_addr += KASAN_SHADOW_SCALE_SIZE;
-		shadow_val = *(u8 *)kasan_mem_to_shadow(first_bad_addr);
-	}
-	return first_bad_addr;
+	while (p < addr + size && !(*(u8 *)kasan_mem_to_shadow(p)))
+		p += KASAN_SHADOW_SCALE_SIZE;
+	return p;
 }
 
 static const char *get_shadow_bug_type(struct kasan_access_info *info)
@@ -50,9 +47,6 @@ static const char *get_shadow_bug_type(struct kasan_access_info *info)
 	const char *bug_type = "unknown-crash";
 	u8 *shadow_addr;
 
-	info->first_bad_addr = find_first_bad_addr(info->access_addr,
-						info->access_size);
-
 	shadow_addr = (u8 *)kasan_mem_to_shadow(info->first_bad_addr);
 
 	/*
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index 50adcab463f2..9b567f742539 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -119,6 +119,7 @@ void kasan_poison_shadow(const void *address, size_t size, u8 value);
 void check_memory_region(unsigned long addr, size_t size, bool write,
 				unsigned long ret_ip);
 
+void *find_first_bad_addr(void *addr, size_t size);
 const char *get_bug_type(struct kasan_access_info *info);
 
 void kasan_report(unsigned long addr, size_t size,
@@ -139,6 +140,8 @@ static inline void quarantine_remove_cache(struct kmem_cache *cache) { }
 
 #ifdef CONFIG_KASAN_SW_TAGS
 
+void print_tags(u8 addr_tag, const void *addr);
+
 #define KASAN_PTR_TAG_SHIFT 56
 #define KASAN_PTR_TAG_MASK (0xFFUL << KASAN_PTR_TAG_SHIFT)
 
@@ -166,6 +169,8 @@ static inline void *reset_tag(const void *addr)
 
 #else /* CONFIG_KASAN_SW_TAGS */
 
+static inline void print_tags(u8 addr_tag, const void *addr) { }
+
 static inline u8 random_tag(void)
 {
 	return 0;
diff --git a/mm/kasan/report.c b/mm/kasan/report.c
index 64a74f334c45..214d85035f99 100644
--- a/mm/kasan/report.c
+++ b/mm/kasan/report.c
@@ -64,11 +64,10 @@ static int __init kasan_set_multi_shot(char *str)
 }
 __setup("kasan_multi_shot", kasan_set_multi_shot);
 
-static void print_error_description(struct kasan_access_info *info,
-					const char *bug_type)
+static void print_error_description(struct kasan_access_info *info)
 {
 	pr_err("BUG: KASAN: %s in %pS\n",
-		bug_type, (void *)info->ip);
+		get_bug_type(info), (void *)info->ip);
 	pr_err("%s of size %zu at addr %px by task %s/%d\n",
 		info->is_write ? "Write" : "Read", info->access_size,
 		info->access_addr, current->comm, task_pid_nr(current));
@@ -272,6 +271,8 @@ void kasan_report_invalid_free(void *object, unsigned long ip)
 
 	start_report(&flags);
 	pr_err("BUG: KASAN: double-free or invalid-free in %pS\n", (void *)ip);
+	print_tags(get_tag(object), reset_tag(object));
+	object = reset_tag(object);
 	pr_err("\n");
 	print_address_description(object);
 	pr_err("\n");
@@ -279,41 +280,45 @@ void kasan_report_invalid_free(void *object, unsigned long ip)
 	end_report(&flags);
 }
 
-static void kasan_report_error(struct kasan_access_info *info)
-{
-	unsigned long flags;
-
-	start_report(&flags);
-
-	print_error_description(info, get_bug_type(info));
-	pr_err("\n");
-
-	if (!addr_has_shadow(info->access_addr)) {
-		dump_stack();
-	} else {
-		print_address_description((void *)info->access_addr);
-		pr_err("\n");
-		print_shadow_for_address(info->first_bad_addr);
-	}
-
-	end_report(&flags);
-}
-
 void kasan_report(unsigned long addr, size_t size,
 		bool is_write, unsigned long ip)
 {
 	struct kasan_access_info info;
+	void *tagged_addr;
+	void *untagged_addr;
+	unsigned long flags;
 
 	if (likely(!report_enabled()))
 		return;
 
 	disable_trace_on_warning();
 
-	info.access_addr = (void *)addr;
-	info.first_bad_addr = (void *)addr;
+	tagged_addr = (void *)addr;
+	untagged_addr = reset_tag(tagged_addr);
+
+	info.access_addr = tagged_addr;
+	if (addr_has_shadow(untagged_addr))
+		info.first_bad_addr = find_first_bad_addr(tagged_addr, size);
+	else
+		info.first_bad_addr = untagged_addr;
 	info.access_size = size;
 	info.is_write = is_write;
 	info.ip = ip;
 
-	kasan_report_error(&info);
+	start_report(&flags);
+
+	print_error_description(&info);
+	if (addr_has_shadow(untagged_addr))
+		print_tags(get_tag(tagged_addr), info.first_bad_addr);
+	pr_err("\n");
+
+	if (addr_has_shadow(untagged_addr)) {
+		print_address_description(untagged_addr);
+		pr_err("\n");
+		print_shadow_for_address(info.first_bad_addr);
+	} else {
+		dump_stack();
+	}
+
+	end_report(&flags);
 }
diff --git a/mm/kasan/tags_report.c b/mm/kasan/tags_report.c
index 8af15e87d3bc..573c51d20d09 100644
--- a/mm/kasan/tags_report.c
+++ b/mm/kasan/tags_report.c
@@ -37,3 +37,21 @@ const char *get_bug_type(struct kasan_access_info *info)
 {
 	return "invalid-access";
 }
+
+void *find_first_bad_addr(void *addr, size_t size)
+{
+	u8 tag = get_tag(addr);
+	void *p = reset_tag(addr);
+	void *end = p + size;
+
+	while (p < end && tag == *(u8 *)kasan_mem_to_shadow(p))
+		p += KASAN_SHADOW_SCALE_SIZE;
+	return p;
+}
+
+void print_tags(u8 addr_tag, const void *addr)
+{
+	u8 *shadow = (u8 *)kasan_mem_to_shadow(addr);
+
+	pr_err("Pointer tag: [%02x], memory tag: [%02x]\n", addr_tag, *shadow);
+}
-- 
2.19.0.444.g18242da7ef-goog

================================================================================


################################################################################

=== Thread: [PATCH v9 16/20] kasan: add hooks implementation for tag-based mode ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 16/20] kasan: add hooks implementation for tag-based mode
Date: Fri, 21 Sep 2018 15:13:38 +0000
Message-ID: <60a361c80907e05baca360e4e1c40ee3996bda06.1537542735.git.andreyknvl () google ! com>
--------------------
This commit adds tag-based KASAN specific hooks implementation and
adjusts common generic and tag-based KASAN ones.

1. When a new slab cache is created, tag-based KASAN rounds up the size of
   the objects in this cache to KASAN_SHADOW_SCALE_SIZE (== 16).

2. On each kmalloc tag-based KASAN generates a random tag, sets the shadow
   memory, that corresponds to this object to this tag, and embeds this
   tag value into the top byte of the returned pointer.

3. On each kfree tag-based KASAN poisons the shadow memory with a random
   tag to allow detection of use-after-free bugs.

The rest of the logic of the hook implementation is very much similar to
the one provided by generic KASAN. Tag-based KASAN saves allocation and
free stack metadata to the slab object the same way generic KASAN does.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/common.c | 116 ++++++++++++++++++++++++++++++++++++++--------
 mm/kasan/kasan.h  |   8 ++++
 mm/kasan/tags.c   |  48 +++++++++++++++++++
 3 files changed, 153 insertions(+), 19 deletions(-)

diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 7134e75447ff..27f0cae336c9 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -140,6 +140,13 @@ void kasan_poison_shadow(const void *address, size_t size, u8 value)
 {
 	void *shadow_start, *shadow_end;
 
+	/*
+	 * Perform shadow offset calculation based on untagged address, as
+	 * some of the callers (e.g. kasan_poison_object_data) pass tagged
+	 * addresses to this function.
+	 */
+	address = reset_tag(address);
+
 	shadow_start = kasan_mem_to_shadow(address);
 	shadow_end = kasan_mem_to_shadow(address + size);
 
@@ -148,11 +155,24 @@ void kasan_poison_shadow(const void *address, size_t size, u8 value)
 
 void kasan_unpoison_shadow(const void *address, size_t size)
 {
-	kasan_poison_shadow(address, size, 0);
+	u8 tag = get_tag(address);
+
+	/*
+	 * Perform shadow offset calculation based on untagged address, as
+	 * some of the callers (e.g. kasan_unpoison_object_data) pass tagged
+	 * addresses to this function.
+	 */
+	address = reset_tag(address);
+
+	kasan_poison_shadow(address, size, tag);
 
 	if (size & KASAN_SHADOW_MASK) {
 		u8 *shadow = (u8 *)kasan_mem_to_shadow(address + size);
-		*shadow = size & KASAN_SHADOW_MASK;
+
+		if (IS_ENABLED(CONFIG_KASAN_SW_TAGS))
+			*shadow = tag;
+		else
+			*shadow = size & KASAN_SHADOW_MASK;
 	}
 }
 
@@ -200,8 +220,9 @@ void kasan_unpoison_stack_above_sp_to(const void *watermark)
 
 void kasan_alloc_pages(struct page *page, unsigned int order)
 {
-	if (likely(!PageHighMem(page)))
-		kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
+	if (unlikely(PageHighMem(page)))
+		return;
+	kasan_unpoison_shadow(page_address(page), PAGE_SIZE << order);
 }
 
 void kasan_free_pages(struct page *page, unsigned int order)
@@ -218,6 +239,9 @@ void kasan_free_pages(struct page *page, unsigned int order)
  */
 static inline unsigned int optimal_redzone(unsigned int object_size)
 {
+	if (IS_ENABLED(CONFIG_KASAN_SW_TAGS))
+		return 0;
+
 	return
 		object_size <= 64        - 16   ? 16 :
 		object_size <= 128       - 32   ? 32 :
@@ -232,6 +256,7 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 			slab_flags_t *flags)
 {
 	unsigned int orig_size = *size;
+	unsigned int redzone_size;
 	int redzone_adjust;
 
 	/* Add alloc meta. */
@@ -239,20 +264,20 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 	*size += sizeof(struct kasan_alloc_meta);
 
 	/* Add free meta. */
-	if (cache->flags & SLAB_TYPESAFE_BY_RCU || cache->ctor ||
-	    cache->object_size < sizeof(struct kasan_free_meta)) {
+	if (IS_ENABLED(CONFIG_KASAN_GENERIC) &&
+	    (cache->flags & SLAB_TYPESAFE_BY_RCU || cache->ctor ||
+	     cache->object_size < sizeof(struct kasan_free_meta))) {
 		cache->kasan_info.free_meta_offset = *size;
 		*size += sizeof(struct kasan_free_meta);
 	}
-	redzone_adjust = optimal_redzone(cache->object_size) -
-		(*size - cache->object_size);
 
+	redzone_size = optimal_redzone(cache->object_size);
+	redzone_adjust = redzone_size -	(*size - cache->object_size);
 	if (redzone_adjust > 0)
 		*size += redzone_adjust;
 
 	*size = min_t(unsigned int, KMALLOC_MAX_SIZE,
-			max(*size, cache->object_size +
-					optimal_redzone(cache->object_size)));
+			max(*size, cache->object_size + redzone_size));
 
 	/*
 	 * If the metadata doesn't fit, don't enable KASAN at all.
@@ -265,6 +290,8 @@ void kasan_cache_create(struct kmem_cache *cache, unsigned int *size,
 		return;
 	}
 
+	cache->align = round_up(cache->align, KASAN_SHADOW_SCALE_SIZE);
+
 	*flags |= SLAB_KASAN;
 }
 
@@ -309,6 +336,32 @@ void kasan_poison_object_data(struct kmem_cache *cache, void *object)
 			KASAN_KMALLOC_REDZONE);
 }
 
+/*
+ * Since it's desirable to only call object contructors once during slab
+ * allocation, we preassign tags to all such objects. Also preassign tags for
+ * SLAB_TYPESAFE_BY_RCU slabs to avoid use-after-free reports.
+ * For SLAB allocator we can't preassign tags randomly since the freelist is
+ * stored as an array of indexes instead of a linked list. Assign tags based
+ * on objects indexes, so that objects that are next to each other get
+ * different tags.
+ * After a tag is assigned, the object always gets allocated with the same tag.
+ * The reason is that we can't change tags for objects with constructors on
+ * reallocation (even for non-SLAB_TYPESAFE_BY_RCU), because the constructor
+ * code can save the pointer to the object somewhere (e.g. in the object
+ * itself). Then if we retag it, the old saved pointer will become invalid.
+ */
+static u8 assign_tag(struct kmem_cache *cache, const void *object, bool new)
+{
+	if (!cache->ctor && !(cache->flags & SLAB_TYPESAFE_BY_RCU))
+		return new ? KASAN_TAG_KERNEL : random_tag();
+
+#ifdef CONFIG_SLAB
+	return (u8)obj_to_index(cache, virt_to_page(object), (void *)object);
+#else
+	return new ? random_tag() : get_tag(object);
+#endif
+}
+
 void *kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
 {
 	struct kasan_alloc_meta *alloc_info;
@@ -319,6 +372,9 @@ void *kasan_init_slab_obj(struct kmem_cache *cache, const void *object)
 	alloc_info = get_alloc_info(cache, object);
 	__memset(alloc_info, 0, sizeof(*alloc_info));
 
+	if (IS_ENABLED(CONFIG_KASAN_SW_TAGS))
+		object = set_tag(object, assign_tag(cache, object, true));
+
 	return (void *)object;
 }
 
@@ -327,15 +383,30 @@ void *kasan_slab_alloc(struct kmem_cache *cache, void *object, gfp_t flags)
 	return kasan_kmalloc(cache, object, cache->object_size, flags);
 }
 
+static inline bool shadow_invalid(u8 tag, s8 shadow_byte)
+{
+	if (IS_ENABLED(CONFIG_KASAN_GENERIC))
+		return shadow_byte < 0 ||
+			shadow_byte >= KASAN_SHADOW_SCALE_SIZE;
+	else
+		return tag != (u8)shadow_byte;
+}
+
 static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
 			      unsigned long ip, bool quarantine)
 {
 	s8 shadow_byte;
+	u8 tag;
+	void *tagged_object;
 	unsigned long rounded_up_size;
 
+	tag = get_tag(object);
+	tagged_object = object;
+	object = reset_tag(object);
+
 	if (unlikely(nearest_obj(cache, virt_to_head_page(object), object) !=
 	    object)) {
-		kasan_report_invalid_free(object, ip);
+		kasan_report_invalid_free(tagged_object, ip);
 		return true;
 	}
 
@@ -344,20 +415,22 @@ static bool __kasan_slab_free(struct kmem_cache *cache, void *object,
 		return false;
 
 	shadow_byte = READ_ONCE(*(s8 *)kasan_mem_to_shadow(object));
-	if (shadow_byte < 0 || shadow_byte >= KASAN_SHADOW_SCALE_SIZE) {
-		kasan_report_invalid_free(object, ip);
+	if (shadow_invalid(tag, shadow_byte)) {
+		kasan_report_invalid_free(tagged_object, ip);
 		return true;
 	}
 
 	rounded_up_size = round_up(cache->object_size, KASAN_SHADOW_SCALE_SIZE);
 	kasan_poison_shadow(object, rounded_up_size, KASAN_KMALLOC_FREE);
 
-	if (!quarantine || unlikely(!(cache->flags & SLAB_KASAN)))
+	if ((IS_ENABLED(CONFIG_KASAN_GENERIC) && !quarantine) ||
+			unlikely(!(cache->flags & SLAB_KASAN)))
 		return false;
 
 	set_track(&get_alloc_info(cache, object)->free_track, GFP_NOWAIT);
 	quarantine_put(get_free_info(cache, object), cache);
-	return true;
+
+	return IS_ENABLED(CONFIG_KASAN_GENERIC);
 }
 
 bool kasan_slab_free(struct kmem_cache *cache, void *object, unsigned long ip)
@@ -370,6 +443,7 @@ void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 {
 	unsigned long redzone_start;
 	unsigned long redzone_end;
+	u8 tag;
 
 	if (gfpflags_allow_blocking(flags))
 		quarantine_reduce();
@@ -382,14 +456,18 @@ void *kasan_kmalloc(struct kmem_cache *cache, const void *object, size_t size,
 	redzone_end = round_up((unsigned long)object + cache->object_size,
 				KASAN_SHADOW_SCALE_SIZE);
 
-	kasan_unpoison_shadow(object, size);
+	if (IS_ENABLED(CONFIG_KASAN_SW_TAGS))
+		tag = assign_tag(cache, object, false);
+
+	/* Tag is ignored in set_tag without CONFIG_KASAN_SW_TAGS */
+	kasan_unpoison_shadow(set_tag(object, tag), size);
 	kasan_poison_shadow((void *)redzone_start, redzone_end - redzone_start,
 		KASAN_KMALLOC_REDZONE);
 
 	if (cache->flags & SLAB_KASAN)
 		set_track(&get_alloc_info(cache, object)->alloc_track, flags);
 
-	return (void *)object;
+	return set_tag(object, tag);
 }
 EXPORT_SYMBOL(kasan_kmalloc);
 
@@ -439,7 +517,7 @@ void kasan_poison_kfree(void *ptr, unsigned long ip)
 	page = virt_to_head_page(ptr);
 
 	if (unlikely(!PageSlab(page))) {
-		if (ptr != page_address(page)) {
+		if (reset_tag(ptr) != page_address(page)) {
 			kasan_report_invalid_free(ptr, ip);
 			return;
 		}
@@ -452,7 +530,7 @@ void kasan_poison_kfree(void *ptr, unsigned long ip)
 
 void kasan_kfree_large(void *ptr, unsigned long ip)
 {
-	if (ptr != page_address(virt_to_head_page(ptr)))
+	if (reset_tag(ptr) != page_address(virt_to_head_page(ptr)))
 		kasan_report_invalid_free(ptr, ip);
 	/* The object will be poisoned by page_alloc. */
 }
diff --git a/mm/kasan/kasan.h b/mm/kasan/kasan.h
index 9b567f742539..0b27ec036e79 100644
--- a/mm/kasan/kasan.h
+++ b/mm/kasan/kasan.h
@@ -12,10 +12,18 @@
 #define KASAN_TAG_INVALID	0xFE /* inaccessible memory tag */
 #define KASAN_TAG_MAX		0xFD /* maximum value for random tags */
 
+#ifdef CONFIG_KASAN_GENERIC
 #define KASAN_FREE_PAGE         0xFF  /* page was freed */
 #define KASAN_PAGE_REDZONE      0xFE  /* redzone for kmalloc_large allocations */
 #define KASAN_KMALLOC_REDZONE   0xFC  /* redzone inside slub object */
 #define KASAN_KMALLOC_FREE      0xFB  /* object was freed (kmem_cache_free/kfree) */
+#else
+#define KASAN_FREE_PAGE         KASAN_TAG_INVALID
+#define KASAN_PAGE_REDZONE      KASAN_TAG_INVALID
+#define KASAN_KMALLOC_REDZONE   KASAN_TAG_INVALID
+#define KASAN_KMALLOC_FREE      KASAN_TAG_INVALID
+#endif
+
 #define KASAN_GLOBAL_REDZONE    0xFA  /* redzone for global variable */
 
 /*
diff --git a/mm/kasan/tags.c b/mm/kasan/tags.c
index 700323946867..a3cca11e4fed 100644
--- a/mm/kasan/tags.c
+++ b/mm/kasan/tags.c
@@ -78,15 +78,60 @@ void *kasan_reset_tag(const void *addr)
 void check_memory_region(unsigned long addr, size_t size, bool write,
 				unsigned long ret_ip)
 {
+	u8 tag;
+	u8 *shadow_first, *shadow_last, *shadow;
+	void *untagged_addr;
+
+	if (unlikely(size == 0))
+		return;
+
+	tag = get_tag((const void *)addr);
+
+	/*
+	 * Ignore accesses for pointers tagged with 0xff (native kernel
+	 * pointer tag) to suppress false positives caused by kmap.
+	 *
+	 * Some kernel code was written to account for archs that don't keep
+	 * high memory mapped all the time, but rather map and unmap particular
+	 * pages when needed. Instead of storing a pointer to the kernel memory,
+	 * this code saves the address of the page structure and offset within
+	 * that page for later use. Those pages are then mapped and unmapped
+	 * with kmap/kunmap when necessary and virt_to_page is used to get the
+	 * virtual address of the page. For arm64 (that keeps the high memory
+	 * mapped all the time), kmap is turned into a page_address call.
+
+	 * The issue is that with use of the page_address + virt_to_page
+	 * sequence the top byte value of the original pointer gets lost (gets
+	 * set to KASAN_TAG_KERNEL (0xFF)).
+	 */
+	if (tag == KASAN_TAG_KERNEL)
+		return;
+
+	untagged_addr = reset_tag((const void *)addr);
+	if (unlikely(untagged_addr <
+			kasan_shadow_to_mem((void *)KASAN_SHADOW_START))) {
+		kasan_report(addr, size, write, ret_ip);
+		return;
+	}
+	shadow_first = kasan_mem_to_shadow(untagged_addr);
+	shadow_last = kasan_mem_to_shadow(untagged_addr + size - 1);
+	for (shadow = shadow_first; shadow <= shadow_last; shadow++) {
+		if (*shadow != tag) {
+			kasan_report(addr, size, write, ret_ip);
+			return;
+		}
+	}
 }
 
 #define DEFINE_HWASAN_LOAD_STORE(size)					\
 	void __hwasan_load##size##_noabort(unsigned long addr)		\
 	{								\
+		check_memory_region(addr, size, false, _RET_IP_);	\
 	}								\
 	EXPORT_SYMBOL(__hwasan_load##size##_noabort);			\
 	void __hwasan_store##size##_noabort(unsigned long addr)		\
 	{								\
+		check_memory_region(addr, size, true, _RET_IP_);	\
 	}								\
 	EXPORT_SYMBOL(__hwasan_store##size##_noabort)
 
@@ -98,15 +143,18 @@ DEFINE_HWASAN_LOAD_STORE(16);
 
 void __hwasan_loadN_noabort(unsigned long addr, unsigned long size)
 {
+	check_memory_region(addr, size, false, _RET_IP_);
 }
 EXPORT_SYMBOL(__hwasan_loadN_noabort);
 
 void __hwasan_storeN_noabort(unsigned long addr, unsigned long size)
 {
+	check_memory_region(addr, size, true, _RET_IP_);
 }
 EXPORT_SYMBOL(__hwasan_storeN_noabort);
 
 void __hwasan_tag_memory(unsigned long addr, u8 tag, unsigned long size)
 {
+	kasan_poison_shadow((void *)addr, size, tag);
 }
 EXPORT_SYMBOL(__hwasan_tag_memory);
-- 
2.19.0.444.g18242da7ef-goog

================================================================================


################################################################################

=== Thread: [PATCH v9 17/20] kasan, arm64: add brk handler for inline instrumentation ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 17/20] kasan, arm64: add brk handler for inline instrumentation
Date: Fri, 21 Sep 2018 15:13:39 +0000
Message-ID: <31b54fe2d9b950e9e8c9cd303808db35c730548c.1537542735.git.andreyknvl () google ! com>
--------------------
Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
memory into the generated code, instead of inserting a callback) generates
a brk instruction when a tag mismatch is detected.

This commit adds a tag-based KASAN specific brk handler, that decodes the
immediate value passed to the brk instructions (to extract information
about the memory access that triggered the mismatch), reads the register
values (x0 contains the guilty address) and reports the bug.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/include/asm/brk-imm.h |  2 +
 arch/arm64/kernel/traps.c        | 68 +++++++++++++++++++++++++++++++-
 include/linux/kasan.h            |  3 ++
 3 files changed, 71 insertions(+), 2 deletions(-)

diff --git a/arch/arm64/include/asm/brk-imm.h b/arch/arm64/include/asm/brk-imm.h
index ed693c5bcec0..2945fe6cd863 100644
--- a/arch/arm64/include/asm/brk-imm.h
+++ b/arch/arm64/include/asm/brk-imm.h
@@ -16,10 +16,12 @@
  * 0x400: for dynamic BRK instruction
  * 0x401: for compile time BRK instruction
  * 0x800: kernel-mode BUG() and WARN() traps
+ * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff)
  */
 #define FAULT_BRK_IMM			0x100
 #define KGDB_DYN_DBG_BRK_IMM		0x400
 #define KGDB_COMPILED_DBG_BRK_IMM	0x401
 #define BUG_BRK_IMM			0x800
+#define KASAN_BRK_IMM			0x900
 
 #endif
diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 039e9ff379cc..ca0c00f5b6dd 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -35,6 +35,7 @@
 #include <linux/sizes.h>
 #include <linux/syscalls.h>
 #include <linux/mm_types.h>
+#include <linux/kasan.h>
 
 #include <asm/atomic.h>
 #include <asm/bug.h>
@@ -269,10 +270,14 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
 	}
 }
 
-void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+void __arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
 {
 	regs->pc += size;
+}
 
+void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+{
+	__arm64_skip_faulting_instruction(regs, size);
 	/*
 	 * If we were single stepping, we want to get the step exception after
 	 * we return from the trap.
@@ -775,7 +780,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
 	}
 
 	/* If thread survives, skip over the BUG instruction and continue: */
-	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 	return DBG_HOOK_HANDLED;
 }
 
@@ -785,6 +790,58 @@ static struct break_hook bug_break_hook = {
 	.fn = bug_handler,
 };
 
+#ifdef CONFIG_KASAN_SW_TAGS
+
+#define KASAN_ESR_RECOVER	0x20
+#define KASAN_ESR_WRITE	0x10
+#define KASAN_ESR_SIZE_MASK	0x0f
+#define KASAN_ESR_SIZE(esr)	(1 << ((esr) & KASAN_ESR_SIZE_MASK))
+
+static int kasan_handler(struct pt_regs *regs, unsigned int esr)
+{
+	bool recover = esr & KASAN_ESR_RECOVER;
+	bool write = esr & KASAN_ESR_WRITE;
+	size_t size = KASAN_ESR_SIZE(esr);
+	u64 addr = regs->regs[0];
+	u64 pc = regs->pc;
+
+	if (user_mode(regs))
+		return DBG_HOOK_ERROR;
+
+	kasan_report(addr, size, write, pc);
+
+	/*
+	 * The instrumentation allows to control whether we can proceed after
+	 * a crash was detected. This is done by passing the -recover flag to
+	 * the compiler. Disabling recovery allows to generate more compact
+	 * code.
+	 *
+	 * Unfortunately disabling recovery doesn't work for the kernel right
+	 * now. KASAN reporting is disabled in some contexts (for example when
+	 * the allocator accesses slab object metadata; this is controlled by
+	 * current->kasan_depth). All these accesses are detected by the tool,
+	 * even though the reports for them are not printed.
+	 *
+	 * This is something that might be fixed at some point in the future.
+	 */
+	if (!recover)
+		die("Oops - KASAN", regs, 0);
+
+	/* If thread survives, skip over the brk instruction and continue: */
+	__arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	return DBG_HOOK_HANDLED;
+}
+
+#define KASAN_ESR_VAL (0xf2000000 | KASAN_BRK_IMM)
+#define KASAN_ESR_MASK 0xffffff00
+
+static struct break_hook kasan_break_hook = {
+	.esr_val = KASAN_ESR_VAL,
+	.esr_mask = KASAN_ESR_MASK,
+	.fn = kasan_handler,
+};
+#endif
+
 /*
  * Initial handler for AArch64 BRK exceptions
  * This handler only used until debug_traps_init().
@@ -792,6 +849,10 @@ static struct break_hook bug_break_hook = {
 int __init early_brk64(unsigned long addr, unsigned int esr,
 		struct pt_regs *regs)
 {
+#ifdef CONFIG_KASAN_SW_TAGS
+	if ((esr & KASAN_ESR_MASK) == KASAN_ESR_VAL)
+		return kasan_handler(regs, esr) != DBG_HOOK_HANDLED;
+#endif
 	return bug_handler(regs, esr) != DBG_HOOK_HANDLED;
 }
 
@@ -799,4 +860,7 @@ int __init early_brk64(unsigned long addr, unsigned int esr,
 void __init trap_init(void)
 {
 	register_break_hook(&bug_break_hook);
+#ifdef CONFIG_KASAN_SW_TAGS
+	register_break_hook(&kasan_break_hook);
+#endif
 }
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index 4c9d6f9029f2..d5a2a7f1f72c 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -173,6 +173,9 @@ void kasan_init_tags(void);
 
 void *kasan_reset_tag(const void *addr);
 
+void kasan_report(unsigned long addr, size_t size,
+		bool is_write, unsigned long ip);
+
 #else /* CONFIG_KASAN_SW_TAGS */
 
 static inline void kasan_init_tags(void) { }
-- 
2.19.0.444.g18242da7ef-goog

================================================================================


################################################################################

=== Thread: [PATCH v9 19/20] kasan: update documentation ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 19/20] kasan: update documentation
Date: Fri, 21 Sep 2018 15:13:41 +0000
Message-ID: <47b386e1715275f60ea55f93eb00deb1f3451b3a.1537542735.git.andreyknvl () google ! com>
--------------------
This patch updates KASAN documentation to reflect the addition of the new
tag-based mode.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 Documentation/dev-tools/kasan.rst | 232 ++++++++++++++++++------------
 1 file changed, 138 insertions(+), 94 deletions(-)

diff --git a/Documentation/dev-tools/kasan.rst b/Documentation/dev-tools/kasan.rst
index aabc8738b3d8..a407e18afd32 100644
--- a/Documentation/dev-tools/kasan.rst
+++ b/Documentation/dev-tools/kasan.rst
@@ -4,15 +4,25 @@ The Kernel Address Sanitizer (KASAN)
 Overview
 --------
 
-KernelAddressSANitizer (KASAN) is a dynamic memory error detector. It provides
-a fast and comprehensive solution for finding use-after-free and out-of-bounds
-bugs.
+KernelAddressSANitizer (KASAN) is a dynamic memory error detector designed to
+find out-of-bound and use-after-free bugs. KASAN has two modes: generic KASAN
+(similar to userspace ASan) and software tag-based KASAN (similar to userspace
+HWASan).
 
-KASAN uses compile-time instrumentation for checking every memory access,
-therefore you will need a GCC version 4.9.2 or later. GCC 5.0 or later is
-required for detection of out-of-bounds accesses to stack or global variables.
+KASAN uses compile-time instrumentation to insert validity checks before every
+memory access, and therefore requires a compiler version that supports that.
 
-Currently KASAN is supported only for the x86_64 and arm64 architectures.
+Generic KASAN is supported in both GCC and Clang. With GCC it requires version
+4.9.2 or later for basic support and version 5.0 or later for detection of
+out-of-bounds accesses for stack and global variables and for inline
+instrumentation mode (see the Usage section). With Clang it requires version
+3.7.0 or later and it doesn't support detection of out-of-bounds accesses for
+global variables yet.
+
+Tag-based KASAN is only supported in Clang and requires version 7.0.0 or later.
+
+Currently generic KASAN is supported for the x86_64, arm64 and xtensa
+architectures, and tag-based KASAN is supported only for arm64.
 
 Usage
 -----
@@ -21,12 +31,14 @@ To enable KASAN configure kernel with::
 
 	  CONFIG_KASAN = y
 
-and choose between CONFIG_KASAN_OUTLINE and CONFIG_KASAN_INLINE. Outline and
-inline are compiler instrumentation types. The former produces smaller binary
-the latter is 1.1 - 2 times faster. Inline instrumentation requires a GCC
-version 5.0 or later.
+and choose between CONFIG_KASAN_GENERIC (to enable generic KASAN) and
+CONFIG_KASAN_SW_TAGS (to enable software tag-based KASAN).
 
-KASAN works with both SLUB and SLAB memory allocators.
+You also need to choose between CONFIG_KASAN_OUTLINE and CONFIG_KASAN_INLINE.
+Outline and inline are compiler instrumentation types. The former produces
+smaller binary while the latter is 1.1 - 2 times faster.
+
+Both KASAN modes work with both SLUB and SLAB memory allocators.
 For better bug detection and nicer reporting, enable CONFIG_STACKTRACE.
 
 To disable instrumentation for specific files or directories, add a line
@@ -43,85 +55,85 @@ similar to the following to the respective kernel Makefile:
 Error reports
 ~~~~~~~~~~~~~
 
-A typical out of bounds access report looks like this::
+A typical out-of-bounds access generic KASAN report looks like this::
 
     ==================================================================
-    BUG: AddressSanitizer: out of bounds access in kmalloc_oob_right+0x65/0x75 [test_kasan] at addr ffff8800693bc5d3
-    Write of size 1 by task modprobe/1689
-    =============================================================================
-    BUG kmalloc-128 (Not tainted): kasan error
-    -----------------------------------------------------------------------------
-
-    Disabling lock debugging due to kernel taint
-    INFO: Allocated in kmalloc_oob_right+0x3d/0x75 [test_kasan] age=0 cpu=0 pid=1689
-     __slab_alloc+0x4b4/0x4f0
-     kmem_cache_alloc_trace+0x10b/0x190
-     kmalloc_oob_right+0x3d/0x75 [test_kasan]
-     init_module+0x9/0x47 [test_kasan]
-     do_one_initcall+0x99/0x200
-     load_module+0x2cb3/0x3b20
-     SyS_finit_module+0x76/0x80
-     system_call_fastpath+0x12/0x17
-    INFO: Slab 0xffffea0001a4ef00 objects=17 used=7 fp=0xffff8800693bd728 flags=0x100000000004080
-    INFO: Object 0xffff8800693bc558 @offset=1368 fp=0xffff8800693bc720
-
-    Bytes b4 ffff8800693bc548: 00 00 00 00 00 00 00 00 5a 5a 5a 5a 5a 5a 5a 5a  ........ZZZZZZZZ
-    Object ffff8800693bc558: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b  kkkkkkkkkkkkkkkk
-    Object ffff8800693bc568: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b  kkkkkkkkkkkkkkkk
-    Object ffff8800693bc578: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b  kkkkkkkkkkkkkkkk
-    Object ffff8800693bc588: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b  kkkkkkkkkkkkkkkk
-    Object ffff8800693bc598: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b  kkkkkkkkkkkkkkkk
-    Object ffff8800693bc5a8: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b  kkkkkkkkkkkkkkkk
-    Object ffff8800693bc5b8: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b  kkkkkkkkkkkkkkkk
-    Object ffff8800693bc5c8: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b a5  kkkkkkkkkkkkkkk.
-    Redzone ffff8800693bc5d8: cc cc cc cc cc cc cc cc                          ........
-    Padding ffff8800693bc718: 5a 5a 5a 5a 5a 5a 5a 5a                          ZZZZZZZZ
-    CPU: 0 PID: 1689 Comm: modprobe Tainted: G    B          3.18.0-rc1-mm1+ #98
-    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.7.5-0-ge51488c-20140602_164612-nilsson.home.kraxel.org 04/01/2014
-     ffff8800693bc000 0000000000000000 ffff8800693bc558 ffff88006923bb78
-     ffffffff81cc68ae 00000000000000f3 ffff88006d407600 ffff88006923bba8
-     ffffffff811fd848 ffff88006d407600 ffffea0001a4ef00 ffff8800693bc558
+    BUG: KASAN: slab-out-of-bounds in kmalloc_oob_right+0xa8/0xbc [test_kasan]
+    Write of size 1 at addr ffff8801f44ec37b by task insmod/2760
+    
+    CPU: 1 PID: 2760 Comm: insmod Not tainted 4.19.0-rc3+ #698
+    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.10.2-1 04/01/2014
     Call Trace:
-     [<ffffffff81cc68ae>] dump_stack+0x46/0x58
-     [<ffffffff811fd848>] print_trailer+0xf8/0x160
-     [<ffffffffa00026a7>] ? kmem_cache_oob+0xc3/0xc3 [test_kasan]
-     [<ffffffff811ff0f5>] object_err+0x35/0x40
-     [<ffffffffa0002065>] ? kmalloc_oob_right+0x65/0x75 [test_kasan]
-     [<ffffffff8120b9fa>] kasan_report_error+0x38a/0x3f0
-     [<ffffffff8120a79f>] ? kasan_poison_shadow+0x2f/0x40
-     [<ffffffff8120b344>] ? kasan_unpoison_shadow+0x14/0x40
-     [<ffffffff8120a79f>] ? kasan_poison_shadow+0x2f/0x40
-     [<ffffffffa00026a7>] ? kmem_cache_oob+0xc3/0xc3 [test_kasan]
-     [<ffffffff8120a995>] __asan_store1+0x75/0xb0
-     [<ffffffffa0002601>] ? kmem_cache_oob+0x1d/0xc3 [test_kasan]
-     [<ffffffffa0002065>] ? kmalloc_oob_right+0x65/0x75 [test_kasan]
-     [<ffffffffa0002065>] kmalloc_oob_right+0x65/0x75 [test_kasan]
-     [<ffffffffa00026b0>] init_module+0x9/0x47 [test_kasan]
-     [<ffffffff810002d9>] do_one_initcall+0x99/0x200
-     [<ffffffff811e4e5c>] ? __vunmap+0xec/0x160
-     [<ffffffff81114f63>] load_module+0x2cb3/0x3b20
-     [<ffffffff8110fd70>] ? m_show+0x240/0x240
-     [<ffffffff81115f06>] SyS_finit_module+0x76/0x80
-     [<ffffffff81cd3129>] system_call_fastpath+0x12/0x17
+     dump_stack+0x94/0xd8
+     print_address_description+0x73/0x280
+     kasan_report+0x144/0x187
+     __asan_report_store1_noabort+0x17/0x20
+     kmalloc_oob_right+0xa8/0xbc [test_kasan]
+     kmalloc_tests_init+0x16/0x700 [test_kasan]
+     do_one_initcall+0xa5/0x3ae
+     do_init_module+0x1b6/0x547
+     load_module+0x75df/0x8070
+     __do_sys_init_module+0x1c6/0x200
+     __x64_sys_init_module+0x6e/0xb0
+     do_syscall_64+0x9f/0x2c0
+     entry_SYSCALL_64_after_hwframe+0x44/0xa9
+    RIP: 0033:0x7f96443109da
+    RSP: 002b:00007ffcf0b51b08 EFLAGS: 00000202 ORIG_RAX: 00000000000000af
+    RAX: ffffffffffffffda RBX: 000055dc3ee521a0 RCX: 00007f96443109da
+    RDX: 00007f96445cff88 RSI: 0000000000057a50 RDI: 00007f9644992000
+    RBP: 000055dc3ee510b0 R08: 0000000000000003 R09: 0000000000000000
+    R10: 00007f964430cd0a R11: 0000000000000202 R12: 00007f96445cff88
+    R13: 000055dc3ee51090 R14: 0000000000000000 R15: 0000000000000000
+    
+    Allocated by task 2760:
+     save_stack+0x43/0xd0
+     kasan_kmalloc+0xa7/0xd0
+     kmem_cache_alloc_trace+0xe1/0x1b0
+     kmalloc_oob_right+0x56/0xbc [test_kasan]
+     kmalloc_tests_init+0x16/0x700 [test_kasan]
+     do_one_initcall+0xa5/0x3ae
+     do_init_module+0x1b6/0x547
+     load_module+0x75df/0x8070
+     __do_sys_init_module+0x1c6/0x200
+     __x64_sys_init_module+0x6e/0xb0
+     do_syscall_64+0x9f/0x2c0
+     entry_SYSCALL_64_after_hwframe+0x44/0xa9
+    
+    Freed by task 815:
+     save_stack+0x43/0xd0
+     __kasan_slab_free+0x135/0x190
+     kasan_slab_free+0xe/0x10
+     kfree+0x93/0x1a0
+     umh_complete+0x6a/0xa0
+     call_usermodehelper_exec_async+0x4c3/0x640
+     ret_from_fork+0x35/0x40
+    
+    The buggy address belongs to the object at ffff8801f44ec300
+     which belongs to the cache kmalloc-128 of size 128
+    The buggy address is located 123 bytes inside of
+     128-byte region [ffff8801f44ec300, ffff8801f44ec380)
+    The buggy address belongs to the page:
+    page:ffffea0007d13b00 count:1 mapcount:0 mapping:ffff8801f7001640 index:0x0
+    flags: 0x200000000000100(slab)
+    raw: 0200000000000100 ffffea0007d11dc0 0000001a0000001a ffff8801f7001640
+    raw: 0000000000000000 0000000080150015 00000001ffffffff 0000000000000000
+    page dumped because: kasan: bad access detected
+    
     Memory state around the buggy address:
-     ffff8800693bc300: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
-     ffff8800693bc380: fc fc 00 00 00 00 00 00 00 00 00 00 00 00 00 fc
-     ffff8800693bc400: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
-     ffff8800693bc480: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
-     ffff8800693bc500: fc fc fc fc fc fc fc fc fc fc fc 00 00 00 00 00
-    >ffff8800693bc580: 00 00 00 00 00 00 00 00 00 00 03 fc fc fc fc fc
-                                                 ^
-     ffff8800693bc600: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
-     ffff8800693bc680: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc
-     ffff8800693bc700: fc fc fc fc fb fb fb fb fb fb fb fb fb fb fb fb
-     ffff8800693bc780: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
-     ffff8800693bc800: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
+     ffff8801f44ec200: fc fc fc fc fc fc fc fc fb fb fb fb fb fb fb fb
+     ffff8801f44ec280: fb fb fb fb fb fb fb fb fc fc fc fc fc fc fc fc
+    >ffff8801f44ec300: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 03
+                                                                    ^
+     ffff8801f44ec380: fc fc fc fc fc fc fc fc fb fb fb fb fb fb fb fb
+     ffff8801f44ec400: fb fb fb fb fb fb fb fb fc fc fc fc fc fc fc fc
     ==================================================================
 
-The header of the report discribe what kind of bug happened and what kind of
-access caused it. It's followed by the description of the accessed slub object
-(see 'SLUB Debug output' section in Documentation/vm/slub.rst for details) and
-the description of the accessed memory page.
+The header of the report provides a short summary of what kind of bug happened
+and what kind of access caused it. It's followed by a stack trace of the bad
+access, a stack trace of where the accessed memory was allocated (in case bad
+access happens on a slab object), and a stack trace of where the object was
+freed (in case of a use-after-free bug report). Next comes a description of
+the accessed slab object and information about the accessed memory page.
 
 In the last section the report shows memory state around the accessed address.
 Reading this part requires some understanding of how KASAN works.
@@ -138,18 +150,24 @@ inaccessible memory like redzones or freed memory (see mm/kasan/kasan.h).
 In the report above the arrows point to the shadow byte 03, which means that
 the accessed address is partially accessible.
 
+For tag-based KASAN this last report section shows the memory tags around the
+accessed address (see Implementation details section).
+
 
 Implementation details
 ----------------------
 
+Generic KASAN
+~~~~~~~~~~~~~
+
 From a high level, our approach to memory error detection is similar to that
 of kmemcheck: use shadow memory to record whether each byte of memory is safe
-to access, and use compile-time instrumentation to check shadow memory on each
-memory access.
+to access, and use compile-time instrumentation to insert checks of shadow
+memory on each memory access.
 
-AddressSanitizer dedicates 1/8 of kernel memory to its shadow memory
-(e.g. 16TB to cover 128TB on x86_64) and uses direct mapping with a scale and
-offset to translate a memory address to its corresponding shadow address.
+Generic KASAN dedicates 1/8th of kernel memory to its shadow memory (e.g. 16TB
+to cover 128TB on x86_64) and uses direct mapping with a scale and offset to
+translate a memory address to its corresponding shadow address.
 
 Here is the function which translates an address to its corresponding shadow
 address::
@@ -162,12 +180,38 @@ address::
 
 where ``KASAN_SHADOW_SCALE_SHIFT = 3``.
 
-Compile-time instrumentation used for checking memory accesses. Compiler inserts
-function calls (__asan_load*(addr), __asan_store*(addr)) before each memory
-access of size 1, 2, 4, 8 or 16. These functions check whether memory access is
-valid or not by checking corresponding shadow memory.
+Compile-time instrumentation is used to insert memory access checks. Compiler
+inserts function calls (__asan_load*(addr), __asan_store*(addr)) before each
+memory access of size 1, 2, 4, 8 or 16. These functions check whether memory
+access is valid or not by checking corresponding shadow memory.
 
 GCC 5.0 has possibility to perform inline instrumentation. Instead of making
 function calls GCC directly inserts the code to check the shadow memory.
 This option significantly enlarges kernel but it gives x1.1-x2 performance
 boost over outline instrumented kernel.
+
+Software tag-based KASAN
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+Tag-based KASAN uses the Top Byte Ignore (TBI) feature of modern arm64 CPUs to
+store a pointer tag in the top byte of kernel pointers. Like generic KASAN it
+uses shadow memory to store memory tags associated with each 16-byte memory
+cell (therefore it dedicates 1/16th of the kernel memory for shadow memory).
+
+On each memory allocation tag-based KASAN generates a random tag, tags the
+allocated memory with this tag, and embeds this tag into the returned pointer.
+Software tag-based KASAN uses compile-time instrumentation to insert checks
+before each memory access. These checks make sure that tag of the memory that
+is being accessed is equal to tag of the pointer that is used to access this
+memory. In case of a tag mismatch tag-based KASAN prints a bug report.
+
+Software tag-based KASAN also has two instrumentation modes (outline, that
+emits callbacks to check memory accesses; and inline, that performs the shadow
+memory checks inline). With outline instrumentation mode, a bug report is
+simply printed from the function that performs the access check. With inline
+instrumentation a brk instruction is emitted by the compiler, and a dedicated
+brk handler is used to print bug reports.
+
+A potential expansion of this mode is a hardware tag-based mode, which would
+use hardware memory tagging support instead of compiler instrumentation and
+manual shadow memory manipulation.
-- 
2.19.0.444.g18242da7ef-goog

================================================================================


################################################################################

=== Thread: [PATCH v9 20/20] kasan: add SPDX-License-Identifier mark to source files ===

From: Andrey Konovalov <andreyknvl () google ! com>
To: linux-sparse
Subject: [PATCH v9 20/20] kasan: add SPDX-License-Identifier mark to source files
Date: Fri, 21 Sep 2018 15:13:42 +0000
Message-ID: <6f0b4d6d72028c0745ac044722af07cce6a795c8.1537542735.git.andreyknvl () google ! com>
--------------------
This patch adds a "SPDX-License-Identifier: GPL-2.0" mark to all source
files under mm/kasan.

Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 mm/kasan/common.c         | 1 +
 mm/kasan/generic.c        | 1 +
 mm/kasan/generic_report.c | 1 +
 mm/kasan/init.c           | 1 +
 mm/kasan/quarantine.c     | 1 +
 mm/kasan/report.c         | 1 +
 mm/kasan/tags.c           | 1 +
 mm/kasan/tags_report.c    | 1 +
 8 files changed, 8 insertions(+)

diff --git a/mm/kasan/common.c b/mm/kasan/common.c
index 195ca385cf7a..c469fc4c9e2b 100644
--- a/mm/kasan/common.c
+++ b/mm/kasan/common.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains common generic and tag-based KASAN code.
  *
diff --git a/mm/kasan/generic.c b/mm/kasan/generic.c
index b8de6d33c55c..ccb6207276e3 100644
--- a/mm/kasan/generic.c
+++ b/mm/kasan/generic.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains core generic KASAN code.
  *
diff --git a/mm/kasan/generic_report.c b/mm/kasan/generic_report.c
index a4604cceae59..5e12035888f2 100644
--- a/mm/kasan/generic_report.c
+++ b/mm/kasan/generic_report.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains generic KASAN specific error reporting code.
  *
diff --git a/mm/kasan/init.c b/mm/kasan/init.c
index 7a2a2f13f86f..b3c068ab2a85 100644
--- a/mm/kasan/init.c
+++ b/mm/kasan/init.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains some kasan initialization code.
  *
diff --git a/mm/kasan/quarantine.c b/mm/kasan/quarantine.c
index 3a8ddf8baf7d..0e4dc1a22615 100644
--- a/mm/kasan/quarantine.c
+++ b/mm/kasan/quarantine.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * KASAN quarantine.
  *
diff --git a/mm/kasan/report.c b/mm/kasan/report.c
index 214d85035f99..ca9418fe9232 100644
--- a/mm/kasan/report.c
+++ b/mm/kasan/report.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains common generic and tag-based KASAN error reporting code.
  *
diff --git a/mm/kasan/tags.c b/mm/kasan/tags.c
index a3cca11e4fed..7b7c21d40851 100644
--- a/mm/kasan/tags.c
+++ b/mm/kasan/tags.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains core tag-based KASAN code.
  *
diff --git a/mm/kasan/tags_report.c b/mm/kasan/tags_report.c
index 573c51d20d09..8eaf5f722271 100644
--- a/mm/kasan/tags_report.c
+++ b/mm/kasan/tags_report.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains tag-based KASAN specific error reporting code.
  *
-- 
2.19.0.444.g18242da7ef-goog

================================================================================


################################################################################

=== Thread: [PATCH] Add testcases for bitwise cast on pointer ===

From: Thiebaud Weksteen <tweek () google ! com>
To: linux-sparse
Subject: [PATCH] Add testcases for bitwise cast on pointer
Date: Wed, 25 Apr 2018 12:02:17 +0000
Message-ID: <20180425120217.90744-1-tweek () google ! com>
--------------------
Signed-off-by: Thiebaud Weksteen <tweek@google.com>
---
 validation/bitwise-cast.c | 46 +++++++++++++++++++++++++++++++--------
 1 file changed, 37 insertions(+), 9 deletions(-)

diff --git a/validation/bitwise-cast.c b/validation/bitwise-cast.c
index baeca29..8eaabb7 100644
--- a/validation/bitwise-cast.c
+++ b/validation/bitwise-cast.c
@@ -18,8 +18,6 @@ static __be32 bar(void)
 /* Implicit casts of nonzero, bad */
 static __be32 baz(void)
 {
-	__be32 x = 0x2a;
-
 	return 99;
 }
 
@@ -29,16 +27,46 @@ static __be32 quux(void)
 	return (__be32)1729;
 }
 
+/* Explicit case of nonzero forced, legal */
+static __be32 quuy(void)
+{
+	return (__attribute__((force)) __be32) 1730;
+}
+
+static u32 global_i = 31337;
+
+/* Implicit cast of pointer, bad */
+static __be32* quuz(void)
+{
+	u32* x = &global_i;
+	return x;
+}
+
+/* Explicit cast of pointer, bad */
+static __be32* quva(void)
+{
+	u32* x = &global_i;
+	return (__be32 *) x;
+}
+
+/* Explicit cast of pointer forced, legal */
+static __be32* quvb(void)
+{
+	u32* x = &global_i;
+	return (__attribute__((force)) __be32 *) x;
+}
+
 /*
  * check-name: conversions to bitwise types
  * check-command: sparse -Wbitwise $file
  * check-error-start
-bitwise-cast.c:21:20: warning: incorrect type in initializer (different base types)
-bitwise-cast.c:21:20:    expected restricted __be32 [usertype] x
-bitwise-cast.c:21:20:    got int
-bitwise-cast.c:23:16: warning: incorrect type in return expression (different base types)
-bitwise-cast.c:23:16:    expected restricted __be32
-bitwise-cast.c:23:16:    got int
-bitwise-cast.c:29:17: warning: cast to restricted __be32
+bitwise-cast.c:22:16: warning: incorrect type in return expression (different base types)
+bitwise-cast.c:22:16:    expected restricted __be32
+bitwise-cast.c:22:16:    got int
+bitwise-cast.c:28:17: warning: cast to restricted __be32
+bitwise-cast.c:43:16: warning: incorrect type in return expression (different base types)
+bitwise-cast.c:43:16:    expected restricted __be32 [usertype] *
+bitwise-cast.c:43:16:    got unsigned int [usertype] *x
+bitwise-cast.c:50:17: warning: cast to restricted __be32 *
  * check-error-end
  */
-- 
2.17.0.484.g0c8726318c-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH] Add testcases for bitwise cast on pointer
Date: Thu, 26 Apr 2018 19:22:16 +0000
Message-ID: <20180426192215.p3sjv5ura6oegqrw () ltop ! local>
--------------------
Thanks for the patch.
I've changed it a bit though, putting the pointers part in
a different fine, in order to not mix the tests with simple
__bitwise types (which are OK) with those on __bitwise pointers
(which fail). You can see the result on:
	git://github.com/lucvoo/sparse-dev.git bug-restricted-ptr


Cheers,
-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] Add testcases for bitwise cast on pointer
Date: Sat, 15 Dec 2018 23:32:18 +0000
Message-ID: <20181215233218.43553-1-luc.vanoostenryck () gmail ! com>
--------------------
From: Thiebaud Weksteen <tweek@google.com>

since it seems that the strict type checking is not done
on pointers to restricted types.

Signed-off-by: Thiebaud Weksteen <tweek@google.com>
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/bitwise-cast-ptr.c | 34 ++++++++++++++++++++++++++++++++++
 validation/bitwise-cast.c     |  6 ++++++
 2 files changed, 40 insertions(+)
 create mode 100644 validation/bitwise-cast-ptr.c

diff --git a/validation/bitwise-cast-ptr.c b/validation/bitwise-cast-ptr.c
new file mode 100644
index 000000000..c390092ee
--- /dev/null
+++ b/validation/bitwise-cast-ptr.c
@@ -0,0 +1,34 @@
+#define __bitwise	__attribute__((bitwise))
+#define __force		__attribute__((force))
+
+typedef unsigned int u32;
+typedef unsigned int __bitwise __be32;
+
+static __be32* tobi(u32 *x)
+{
+	return x;			// should warn, implicit cast
+}
+
+static __be32* tobe(u32 *x)
+{
+	return (__be32 *) x;		// should warn, explicit cast
+}
+
+static __be32* tobf(u32 *x)
+{
+	return (__force __be32 *) x;	// should not warn, forced cast
+	return (__be32 __force *) x;	// should not warn, forced cast
+}
+
+/*
+ * check-name: cast of bitwise pointers
+ * check-command: sparse -Wbitwise $file
+ * check-known-to-fail
+ *
+ * check-error-start
+bitwise-cast-ptr.c:9:16: warning: incorrect type in return expression (different base types)
+bitwise-cast-ptr.c:9:16:    expected restricted __be32 [usertype] *
+bitwise-cast-ptr.c:9:16:    got unsigned int [usertype] *x
+bitwise-cast-ptr.c:14:17: warning: cast to restricted __be32 [usertype] *
+ * check-error-end
+ */
diff --git a/validation/bitwise-cast.c b/validation/bitwise-cast.c
index baeca29e7..0583461cb 100644
--- a/validation/bitwise-cast.c
+++ b/validation/bitwise-cast.c
@@ -29,6 +29,12 @@ static __be32 quux(void)
 	return (__be32)1729;
 }
 
+/* Explicit case of nonzero forced, legal */
+static __be32 quuy(void)
+{
+	return (__attribute__((force)) __be32) 1730;
+}
+
 /*
  * check-name: conversions to bitwise types
  * check-command: sparse -Wbitwise $file
-- 
2.20.0

================================================================================


################################################################################

=== Thread: [PATCH] Honor CPPFLAGS ===

From: =?UTF-8?q?Uwe=20Kleine-K=C3=B6nig?= <uwe () kleine-koenig ! org>
To: linux-sparse
Subject: [PATCH] Honor CPPFLAGS
Date: Thu, 19 Apr 2018 04:54:18 +0000
Message-ID: <20180419045418.23676-1-uwe () kleine-koenig ! org>
--------------------
CPPFLAGS is a standard variable that should be passed to the compiler.

Debian's package build tools pass "-Wdate-time -D_FORTIFY_SOURCE=2" in this
variable.

Signed-off-by: Uwe Kleine-KÃ¶nig <uwe@kleine-koenig.org>
---
 Makefile | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/Makefile b/Makefile
index 0bd370b8dc0e..a72802ff986a 100644
--- a/Makefile
+++ b/Makefile
@@ -213,10 +213,10 @@ c2xml.o c2xml.sc: PKG_CFLAGS += $(LIBXML_CFLAGS)
 pre-process.sc: CHECKER_FLAGS += -Wno-vla
 
 %.o: %.c $(LIB_H)
-	$(QUIET_CC)$(CC) -o $@ -c $(ALL_CFLAGS) $<
+	$(QUIET_CC)$(CC) -o $@ -c $(ALL_CFLAGS) $(CPPFLAGS) $<
 
 %.sc: %.c sparse
-	$(QUIET_CHECK) $(CHECKER) $(CHECKER_FLAGS) -c $(ALL_CFLAGS) $<
+	$(QUIET_CHECK) $(CHECKER) $(CHECKER_FLAGS) -c $(ALL_CFLAGS) $(CPPFLAGS) $<
 
 ALL_OBJS :=  $(LIB_OBJS) $(foreach p,$(PROGRAMS),$(p).o $($(p)_EXTRA_DEPS))
 selfcheck: $(ALL_OBJS:.o=.sc)
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH] Honor CPPFLAGS
Date: Thu, 19 Apr 2018 07:54:32 +0000
Message-ID: <20180419075419.4ugtz3pqukval34d () ltop ! local>
--------------------
On Thu, Apr 19, 2018 at 06:54:18AM +0200, Uwe Kleine-König wrote:
> CPPFLAGS is a standard variable that should be passed to the compiler.
> 
> Debian's package build tools pass "-Wdate-time -D_FORTIFY_SOURCE=2" in this
> variable.
> 
> Signed-off-by: Uwe Kleine-König <uwe@kleine-koenig.org>

Acked-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] Makefile: Use find's -delete instead of -exec rm {} \; ===

From: =?UTF-8?q?Uwe=20Kleine-K=C3=B6nig?= <uwe () kleine-koenig ! org>
To: linux-sparse
Subject: [PATCH] Makefile: Use find's -delete instead of -exec rm {} \;
Date: Thu, 19 Apr 2018 05:19:30 +0000
Message-ID: <20180419051930.25468-1-uwe () kleine-koenig ! org>
--------------------
find knows how to delete files without the need to fork.

Signed-off-by: Uwe Kleine-KÃ¶nig <uwe@kleine-koenig.org>
---
 Makefile | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/Makefile b/Makefile
index faa6ea8d4551..c6bfe035076b 100644
--- a/Makefile
+++ b/Makefile
@@ -243,4 +243,4 @@ clean-check:
 	                 -o -name "*.c.error.got" \
 	                 -o -name "*.c.error.diff" \
 			 -o -name "*.o" \
-	                 \) -exec rm {} \;
+	                 \) -delete
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] add test case bug expand union ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] add test case bug expand union
Date: Mon, 16 Apr 2018 14:40:29 +0000
Message-ID: <20180416144029.84231-1-luc.vanoostenryck () gmail ! com>
--------------------
constant_symbol_value() needs several conditions before returning
a valid value.

However not all conditions are always met. One such case occurs when
an union is involved and the value we look after is of a different
type than the initializer.

Add two test cases showing (some of) the problems.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/bug-expand-union0.c | 21 +++++++++++++++++++++
 validation/bug-expand-union1.c | 20 ++++++++++++++++++++
 2 files changed, 41 insertions(+)
 create mode 100644 validation/bug-expand-union0.c
 create mode 100644 validation/bug-expand-union1.c

diff --git a/validation/bug-expand-union0.c b/validation/bug-expand-union0.c
new file mode 100644
index 000000000..dd6d60c3e
--- /dev/null
+++ b/validation/bug-expand-union0.c
@@ -0,0 +1,21 @@
+union u {
+	char c;
+	float f;
+};
+
+static int foo(void)
+{
+	union u u = { .f = 0.123 };
+	return u.c;
+}
+
+/*
+ * check-name: bug-expand-union
+ * check description: must not infer the value from the float
+ * check-command: test-linearize $file
+ * check-known-to-fail
+ *
+ * check-output-ignore
+ * check-output-contains: load\\.
+ * check-output-excludes: ret\\..*\\$
+ */
diff --git a/validation/bug-expand-union1.c b/validation/bug-expand-union1.c
new file mode 100644
index 000000000..582a1f4f8
--- /dev/null
+++ b/validation/bug-expand-union1.c
@@ -0,0 +1,20 @@
+union u {
+	int i;
+	float f;
+};
+
+static int foo(void)
+{
+	union u u = { .f = 0.123 };
+	return u.i;
+}
+
+/*
+ * check-name: bug-expand-union
+ * check description: must not infer the value from the float
+ * check-command: test-linearize $file
+ * check-known-to-fail
+ *
+ * check-output-ignore
+ * check-output-contains: load\\.
+ */
-- 
2.16.3

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] add test for integer-const-expr-ness ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] add test for integer-const-expr-ness
Date: Thu, 31 May 2018 04:08:17 +0000
Message-ID: <20180531040817.38001-1-luc.vanoostenryck () gmail ! com>
--------------------
This use Martin's awesome macro to test if sparse's
notion of integer-const-expr is the same as GCC's.

It test also that the result of this macro is itself a
constant integer expression.

Awesome-macro-by: Martin Uecker <Martin.Uecker@med.uni-goettingen.de>
Test-originally-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/integer-const-expr.c | 85 +++++++++++++++++++++++++++++++++
 1 file changed, 85 insertions(+)
 create mode 100644 validation/integer-const-expr.c

diff --git a/validation/integer-const-expr.c b/validation/integer-const-expr.c
new file mode 100644
index 000000000..f41aa806a
--- /dev/null
+++ b/validation/integer-const-expr.c
@@ -0,0 +1,85 @@
+extern void *malloc(unsigned long);
+
+static inline __attribute__((__const__)) unsigned squarec(unsigned n)
+{
+        return n*n;
+}
+
+static inline unsigned square(unsigned n)
+{
+        return n*n;
+}
+
+static inline unsigned long long bignum(void)
+{
+        return 1000000000000ULL;
+}
+
+static inline __attribute__((__const__)) unsigned long long bignumc(void)
+{
+        return 1000000000000ULL;
+}
+
+// test if x is an integer constant expression [C99,C11 6.6p6]
+#define ICE_P(x) \
+    (__builtin_types_compatible_p(typeof(0?((void*)((long)(x)*0l)):(int*)1),int*))
+
+#define CHX_P(X)	__builtin_choose_expr(ICE_P(X), 1, 0)
+#define CST_P(X)	__builtin_constant_p(ICE_P(X))
+
+#define TEST(R, X)	_Static_assert(ICE_P(X) == R, "ICE_P(" #X ") => " #R);	\
+			_Static_assert(ICE_P(ICE_P(X)), "ICE_P2(" #X ")");	\
+			_Static_assert(CHX_P(X) == R, "CHX_P(" #X ") => " #R);	\
+			_Static_assert(CST_P(X) == 1, "CST_P(" #X ")")
+
+int main(int argc, char *argv[])
+{
+        char fla[3];
+        char vla[argc++];
+        char **p, **q;
+        int x = 5, y = 8;
+        void *v;
+
+        p = &argv[3];
+        q = &argv[6];
+
+        TEST(1, 4);
+        TEST(1, sizeof(long));
+        TEST(1, 5ull - 3u);
+        TEST(1, 3.2);
+        TEST(1, sizeof(fla));
+
+        TEST(0, square(2));
+        TEST(0, square(argc));
+        TEST(0, squarec(2));
+        TEST(0, squarec(argc));
+        TEST(0, 1+argc-argc);
+        TEST(0, 1+argc+argc+1-argc-argc);
+        TEST(0, bignum() - 1);
+        TEST(0, 0*bignum());
+        TEST(0, 0*bignumc());
+        TEST(0, sizeof(vla));
+        TEST(0, p);
+        TEST(0, p < q);
+        TEST(0, p++);
+        TEST(0, main);
+        TEST(0, malloc(8));
+        TEST(0, v = malloc(8));
+        TEST(0, v);
+        TEST(0, x++);
+        TEST(0, y++);
+        TEST(0, (3, 2, 1));
+        TEST(0, ({x++; 0; }));
+        TEST(0, ({square(y--); 0; }));
+        TEST(0, (square(x), 3));
+        TEST(0, (squarec(x), 3));
+        TEST(0, ({squarec(x); 3;}));
+        TEST(0, ({squarec(x);}));
+
+        return 0;
+}
+
+/*
+ * check-name: integer-const-expr
+ * check-command: sparse -Wno-vla $file
+ */
-- 
2.17.1

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] add testcases for packed bitfields ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] add testcases for packed bitfields
Date: Mon, 16 Apr 2018 14:12:34 +0000
Message-ID: <20180416141234.77051-1-luc.vanoostenryck () gmail ! com>
--------------------
Currently sparse ignore type attributes, like for example,
defining a packed structure.

Add a testcase for this.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/packed-bitfield.c | 65 ++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 65 insertions(+)
 create mode 100644 validation/packed-bitfield.c

diff --git a/validation/packed-bitfield.c b/validation/packed-bitfield.c
new file mode 100644
index 000000000..914b7f86a
--- /dev/null
+++ b/validation/packed-bitfield.c
@@ -0,0 +1,65 @@
+#define alignof(X)	__alignof__(X)
+#define __packed	__attribute__((packed))
+
+struct sa {
+	int a:7;
+	int c:10;
+	int b:2;
+} __packed;
+_Static_assert(alignof(struct sa) == 1, "alignof(struct sa)");
+
+struct __packed sb {
+	int a:7;
+	int c:10;
+	int b:2;
+};
+_Static_assert(alignof(struct sb) == 1, "alignof(struct sb)");
+
+static int get_size(void)
+{
+	return sizeof(struct sa);
+}
+
+static void chk_align(struct sa sa, struct sa *p)
+{
+	_Static_assert(__alignof__(sa) == 1, "alignof(sa)");
+	_Static_assert(__alignof__(*p) == 1, "alignof(*p)");
+}
+
+static int fp0(struct sa *sa)
+{
+	return sa->c;
+}
+
+static int fpx(struct sa *sa, int idx)
+{
+	return sa[idx].c;
+}
+
+static int fglobal(void)
+{
+	extern struct sa g;
+	return g.c;
+}
+
+static struct sa l;
+static int flocal(void)
+{
+	return l.c;
+}
+
+
+int main(void)
+{
+	extern void fun(struct sa *);
+	struct sa sa = { 0 };
+
+	fun(&sa);
+	return 0;
+}
+
+/*
+ * check-name: packed-bitfield
+ * check-command: sparse -m32 $file
+ * check-known-to-fail
+ */
-- 
2.16.3

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] alloc: check if size is too big ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] alloc: check if size is too big
Date: Tue, 17 Apr 2018 08:30:50 +0000
Message-ID: <20180417083050.8027-1-luc.vanoostenryck () gmail ! com>
--------------------
The allocate functions can take an extra size in arguments
used to allocate some variable extta space at the end of the
allocated structure. In the common case this extra size is zero
and if not it should be relatively small. In all cases the total
size must be smaller than the 'chunking factor' (the size of the
block ued t do these allocations).

However, this total size is not tested and all kinds of interesting
failures can be produced if it is too big.

Fix this by adding a test and die in case of failure.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 allocate.c | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/allocate.c b/allocate.c
index 0cc556307..152fa8964 100644
--- a/allocate.c
+++ b/allocate.c
@@ -103,6 +103,8 @@ void *allocate(struct allocator_struct *desc, unsigned int size)
 		struct allocation_blob *newblob = blob_alloc(chunking);
 		if (!newblob)
 			die("out of memory");
+		if (size > chunking)
+			die("alloc too big");
 		desc->total_bytes += chunking;
 		newblob->next = blob;
 		blob = newblob;
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] as-name: document that identifiers are OK for address spaces ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] as-name: document that identifiers are OK for address spaces
Date: Sat, 22 Dec 2018 00:18:34 +0000
Message-ID: <20181222001834.40089-1-luc.vanoostenryck () gmail ! com>
--------------------
A previous series allowed to used an indentifier to denotate an
address space but this wasn't documented.

Document it now in the manpage.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 sparse.1 | 7 ++++---
 1 file changed, 4 insertions(+), 3 deletions(-)

diff --git a/sparse.1 b/sparse.1
index 096c5b08c..91cd975e3 100644
--- a/sparse.1
+++ b/sparse.1
@@ -37,9 +37,10 @@ Turn all sparse warnings into errors.
 Warn about code which mixes pointers to different address spaces.
 
 Sparse allows an extended attribute
-.BI __attribute__((address_space( num )))
-on pointers, which designates a pointer target in address space \fInum\fR (a
-constant integer).  With \fB\-Waddress\-space\fR, Sparse treats pointers with
+.BI __attribute__((address_space( id )))
+on pointers, which designates a pointer target in address space \fIid\fR (an
+identifier or a constant integer).
+With \fB\-Waddress\-space\fR, Sparse treats pointers with
 identical target types but different address spaces as distinct types.  To
 override this warning, such as for functions which convert pointers between
 address spaces, use a type that includes \fB__attribute__((force))\fR.
-- 
2.20.0

================================================================================


################################################################################

=== Thread: [PATCH] ban use of 'true' or 'false' ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] ban use of 'true' or 'false'
Date: Tue, 20 Feb 2018 11:36:03 +0000
Message-ID: <20180220113603.13007-1-luc.vanoostenryck () gmail ! com>
--------------------
The idea being, of course, to be able for some functions to return
a bool, making clear what's their possible returned values.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 compile-i386.c | 18 +++++++++---------
 evaluate.c     | 24 ++++++++++++------------
 expand.c       | 18 +++++++++---------
 flow.c         | 15 +++++++--------
 inline.c       | 26 +++++++++++++-------------
 lib.h          |  1 +
 linearize.c    | 12 ++++++------
 pre-process.c  |  4 ++--
 show-parse.c   |  6 +++---
 simplify.c     | 14 ++++++--------
 10 files changed, 68 insertions(+), 70 deletions(-)

diff --git a/compile-i386.c b/compile-i386.c
index 9c6c18603..2ee7b35ec 100644
--- a/compile-i386.c
+++ b/compile-i386.c
@@ -1554,7 +1554,7 @@ static struct storage *emit_return_stmt(struct statement *stmt)
 
 static struct storage *emit_conditional_expr(struct expression *expr)
 {
-	struct storage *cond, *true = NULL, *false = NULL;
+	struct storage *cond, *stot = NULL, *stof = NULL;
 	struct storage *new = stack_alloc(expr->ctype->bit_size / 8);
 	int target_false, cond_end;
 
@@ -1563,16 +1563,16 @@ static struct storage *emit_conditional_expr(struct expression *expr)
 	target_false = emit_conditional_test(cond);
 
 	/* handle if-true part of the expression */
-	true = x86_expression(expr->cond_true);
+	stot = x86_expression(expr->cond_true);
 
-	emit_copy(new, true, expr->ctype);
+	emit_copy(new, stot, expr->ctype);
 
 	cond_end = emit_conditional_end(target_false);
 
 	/* handle if-false part of the expression */
-	false = x86_expression(expr->cond_false);
+	stof = x86_expression(expr->cond_false);
 
-	emit_copy(new, false, expr->ctype);
+	emit_copy(new, stof, expr->ctype);
 
 	/* end of conditional; jump target for if-true branch */
 	emit_label(cond_end, "end conditional");
@@ -1583,15 +1583,15 @@ static struct storage *emit_conditional_expr(struct expression *expr)
 static struct storage *emit_select_expr(struct expression *expr)
 {
 	struct storage *cond = x86_expression(expr->conditional);
-	struct storage *true = x86_expression(expr->cond_true);
-	struct storage *false = x86_expression(expr->cond_false);
+	struct storage *stot = x86_expression(expr->cond_true);
+	struct storage *stof = x86_expression(expr->cond_false);
 	struct storage *reg_cond, *reg_true, *reg_false;
 	struct storage *new = stack_alloc(4);
 
 	emit_comment("begin SELECT");
 	reg_cond = get_reg_value(cond, get_regclass(expr->conditional));
-	reg_true = get_reg_value(true, get_regclass(expr));
-	reg_false = get_reg_value(false, get_regclass(expr));
+	reg_true = get_reg_value(stot, get_regclass(expr));
+	reg_false = get_reg_value(stof, get_regclass(expr));
 
 	/*
 	 * Do the actual select: check the conditional for zero,
diff --git a/evaluate.c b/evaluate.c
index bd10c6d9b..4e1dffe9c 100644
--- a/evaluate.c
+++ b/evaluate.c
@@ -1140,7 +1140,7 @@ OK:
  */
 static struct symbol *evaluate_conditional_expression(struct expression *expr)
 {
-	struct expression **true;
+	struct expression **cond;
 	struct symbol *ctype, *ltype, *rtype, *lbase, *rbase;
 	int lclass, rclass;
 	const char * typediff;
@@ -1154,16 +1154,16 @@ static struct symbol *evaluate_conditional_expression(struct expression *expr)
 	ctype = degenerate(expr->conditional);
 	rtype = degenerate(expr->cond_false);
 
-	true = &expr->conditional;
+	cond = &expr->conditional;
 	ltype = ctype;
 	if (expr->cond_true) {
 		if (!evaluate_expression(expr->cond_true))
 			return NULL;
 		ltype = degenerate(expr->cond_true);
-		true = &expr->cond_true;
+		cond = &expr->cond_true;
 	}
 
-	expr->flags = (expr->conditional->flags & (*true)->flags &
+	expr->flags = (expr->conditional->flags & (*cond)->flags &
 			expr->cond_false->flags & ~CEF_CONST_MASK);
 	/*
 	 * A conditional operator yields a particular constant
@@ -1179,32 +1179,32 @@ static struct symbol *evaluate_conditional_expression(struct expression *expr)
 	 * address constants, mark the result as an address constant.
 	 */
 	if (expr->conditional->flags & (CEF_ACE | CEF_ADDR))
-		expr->flags = (*true)->flags & expr->cond_false->flags & ~CEF_CONST_MASK;
+		expr->flags = (*cond)->flags & expr->cond_false->flags & ~CEF_CONST_MASK;
 
 	lclass = classify_type(ltype, &ltype);
 	rclass = classify_type(rtype, &rtype);
 	if (lclass & rclass & TYPE_NUM) {
-		ctype = usual_conversions('?', *true, expr->cond_false,
+		ctype = usual_conversions('?', *cond, expr->cond_false,
 					  lclass, rclass, ltype, rtype);
-		*true = cast_to(*true, ctype);
+		*cond = cast_to(*cond, ctype);
 		expr->cond_false = cast_to(expr->cond_false, ctype);
 		goto out;
 	}
 
 	if ((lclass | rclass) & TYPE_PTR) {
-		int is_null1 = is_null_pointer_constant(*true);
+		int is_null1 = is_null_pointer_constant(*cond);
 		int is_null2 = is_null_pointer_constant(expr->cond_false);
 
 		if (is_null1 && is_null2) {
-			*true = cast_to(*true, &ptr_ctype);
+			*cond = cast_to(*cond, &ptr_ctype);
 			expr->cond_false = cast_to(expr->cond_false, &ptr_ctype);
 			ctype = &ptr_ctype;
 			goto out;
 		}
 		if (is_null1 && (rclass & TYPE_PTR)) {
 			if (is_null1 == 2)
-				bad_null(*true);
-			*true = cast_to(*true, rtype);
+				bad_null(*cond);
+			*cond = cast_to(*cond, rtype);
 			ctype = rtype;
 			goto out;
 		}
@@ -1284,7 +1284,7 @@ Qual:
 		sym->ctype.modifiers |= qual;
 		ctype = sym;
 	}
-	*true = cast_to(*true, ctype);
+	*cond = cast_to(*cond, ctype);
 	expr->cond_false = cast_to(expr->cond_false, ctype);
 	goto out;
 }
diff --git a/expand.c b/expand.c
index d44aec242..4c68c98ca 100644
--- a/expand.c
+++ b/expand.c
@@ -529,27 +529,27 @@ static int expand_compare(struct expression *expr)
 static int expand_conditional(struct expression *expr)
 {
 	struct expression *cond = expr->conditional;
-	struct expression *true = expr->cond_true;
-	struct expression *false = expr->cond_false;
+	struct expression *valt = expr->cond_true;
+	struct expression *valf = expr->cond_false;
 	int cost, cond_cost;
 
 	cond_cost = expand_expression(cond);
 	if (cond->type == EXPR_VALUE) {
 		unsigned flags = expr->flags;
 		if (!cond->value)
-			true = false;
-		if (!true)
-			true = cond;
-		cost = expand_expression(true);
-		*expr = *true;
+			valt = valf;
+		if (!valt)
+			valt = cond;
+		cost = expand_expression(valt);
+		*expr = *valt;
 		expr->flags = flags;
 		if (expr->type == EXPR_VALUE)
 			expr->taint |= cond->taint;
 		return cost;
 	}
 
-	cost = expand_expression(true);
-	cost += expand_expression(false);
+	cost = expand_expression(valt);
+	cost += expand_expression(valf);
 
 	if (cost < SELECT_COST) {
 		expr->type = EXPR_SELECT;
diff --git a/flow.c b/flow.c
index 495b118d2..5f7054cdf 100644
--- a/flow.c
+++ b/flow.c
@@ -131,7 +131,7 @@ static int try_to_simplify_bb(struct basic_block *bb, struct instruction *first,
 		struct basic_block *source, *target;
 		pseudo_t pseudo;
 		struct instruction *br;
-		int true;
+		int cond;
 
 		if (!def)
 			continue;
@@ -144,10 +144,10 @@ static int try_to_simplify_bb(struct basic_block *bb, struct instruction *first,
 			continue;
 		if (br->opcode != OP_CBR && br->opcode != OP_BR)
 			continue;
-		true = pseudo_truth_value(pseudo);
-		if (true < 0)
+		cond = pseudo_truth_value(pseudo);
+		if (cond < 0)
 			continue;
-		target = true ? second->bb_true : second->bb_false;
+		target = cond ? second->bb_true : second->bb_false;
 		if (bb_depends_on(target, bb))
 			continue;
 		if (bb_depends_on_phi(target, bb))
@@ -209,7 +209,7 @@ static int simplify_phi_branch(struct basic_block *bb, struct instruction *br)
 }
 
 static int simplify_branch_branch(struct basic_block *bb, struct instruction *br,
-	struct basic_block **target_p, int true)
+	struct basic_block **target_p, int bb_true)
 {
 	struct basic_block *target = *target_p, *final;
 	struct instruction *insn;
@@ -225,7 +225,7 @@ static int simplify_branch_branch(struct basic_block *bb, struct instruction *br
 	 * Now we just need to see if we can rewrite the branch..
 	 */
 	retval = 0;
-	final = true ? insn->bb_true : insn->bb_false;
+	final = bb_true ? insn->bb_true : insn->bb_false;
 	if (bb_has_side_effects(target))
 		goto try_to_rewrite_target;
 	if (bb_depends_on(final, target))
@@ -852,13 +852,12 @@ static struct basic_block * rewrite_branch_bb(struct basic_block *bb, struct ins
 {
 	struct basic_block *parent;
 	struct basic_block *target = br->bb_true;
-	struct basic_block *false = br->bb_false;
 
 	if (br->opcode == OP_CBR) {
 		pseudo_t cond = br->cond;
 		if (cond->type != PSEUDO_VAL)
 			return NULL;
-		target = cond->value ? target : false;
+		target = cond->value ? target : br->bb_false;
 	}
 
 	/*
diff --git a/inline.c b/inline.c
index b999dbac3..28c3afb1a 100644
--- a/inline.c
+++ b/inline.c
@@ -180,14 +180,14 @@ static struct expression * copy_expression(struct expression *expr)
 	case EXPR_SELECT:
 	case EXPR_CONDITIONAL: {
 		struct expression *cond = copy_expression(expr->conditional);
-		struct expression *true = copy_expression(expr->cond_true);
-		struct expression *false = copy_expression(expr->cond_false);
-		if (cond == expr->conditional && true == expr->cond_true && false == expr->cond_false)
+		struct expression *valt = copy_expression(expr->cond_true);
+		struct expression *valf = copy_expression(expr->cond_false);
+		if (cond == expr->conditional && valt == expr->cond_true && valf == expr->cond_false)
 			break;
 		expr = dup_expression(expr);
 		expr->conditional = cond;
-		expr->cond_true = true;
-		expr->cond_false = false;
+		expr->cond_true = valt;
+		expr->cond_false = valf;
 		break;
 	}
 
@@ -366,20 +366,20 @@ static struct statement *copy_one_statement(struct statement *stmt)
 	}
 	case STMT_IF: {
 		struct expression *cond = stmt->if_conditional;
-		struct statement *true = stmt->if_true;
-		struct statement *false = stmt->if_false;
+		struct statement *valt = stmt->if_true;
+		struct statement *valf = stmt->if_false;
 
 		cond = copy_expression(cond);
-		true = copy_one_statement(true);
-		false = copy_one_statement(false);
+		valt = copy_one_statement(valt);
+		valf = copy_one_statement(valf);
 		if (stmt->if_conditional == cond &&
-		    stmt->if_true == true &&
-		    stmt->if_false == false)
+		    stmt->if_true == valt &&
+		    stmt->if_false == valf)
 			break;
 		stmt = dup_statement(stmt);
 		stmt->if_conditional = cond;
-		stmt->if_true = true;
-		stmt->if_false = false;
+		stmt->if_true = valt;
+		stmt->if_false = valf;
 		break;
 	}
 	case STMT_RETURN: {
diff --git a/lib.h b/lib.h
index 271b7ae12..25559aaec 100644
--- a/lib.h
+++ b/lib.h
@@ -1,6 +1,7 @@
 #ifndef LIB_H
 #define LIB_H
 
+#include <stdbool.h>
 #include <stdlib.h>
 #include <stddef.h>
 
diff --git a/linearize.c b/linearize.c
index eb4e68c21..211dabf48 100644
--- a/linearize.c
+++ b/linearize.c
@@ -1367,19 +1367,19 @@ static pseudo_t linearize_cond_branch(struct entrypoint *ep, struct expression *
 
 static pseudo_t linearize_select(struct entrypoint *ep, struct expression *expr)
 {
-	pseudo_t cond, true, false, res;
+	pseudo_t cond, valt, valf, res;
 	struct instruction *insn;
 
-	true = linearize_expression(ep, expr->cond_true);
-	false = linearize_expression(ep, expr->cond_false);
+	valt = linearize_expression(ep, expr->cond_true);
+	valf = linearize_expression(ep, expr->cond_false);
 	cond = linearize_expression(ep, expr->conditional);
 
 	insn = alloc_typed_instruction(OP_SEL, expr->ctype);
 	if (!expr->cond_true)
-		true = cond;
+		valt = cond;
 	use_pseudo(insn, cond, &insn->src1);
-	use_pseudo(insn, true, &insn->src2);
-	use_pseudo(insn, false, &insn->src3);
+	use_pseudo(insn, valt, &insn->src2);
+	use_pseudo(insn, valf, &insn->src3);
 
 	res = alloc_pseudo(insn);
 	insn->target = res;
diff --git a/pre-process.c b/pre-process.c
index 8800dce53..c6c6cdada 100644
--- a/pre-process.c
+++ b/pre-process.c
@@ -1427,13 +1427,13 @@ static int handle_strong_undef(struct stream *stream, struct token **line, struc
 	return do_handle_undef(stream, line, token, SYM_ATTR_STRONG);
 }
 
-static int preprocessor_if(struct stream *stream, struct token *token, int true)
+static int preprocessor_if(struct stream *stream, struct token *token, int cond)
 {
 	token_type(token) = false_nesting ? TOKEN_SKIP_GROUPS : TOKEN_IF;
 	free_preprocessor_line(token->next);
 	token->next = stream->top_if;
 	stream->top_if = token;
-	if (false_nesting || true != 1)
+	if (false_nesting || cond != 1)
 		false_nesting++;
 	return 0;
 }
diff --git a/show-parse.c b/show-parse.c
index 9b5225da7..72d3f3854 100644
--- a/show-parse.c
+++ b/show-parse.c
@@ -1009,11 +1009,11 @@ static int show_label_expr(struct expression *expr)
 static int show_conditional_expr(struct expression *expr)
 {
 	int cond = show_expression(expr->conditional);
-	int true = show_expression(expr->cond_true);
-	int false = show_expression(expr->cond_false);
+	int valt = show_expression(expr->cond_true);
+	int valf = show_expression(expr->cond_false);
 	int new = new_pseudo();
 
-	printf("[v%d]\tcmov.%d\t\tv%d,v%d,v%d\n", cond, expr->ctype->bit_size, new, true, false);
+	printf("[v%d]\tcmov.%d\t\tv%d,v%d,v%d\n", cond, expr->ctype->bit_size, new, valt, valf);
 	return new;
 }
 
diff --git a/simplify.c b/simplify.c
index 72f4da8a1..6cb667c45 100644
--- a/simplify.c
+++ b/simplify.c
@@ -1057,10 +1057,9 @@ static int simplify_cond_branch(struct instruction *br, pseudo_t cond, struct in
 	use_pseudo(br, *pp, &br->cond);
 	remove_usage(cond, &br->cond);
 	if (def->opcode == OP_SET_EQ) {
-		struct basic_block *true = br->bb_true;
-		struct basic_block *false = br->bb_false;
-		br->bb_false = true;
-		br->bb_true = false;
+		struct basic_block *tmp = br->bb_true;
+		br->bb_true = br->bb_false;
+		br->bb_false = tmp;
 	}
 	return REPEAT_CSE;
 }
@@ -1111,10 +1110,9 @@ static int simplify_branch(struct instruction *insn)
 					return REPEAT_CSE;
 				}
 				if (val2) {
-					struct basic_block *true = insn->bb_true;
-					struct basic_block *false = insn->bb_false;
-					insn->bb_false = true;
-					insn->bb_true = false;
+					struct basic_block *tmp = insn->bb_true;
+					insn->bb_true = insn->bb_false;
+					insn->bb_false = tmp;
 				}
 				use_pseudo(insn, def->src1, &insn->cond);
 				remove_usage(cond, &insn->cond);
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] build: check if sparse-llvm needs libc++ ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] build: check if sparse-llvm needs libc++
Date: Tue, 18 Dec 2018 20:23:52 +0000
Message-ID: <20181218202352.48006-1-luc.vanoostenryck () gmail ! com>
--------------------
The output of 'llvm-config --system-libs' is not really complete
as libc++ may be needed but not reported as such by this command.

So, use the output of 'llvm-config --cxxflags' to check this.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Makefile | 1 +
 1 file changed, 1 insertion(+)

diff --git a/Makefile b/Makefile
index beaff7a98..daac0297c 100644
--- a/Makefile
+++ b/Makefile
@@ -163,6 +163,7 @@ LLVM_LDFLAGS := $(shell $(LLVM_CONFIG) --ldflags)
 LLVM_CFLAGS := -I$(shell $(LLVM_CONFIG) --includedir)
 LLVM_LIBS := $(shell $(LLVM_CONFIG) --libs)
 LLVM_LIBS += $(shell $(LLVM_CONFIG) --system-libs 2>/dev/null)
+LLVM_LIBS += $(shell $(LLVM_CONFIG) --cxxflags | grep -q -e '-stdlib=libc++' && echo -lc++)
 PROGRAMS += $(LLVM_PROGS)
 INST_PROGRAMS += sparse-llvm sparsec
 sparse-llvm-cflags := $(LLVM_CFLAGS)
-- 
2.20.0

================================================================================

From: Josh Triplett <josh () joshtriplett ! org>
To: linux-sparse
Subject: Re: [PATCH] build: check if sparse-llvm needs libc++
Date: Tue, 18 Dec 2018 21:06:24 +0000
Message-ID: <20181218210624.GA25254 () localhost>
--------------------
On Tue, Dec 18, 2018 at 09:23:52PM +0100, Luc Van Oostenryck wrote:
> The output of 'llvm-config --system-libs' is not really complete
> as libc++ may be needed but not reported as such by this command.
> 
> So, use the output of 'llvm-config --cxxflags' to check this.
> 
> Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> ---
>  Makefile | 1 +
>  1 file changed, 1 insertion(+)
> 
> diff --git a/Makefile b/Makefile
> index beaff7a98..daac0297c 100644
> --- a/Makefile
> +++ b/Makefile
> @@ -163,6 +163,7 @@ LLVM_LDFLAGS := $(shell $(LLVM_CONFIG) --ldflags)
>  LLVM_CFLAGS := -I$(shell $(LLVM_CONFIG) --includedir)
>  LLVM_LIBS := $(shell $(LLVM_CONFIG) --libs)
>  LLVM_LIBS += $(shell $(LLVM_CONFIG) --system-libs 2>/dev/null)
> +LLVM_LIBS += $(shell $(LLVM_CONFIG) --cxxflags | grep -q -e '-stdlib=libc++' && echo -lc++)

Please pass -F to grep here.
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH] build: check if sparse-llvm needs libc++
Date: Tue, 18 Dec 2018 21:43:36 +0000
Message-ID: <20181218214335.yre7oupyw7yevijv () ltop ! local>
--------------------
On Tue, Dec 18, 2018 at 01:06:24PM -0800, Josh Triplett wrote:
> On Tue, Dec 18, 2018 at 09:23:52PM +0100, Luc Van Oostenryck wrote:
> > The output of 'llvm-config --system-libs' is not really complete
> > as libc++ may be needed but not reported as such by this command.
> > 
> > So, use the output of 'llvm-config --cxxflags' to check this.
> > 
> > Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> > ---
> >  Makefile | 1 +
> >  1 file changed, 1 insertion(+)
> > 
> > diff --git a/Makefile b/Makefile
> > index beaff7a98..daac0297c 100644
> > --- a/Makefile
> > +++ b/Makefile
> > @@ -163,6 +163,7 @@ LLVM_LDFLAGS := $(shell $(LLVM_CONFIG) --ldflags)
> >  LLVM_CFLAGS := -I$(shell $(LLVM_CONFIG) --includedir)
> >  LLVM_LIBS := $(shell $(LLVM_CONFIG) --libs)
> >  LLVM_LIBS += $(shell $(LLVM_CONFIG) --system-libs 2>/dev/null)
> > +LLVM_LIBS += $(shell $(LLVM_CONFIG) --cxxflags | grep -q -e '-stdlib=libc++' && echo -lc++)
> 
> Please pass -F to grep here.

Yes, of course.
Thanks for noticing this.

-- Luc
================================================================================


################################################################################

=== Thread: [PATCH] build: only need includedir from llvm-config ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] build: only need includedir from llvm-config
Date: Tue, 18 Dec 2018 20:23:34 +0000
Message-ID: <20181218202334.47958-1-luc.vanoostenryck () gmail ! com>
--------------------
sparse-llvm doesn't need to full output of 'llvm-config --cflags',
it only needs where LLVM's header files can be found.

So, use 'llvm-config --includedir' instead.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Makefile | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/Makefile b/Makefile
index 013423435..beaff7a98 100644
--- a/Makefile
+++ b/Makefile
@@ -160,7 +160,7 @@ ifeq ($(shell expr "$(LLVM_VERSION)" : '[3-9]\.'),2)
 LLVM_PROGS := sparse-llvm
 $(LLVM_PROGS): LD := g++
 LLVM_LDFLAGS := $(shell $(LLVM_CONFIG) --ldflags)
-LLVM_CFLAGS := $(shell $(LLVM_CONFIG) --cflags | sed -e "s/-DNDEBUG//g" | sed -e "s/-pedantic//g")
+LLVM_CFLAGS := -I$(shell $(LLVM_CONFIG) --includedir)
 LLVM_LIBS := $(shell $(LLVM_CONFIG) --libs)
 LLVM_LIBS += $(shell $(LLVM_CONFIG) --system-libs 2>/dev/null)
 PROGRAMS += $(LLVM_PROGS)
-- 
2.20.0

================================================================================


################################################################################

=== Thread: [PATCH] cast: optimize away casts to/from pointers ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] cast: optimize away casts to/from pointers
Date: Tue, 26 Jun 2018 22:55:00 +0000
Message-ID: <20180626225500.96797-1-luc.vanoostenryck () gmail ! com>
--------------------
Now that all casts to or from a pointer are between a pointer
and a pointer-sized unsigned integer, from an optimization
PoV, they are all no-ops.

So, optimize them away at simplification time.

Note: casts between pointers (OP_PTRCAST) should also be
      optimized away but the original type is used for a
      number a things (for example in check_access()) and
      can't be optimized away so simply (yet).

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c                     |  5 +++--
 sparse-llvm.c                  |  4 ++++
 validation/cast-weirds.c       |  6 ++----
 validation/linear/cast-kinds.c | 24 ++++++++----------------
 validation/optim/cast-nop.c    | 18 ++++++++++++++++++
 5 files changed, 35 insertions(+), 22 deletions(-)
 create mode 100644 validation/optim/cast-nop.c

diff --git a/simplify.c b/simplify.c
index ad32fe08a..a91835552 100644
--- a/simplify.c
+++ b/simplify.c
@@ -1210,12 +1210,13 @@ int simplify_instruction(struct instruction *insn)
 	case OP_FCVTU: case OP_FCVTS:
 	case OP_UCVTF: case OP_SCVTF:
 	case OP_FCVTF:
-	case OP_UTPTR:
-	case OP_PTRTU:
 	case OP_PTRCAST:
 		if (dead_insn(insn, &insn->src, NULL, NULL))
 			return REPEAT_CSE;
 		break;
+	case OP_UTPTR:
+	case OP_PTRTU:
+		return replace_with_pseudo(insn, insn->src);
 	case OP_PHI:
 		if (dead_insn(insn, NULL, NULL, NULL)) {
 			kill_use_list(insn->phi_list);
diff --git a/sparse-llvm.c b/sparse-llvm.c
index 937f4490c..9560713a3 100644
--- a/sparse-llvm.c
+++ b/sparse-llvm.c
@@ -660,6 +660,10 @@ static void output_op_compare(struct function *fn, struct instruction *insn)
 	case LLVMIntegerTypeKind: {
 		LLVMIntPredicate op = translate_op(insn->opcode);
 
+		if (LLVMGetTypeKind(LLVMTypeOf(rhs)) == LLVMPointerTypeKind) {
+			LLVMTypeRef ltype = LLVMTypeOf(lhs);
+			rhs = LLVMBuildPtrToInt(fn->builder, rhs, ltype, "");
+		}
 		target = LLVMBuildICmp(fn->builder, op, lhs, rhs, target_name);
 		break;
 	}
diff --git a/validation/cast-weirds.c b/validation/cast-weirds.c
index a99c65d25..df62aafd8 100644
--- a/validation/cast-weirds.c
+++ b/validation/cast-weirds.c
@@ -21,16 +21,14 @@ int_2_iptr:
 .L0:
 	<entry-point>
 	sext.64     %r2 <- (32) %arg1
-	utptr.64    %r3 <- (64) %r2
-	ret.64      %r3
+	ret.64      %r2
 
 
 uint_2_iptr:
 .L2:
 	<entry-point>
 	zext.64     %r6 <- (32) %arg1
-	utptr.64    %r7 <- (64) %r6
-	ret.64      %r7
+	ret.64      %r6
 
 
 int_2_vptr:
diff --git a/validation/linear/cast-kinds.c b/validation/linear/cast-kinds.c
index 5df307bc6..b144dc7ef 100644
--- a/validation/linear/cast-kinds.c
+++ b/validation/linear/cast-kinds.c
@@ -88,8 +88,7 @@ vptr_2_int:
 iptr_2_int:
 .L8:
 	<entry-point>
-	ptrtu.64    %r13 <- (64) %arg1
-	trunc.32    %r14 <- (64) %r13
+	trunc.32    %r14 <- (64) %arg1
 	ret.32      %r14
 
 
@@ -137,8 +136,7 @@ vptr_2_uint:
 iptr_2_uint:
 .L22:
 	<entry-point>
-	ptrtu.64    %r34 <- (64) %arg1
-	trunc.32    %r35 <- (64) %r34
+	trunc.32    %r35 <- (64) %arg1
 	ret.32      %r35
 
 
@@ -185,8 +183,7 @@ vptr_2_long:
 iptr_2_long:
 .L36:
 	<entry-point>
-	ptrtu.64    %r54 <- (64) %arg1
-	ret.64      %r54
+	ret.64      %arg1
 
 
 float_2_long:
@@ -232,8 +229,7 @@ vptr_2_ulong:
 iptr_2_ulong:
 .L50:
 	<entry-point>
-	ptrtu.64    %r73 <- (64) %arg1
-	ret.64      %r73
+	ret.64      %arg1
 
 
 float_2_ulong:
@@ -286,30 +282,26 @@ int_2_iptr:
 .L66:
 	<entry-point>
 	sext.64     %r94 <- (32) %arg1
-	utptr.64    %r95 <- (64) %r94
-	ret.64      %r95
+	ret.64      %r94
 
 
 uint_2_iptr:
 .L68:
 	<entry-point>
 	zext.64     %r98 <- (32) %arg1
-	utptr.64    %r99 <- (64) %r98
-	ret.64      %r99
+	ret.64      %r98
 
 
 long_2_iptr:
 .L70:
 	<entry-point>
-	utptr.64    %r102 <- (64) %arg1
-	ret.64      %r102
+	ret.64      %arg1
 
 
 ulong_2_iptr:
 .L72:
 	<entry-point>
-	utptr.64    %r105 <- (64) %arg1
-	ret.64      %r105
+	ret.64      %arg1
 
 
 vptr_2_iptr:
diff --git a/validation/optim/cast-nop.c b/validation/optim/cast-nop.c
new file mode 100644
index 000000000..7741b7a72
--- /dev/null
+++ b/validation/optim/cast-nop.c
@@ -0,0 +1,18 @@
+static long p2l(long *p)
+{
+	return (long) p;
+}
+
+static long *l2p(long l)
+{
+	return (long*)l;
+}
+
+/*
+ * check-name: cast-nop
+ * check-command: test-linearize -Wno-decl $file
+ *
+ * check-output-ignore
+ * check-output-excludes: utptr\\.
+ * check-output-excludes: ptrtu\\.
+ */
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] cse: let equivalent cast hash & compare identically ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] cse: let equivalent cast hash & compare identically
Date: Tue, 26 Jun 2018 22:48:55 +0000
Message-ID: <20180626224855.96719-1-luc.vanoostenryck () gmail ! com>
--------------------
Now that cast instructions are more finely grained, it's
not needed to compare the original type of the casts,
only the original size can matter.

So, do not hash & compare the original types but only the
orignal sizes. This allow much more casts instructions to be
CSEed away.

Note: like noted in the code, even the original size shouldn't
      matter as identical sources should implies identical
      original sizes but this can't yet be guaranted.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---

The patch is available for review & testing in the Git repository at:
  git://github.com/lucvoo/sparse-dev.git cse-cast
before being promoted to the stable tree at:
  git://github.com/lucvoo/sparse.git master

----------------------------------------------------------------
 cse.c | 28 ++++++++++++++++------------
 1 file changed, 16 insertions(+), 12 deletions(-)

diff --git a/cse.c b/cse.c
index 33cc40dc7..e69fc81fb 100644
--- a/cse.c
+++ b/cse.c
@@ -95,14 +95,12 @@ void cse_collect(struct instruction *insn)
 	case OP_TRUNC:
 	case OP_PTRCAST:
 	case OP_UTPTR: case OP_PTRTU:
-		/*
-		 * This is crap! Many "orig_types" are the
-		 * same as far as casts go, we should generate
-		 * some kind of "type hash" that is identical
-		 * for identical casts
-		 */
-		hash += hashval(insn->orig_type);
+		if (!insn->orig_type || insn->orig_type->bit_size < 0)
+			return;
 		hash += hashval(insn->src);
+
+		// Note: see corresponding line in insn_compare()
+		hash += hashval(insn->orig_type->bit_size);
 		break;
 
 	/* Other */
@@ -165,6 +163,7 @@ static int insn_compare(const void *_i1, const void *_i2)
 {
 	const struct instruction *i1 = _i1;
 	const struct instruction *i2 = _i2;
+	int size1, size2;
 	int diff;
 
 	if (i1->opcode != i2->opcode)
@@ -243,13 +242,18 @@ static int insn_compare(const void *_i1, const void *_i2)
 	case OP_TRUNC:
 	case OP_PTRCAST:
 	case OP_UTPTR: case OP_PTRTU:
-		/*
-		 * This is crap! See the comments on hashing.
-		 */
-		if (i1->orig_type != i2->orig_type)
-			return i1->orig_type < i2->orig_type ? -1 : 1;
 		if (i1->src != i2->src)
 			return i1->src < i2->src ? -1 : 1;
+
+		// Note: if it can be guaranted that identical ->src
+		// implies identical orig_type->bit_size, then this
+		// test and the hashing of the original size in
+		// cse_collect() are not needed.
+		// It must be generaly true but it isn't guaranted (yet).
+		size1 = i1->orig_type->bit_size;
+		size2 = i2->orig_type->bit_size;
+		if (size1 != size2)
+			return size1 < size2 ? -1 : 1;
 		break;
 
 	default:
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] disable sparse warnings about unknown attributes ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-kbuild
Subject: [PATCH] disable sparse warnings about unknown attributes
Date: Thu, 15 Feb 2018 21:07:50 +0000
Message-ID: <20180215210750.28813-1-luc.vanoostenryck () gmail ! com>
--------------------
Currently, sparse issues warnings on code using an attribute
it doesn't know about.

One of the problem with this is that these warnings have no
value for the developer, it's just noise for him. At best these
warnings tell something about some deficiencies of sparse itself
but not about a potential problem with code analyzed.

A second problem with this is that sparse release are, alas,
less frequent than new attributes are added to GCC.

So, avoid the noise by asking sparse to not warn about
attributes it doesn't know about.

Reference: https://marc.info/?l=linux-sparse&m=151871600016790
Reference: https://marc.info/?l=linux-sparse&m=151871725417322
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Makefile | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/Makefile b/Makefile
index 79ad2bfa2..8d9a7374c 100644
--- a/Makefile
+++ b/Makefile
@@ -388,7 +388,7 @@ PYTHON		= python
 CHECK		= sparse
 
 CHECKFLAGS     := -D__linux__ -Dlinux -D__STDC__ -Dunix -D__unix__ \
-		  -Wbitwise -Wno-return-void $(CF)
+		  -Wbitwise -Wno-return-void -Wno-unknown-attribute $(CF)
 NOSTDINC_FLAGS  =
 CFLAGS_MODULE   =
 AFLAGS_MODULE   =
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-kbuild" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] disable sparse warnings about unknown attributes
Date: Thu, 15 Feb 2018 21:07:50 +0000
Message-ID: <20180215210750.28813-1-luc.vanoostenryck () gmail ! com>
--------------------
Currently, sparse issues warnings on code using an attribute
it doesn't know about.

One of the problem with this is that these warnings have no
value for the developer, it's just noise for him. At best these
warnings tell something about some deficiencies of sparse itself
but not about a potential problem with code analyzed.

A second problem with this is that sparse release are, alas,
less frequent than new attributes are added to GCC.

So, avoid the noise by asking sparse to not warn about
attributes it doesn't know about.

Reference: https://marc.info/?l=linux-sparse&m=151871600016790
Reference: https://marc.info/?l=linux-sparse&m=151871725417322
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Makefile | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/Makefile b/Makefile
index 79ad2bfa2..8d9a7374c 100644
--- a/Makefile
+++ b/Makefile
@@ -388,7 +388,7 @@ PYTHON		= python
 CHECK		= sparse
 
 CHECKFLAGS     := -D__linux__ -Dlinux -D__STDC__ -Dunix -D__unix__ \
-		  -Wbitwise -Wno-return-void $(CF)
+		  -Wbitwise -Wno-return-void -Wno-unknown-attribute $(CF)
 NOSTDINC_FLAGS  =
 CFLAGS_MODULE   =
 AFLAGS_MODULE   =
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Randy Dunlap <rdunlap () infradead ! org>
To: linux-kbuild
Subject: Re: [PATCH] disable sparse warnings about unknown attributes
Date: Thu, 15 Feb 2018 23:21:14 +0000
Message-ID: <c215cf31-ba19-49ff-3da9-268e64ea1ba0 () infradead ! org>
--------------------
On 02/15/2018 01:07 PM, Luc Van Oostenryck wrote:
> Currently, sparse issues warnings on code using an attribute
> it doesn't know about.
> 
> One of the problem with this is that these warnings have no
> value for the developer, it's just noise for him. At best these
> warnings tell something about some deficiencies of sparse itself
> but not about a potential problem with code analyzed.
> 
> A second problem with this is that sparse release are, alas,
> less frequent than new attributes are added to GCC.
> 
> So, avoid the noise by asking sparse to not warn about
> attributes it doesn't know about.
> 
> Reference: https://marc.info/?l=linux-sparse&m=151871600016790
> Reference: https://marc.info/?l=linux-sparse&m=151871725417322
> Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> ---
>  Makefile | 2 +-
>  1 file changed, 1 insertion(+), 1 deletion(-)
> 
> diff --git a/Makefile b/Makefile
> index 79ad2bfa2..8d9a7374c 100644
> --- a/Makefile
> +++ b/Makefile
> @@ -388,7 +388,7 @@ PYTHON		= python
>  CHECK		= sparse
>  
>  CHECKFLAGS     := -D__linux__ -Dlinux -D__STDC__ -Dunix -D__unix__ \
> -		  -Wbitwise -Wno-return-void $(CF)
> +		  -Wbitwise -Wno-return-void -Wno-unknown-attribute $(CF)
>  NOSTDINC_FLAGS  =
>  CFLAGS_MODULE   =
>  AFLAGS_MODULE   =
> 

Acked-by: Randy Dunlap <rdunlap@infradead.org>
Tested-by: Randy Dunlap <rdunlap@infradead.org>

thanks,
-- 
~Randy
--
To unsubscribe from this list: send the line "unsubscribe linux-kbuild" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Randy Dunlap <rdunlap () infradead ! org>
To: linux-kernel
Subject: Re: [PATCH] disable sparse warnings about unknown attributes
Date: Thu, 15 Feb 2018 23:21:14 +0000
Message-ID: <c215cf31-ba19-49ff-3da9-268e64ea1ba0 () infradead ! org>
--------------------
On 02/15/2018 01:07 PM, Luc Van Oostenryck wrote:
> Currently, sparse issues warnings on code using an attribute
> it doesn't know about.
> 
> One of the problem with this is that these warnings have no
> value for the developer, it's just noise for him. At best these
> warnings tell something about some deficiencies of sparse itself
> but not about a potential problem with code analyzed.
> 
> A second problem with this is that sparse release are, alas,
> less frequent than new attributes are added to GCC.
> 
> So, avoid the noise by asking sparse to not warn about
> attributes it doesn't know about.
> 
> Reference: https://marc.info/?l=linux-sparse&m=151871600016790
> Reference: https://marc.info/?l=linux-sparse&m=151871725417322
> Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> ---
>  Makefile | 2 +-
>  1 file changed, 1 insertion(+), 1 deletion(-)
> 
> diff --git a/Makefile b/Makefile
> index 79ad2bfa2..8d9a7374c 100644
> --- a/Makefile
> +++ b/Makefile
> @@ -388,7 +388,7 @@ PYTHON		= python
>  CHECK		= sparse
>  
>  CHECKFLAGS     := -D__linux__ -Dlinux -D__STDC__ -Dunix -D__unix__ \
> -		  -Wbitwise -Wno-return-void $(CF)
> +		  -Wbitwise -Wno-return-void -Wno-unknown-attribute $(CF)
>  NOSTDINC_FLAGS  =
>  CFLAGS_MODULE   =
>  AFLAGS_MODULE   =
> 

Acked-by: Randy Dunlap <rdunlap@infradead.org>
Tested-by: Randy Dunlap <rdunlap@infradead.org>

thanks,
-- 
~Randy
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: [PATCH] disable sparse warnings about unknown attributes
Date: Tue, 20 Feb 2018 14:50:02 +0000
Message-ID: <CAK7LNAR6Pizog=NBVjYfF1owSntjOb4o7qV4WNv7d3j3LxongA () mail ! gmail ! com>
--------------------
2018-02-16 6:07 GMT+09:00 Luc Van Oostenryck <luc.vanoostenryck@gmail.com>:
> Currently, sparse issues warnings on code using an attribute
> it doesn't know about.
>
> One of the problem with this is that these warnings have no
> value for the developer, it's just noise for him. At best these
> warnings tell something about some deficiencies of sparse itself
> but not about a potential problem with code analyzed.
>
> A second problem with this is that sparse release are, alas,
> less frequent than new attributes are added to GCC.
>
> So, avoid the noise by asking sparse to not warn about
> attributes it doesn't know about.
>
> Reference: https://marc.info/?l=linux-sparse&m=151871600016790
> Reference: https://marc.info/?l=linux-sparse&m=151871725417322
> Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> ---
>  Makefile | 2 +-
>  1 file changed, 1 insertion(+), 1 deletion(-)
>

Applied to linux-kbuild/kbuild.

Thanks!


-- 
Best Regards
Masahiro Yamada
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kbuild
Subject: Re: [PATCH] disable sparse warnings about unknown attributes
Date: Tue, 20 Feb 2018 14:50:02 +0000
Message-ID: <CAK7LNAR6Pizog=NBVjYfF1owSntjOb4o7qV4WNv7d3j3LxongA () mail ! gmail ! com>
--------------------
2018-02-16 6:07 GMT+09:00 Luc Van Oostenryck <luc.vanoostenryck@gmail.com>:
> Currently, sparse issues warnings on code using an attribute
> it doesn't know about.
>
> One of the problem with this is that these warnings have no
> value for the developer, it's just noise for him. At best these
> warnings tell something about some deficiencies of sparse itself
> but not about a potential problem with code analyzed.
>
> A second problem with this is that sparse release are, alas,
> less frequent than new attributes are added to GCC.
>
> So, avoid the noise by asking sparse to not warn about
> attributes it doesn't know about.
>
> Reference: https://marc.info/?l=linux-sparse&m=151871600016790
> Reference: https://marc.info/?l=linux-sparse&m=151871725417322
> Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> ---
>  Makefile | 2 +-
>  1 file changed, 1 insertion(+), 1 deletion(-)
>

Applied to linux-kbuild/kbuild.

Thanks!


-- 
Best Regards
Masahiro Yamada
--
To unsubscribe from this list: send the line "unsubscribe linux-kbuild" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] do not to ignore old preprocessor testcases ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] do not to ignore old preprocessor testcases
Date: Tue, 17 Apr 2018 08:54:40 +0000
Message-ID: <20180417085440.9966-1-luc.vanoostenryck () gmail ! com>
--------------------
validation/{phase2/backslash,phase3/comments} are two ancient
testcases that predate ./test-suite and they are ignored by
the testsuite because they do not have a .c extension.

Change this by:
- renaming them with a .c
- moving them to validation/preprocessor/
- adding the testsuite tags & results to them
- remove comments about their previous status

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 .../phase2-backslash.c}                       | 29 +++++++++++--------
 .../phase3-comments.c}                        | 12 ++++++--
 2 files changed, 27 insertions(+), 14 deletions(-)
 rename validation/{phase2/backslash => preprocessor/phase2-backslash.c} (75%)
 rename validation/{phase3/comments => preprocessor/phase3-comments.c} (51%)

diff --git a/validation/phase2/backslash b/validation/preprocessor/phase2-backslash.c
similarity index 75%
rename from validation/phase2/backslash
rename to validation/preprocessor/phase2-backslash.c
index 29c85b4d0..21d94d7dc 100644
--- a/validation/phase2/backslash
+++ b/validation/preprocessor/phase2-backslash.c
@@ -17,11 +17,26 @@
  * the rest of tokenizer.
  */
 
+/*
+ * check-name: phase2-backslash
+ * check-command: sparse -E $file
+ *
+ * check-output-start
+
+"\a"
+1
+D
+'\a'
+ * check-output-end
+ *
+ * check-error-start
+preprocessor/phase2-backslash.c:68:0: warning: backslash-newline at end of file
+ * check-error-end
+ */
+
 #define A(x) #x
 #define B(x) A(x)
 /* This should result in "\a" */
-/* XXX: currently sparse produces "a" */
-/* Partially fixed: now it gives "\\a", which is a separate problem */
 B(\a)
 
 #define C\
@@ -32,31 +47,21 @@ C
 #define D\
 1
 /* And this should give D, since '\n' is removed and we get no whitespace */
-/* XXX: currently sparse produces 1 */
-/* Fixed */
 D
 
 #define E '\\
 a'
 /* This should give '\a' - with no warnings issued */
-/* XXX: currently sparse complains a lot and ends up producing a */
-/* Fixed */
 E
 
 /* This should give nothing */
-/* XXX: currently sparse produces more junk */
-/* Fixed */
 // junk \
 more junk
 
 /* This should also give nothing */
-/* XXX: currently sparse produces / * comment * / */
-/* Fixed */
 /\
 * comment *\
 /
 
 /* And this should complain since final newline should not be eaten by '\\' */
-/* XXX: currently sparse does not notice */
-/* Fixed */
 \
diff --git a/validation/phase3/comments b/validation/preprocessor/phase3-comments.c
similarity index 51%
rename from validation/phase3/comments
rename to validation/preprocessor/phase3-comments.c
index 8f51a307b..7106b480a 100644
--- a/validation/phase3/comments
+++ b/validation/preprocessor/phase3-comments.c
@@ -3,7 +3,15 @@
  */
 
 /* This should give nothing */
-/* XXX: currently sparse produces Y */
-/* Fixed */
 #define X /*
  */ Y
+
+/*
+ * check-name: phase3-comments
+ * check-command: sparse -E $file
+ *
+ * check-output-start
+
+
+ * check-output-end
+ */
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] doc: fix list formatting ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] doc: fix list formatting
Date: Sat, 22 Dec 2018 00:06:59 +0000
Message-ID: <20181222000659.39792-1-luc.vanoostenryck () gmail ! com>
--------------------
Sphinx gives a warning on if_convert_phi()'s autodoc because
of some 'unknown indentation' caused by using the wrong marker
('#' instead of '*' or '#.').

Use the right markup: '*'.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c | 5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)

diff --git a/simplify.c b/simplify.c
index 6397f4268..7850bcdc6 100644
--- a/simplify.c
+++ b/simplify.c
@@ -179,10 +179,11 @@ static int if_convert_phi(struct instruction *insn)
 // @return: the unique result if the phi-node is trivial, NULL otherwise
 //
 // A phi-node is trivial if it has a single possible result:
-//	# all operands are the same
-//	# the operands are themselves defined by a chain or cycle of phi-nodes
+//	* all operands are the same
+//	* the operands are themselves defined by a chain or cycle of phi-nodes
 //		and the set of all operands involved contains a single value
 //		not defined by these phi-nodes
+//
 // Since the result is unique, these phi-nodes can be removed.
 static pseudo_t trivial_phi(pseudo_t pseudo, struct instruction *insn, struct pseudo_list **list)
 {
-- 
2.20.0

================================================================================


################################################################################

=== Thread: [PATCH] evaluate: fix dereferencing function pointers ===

From: Jann Horn <jannh () google ! com>
To: linux-sparse
Subject: [PATCH] evaluate: fix dereferencing function pointers
Date: Thu, 22 Feb 2018 01:11:44 +0000
Message-ID: <20180222011144.201194-1-jannh () google ! com>
--------------------
If you dereference a function pointer, you get a function pointer back.
AFAICS there are >=69 places in the kernel that unnecessarily dereference a
function pointer before calling through it.

Output of testcase when applied to previous version:

=============================================
$ ./test-suite single linear/fn-ptr-deref.c
     TEST    dereference function pointer (linear/fn-ptr-deref.c)
error: actual output text does not match expected output text.
error: see linear/fn-ptr-deref.c.output.* for further investigation.
--- linear/fn-ptr-deref.c.output.expected	2018-02-22 01:26:20.592985408 +0100
+++ linear/fn-ptr-deref.c.output.got	2018-02-22 01:26:20.588985440 +0100
@@ -2,7 +2,8 @@
 .L0:
 	<entry-point>
 	call        func_a
-	call        func_a
+	load        %r4 <- 0[func_a]
+	call        VOID
 	ret


linear/fn-ptr-deref.c failed !
=============================================

Signed-off-by: Jann Horn <jannh@google.com>
---
 evaluate.c                       |  6 ++++++
 validation/linear/fn-ptr-deref.c | 22 ++++++++++++++++++++++
 2 files changed, 28 insertions(+)
 create mode 100644 validation/linear/fn-ptr-deref.c

diff --git a/evaluate.c b/evaluate.c
index b96696d..3859bfc 100644
--- a/evaluate.c
+++ b/evaluate.c
@@ -1776,6 +1776,12 @@ static struct symbol *evaluate_dereference(struct expression *expr)
 	if (ctype->type == SYM_NODE)
 		ctype = ctype->ctype.base_type;
 
+	/* Dereferencing a function pointer does nothing. */
+	if (ctype->type == SYM_PTR && ctype->ctype.base_type->type == SYM_FN) {
+		*expr = *op;
+		return expr->ctype;
+	}
+
 	node = alloc_symbol(expr->pos, SYM_NODE);
 	target = ctype->ctype.base_type;
 
diff --git a/validation/linear/fn-ptr-deref.c b/validation/linear/fn-ptr-deref.c
new file mode 100644
index 0000000..9352d1e
--- /dev/null
+++ b/validation/linear/fn-ptr-deref.c
@@ -0,0 +1,22 @@
+void func_a(void);
+static void function_foobar(int n)
+{
+	void (*indir_func_var)(void) = func_a;
+	indir_func_var();
+	(*indir_func_var)();
+}
+/*
+ * check-name: dereference function pointer
+ * check-command: test-linearize $file
+ *
+ * check-output-start
+function_foobar:
+.L0:
+	<entry-point>
+	call        func_a
+	call        func_a
+	ret
+
+
+ * check-output-end
+ */
-- 
2.16.1.291.g4437f3f132-goog

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH] evaluate: fix dereferencing function pointers
Date: Thu, 22 Feb 2018 02:49:09 +0000
Message-ID: <20180222024908.xlo45kyc7s7wwaej () ltop ! local>
--------------------
On Thu, Feb 22, 2018 at 02:11:44AM +0100, Jann Horn wrote:
> If you dereference a function pointer, you get a function pointer back.
> AFAICS there are >=69 places in the kernel that unnecessarily dereference a
> function pointer before calling through it.

Hi,

Yes. This is in fact a known problem.
A fix for it and some others related to problems with
dereferences, as well as a bunch of test cases, can be
found in my development tree:
	https://github.com/lucvoo/sparse-dev/tree/master

The patches related to this issues are the ones just before
commit 1e1a172fe.


Best regards,
-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Jann Horn <jannh () google ! com>
To: linux-sparse
Subject: Re: [PATCH] evaluate: fix dereferencing function pointers
Date: Thu, 22 Feb 2018 03:02:20 +0000
Message-ID: <CAG48ez0GNtV6xub3G=fuzaO1u0=z4WuaxS-kwDZH1xKKz+aiMQ () mail ! gmail ! com>
--------------------
On Thu, Feb 22, 2018 at 3:49 AM, Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
> On Thu, Feb 22, 2018 at 02:11:44AM +0100, Jann Horn wrote:
>> If you dereference a function pointer, you get a function pointer back.
>> AFAICS there are >=69 places in the kernel that unnecessarily dereference a
>> function pointer before calling through it.
>
> Hi,
>
> Yes. This is in fact a known problem.
> A fix for it and some others related to problems with
> dereferences, as well as a bunch of test cases, can be
> found in my development tree:
>         https://github.com/lucvoo/sparse-dev/tree/master
>
> The patches related to this issues are the ones just before
> commit 1e1a172fe.

Ah, I see.
https://github.com/lucvoo/sparse-dev/commit/eeacb2cb90b2a6275935bb61613383413a9d2779
and
https://github.com/lucvoo/sparse-dev/commit/2be50b1451d954b4ea4579bcb7173e57d8a07161
. Thanks for pointing me to those!

Are there more trees that would be helpful to know about? I assumed
that the master branch of
https://git.kernel.org/pub/scm/devel/sparse/sparse.git/ would be a
reasonable place to start development from, but apparently that's not
true, given that you have three months' worth of patches in your tree?
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH] evaluate: fix dereferencing function pointers
Date: Thu, 22 Feb 2018 03:20:15 +0000
Message-ID: <20180222032014.vt335h7vqipt2ugj () ltop ! local>
--------------------
On Thu, Feb 22, 2018 at 04:02:20AM +0100, Jann Horn wrote:
> On Thu, Feb 22, 2018 at 3:49 AM, Luc Van Oostenryck
> <luc.vanoostenryck@gmail.com> wrote:
> > On Thu, Feb 22, 2018 at 02:11:44AM +0100, Jann Horn wrote:
> >> If you dereference a function pointer, you get a function pointer back.
> >> AFAICS there are >=69 places in the kernel that unnecessarily dereference a
> >> function pointer before calling through it.
> >
> > Hi,
> >
> > Yes. This is in fact a known problem.
> > A fix for it and some others related to problems with
> > dereferences, as well as a bunch of test cases, can be
> > found in my development tree:
> >         https://github.com/lucvoo/sparse-dev/tree/master
> >
> > The patches related to this issues are the ones just before
> > commit 1e1a172fe.
> 
> Ah, I see.
> https://github.com/lucvoo/sparse-dev/commit/eeacb2cb90b2a6275935bb61613383413a9d2779
> and
> https://github.com/lucvoo/sparse-dev/commit/2be50b1451d954b4ea4579bcb7173e57d8a07161
> . Thanks for pointing me to those!
> 
> Are there more trees that would be helpful to know about? I assumed
> that the master branch of
> https://git.kernel.org/pub/scm/devel/sparse/sparse.git/ would be a
> reasonable place to start development from, but apparently that's not
> true, given that you have three months' worth of patches in your tree?

I don't think so.
I have the tree (which I would have used here above)
	https://github.com/lucvoo/sparse
which is a stable tree (well tested and never rebased)
containing my patches and the few other patches sent to the mailing list.
The branch point with the official tree is indeed a few months ago.
My other tree is at:
	https://github.com/lucvoo/sparse-dev
and it's a non-stable development tree (topics branches are not
finam, and may be rebased, 'tip' is a quasi stable branch I use
to aggregate topic branches and is sometimes rebased, 'master'
is the same as the stable tree).

-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] fix accesses through incorrect union members ===

From: Jann Horn <jannh () google ! com>
To: linux-sparse
Subject: Re: [PATCH] fix accesses through incorrect union members
Date: Fri, 23 Feb 2018 22:08:08 +0000
Message-ID: <CAG48ez3t5Phsj46a8X0yi29D27RTyeThHR5xtGVipE5r0rqBgQ () mail ! gmail ! com>
--------------------
On Fri, Feb 23, 2018 at 11:06 PM, Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
> On Fri, Feb 23, 2018 at 10:55:13PM +0100, Jann Horn wrote:
>> No functional change - ->src1 and ->phi_src are at the same offset and have
>> the same type.
>> Discovered and debugged by compiling with "-Dunion=struct" in CFLAGS, which
>> I originally did because I wanted to easily see in GDB which union fields
>> I'm supposed to be accessing.
>
> There is some doc helping a bit about this on my tree:
>         https://github.com/lucvoo/sparse/blob/master/Documentation/IR.md

Ah, thanks. That looks helpful.
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] fix crash on 'goto <reserved word>' ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] fix crash on 'goto <reserved word>'
Date: Sun, 27 May 2018 00:11:47 +0000
Message-ID: <20180527001147.20040-1-luc.vanoostenryck () gmail ! com>
--------------------
On code like 'goto <some reserved code>', bind_symbol() reports
an error and doesn't bind the label's ident to the goto's label
symbol.

However, at evaluation time, the ident is unconditionally
dereferenced.

Avoid the crash by checking for a null ident before dereferencing it.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 evaluate.c                 |  2 +-
 validation/goto-reserved.c | 12 ++++++++++++
 2 files changed, 13 insertions(+), 1 deletion(-)
 create mode 100644 validation/goto-reserved.c

diff --git a/evaluate.c b/evaluate.c
index edace120f..c9ed663dd 100644
--- a/evaluate.c
+++ b/evaluate.c
@@ -3593,7 +3593,7 @@ static void evaluate_goto_statement(struct statement *stmt)
 {
 	struct symbol *label = stmt->goto_label;
 
-	if (label && !label->stmt && !lookup_keyword(label->ident, NS_KEYWORD))
+	if (label && !label->stmt && label->ident && !lookup_keyword(label->ident, NS_KEYWORD))
 		sparse_error(stmt->pos, "label '%s' was not declared", show_ident(label->ident));
 
 	evaluate_expression(stmt->goto_expression);
diff --git a/validation/goto-reserved.c b/validation/goto-reserved.c
new file mode 100644
index 000000000..fbaf03e14
--- /dev/null
+++ b/validation/goto-reserved.c
@@ -0,0 +1,12 @@
+static void foo(void)
+{
+	goto return;
+}
+
+/*
+ * check-name: goto-reserved
+ *
+ * check-error-start
+goto-reserved.c:3:14: error: Trying to use reserved word 'return' as identifier
+ * check-error-end
+ */
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] fix dominance testing of mixed types ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] fix dominance testing of mixed types
Date: Sun, 18 Mar 2018 13:55:31 +0000
Message-ID: <20180318135531.10834-1-luc.vanoostenryck () gmail ! com>
--------------------
The function dominates() is used to test the dominance of
one instruction over another one. To do this different things
need to be checked, like:
- same symbol or not
- same address & same offset
- partial overlapping

One case is not covered though: when type cohercion is done
via a pointer to an union (IOW when a memory location is used
to store one type and is then read back as another type).
For example, the location is first stored as a float and then is
read back as an integer of the same size. Currently, sparse
will consider that the store effectively dominates the load
which will then allow to simplify away the load and use the
pseudo holding the float value for the expected integer.

There is surely several ways to fix this problem.
The solution used in this patch is to check if both instructions
are of of the same floating-pointness.

Note: this solution is probably incomplete. For example, what
      about compound types?

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 flow.c                                             |  4 +++
 validation/mem2reg/access-union0.c                 | 37 ++++++++++++++++++++++
 validation/mem2reg/access-union1.c                 | 36 +++++++++++++++++++++
 validation/mem2reg/access-union2.c                 | 26 +++++++++++++++
 validation/mem2reg/access-union3.c                 | 24 ++++++++++++++
 .../mem2reg/{init-local.c => init-local-float.c}   | 14 ++------
 validation/mem2reg/init-local-int.c                | 18 +++++++++++
 validation/mem2reg/init-local-union0.c             |  1 +
 8 files changed, 148 insertions(+), 12 deletions(-)
 create mode 100644 validation/mem2reg/access-union0.c
 create mode 100644 validation/mem2reg/access-union1.c
 create mode 100644 validation/mem2reg/access-union2.c
 create mode 100644 validation/mem2reg/access-union3.c
 rename validation/mem2reg/{init-local.c => init-local-float.c} (53%)
 create mode 100644 validation/mem2reg/init-local-int.c

diff --git a/flow.c b/flow.c
index f928c2684..32e30670c 100644
--- a/flow.c
+++ b/flow.c
@@ -367,6 +367,10 @@ int dominates(pseudo_t pseudo, struct instruction *insn, struct instruction *dom
 			return 0;
 		return -1;
 	}
+	if (!insn->type || !dom->type)
+		return -1;
+	if (is_float_type(insn->type) != is_float_type(dom->type))
+		return -1;
 	return 1;
 }
 
diff --git a/validation/mem2reg/access-union0.c b/validation/mem2reg/access-union0.c
new file mode 100644
index 000000000..b04326cea
--- /dev/null
+++ b/validation/mem2reg/access-union0.c
@@ -0,0 +1,37 @@
+typedef unsigned  int u32;
+typedef unsigned long u64;
+
+void use(u32);
+
+u32 f0(void)
+{
+	union {
+		u64 b;
+		u32 a[2];
+	} u;
+
+	u.b = 1;
+	return u.a[0];
+}
+
+
+u32 f1(void)
+{
+	union {
+		u64 b;
+		u32 a[2];
+	} u;
+
+	u.b = 1;
+	return u.a[1];
+}
+
+/*
+ * check-name: access-union0
+ * check-command: test-linearize -m64 -Wno-decl $file
+ *
+ * check-output-ignore
+ * check-output-pattern(2): load\\.
+ * check-output-pattern(2): store\\.
+ * check-output-excludes: ret.32 *\\$1
+ */
diff --git a/validation/mem2reg/access-union1.c b/validation/mem2reg/access-union1.c
new file mode 100644
index 000000000..e6d8c820e
--- /dev/null
+++ b/validation/mem2reg/access-union1.c
@@ -0,0 +1,36 @@
+void use(int);
+
+double foo(void)
+{
+	union {
+		double	d;
+		int	i;
+	} u;
+
+	u.d = 1.0;
+	use(u.i);
+	u.d = 0.0;
+	return u.d;
+}
+
+/*
+ * check-name: access-union1
+ * check-command: test-linearize -Wno-decl $file
+ * check-known-to-fail
+ *
+ * check-output-start
+foo:
+.L0:
+	<entry-point>
+	setfval.64  %r1 <- 1.000000
+	store.64    %r1 <- 0[u]
+	load.32     %r2 <- 0[u]
+	call        use, %r2
+	setfval.64  %r3 <- 0.000000
+	ret.64      %r3
+
+
+ * check-output-end
+ * check-output-pattern(1): load\\.
+ * check-output-pattern(1): store\\.
+ */
diff --git a/validation/mem2reg/access-union2.c b/validation/mem2reg/access-union2.c
new file mode 100644
index 000000000..7c119ffbd
--- /dev/null
+++ b/validation/mem2reg/access-union2.c
@@ -0,0 +1,26 @@
+typedef double        dbl;
+typedef unsigned long u64;
+
+void use(u64);
+
+u64 foo(void)
+{
+	union {
+		dbl f;
+		u64 i;
+	} u;
+
+	u.f = 1.0;
+	use(u.i);
+	u.f = 0.0;
+	return u.f;
+}
+
+/*
+ * check-name: access-union2
+ * check-command: test-linearize -Wno-decl $file
+ *
+ * check-output-ignore
+ * check-output-pattern(1): load\\.
+ * check-output-pattern(1): store\\.
+ */
diff --git a/validation/mem2reg/access-union3.c b/validation/mem2reg/access-union3.c
new file mode 100644
index 000000000..d977493ab
--- /dev/null
+++ b/validation/mem2reg/access-union3.c
@@ -0,0 +1,24 @@
+typedef double        dbl;
+typedef unsigned long u64;
+
+void use(u64);
+
+static dbl foo(void)
+{
+	union {
+		dbl f;
+		u64 i;
+	} u;
+
+	u.i = 123;
+	return u.f;
+}
+
+/*
+ * check-name: access-union3
+ * check-command: test-linearize -fdump-ir=mem2reg $file
+ *
+ * check-output-ignore
+ * check-output-pattern(1): load\\.
+ * check-output-pattern(1): store\\.
+ */
diff --git a/validation/mem2reg/init-local.c b/validation/mem2reg/init-local-float.c
similarity index 53%
rename from validation/mem2reg/init-local.c
rename to validation/mem2reg/init-local-float.c
index d51c9247a..2642a204a 100644
--- a/validation/mem2reg/init-local.c
+++ b/validation/mem2reg/init-local-float.c
@@ -1,14 +1,4 @@
-int ssimple(void)
-{
-	struct {
-		int a;
-	} s;
-
-	s.a = 1;
-	return s.a;
-}
-
-double sdouble(void)
+static double local_float(void)
 {
 	struct {
 		double a;
@@ -20,7 +10,7 @@ double sdouble(void)
 
 /*
  * check-name: init-local
- * check-command: test-linearize -Wno-decl -fdump-ir=mem2reg $file
+ * check-command: test-linearize $file
  * check-output-ignore
  * check-output-excludes: load\\.
  * check-output-excludes: store\\.
diff --git a/validation/mem2reg/init-local-int.c b/validation/mem2reg/init-local-int.c
new file mode 100644
index 000000000..285f3199f
--- /dev/null
+++ b/validation/mem2reg/init-local-int.c
@@ -0,0 +1,18 @@
+static int local_int(void)
+{
+	struct {
+		int a;
+	} s;
+
+	s.a = 1;
+	return s.a;
+}
+
+/*
+ * check-name: init-local-int
+ * check-command: test-linearize $file
+ * check-output-ignore
+ * check-output-excludes: load\\.
+ * check-output-excludes: store\\.
+ * check-output-contains: ret.32 *\\$1
+ */
diff --git a/validation/mem2reg/init-local-union0.c b/validation/mem2reg/init-local-union0.c
index 3a57e781f..9a3b87d46 100644
--- a/validation/mem2reg/init-local-union0.c
+++ b/validation/mem2reg/init-local-union0.c
@@ -15,4 +15,5 @@ double uintfloat(void)
  * check-output-ignore
  * check-output-pattern(1): store\\.32
  * check-output-pattern(1): load\\.64
+ * check-output-excludes: ret\\.*\\$1
  */
-- 
2.16.2

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Christopher Li <sparse () chrisli ! org>
To: linux-sparse
Subject: Re: [PATCH] fix dominance testing of mixed types
Date: Mon, 19 Mar 2018 09:19:51 +0000
Message-ID: <CANeU7Qn2WUvxAC81w4GWQmY+0zjAoET6RSt8ryzNAxbOcxiMjQ () mail ! gmail ! com>
--------------------
On Sun, Mar 18, 2018 at 6:55 AM, Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
> The function dominates() is used to test the dominance of
> one instruction over another one. To do this different things
> need to be checked, like:
> - same symbol or not
> - same address & same offset
> - partial overlapping
>
> One case is not covered though: when type cohercion is done
> via a pointer to an union (IOW when a memory location is used
> to store one type and is then read back as another type).
> For example, the location is first stored as a float and then is
> read back as an integer of the same size. Currently, sparse
> will consider that the store effectively dominates the load
> which will then allow to simplify away the load and use the
> pseudo holding the float value for the expected integer.
>
> There is surely several ways to fix this problem.
> The solution used in this patch is to check if both instructions
> are of of the same floating-pointness.
>
> Note: this solution is probably incomplete. For example, what
>       about compound types?

Thanks for the patch. The test case is definitly good to have.
The solution looks likely work for your test case.

The more general topic is pointer alias analyse.

Chris
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH] fix dominance testing of mixed types
Date: Mon, 19 Mar 2018 17:03:58 +0000
Message-ID: <20180319170356.wf6utd2n2xy5islg () ltop ! local>
--------------------
On Mon, Mar 19, 2018 at 02:19:51AM -0700, Christopher Li wrote:
> On Sun, Mar 18, 2018 at 6:55 AM, Luc Van Oostenryck
> <luc.vanoostenryck@gmail.com> wrote:
> > The function dominates() is used to test the dominance of
> > one instruction over another one. To do this different things
> > need to be checked, like:
> > - same symbol or not
> > - same address & same offset
> > - partial overlapping
> >
> > One case is not covered though: when type cohercion is done
> > via a pointer to an union (IOW when a memory location is used
> > to store one type and is then read back as another type).
> > For example, the location is first stored as a float and then is
> > read back as an integer of the same size. Currently, sparse
> > will consider that the store effectively dominates the load
> > which will then allow to simplify away the load and use the
> > pseudo holding the float value for the expected integer.

...

> > Note: this solution is probably incomplete. For example, what
> >       about compound types?
> 
> Thanks for the patch. The test case is definitly good to have.
> The solution looks likely work for your test case.
> 
> The more general topic is pointer alias analyse.

But AA is irrelevant to the problem here. AA will tell you *if*
two pointers may or must alias each other. This is needed for a
number of things but the problem here is different: we already
*know* that (or only concerned when) the two locations *are* the
same (if there is a doubt about this  dominates() returns -1)
but still we must not conclude that the store dominates the load.
In fact, we can conclude that it dominates if we want so, but
what must not be done is the corresponding memop simplification.

The patch cover the case 'floating-point against anything else'
and my question was along the line:
    What if we have more complex situations with *same*
    location but mixed types? What are the memop simplifications
    that may be done and which must not be done?
For example what if we have an union of union or an union holding
a struct holding an union of a int & a float (all of the same size)?

I have the feeling that this test should move where the memop
simplification is done. I'll see.

-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] fix killing OP_SWITCH ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] fix killing OP_SWITCH
Date: Tue, 26 Jun 2018 22:43:19 +0000
Message-ID: <20180626224319.96586-1-luc.vanoostenryck () gmail ! com>
--------------------
Currently OP_SWITCHes are only handled by default in kill_insn().
In consequence, when killed, OP_SWITCHes leave a fake usage on the
condition.

Fix this by removing the condition's usage when killing an OP_SWITCH.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c               |  1 +
 validation/kill-switch.c | 20 ++++++++++++++++++++
 2 files changed, 21 insertions(+)
 create mode 100644 validation/kill-switch.c

diff --git a/simplify.c b/simplify.c
index ad32fe08a..048f7c3aa 100644
--- a/simplify.c
+++ b/simplify.c
@@ -270,6 +270,7 @@ int kill_insn(struct instruction *insn, int force)
 		break;
 
 	case OP_CBR:
+	case OP_SWITCH:
 	case OP_COMPUTEDGOTO:
 		kill_use(&insn->cond);
 		break;
diff --git a/validation/kill-switch.c b/validation/kill-switch.c
new file mode 100644
index 000000000..f6b879b31
--- /dev/null
+++ b/validation/kill-switch.c
@@ -0,0 +1,20 @@
+static struct s {
+	unsigned f:1;
+} s;
+
+static void foo(void)
+{
+	switch (s.f) {
+	case 0:
+		;
+	}
+}
+
+/*
+ * check-name: kill-switch
+ * check-command: test-linearize $file
+ *
+ * check-output-ignore
+ * check-output-excludes: load\\.
+ * check-output-excludes: trunc\\.
+ */
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] fix missing checks for deleted instructions ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] fix missing checks for deleted instructions
Date: Tue, 17 Apr 2018 08:31:00 +0000
Message-ID: <20180417083100.8124-1-luc.vanoostenryck () gmail ! com>
--------------------
Instructions with a null ->bb are instructions which have
been killed. As such, they must thus always be ignored.

Fix this by adding the missing checks for null ->bb when
looping over all the instructions of a basic block.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 graph.c  | 2 ++
 sparse.c | 2 ++
 2 files changed, 4 insertions(+)

diff --git a/graph.c b/graph.c
index c9af9a580..a24c6e177 100644
--- a/graph.c
+++ b/graph.c
@@ -74,6 +74,8 @@ static void graph_ep(struct entrypoint *ep)
 
 		/* List loads and stores */
 		FOR_EACH_PTR(bb->insns, insn) {
+			if (!insn->bb)
+				continue;
 			switch(insn->opcode) {
 			case OP_STORE:
 				if (insn->symbol->type == PSEUDO_SYM) {
diff --git a/sparse.c b/sparse.c
index 768b91253..056d14ff6 100644
--- a/sparse.c
+++ b/sparse.c
@@ -47,6 +47,8 @@ static int context_increase(struct basic_block *bb, int entry)
 
 	FOR_EACH_PTR(bb->insns, insn) {
 		int val;
+		if (!insn->bb)
+			continue;
 		if (insn->opcode != OP_CONTEXT)
 			continue;
 		val = insn->increment;
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] fix: don't dump pointer value in error message ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] fix: don't dump pointer value in error message
Date: Tue, 17 Apr 2018 08:30:56 +0000
Message-ID: <20180417083056.8077-1-luc.vanoostenryck () gmail ! com>
--------------------
In evaluate_symbol_type(), an error message is issued
if a SYM_UNINITIALIZED symbol is reached. Good.

However, the error message display the address of the
incriminated symbol which isn't much useful for finding
the origin of the problem.

Fix this by displaying the symbol's typename instead.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 symbol.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/symbol.c b/symbol.c
index 6cfaf2c8f..e14d2306a 100644
--- a/symbol.c
+++ b/symbol.c
@@ -504,7 +504,7 @@ struct symbol *examine_symbol_type(struct symbol * sym)
 		sparse_error(sym->pos, "ctype on preprocessor command? (%s)", show_ident(sym->ident));
 		return NULL;
 	case SYM_UNINITIALIZED:
-		sparse_error(sym->pos, "ctype on uninitialized symbol %p", sym);
+		sparse_error(sym->pos, "ctype on uninitialized symbol '%s'", show_typename(sym));
 		return NULL;
 	case SYM_RESTRICT:
 		examine_base_type(sym);
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] fix: remove dead OP_{SETVAL,SETFVAL,SLICE} ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] fix: remove dead OP_{SETVAL,SETFVAL,SLICE}
Date: Sat, 30 Jun 2018 18:27:53 +0000
Message-ID: <20180630182753.19045-1-luc.vanoostenryck () gmail ! com>
--------------------
At linearization (and maybe in some other situations), dead
intructions are created. Currently, it's the simplification
phase's job to detect and remove these dead instructions
(cfr. dead_insn()).

However, several intructions, not otherwise concerned by
simplification are never so detected and thus never removed.

Add those instructions to simplify_instruction() and call
dead_insn() on them to remove them if dead.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c | 9 +++++++++
 1 file changed, 9 insertions(+)

diff --git a/simplify.c b/simplify.c
index 6f2eb1182..a399a9355 100644
--- a/simplify.c
+++ b/simplify.c
@@ -1222,6 +1222,15 @@ int simplify_instruction(struct instruction *insn)
 		if (dead_insn(insn, &insn->src, NULL, NULL))
 			return REPEAT_CSE;
 		break;
+	case OP_SLICE:
+		if (dead_insn(insn, &insn->src, NULL, NULL))
+			return REPEAT_CSE;
+		break;
+	case OP_SETVAL:
+	case OP_SETFVAL:
+		if (dead_insn(insn, NULL, NULL, NULL))
+			return REPEAT_CSE;
+		break;
 	case OP_PHI:
 		if (dead_insn(insn, NULL, NULL, NULL)) {
 			kill_use_list(insn->phi_list);
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] fix: remove usage when killing symaddr (part 1) ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] fix: remove usage when killing symaddr (part 1)
Date: Tue, 17 Apr 2018 08:31:09 +0000
Message-ID: <20180417083109.8220-1-luc.vanoostenryck () gmail ! com>
--------------------
When killing OP_SYMADDRs, their usage must be removed. Otherwise
fake users remain and this may inhibit some memop conversions.

Fix this by adding the missing kill_use().

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c | 1 +
 1 file changed, 1 insertion(+)

diff --git a/simplify.c b/simplify.c
index 78ba9c61f..36e34bc3e 100644
--- a/simplify.c
+++ b/simplify.c
@@ -269,6 +269,7 @@ int kill_insn(struct instruction *insn, int force)
 		break;
 
 	case OP_SYMADDR:
+		kill_use(&insn->symbol);
 		repeat_phase |= REPEAT_SYMBOL_CLEANUP;
 		break;
 
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] fix: remove usage when killing symaddr (part 2) ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] fix: remove usage when killing symaddr (part 2)
Date: Tue, 17 Apr 2018 08:31:14 +0000
Message-ID: <20180417083114.8269-1-luc.vanoostenryck () gmail ! com>
--------------------
When killing OP_SYMADDRs, their usage must be removed. Otherwise
fake users remain and this may inhibit some memop conversions.

However, in simplify_instruction() when dead_insn() is called
on OP_SYMADDR the symbol's usage is ignored.

Fix this by adding the missing &insn->symbol to this dead_insn().

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/simplify.c b/simplify.c
index 36e34bc3e..2c29bc302 100644
--- a/simplify.c
+++ b/simplify.c
@@ -1217,7 +1217,7 @@ int simplify_instruction(struct instruction *insn)
 	case OP_STORE:
 		return simplify_memop(insn);
 	case OP_SYMADDR:
-		if (dead_insn(insn, NULL, NULL, NULL))
+		if (dead_insn(insn, &insn->symbol, NULL, NULL))
 			return REPEAT_CSE | REPEAT_SYMBOL_CLEANUP;
 		return replace_with_pseudo(insn, insn->symbol);
 	case OP_CAST:
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] kbuild, x86: revert macros in extended asm workarounds ===

From: Peter Zijlstra <peterz () infradead ! org>
To: linux-kernel
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Thu, 13 Dec 2018 10:51:46 +0000
Message-ID: <20181213105146.GH5289 () hirez ! programming ! kicks-ass ! net>
--------------------
On Thu, Dec 13, 2018 at 06:17:41PM +0900, Masahiro Yamada wrote:
> Revert the following commits:
> 
> - 5bdcd510c2ac9efaf55c4cbd8d46421d8e2320cd
>   ("x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs")
> 
> - d5a581d84ae6b8a4a740464b80d8d9cf1e7947b2
>   ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
> 
> - 0474d5d9d2f7f3b11262f7bf87d0e7314ead9200.
>   ("x86/extable: Macrofy inline assembly code to work around GCC inlining bugs")
> 
> - 494b5168f2de009eb80f198f668da374295098dd.
>   ("x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops")
> 
> - f81f8ad56fd1c7b99b2ed1c314527f7d9ac447c6.
>   ("x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs")
> 
> - 77f48ec28e4ccff94d2e5f4260a83ac27a7f3099.
>   ("x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs")
> 
> - 9e1725b410594911cc5981b6c7b4cea4ec054ca8.
>   ("x86/refcount: Work around GCC inlining bug")
>   (Conflicts: arch/x86/include/asm/refcount.h)
> 
> - c06c4d8090513f2974dfdbed2ac98634357ac475.
>   ("x86/objtool: Use asm macros to work around GCC inlining bugs")
> 
> - 77b0bf55bc675233d22cd5df97605d516d64525e.
>   ("kbuild/Makefile: Prepare for using macros in inline assembly code to work around asm() related GCC inlining bugs")
> 

I don't think we want to blindly revert all that. Some of them actually
made sense and did clean up things irrespective of the asm-inline issue.

In particular I like the jump-label one. The cpufeature one OTOh, yeah,
I'd love to get that reverted.

And as a note; the normal commit quoting style is:

  d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
================================================================================

From: Peter Zijlstra <peterz () infradead ! org>
To: linux-arch
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Thu, 13 Dec 2018 10:51:46 +0000
Message-ID: <20181213105146.GH5289 () hirez ! programming ! kicks-ass ! net>
--------------------
On Thu, Dec 13, 2018 at 06:17:41PM +0900, Masahiro Yamada wrote:
> Revert the following commits:
> 
> - 5bdcd510c2ac9efaf55c4cbd8d46421d8e2320cd
>   ("x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs")
> 
> - d5a581d84ae6b8a4a740464b80d8d9cf1e7947b2
>   ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
> 
> - 0474d5d9d2f7f3b11262f7bf87d0e7314ead9200.
>   ("x86/extable: Macrofy inline assembly code to work around GCC inlining bugs")
> 
> - 494b5168f2de009eb80f198f668da374295098dd.
>   ("x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops")
> 
> - f81f8ad56fd1c7b99b2ed1c314527f7d9ac447c6.
>   ("x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs")
> 
> - 77f48ec28e4ccff94d2e5f4260a83ac27a7f3099.
>   ("x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs")
> 
> - 9e1725b410594911cc5981b6c7b4cea4ec054ca8.
>   ("x86/refcount: Work around GCC inlining bug")
>   (Conflicts: arch/x86/include/asm/refcount.h)
> 
> - c06c4d8090513f2974dfdbed2ac98634357ac475.
>   ("x86/objtool: Use asm macros to work around GCC inlining bugs")
> 
> - 77b0bf55bc675233d22cd5df97605d516d64525e.
>   ("kbuild/Makefile: Prepare for using macros in inline assembly code to work around asm() related GCC inlining bugs")
> 

I don't think we want to blindly revert all that. Some of them actually
made sense and did clean up things irrespective of the asm-inline issue.

In particular I like the jump-label one. The cpufeature one OTOh, yeah,
I'd love to get that reverted.

And as a note; the normal commit quoting style is:

  d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
================================================================================

From: Peter Zijlstra <peterz () infradead ! org>
To: linux-kbuild
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Thu, 13 Dec 2018 10:51:46 +0000
Message-ID: <20181213105146.GH5289 () hirez ! programming ! kicks-ass ! net>
--------------------
On Thu, Dec 13, 2018 at 06:17:41PM +0900, Masahiro Yamada wrote:
> Revert the following commits:
> 
> - 5bdcd510c2ac9efaf55c4cbd8d46421d8e2320cd
>   ("x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs")
> 
> - d5a581d84ae6b8a4a740464b80d8d9cf1e7947b2
>   ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
> 
> - 0474d5d9d2f7f3b11262f7bf87d0e7314ead9200.
>   ("x86/extable: Macrofy inline assembly code to work around GCC inlining bugs")
> 
> - 494b5168f2de009eb80f198f668da374295098dd.
>   ("x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops")
> 
> - f81f8ad56fd1c7b99b2ed1c314527f7d9ac447c6.
>   ("x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs")
> 
> - 77f48ec28e4ccff94d2e5f4260a83ac27a7f3099.
>   ("x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs")
> 
> - 9e1725b410594911cc5981b6c7b4cea4ec054ca8.
>   ("x86/refcount: Work around GCC inlining bug")
>   (Conflicts: arch/x86/include/asm/refcount.h)
> 
> - c06c4d8090513f2974dfdbed2ac98634357ac475.
>   ("x86/objtool: Use asm macros to work around GCC inlining bugs")
> 
> - 77b0bf55bc675233d22cd5df97605d516d64525e.
>   ("kbuild/Makefile: Prepare for using macros in inline assembly code to work around asm() related GCC inlining bugs")
> 

I don't think we want to blindly revert all that. Some of them actually
made sense and did clean up things irrespective of the asm-inline issue.

In particular I like the jump-label one. The cpufeature one OTOh, yeah,
I'd love to get that reverted.

And as a note; the normal commit quoting style is:

  d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
================================================================================

From: Peter Zijlstra <peterz () infradead ! org>
To: linux-sparse
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Thu, 13 Dec 2018 10:51:46 +0000
Message-ID: <20181213105146.GH5289 () hirez ! programming ! kicks-ass ! net>
--------------------
On Thu, Dec 13, 2018 at 06:17:41PM +0900, Masahiro Yamada wrote:
> Revert the following commits:
> 
> - 5bdcd510c2ac9efaf55c4cbd8d46421d8e2320cd
>   ("x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs")
> 
> - d5a581d84ae6b8a4a740464b80d8d9cf1e7947b2
>   ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
> 
> - 0474d5d9d2f7f3b11262f7bf87d0e7314ead9200.
>   ("x86/extable: Macrofy inline assembly code to work around GCC inlining bugs")
> 
> - 494b5168f2de009eb80f198f668da374295098dd.
>   ("x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops")
> 
> - f81f8ad56fd1c7b99b2ed1c314527f7d9ac447c6.
>   ("x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs")
> 
> - 77f48ec28e4ccff94d2e5f4260a83ac27a7f3099.
>   ("x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs")
> 
> - 9e1725b410594911cc5981b6c7b4cea4ec054ca8.
>   ("x86/refcount: Work around GCC inlining bug")
>   (Conflicts: arch/x86/include/asm/refcount.h)
> 
> - c06c4d8090513f2974dfdbed2ac98634357ac475.
>   ("x86/objtool: Use asm macros to work around GCC inlining bugs")
> 
> - 77b0bf55bc675233d22cd5df97605d516d64525e.
>   ("kbuild/Makefile: Prepare for using macros in inline assembly code to work around asm() related GCC inlining bugs")
> 

I don't think we want to blindly revert all that. Some of them actually
made sense and did clean up things irrespective of the asm-inline issue.

In particular I like the jump-label one. The cpufeature one OTOh, yeah,
I'd love to get that reverted.

And as a note; the normal commit quoting style is:

  d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kernel
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Sat, 15 Dec 2018 00:51:12 +0000
Message-ID: <CAK7LNASB=HvU8DwUQQkz_r3sY1DN8Vv-qfNa54-ZDXSpfvEYpg () mail ! gmail ! com>
--------------------
Hi Peter,

On Thu, Dec 13, 2018 at 7:53 PM Peter Zijlstra <peterz@infradead.org> wrote:
>
> On Thu, Dec 13, 2018 at 06:17:41PM +0900, Masahiro Yamada wrote:
> > Revert the following commits:
> >
> > - 5bdcd510c2ac9efaf55c4cbd8d46421d8e2320cd
> >   ("x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs")
> >
> > - d5a581d84ae6b8a4a740464b80d8d9cf1e7947b2
> >   ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
> >
> > - 0474d5d9d2f7f3b11262f7bf87d0e7314ead9200.
> >   ("x86/extable: Macrofy inline assembly code to work around GCC inlining bugs")
> >
> > - 494b5168f2de009eb80f198f668da374295098dd.
> >   ("x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops")
> >
> > - f81f8ad56fd1c7b99b2ed1c314527f7d9ac447c6.
> >   ("x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs")
> >
> > - 77f48ec28e4ccff94d2e5f4260a83ac27a7f3099.
> >   ("x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs")
> >
> > - 9e1725b410594911cc5981b6c7b4cea4ec054ca8.
> >   ("x86/refcount: Work around GCC inlining bug")
> >   (Conflicts: arch/x86/include/asm/refcount.h)
> >
> > - c06c4d8090513f2974dfdbed2ac98634357ac475.
> >   ("x86/objtool: Use asm macros to work around GCC inlining bugs")
> >
> > - 77b0bf55bc675233d22cd5df97605d516d64525e.
> >   ("kbuild/Makefile: Prepare for using macros in inline assembly code to work around asm() related GCC inlining bugs")
> >
>
> I don't think we want to blindly revert all that. Some of them actually
> made sense and did clean up things irrespective of the asm-inline issue.
>
> In particular I like the jump-label one.

[1] The #error message is unnecessary.

[2] keep STATC_BRANCH_NOP/JMP instead of STATIC_JUMP_IF_TRUE/FALSE



In v2, I will make sure to not re-add [1].
I am not sure about [2].


Do you mean only [1],
or both of them?



> The cpufeature one OTOh, yeah,
> I'd love to get that reverted.
>
> And as a note; the normal commit quoting style is:
>
>   d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")


OK. I will do so in v2.


--
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-kbuild
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Sat, 15 Dec 2018 00:51:12 +0000
Message-ID: <CAK7LNASB=HvU8DwUQQkz_r3sY1DN8Vv-qfNa54-ZDXSpfvEYpg () mail ! gmail ! com>
--------------------
Hi Peter,

On Thu, Dec 13, 2018 at 7:53 PM Peter Zijlstra <peterz@infradead.org> wrote:
>
> On Thu, Dec 13, 2018 at 06:17:41PM +0900, Masahiro Yamada wrote:
> > Revert the following commits:
> >
> > - 5bdcd510c2ac9efaf55c4cbd8d46421d8e2320cd
> >   ("x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs")
> >
> > - d5a581d84ae6b8a4a740464b80d8d9cf1e7947b2
> >   ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
> >
> > - 0474d5d9d2f7f3b11262f7bf87d0e7314ead9200.
> >   ("x86/extable: Macrofy inline assembly code to work around GCC inlining bugs")
> >
> > - 494b5168f2de009eb80f198f668da374295098dd.
> >   ("x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops")
> >
> > - f81f8ad56fd1c7b99b2ed1c314527f7d9ac447c6.
> >   ("x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs")
> >
> > - 77f48ec28e4ccff94d2e5f4260a83ac27a7f3099.
> >   ("x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs")
> >
> > - 9e1725b410594911cc5981b6c7b4cea4ec054ca8.
> >   ("x86/refcount: Work around GCC inlining bug")
> >   (Conflicts: arch/x86/include/asm/refcount.h)
> >
> > - c06c4d8090513f2974dfdbed2ac98634357ac475.
> >   ("x86/objtool: Use asm macros to work around GCC inlining bugs")
> >
> > - 77b0bf55bc675233d22cd5df97605d516d64525e.
> >   ("kbuild/Makefile: Prepare for using macros in inline assembly code to work around asm() related GCC inlining bugs")
> >
>
> I don't think we want to blindly revert all that. Some of them actually
> made sense and did clean up things irrespective of the asm-inline issue.
>
> In particular I like the jump-label one.

[1] The #error message is unnecessary.

[2] keep STATC_BRANCH_NOP/JMP instead of STATIC_JUMP_IF_TRUE/FALSE



In v2, I will make sure to not re-add [1].
I am not sure about [2].


Do you mean only [1],
or both of them?



> The cpufeature one OTOh, yeah,
> I'd love to get that reverted.
>
> And as a note; the normal commit quoting style is:
>
>   d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")


OK. I will do so in v2.


--
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-sparse
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Sat, 15 Dec 2018 00:51:12 +0000
Message-ID: <CAK7LNASB=HvU8DwUQQkz_r3sY1DN8Vv-qfNa54-ZDXSpfvEYpg () mail ! gmail ! com>
--------------------
Hi Peter,

On Thu, Dec 13, 2018 at 7:53 PM Peter Zijlstra <peterz@infradead.org> wrote:
>
> On Thu, Dec 13, 2018 at 06:17:41PM +0900, Masahiro Yamada wrote:
> > Revert the following commits:
> >
> > - 5bdcd510c2ac9efaf55c4cbd8d46421d8e2320cd
> >   ("x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs")
> >
> > - d5a581d84ae6b8a4a740464b80d8d9cf1e7947b2
> >   ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
> >
> > - 0474d5d9d2f7f3b11262f7bf87d0e7314ead9200.
> >   ("x86/extable: Macrofy inline assembly code to work around GCC inlining bugs")
> >
> > - 494b5168f2de009eb80f198f668da374295098dd.
> >   ("x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops")
> >
> > - f81f8ad56fd1c7b99b2ed1c314527f7d9ac447c6.
> >   ("x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs")
> >
> > - 77f48ec28e4ccff94d2e5f4260a83ac27a7f3099.
> >   ("x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs")
> >
> > - 9e1725b410594911cc5981b6c7b4cea4ec054ca8.
> >   ("x86/refcount: Work around GCC inlining bug")
> >   (Conflicts: arch/x86/include/asm/refcount.h)
> >
> > - c06c4d8090513f2974dfdbed2ac98634357ac475.
> >   ("x86/objtool: Use asm macros to work around GCC inlining bugs")
> >
> > - 77b0bf55bc675233d22cd5df97605d516d64525e.
> >   ("kbuild/Makefile: Prepare for using macros in inline assembly code to work around asm() related GCC inlining bugs")
> >
>
> I don't think we want to blindly revert all that. Some of them actually
> made sense and did clean up things irrespective of the asm-inline issue.
>
> In particular I like the jump-label one.

[1] The #error message is unnecessary.

[2] keep STATC_BRANCH_NOP/JMP instead of STATIC_JUMP_IF_TRUE/FALSE



In v2, I will make sure to not re-add [1].
I am not sure about [2].


Do you mean only [1],
or both of them?



> The cpufeature one OTOh, yeah,
> I'd love to get that reverted.
>
> And as a note; the normal commit quoting style is:
>
>   d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")


OK. I will do so in v2.


--
Best Regards
Masahiro Yamada
================================================================================

From: Masahiro Yamada <yamada.masahiro () socionext ! com>
To: linux-arch
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Sat, 15 Dec 2018 00:51:12 +0000
Message-ID: <CAK7LNASB=HvU8DwUQQkz_r3sY1DN8Vv-qfNa54-ZDXSpfvEYpg () mail ! gmail ! com>
--------------------
Hi Peter,

On Thu, Dec 13, 2018 at 7:53 PM Peter Zijlstra <peterz@infradead.org> wrote:
>
> On Thu, Dec 13, 2018 at 06:17:41PM +0900, Masahiro Yamada wrote:
> > Revert the following commits:
> >
> > - 5bdcd510c2ac9efaf55c4cbd8d46421d8e2320cd
> >   ("x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs")
> >
> > - d5a581d84ae6b8a4a740464b80d8d9cf1e7947b2
> >   ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
> >
> > - 0474d5d9d2f7f3b11262f7bf87d0e7314ead9200.
> >   ("x86/extable: Macrofy inline assembly code to work around GCC inlining bugs")
> >
> > - 494b5168f2de009eb80f198f668da374295098dd.
> >   ("x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops")
> >
> > - f81f8ad56fd1c7b99b2ed1c314527f7d9ac447c6.
> >   ("x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs")
> >
> > - 77f48ec28e4ccff94d2e5f4260a83ac27a7f3099.
> >   ("x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs")
> >
> > - 9e1725b410594911cc5981b6c7b4cea4ec054ca8.
> >   ("x86/refcount: Work around GCC inlining bug")
> >   (Conflicts: arch/x86/include/asm/refcount.h)
> >
> > - c06c4d8090513f2974dfdbed2ac98634357ac475.
> >   ("x86/objtool: Use asm macros to work around GCC inlining bugs")
> >
> > - 77b0bf55bc675233d22cd5df97605d516d64525e.
> >   ("kbuild/Makefile: Prepare for using macros in inline assembly code to work around asm() related GCC inlining bugs")
> >
>
> I don't think we want to blindly revert all that. Some of them actually
> made sense and did clean up things irrespective of the asm-inline issue.
>
> In particular I like the jump-label one.

[1] The #error message is unnecessary.

[2] keep STATC_BRANCH_NOP/JMP instead of STATIC_JUMP_IF_TRUE/FALSE



In v2, I will make sure to not re-add [1].
I am not sure about [2].


Do you mean only [1],
or both of them?



> The cpufeature one OTOh, yeah,
> I'd love to get that reverted.
>
> And as a note; the normal commit quoting style is:
>
>   d5a581d84ae6 ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")


OK. I will do so in v2.


--
Best Regards
Masahiro Yamada
================================================================================

From: Nadav Amit <namit () vmware ! com>
To: linux-kernel
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Sun, 16 Dec 2018 02:33:39 +0000
Message-ID: <07BE39B2-1F99-4AE4-97F3-0871A39C5E7D () vmware ! com>
--------------------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================================================================================

From: Nadav Amit <namit () vmware ! com>
To: linux-sparse
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Sun, 16 Dec 2018 02:33:39 +0000
Message-ID: <07BE39B2-1F99-4AE4-97F3-0871A39C5E7D () vmware ! com>
--------------------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================================================================================

From: Nadav Amit <namit () vmware ! com>
To: linux-kbuild
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Sun, 16 Dec 2018 02:33:39 +0000
Message-ID: <07BE39B2-1F99-4AE4-97F3-0871A39C5E7D () vmware ! com>
--------------------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================================================================================

From: Nadav Amit <namit () vmware ! com>
To: linux-arch
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Sun, 16 Dec 2018 02:33:39 +0000
Message-ID: <07BE39B2-1F99-4AE4-97F3-0871A39C5E7D () vmware ! com>
--------------------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================================================================================

From: Borislav Petkov <bp () alien8 ! de>
To: linux-kbuild
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Sun, 16 Dec 2018 10:00:42 +0000
Message-ID: <20181216100042.GA815 () zn ! tnic>
--------------------
On Sun, Dec 16, 2018 at 02:33:39AM +0000, Nadav Amit wrote:
> In general, I think that from the start it was clear that the motivation for
> the patch-set is not just performance and also better code. For example, I
> see no reason to revert the PV-changes or the lock-prefix changes that
> improved the code readability.

One thing that has caught my eye with the asm macros, which actually
decreases readability, is that I can't see the macro properly expanded
when I do

make <filename>.s

For example, I get

#APP
# 164 "./arch/x86/include/asm/cpufeature.h" 1
        STATIC_CPU_HAS bitnum=$8 cap_byte="boot_cpu_data+35(%rip)" feature=123 t_yes=.L75 t_no=.L78 always=117  #, MEM[(const char *)&boot_cpu_data + 35B],,,,
# 0 "" 2
        .loc 11 164 2 view .LVU480
#NO_APP

but I'd like to see the actual asm as it is really helpful when hacking
on inline asm stuff. And I haven't found a way to make gcc expand asm
macros in .s output.

Now, assuming the gcc inline patch will be backported to gcc8, I think
we should be covered on all modern distros going forward. So I think we
should revert at least the more complex macros.

-- 
Regards/Gruss,
    Boris.

Good mailing practices for 400: avoid top-posting and trim the reply.
================================================================================

From: Borislav Petkov <bp () alien8 ! de>
To: linux-arch
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Sun, 16 Dec 2018 10:00:42 +0000
Message-ID: <20181216100042.GA815 () zn ! tnic>
--------------------
On Sun, Dec 16, 2018 at 02:33:39AM +0000, Nadav Amit wrote:
> In general, I think that from the start it was clear that the motivation for
> the patch-set is not just performance and also better code. For example, I
> see no reason to revert the PV-changes or the lock-prefix changes that
> improved the code readability.

One thing that has caught my eye with the asm macros, which actually
decreases readability, is that I can't see the macro properly expanded
when I do

make <filename>.s

For example, I get

#APP
# 164 "./arch/x86/include/asm/cpufeature.h" 1
        STATIC_CPU_HAS bitnum=$8 cap_byte="boot_cpu_data+35(%rip)" feature=123 t_yes=.L75 t_no=.L78 always=117  #, MEM[(const char *)&boot_cpu_data + 35B],,,,
# 0 "" 2
        .loc 11 164 2 view .LVU480
#NO_APP

but I'd like to see the actual asm as it is really helpful when hacking
on inline asm stuff. And I haven't found a way to make gcc expand asm
macros in .s output.

Now, assuming the gcc inline patch will be backported to gcc8, I think
we should be covered on all modern distros going forward. So I think we
should revert at least the more complex macros.

-- 
Regards/Gruss,
    Boris.

Good mailing practices for 400: avoid top-posting and trim the reply.
================================================================================

From: Borislav Petkov <bp () alien8 ! de>
To: linux-sparse
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Sun, 16 Dec 2018 10:00:42 +0000
Message-ID: <20181216100042.GA815 () zn ! tnic>
--------------------
On Sun, Dec 16, 2018 at 02:33:39AM +0000, Nadav Amit wrote:
> In general, I think that from the start it was clear that the motivation for
> the patch-set is not just performance and also better code. For example, I
> see no reason to revert the PV-changes or the lock-prefix changes that
> improved the code readability.

One thing that has caught my eye with the asm macros, which actually
decreases readability, is that I can't see the macro properly expanded
when I do

make <filename>.s

For example, I get

#APP
# 164 "./arch/x86/include/asm/cpufeature.h" 1
        STATIC_CPU_HAS bitnum=$8 cap_byte="boot_cpu_data+35(%rip)" feature=123 t_yes=.L75 t_no=.L78 always=117  #, MEM[(const char *)&boot_cpu_data + 35B],,,,
# 0 "" 2
        .loc 11 164 2 view .LVU480
#NO_APP

but I'd like to see the actual asm as it is really helpful when hacking
on inline asm stuff. And I haven't found a way to make gcc expand asm
macros in .s output.

Now, assuming the gcc inline patch will be backported to gcc8, I think
we should be covered on all modern distros going forward. So I think we
should revert at least the more complex macros.

-- 
Regards/Gruss,
    Boris.

Good mailing practices for 400: avoid top-posting and trim the reply.
================================================================================

From: Borislav Petkov via Virtualization <virtualization () lists ! linux-foundation ! org>
To: linux-virtualization
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Sun, 16 Dec 2018 10:00:42 +0000
Message-ID: <20181216100042.GA815 () zn ! tnic>
--------------------
On Sun, Dec 16, 2018 at 02:33:39AM +0000, Nadav Amit wrote:
> In general, I think that from the start it was clear that the motivation for
> the patch-set is not just performance and also better code. For example, I
> see no reason to revert the PV-changes or the lock-prefix changes that
> improved the code readability.

One thing that has caught my eye with the asm macros, which actually
decreases readability, is that I can't see the macro properly expanded
when I do

make <filename>.s

For example, I get

#APP
# 164 "./arch/x86/include/asm/cpufeature.h" 1
        STATIC_CPU_HAS bitnum=$8 cap_byte="boot_cpu_data+35(%rip)" feature=123 t_yes=.L75 t_no=.L78 always=117  #, MEM[(const char *)&boot_cpu_data + 35B],,,,
# 0 "" 2
        .loc 11 164 2 view .LVU480
#NO_APP

but I'd like to see the actual asm as it is really helpful when hacking
on inline asm stuff. And I haven't found a way to make gcc expand asm
macros in .s output.

Now, assuming the gcc inline patch will be backported to gcc8, I think
we should be covered on all modern distros going forward. So I think we
should revert at least the more complex macros.

-- 
Regards/Gruss,
    Boris.

Good mailing practices for 400: avoid top-posting and trim the reply.
_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
================================================================================

From: Nadav Amit <namit () vmware ! com>
To: linux-kernel
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Mon, 17 Dec 2018 01:00:50 +0000
Message-ID: <834EC537-EC42-4563-AA39-3326681ED968 () vmware ! com>
--------------------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================================================================================

From: Nadav Amit <namit () vmware ! com>
To: linux-kbuild
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Mon, 17 Dec 2018 01:00:50 +0000
Message-ID: <834EC537-EC42-4563-AA39-3326681ED968 () vmware ! com>
--------------------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================================================================================

From: Sedat Dilek <sedat.dilek () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Mon, 17 Dec 2018 09:16:32 +0000
Message-ID: <CA+icZUUnkGBp-3pJxE_s_Oc+pdNBrVc_ngYUwpv7zLEbtE_eVg () mail ! gmail ! com>
--------------------
On Thu, Dec 13, 2018 at 10:19 AM Masahiro Yamada
<yamada.masahiro@socionext.com> wrote:
>
> Revert the following commits:
>
> - 5bdcd510c2ac9efaf55c4cbd8d46421d8e2320cd
>   ("x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs")
>
> - d5a581d84ae6b8a4a740464b80d8d9cf1e7947b2
>   ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
>
> - 0474d5d9d2f7f3b11262f7bf87d0e7314ead9200.
>   ("x86/extable: Macrofy inline assembly code to work around GCC inlining bugs")
>
> - 494b5168f2de009eb80f198f668da374295098dd.
>   ("x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops")
>
> - f81f8ad56fd1c7b99b2ed1c314527f7d9ac447c6.
>   ("x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs")
>
> - 77f48ec28e4ccff94d2e5f4260a83ac27a7f3099.
>   ("x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs")
>
> - 9e1725b410594911cc5981b6c7b4cea4ec054ca8.
>   ("x86/refcount: Work around GCC inlining bug")
>   (Conflicts: arch/x86/include/asm/refcount.h)
>
> - c06c4d8090513f2974dfdbed2ac98634357ac475.
>   ("x86/objtool: Use asm macros to work around GCC inlining bugs")
>
> - 77b0bf55bc675233d22cd5df97605d516d64525e.
>   ("kbuild/Makefile: Prepare for using macros in inline assembly code to work around asm() related GCC inlining bugs")
>
> A few days after those commits applied, discussion started to solve
> the issue more elegantly on the compiler side:
>
>   https://lkml.org/lkml/2018/10/7/92
>
> The "asm inline" was implemented by Segher Boessenkool, and now queued
> up for GCC 9. (People were positive even for back-porting it to older
> compilers).
>
> Since the in-kernel workarounds merged, some issues have been reported:
> breakage of building with distcc/icecc, breakage of distro packages for
> module building. (More fundamentally, we cannot build external modules
> after 'make clean')
>
> Patching around the build system would make the code even uglier.
>
> Given that this issue will be solved in a cleaner way sooner or later,
> let's revert the in-kernel workarounds, and wait for GCC 9.
>
> Reported-by: Logan Gunthorpe <logang@deltatee.com> # distcc
> Reported-by: Sedat Dilek <sedat.dilek@gmail.com> # debian/rpm package

Hi,

I reported the issue with debian package breakage in [1].

I am not subscribed to any involved mailing-list and not following all
the discussions.
I see the situation is not easy as there is especially linux-kbuild
and linux/x86 involved and maybe other interests.
But I am interested in having a fix in v4.20 final and hope this all
still works with LLVM/Clang.

I can offer my help in testing - against Linux v4.20-rc7.
Not sure if all discussed material is in upstream or elsewhere.
What is your suggestion for me as a tester?

Will we have a solution in Linux v4.20 final?

Thanks.

With my best wishes,
- Sedat -

[1] https://marc.info/?t=154212770600037&r=1&w=2

> Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
> Cc: Nadav Amit <namit@vmware.com>
> Cc: Segher Boessenkool <segher@kernel.crashing.org>
> ---
>
> Please consider this for v4.20 release.
> Currently, distro package build is broken.
>
>
>  Makefile                               |  9 +---
>  arch/x86/Makefile                      |  7 ---
>  arch/x86/entry/calling.h               |  2 +-
>  arch/x86/include/asm/alternative-asm.h | 20 +++----
>  arch/x86/include/asm/alternative.h     | 11 +++-
>  arch/x86/include/asm/asm.h             | 53 +++++++++++-------
>  arch/x86/include/asm/bug.h             | 98 +++++++++++++++-------------------
>  arch/x86/include/asm/cpufeature.h      | 82 ++++++++++++----------------
>  arch/x86/include/asm/jump_label.h      | 72 ++++++++++++++++++-------
>  arch/x86/include/asm/paravirt_types.h  | 56 +++++++++----------
>  arch/x86/include/asm/refcount.h        | 81 ++++++++++++----------------
>  arch/x86/kernel/macros.S               | 16 ------
>  include/asm-generic/bug.h              |  8 +--
>  include/linux/compiler.h               | 56 +++++--------------
>  scripts/Kbuild.include                 |  4 +-
>  scripts/mod/Makefile                   |  2 -
>  16 files changed, 262 insertions(+), 315 deletions(-)
>  delete mode 100644 arch/x86/kernel/macros.S
>
> diff --git a/Makefile b/Makefile
> index f2c3423..4cf4c5b 100644
> --- a/Makefile
> +++ b/Makefile
> @@ -1081,7 +1081,7 @@ scripts: scripts_basic scripts_dtc asm-generic gcc-plugins $(autoksyms_h)
>  # version.h and scripts_basic is processed / created.
>
>  # Listed in dependency order
> -PHONY += prepare archprepare macroprepare prepare0 prepare1 prepare2 prepare3
> +PHONY += prepare archprepare prepare0 prepare1 prepare2 prepare3
>
>  # prepare3 is used to check if we are building in a separate output directory,
>  # and if so do:
> @@ -1104,9 +1104,7 @@ prepare2: prepare3 outputmakefile asm-generic
>  prepare1: prepare2 $(version_h) $(autoksyms_h) include/generated/utsrelease.h
>         $(cmd_crmodverdir)
>
> -macroprepare: prepare1 archmacros
> -
> -archprepare: archheaders archscripts macroprepare scripts_basic
> +archprepare: archheaders archscripts prepare1 scripts_basic
>
>  prepare0: archprepare gcc-plugins
>         $(Q)$(MAKE) $(build)=.
> @@ -1174,9 +1172,6 @@ archheaders:
>  PHONY += archscripts
>  archscripts:
>
> -PHONY += archmacros
> -archmacros:
> -
>  PHONY += __headers
>  __headers: $(version_h) scripts_basic uapi-asm-generic archheaders archscripts
>         $(Q)$(MAKE) $(build)=scripts build_unifdef
> diff --git a/arch/x86/Makefile b/arch/x86/Makefile
> index 75ef499..85a66c4 100644
> --- a/arch/x86/Makefile
> +++ b/arch/x86/Makefile
> @@ -232,13 +232,6 @@ archscripts: scripts_basic
>  archheaders:
>         $(Q)$(MAKE) $(build)=arch/x86/entry/syscalls all
>
> -archmacros:
> -       $(Q)$(MAKE) $(build)=arch/x86/kernel arch/x86/kernel/macros.s
> -
> -ASM_MACRO_FLAGS = -Wa,arch/x86/kernel/macros.s
> -export ASM_MACRO_FLAGS
> -KBUILD_CFLAGS += $(ASM_MACRO_FLAGS)
> -
>  ###
>  # Kernel objects
>
> diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h
> index 25e5a6b..20d0885 100644
> --- a/arch/x86/entry/calling.h
> +++ b/arch/x86/entry/calling.h
> @@ -352,7 +352,7 @@ For 32-bit we have the following conventions - kernel is built with
>  .macro CALL_enter_from_user_mode
>  #ifdef CONFIG_CONTEXT_TRACKING
>  #ifdef HAVE_JUMP_LABEL
> -       STATIC_BRANCH_JMP l_yes=.Lafter_call_\@, key=context_tracking_enabled, branch=1
> +       STATIC_JUMP_IF_FALSE .Lafter_call_\@, context_tracking_enabled, def=0
>  #endif
>         call enter_from_user_mode
>  .Lafter_call_\@:
> diff --git a/arch/x86/include/asm/alternative-asm.h b/arch/x86/include/asm/alternative-asm.h
> index 8e4ea39..31b627b 100644
> --- a/arch/x86/include/asm/alternative-asm.h
> +++ b/arch/x86/include/asm/alternative-asm.h
> @@ -7,24 +7,16 @@
>  #include <asm/asm.h>
>
>  #ifdef CONFIG_SMP
> -.macro LOCK_PREFIX_HERE
> +       .macro LOCK_PREFIX
> +672:   lock
>         .pushsection .smp_locks,"a"
>         .balign 4
> -       .long 671f - .          # offset
> +       .long 672b - .
>         .popsection
> -671:
> -.endm
> -
> -.macro LOCK_PREFIX insn:vararg
> -       LOCK_PREFIX_HERE
> -       lock \insn
> -.endm
> +       .endm
>  #else
> -.macro LOCK_PREFIX_HERE
> -.endm
> -
> -.macro LOCK_PREFIX insn:vararg
> -.endm
> +       .macro LOCK_PREFIX
> +       .endm
>  #endif
>
>  /*
> diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h
> index d7faa16..4cd6a3b 100644
> --- a/arch/x86/include/asm/alternative.h
> +++ b/arch/x86/include/asm/alternative.h
> @@ -31,8 +31,15 @@
>   */
>
>  #ifdef CONFIG_SMP
> -#define LOCK_PREFIX_HERE "LOCK_PREFIX_HERE\n\t"
> -#define LOCK_PREFIX "LOCK_PREFIX "
> +#define LOCK_PREFIX_HERE \
> +               ".pushsection .smp_locks,\"a\"\n"       \
> +               ".balign 4\n"                           \
> +               ".long 671f - .\n" /* offset */         \
> +               ".popsection\n"                         \
> +               "671:"
> +
> +#define LOCK_PREFIX LOCK_PREFIX_HERE "\n\tlock; "
> +
>  #else /* ! CONFIG_SMP */
>  #define LOCK_PREFIX_HERE ""
>  #define LOCK_PREFIX ""
> diff --git a/arch/x86/include/asm/asm.h b/arch/x86/include/asm/asm.h
> index 21b0867..6467757b 100644
> --- a/arch/x86/include/asm/asm.h
> +++ b/arch/x86/include/asm/asm.h
> @@ -120,25 +120,12 @@
>  /* Exception table entry */
>  #ifdef __ASSEMBLY__
>  # define _ASM_EXTABLE_HANDLE(from, to, handler)                        \
> -       ASM_EXTABLE_HANDLE from to handler
> -
> -.macro ASM_EXTABLE_HANDLE from:req to:req handler:req
> -       .pushsection "__ex_table","a"
> -       .balign 4
> -       .long (\from) - .
> -       .long (\to) - .
> -       .long (\handler) - .
> +       .pushsection "__ex_table","a" ;                         \
> +       .balign 4 ;                                             \
> +       .long (from) - . ;                                      \
> +       .long (to) - . ;                                        \
> +       .long (handler) - . ;                                   \
>         .popsection
> -.endm
> -#else /* __ASSEMBLY__ */
> -
> -# define _ASM_EXTABLE_HANDLE(from, to, handler)                        \
> -       "ASM_EXTABLE_HANDLE from=" #from " to=" #to             \
> -       " handler=\"" #handler "\"\n\t"
> -
> -/* For C file, we already have NOKPROBE_SYMBOL macro */
> -
> -#endif /* __ASSEMBLY__ */
>
>  # define _ASM_EXTABLE(from, to)                                        \
>         _ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
> @@ -161,7 +148,6 @@
>         _ASM_PTR (entry);                                       \
>         .popsection
>
> -#ifdef __ASSEMBLY__
>  .macro ALIGN_DESTINATION
>         /* check for bad alignment of destination */
>         movl %edi,%ecx
> @@ -185,7 +171,34 @@
>         _ASM_EXTABLE_UA(100b, 103b)
>         _ASM_EXTABLE_UA(101b, 103b)
>         .endm
> -#endif /* __ASSEMBLY__ */
> +
> +#else
> +# define _EXPAND_EXTABLE_HANDLE(x) #x
> +# define _ASM_EXTABLE_HANDLE(from, to, handler)                        \
> +       " .pushsection \"__ex_table\",\"a\"\n"                  \
> +       " .balign 4\n"                                          \
> +       " .long (" #from ") - .\n"                              \
> +       " .long (" #to ") - .\n"                                \
> +       " .long (" _EXPAND_EXTABLE_HANDLE(handler) ") - .\n"    \
> +       " .popsection\n"
> +
> +# define _ASM_EXTABLE(from, to)                                        \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
> +
> +# define _ASM_EXTABLE_UA(from, to)                             \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_uaccess)
> +
> +# define _ASM_EXTABLE_FAULT(from, to)                          \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_fault)
> +
> +# define _ASM_EXTABLE_EX(from, to)                             \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_ext)
> +
> +# define _ASM_EXTABLE_REFCOUNT(from, to)                       \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_refcount)
> +
> +/* For C file, we already have NOKPROBE_SYMBOL macro */
> +#endif
>
>  #ifndef __ASSEMBLY__
>  /*
> diff --git a/arch/x86/include/asm/bug.h b/arch/x86/include/asm/bug.h
> index 5090035..6804d66 100644
> --- a/arch/x86/include/asm/bug.h
> +++ b/arch/x86/include/asm/bug.h
> @@ -4,8 +4,6 @@
>
>  #include <linux/stringify.h>
>
> -#ifndef __ASSEMBLY__
> -
>  /*
>   * Despite that some emulators terminate on UD2, we use it for WARN().
>   *
> @@ -22,15 +20,53 @@
>
>  #define LEN_UD2                2
>
> +#ifdef CONFIG_GENERIC_BUG
> +
> +#ifdef CONFIG_X86_32
> +# define __BUG_REL(val)        ".long " __stringify(val)
> +#else
> +# define __BUG_REL(val)        ".long " __stringify(val) " - 2b"
> +#endif
> +
> +#ifdef CONFIG_DEBUG_BUGVERBOSE
> +
> +#define _BUG_FLAGS(ins, flags)                                         \
> +do {                                                                   \
> +       asm volatile("1:\t" ins "\n"                                    \
> +                    ".pushsection __bug_table,\"aw\"\n"                \
> +                    "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"   \
> +                    "\t"  __BUG_REL(%c0) "\t# bug_entry::file\n"       \
> +                    "\t.word %c1"        "\t# bug_entry::line\n"       \
> +                    "\t.word %c2"        "\t# bug_entry::flags\n"      \
> +                    "\t.org 2b+%c3\n"                                  \
> +                    ".popsection"                                      \
> +                    : : "i" (__FILE__), "i" (__LINE__),                \
> +                        "i" (flags),                                   \
> +                        "i" (sizeof(struct bug_entry)));               \
> +} while (0)
> +
> +#else /* !CONFIG_DEBUG_BUGVERBOSE */
> +
>  #define _BUG_FLAGS(ins, flags)                                         \
>  do {                                                                   \
> -       asm volatile("ASM_BUG ins=\"" ins "\" file=%c0 line=%c1 "       \
> -                    "flags=%c2 size=%c3"                               \
> -                    : : "i" (__FILE__), "i" (__LINE__),                \
> -                        "i" (flags),                                   \
> +       asm volatile("1:\t" ins "\n"                                    \
> +                    ".pushsection __bug_table,\"aw\"\n"                \
> +                    "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"   \
> +                    "\t.word %c0"        "\t# bug_entry::flags\n"      \
> +                    "\t.org 2b+%c1\n"                                  \
> +                    ".popsection"                                      \
> +                    : : "i" (flags),                                   \
>                          "i" (sizeof(struct bug_entry)));               \
>  } while (0)
>
> +#endif /* CONFIG_DEBUG_BUGVERBOSE */
> +
> +#else
> +
> +#define _BUG_FLAGS(ins, flags)  asm volatile(ins)
> +
> +#endif /* CONFIG_GENERIC_BUG */
> +
>  #define HAVE_ARCH_BUG
>  #define BUG()                                                  \
>  do {                                                           \
> @@ -46,54 +82,4 @@ do {                                                         \
>
>  #include <asm-generic/bug.h>
>
> -#else /* __ASSEMBLY__ */
> -
> -#ifdef CONFIG_GENERIC_BUG
> -
> -#ifdef CONFIG_X86_32
> -.macro __BUG_REL val:req
> -       .long \val
> -.endm
> -#else
> -.macro __BUG_REL val:req
> -       .long \val - 2b
> -.endm
> -#endif
> -
> -#ifdef CONFIG_DEBUG_BUGVERBOSE
> -
> -.macro ASM_BUG ins:req file:req line:req flags:req size:req
> -1:     \ins
> -       .pushsection __bug_table,"aw"
> -2:     __BUG_REL val=1b        # bug_entry::bug_addr
> -       __BUG_REL val=\file     # bug_entry::file
> -       .word \line             # bug_entry::line
> -       .word \flags            # bug_entry::flags
> -       .org 2b+\size
> -       .popsection
> -.endm
> -
> -#else /* !CONFIG_DEBUG_BUGVERBOSE */
> -
> -.macro ASM_BUG ins:req file:req line:req flags:req size:req
> -1:     \ins
> -       .pushsection __bug_table,"aw"
> -2:     __BUG_REL val=1b        # bug_entry::bug_addr
> -       .word \flags            # bug_entry::flags
> -       .org 2b+\size
> -       .popsection
> -.endm
> -
> -#endif /* CONFIG_DEBUG_BUGVERBOSE */
> -
> -#else /* CONFIG_GENERIC_BUG */
> -
> -.macro ASM_BUG ins:req file:req line:req flags:req size:req
> -       \ins
> -.endm
> -
> -#endif /* CONFIG_GENERIC_BUG */
> -
> -#endif /* __ASSEMBLY__ */
> -
>  #endif /* _ASM_X86_BUG_H */
> diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h
> index 7d44272..aced6c9 100644
> --- a/arch/x86/include/asm/cpufeature.h
> +++ b/arch/x86/include/asm/cpufeature.h
> @@ -2,10 +2,10 @@
>  #ifndef _ASM_X86_CPUFEATURE_H
>  #define _ASM_X86_CPUFEATURE_H
>
> -#ifdef __KERNEL__
> -#ifndef __ASSEMBLY__
> -
>  #include <asm/processor.h>
> +
> +#if defined(__KERNEL__) && !defined(__ASSEMBLY__)
> +
>  #include <asm/asm.h>
>  #include <linux/bitops.h>
>
> @@ -161,10 +161,37 @@ extern void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int bit);
>   */
>  static __always_inline __pure bool _static_cpu_has(u16 bit)
>  {
> -       asm_volatile_goto("STATIC_CPU_HAS bitnum=%[bitnum] "
> -                         "cap_byte=\"%[cap_byte]\" "
> -                         "feature=%P[feature] t_yes=%l[t_yes] "
> -                         "t_no=%l[t_no] always=%P[always]"
> +       asm_volatile_goto("1: jmp 6f\n"
> +                "2:\n"
> +                ".skip -(((5f-4f) - (2b-1b)) > 0) * "
> +                        "((5f-4f) - (2b-1b)),0x90\n"
> +                "3:\n"
> +                ".section .altinstructions,\"a\"\n"
> +                " .long 1b - .\n"              /* src offset */
> +                " .long 4f - .\n"              /* repl offset */
> +                " .word %P[always]\n"          /* always replace */
> +                " .byte 3b - 1b\n"             /* src len */
> +                " .byte 5f - 4f\n"             /* repl len */
> +                " .byte 3b - 2b\n"             /* pad len */
> +                ".previous\n"
> +                ".section .altinstr_replacement,\"ax\"\n"
> +                "4: jmp %l[t_no]\n"
> +                "5:\n"
> +                ".previous\n"
> +                ".section .altinstructions,\"a\"\n"
> +                " .long 1b - .\n"              /* src offset */
> +                " .long 0\n"                   /* no replacement */
> +                " .word %P[feature]\n"         /* feature bit */
> +                " .byte 3b - 1b\n"             /* src len */
> +                " .byte 0\n"                   /* repl len */
> +                " .byte 0\n"                   /* pad len */
> +                ".previous\n"
> +                ".section .altinstr_aux,\"ax\"\n"
> +                "6:\n"
> +                " testb %[bitnum],%[cap_byte]\n"
> +                " jnz %l[t_yes]\n"
> +                " jmp %l[t_no]\n"
> +                ".previous\n"
>                  : : [feature]  "i" (bit),
>                      [always]   "i" (X86_FEATURE_ALWAYS),
>                      [bitnum]   "i" (1 << (bit & 7)),
> @@ -199,44 +226,5 @@ static __always_inline __pure bool _static_cpu_has(u16 bit)
>  #define CPU_FEATURE_TYPEVAL            boot_cpu_data.x86_vendor, boot_cpu_data.x86, \
>                                         boot_cpu_data.x86_model
>
> -#else /* __ASSEMBLY__ */
> -
> -.macro STATIC_CPU_HAS bitnum:req cap_byte:req feature:req t_yes:req t_no:req always:req
> -1:
> -       jmp 6f
> -2:
> -       .skip -(((5f-4f) - (2b-1b)) > 0) * ((5f-4f) - (2b-1b)),0x90
> -3:
> -       .section .altinstructions,"a"
> -       .long 1b - .            /* src offset */
> -       .long 4f - .            /* repl offset */
> -       .word \always           /* always replace */
> -       .byte 3b - 1b           /* src len */
> -       .byte 5f - 4f           /* repl len */
> -       .byte 3b - 2b           /* pad len */
> -       .previous
> -       .section .altinstr_replacement,"ax"
> -4:
> -       jmp \t_no
> -5:
> -       .previous
> -       .section .altinstructions,"a"
> -       .long 1b - .            /* src offset */
> -       .long 0                 /* no replacement */
> -       .word \feature          /* feature bit */
> -       .byte 3b - 1b           /* src len */
> -       .byte 0                 /* repl len */
> -       .byte 0                 /* pad len */
> -       .previous
> -       .section .altinstr_aux,"ax"
> -6:
> -       testb \bitnum,\cap_byte
> -       jnz \t_yes
> -       jmp \t_no
> -       .previous
> -.endm
> -
> -#endif /* __ASSEMBLY__ */
> -
> -#endif /* __KERNEL__ */
> +#endif /* defined(__KERNEL__) && !defined(__ASSEMBLY__) */
>  #endif /* _ASM_X86_CPUFEATURE_H */
> diff --git a/arch/x86/include/asm/jump_label.h b/arch/x86/include/asm/jump_label.h
> index a5fb34f..21efc9d 100644
> --- a/arch/x86/include/asm/jump_label.h
> +++ b/arch/x86/include/asm/jump_label.h
> @@ -2,6 +2,19 @@
>  #ifndef _ASM_X86_JUMP_LABEL_H
>  #define _ASM_X86_JUMP_LABEL_H
>
> +#ifndef HAVE_JUMP_LABEL
> +/*
> + * For better or for worse, if jump labels (the gcc extension) are missing,
> + * then the entire static branch patching infrastructure is compiled out.
> + * If that happens, the code in here will malfunction.  Raise a compiler
> + * error instead.
> + *
> + * In theory, jump labels and the static branch patching infrastructure
> + * could be decoupled to fix this.
> + */
> +#error asm/jump_label.h included on a non-jump-label kernel
> +#endif
> +
>  #define JUMP_LABEL_NOP_SIZE 5
>
>  #ifdef CONFIG_X86_64
> @@ -20,9 +33,15 @@
>
>  static __always_inline bool arch_static_branch(struct static_key *key, bool branch)
>  {
> -       asm_volatile_goto("STATIC_BRANCH_NOP l_yes=\"%l[l_yes]\" key=\"%c0\" "
> -                         "branch=\"%c1\""
> -                       : :  "i" (key), "i" (branch) : : l_yes);
> +       asm_volatile_goto("1:"
> +               ".byte " __stringify(STATIC_KEY_INIT_NOP) "\n\t"
> +               ".pushsection __jump_table,  \"aw\" \n\t"
> +               _ASM_ALIGN "\n\t"
> +               ".long 1b - ., %l[l_yes] - . \n\t"
> +               _ASM_PTR "%c0 + %c1 - .\n\t"
> +               ".popsection \n\t"
> +               : :  "i" (key), "i" (branch) : : l_yes);
> +
>         return false;
>  l_yes:
>         return true;
> @@ -30,8 +49,14 @@ static __always_inline bool arch_static_branch(struct static_key *key, bool bran
>
>  static __always_inline bool arch_static_branch_jump(struct static_key *key, bool branch)
>  {
> -       asm_volatile_goto("STATIC_BRANCH_JMP l_yes=\"%l[l_yes]\" key=\"%c0\" "
> -                         "branch=\"%c1\""
> +       asm_volatile_goto("1:"
> +               ".byte 0xe9\n\t .long %l[l_yes] - 2f\n\t"
> +               "2:\n\t"
> +               ".pushsection __jump_table,  \"aw\" \n\t"
> +               _ASM_ALIGN "\n\t"
> +               ".long 1b - ., %l[l_yes] - . \n\t"
> +               _ASM_PTR "%c0 + %c1 - .\n\t"
> +               ".popsection \n\t"
>                 : :  "i" (key), "i" (branch) : : l_yes);
>
>         return false;
> @@ -41,26 +66,37 @@ static __always_inline bool arch_static_branch_jump(struct static_key *key, bool
>
>  #else  /* __ASSEMBLY__ */
>
> -.macro STATIC_BRANCH_NOP l_yes:req key:req branch:req
> -.Lstatic_branch_nop_\@:
> -       .byte STATIC_KEY_INIT_NOP
> -.Lstatic_branch_no_after_\@:
> +.macro STATIC_JUMP_IF_TRUE target, key, def
> +.Lstatic_jump_\@:
> +       .if \def
> +       /* Equivalent to "jmp.d32 \target" */
> +       .byte           0xe9
> +       .long           \target - .Lstatic_jump_after_\@
> +.Lstatic_jump_after_\@:
> +       .else
> +       .byte           STATIC_KEY_INIT_NOP
> +       .endif
>         .pushsection __jump_table, "aw"
>         _ASM_ALIGN
> -       .long           .Lstatic_branch_nop_\@ - ., \l_yes - .
> -       _ASM_PTR        \key + \branch - .
> +       .long           .Lstatic_jump_\@ - ., \target - .
> +       _ASM_PTR        \key - .
>         .popsection
>  .endm
>
> -.macro STATIC_BRANCH_JMP l_yes:req key:req branch:req
> -.Lstatic_branch_jmp_\@:
> -       .byte 0xe9
> -       .long \l_yes - .Lstatic_branch_jmp_after_\@
> -.Lstatic_branch_jmp_after_\@:
> +.macro STATIC_JUMP_IF_FALSE target, key, def
> +.Lstatic_jump_\@:
> +       .if \def
> +       .byte           STATIC_KEY_INIT_NOP
> +       .else
> +       /* Equivalent to "jmp.d32 \target" */
> +       .byte           0xe9
> +       .long           \target - .Lstatic_jump_after_\@
> +.Lstatic_jump_after_\@:
> +       .endif
>         .pushsection __jump_table, "aw"
>         _ASM_ALIGN
> -       .long           .Lstatic_branch_jmp_\@ - ., \l_yes - .
> -       _ASM_PTR        \key + \branch - .
> +       .long           .Lstatic_jump_\@ - ., \target - .
> +       _ASM_PTR        \key + 1 - .
>         .popsection
>  .endm
>
> diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h
> index 26942ad..488c596 100644
> --- a/arch/x86/include/asm/paravirt_types.h
> +++ b/arch/x86/include/asm/paravirt_types.h
> @@ -348,11 +348,23 @@ extern struct paravirt_patch_template pv_ops;
>  #define paravirt_clobber(clobber)              \
>         [paravirt_clobber] "i" (clobber)
>
> +/*
> + * Generate some code, and mark it as patchable by the
> + * apply_paravirt() alternate instruction patcher.
> + */
> +#define _paravirt_alt(insn_string, type, clobber)      \
> +       "771:\n\t" insn_string "\n" "772:\n"            \
> +       ".pushsection .parainstructions,\"a\"\n"        \
> +       _ASM_ALIGN "\n"                                 \
> +       _ASM_PTR " 771b\n"                              \
> +       "  .byte " type "\n"                            \
> +       "  .byte 772b-771b\n"                           \
> +       "  .short " clobber "\n"                        \
> +       ".popsection\n"
> +
>  /* Generate patchable code, with the default asm parameters. */
> -#define paravirt_call                                                  \
> -       "PARAVIRT_CALL type=\"%c[paravirt_typenum]\""                   \
> -       " clobber=\"%c[paravirt_clobber]\""                             \
> -       " pv_opptr=\"%c[paravirt_opptr]\";"
> +#define paravirt_alt(insn_string)                                      \
> +       _paravirt_alt(insn_string, "%c[paravirt_typenum]", "%c[paravirt_clobber]")
>
>  /* Simple instruction patching code. */
>  #define NATIVE_LABEL(a,x,b) "\n\t.globl " a #x "_" #b "\n" a #x "_" #b ":\n\t"
> @@ -373,6 +385,16 @@ unsigned native_patch(u8 type, void *ibuf, unsigned long addr, unsigned len);
>  int paravirt_disable_iospace(void);
>
>  /*
> + * This generates an indirect call based on the operation type number.
> + * The type number, computed in PARAVIRT_PATCH, is derived from the
> + * offset into the paravirt_patch_template structure, and can therefore be
> + * freely converted back into a structure offset.
> + */
> +#define PARAVIRT_CALL                                  \
> +       ANNOTATE_RETPOLINE_SAFE                         \
> +       "call *%c[paravirt_opptr];"
> +
> +/*
>   * These macros are intended to wrap calls through one of the paravirt
>   * ops structs, so that they can be later identified and patched at
>   * runtime.
> @@ -509,7 +531,7 @@ int paravirt_disable_iospace(void);
>                 /* since this condition will never hold */              \
>                 if (sizeof(rettype) > sizeof(unsigned long)) {          \
>                         asm volatile(pre                                \
> -                                    paravirt_call                      \
> +                                    paravirt_alt(PARAVIRT_CALL)        \
>                                      post                               \
>                                      : call_clbr, ASM_CALL_CONSTRAINT   \
>                                      : paravirt_type(op),               \
> @@ -519,7 +541,7 @@ int paravirt_disable_iospace(void);
>                         __ret = (rettype)((((u64)__edx) << 32) | __eax); \
>                 } else {                                                \
>                         asm volatile(pre                                \
> -                                    paravirt_call                      \
> +                                    paravirt_alt(PARAVIRT_CALL)        \
>                                      post                               \
>                                      : call_clbr, ASM_CALL_CONSTRAINT   \
>                                      : paravirt_type(op),               \
> @@ -546,7 +568,7 @@ int paravirt_disable_iospace(void);
>                 PVOP_VCALL_ARGS;                                        \
>                 PVOP_TEST_NULL(op);                                     \
>                 asm volatile(pre                                        \
> -                            paravirt_call                              \
> +                            paravirt_alt(PARAVIRT_CALL)                \
>                              post                                       \
>                              : call_clbr, ASM_CALL_CONSTRAINT           \
>                              : paravirt_type(op),                       \
> @@ -664,26 +686,6 @@ struct paravirt_patch_site {
>  extern struct paravirt_patch_site __parainstructions[],
>         __parainstructions_end[];
>
> -#else  /* __ASSEMBLY__ */
> -
> -/*
> - * This generates an indirect call based on the operation type number.
> - * The type number, computed in PARAVIRT_PATCH, is derived from the
> - * offset into the paravirt_patch_template structure, and can therefore be
> - * freely converted back into a structure offset.
> - */
> -.macro PARAVIRT_CALL type:req clobber:req pv_opptr:req
> -771:   ANNOTATE_RETPOLINE_SAFE
> -       call *\pv_opptr
> -772:   .pushsection .parainstructions,"a"
> -       _ASM_ALIGN
> -       _ASM_PTR 771b
> -       .byte \type
> -       .byte 772b-771b
> -       .short \clobber
> -       .popsection
> -.endm
> -
>  #endif /* __ASSEMBLY__ */
>
>  #endif /* _ASM_X86_PARAVIRT_TYPES_H */
> diff --git a/arch/x86/include/asm/refcount.h b/arch/x86/include/asm/refcount.h
> index a8b5e1e..dbaed55 100644
> --- a/arch/x86/include/asm/refcount.h
> +++ b/arch/x86/include/asm/refcount.h
> @@ -4,41 +4,6 @@
>   * x86-specific implementation of refcount_t. Based on PAX_REFCOUNT from
>   * PaX/grsecurity.
>   */
> -
> -#ifdef __ASSEMBLY__
> -
> -#include <asm/asm.h>
> -#include <asm/bug.h>
> -
> -.macro REFCOUNT_EXCEPTION counter:req
> -       .pushsection .text..refcount
> -111:   lea \counter, %_ASM_CX
> -112:   ud2
> -       ASM_UNREACHABLE
> -       .popsection
> -113:   _ASM_EXTABLE_REFCOUNT(112b, 113b)
> -.endm
> -
> -/* Trigger refcount exception if refcount result is negative. */
> -.macro REFCOUNT_CHECK_LT_ZERO counter:req
> -       js 111f
> -       REFCOUNT_EXCEPTION counter="\counter"
> -.endm
> -
> -/* Trigger refcount exception if refcount result is zero or negative. */
> -.macro REFCOUNT_CHECK_LE_ZERO counter:req
> -       jz 111f
> -       REFCOUNT_CHECK_LT_ZERO counter="\counter"
> -.endm
> -
> -/* Trigger refcount exception unconditionally. */
> -.macro REFCOUNT_ERROR counter:req
> -       jmp 111f
> -       REFCOUNT_EXCEPTION counter="\counter"
> -.endm
> -
> -#else /* __ASSEMBLY__ */
> -
>  #include <linux/refcount.h>
>  #include <asm/bug.h>
>
> @@ -50,12 +15,35 @@
>   * central refcount exception. The fixup address for the exception points
>   * back to the regular execution flow in .text.
>   */
> +#define _REFCOUNT_EXCEPTION                            \
> +       ".pushsection .text..refcount\n"                \
> +       "111:\tlea %[var], %%" _ASM_CX "\n"             \
> +       "112:\t" ASM_UD2 "\n"                           \
> +       ASM_UNREACHABLE                                 \
> +       ".popsection\n"                                 \
> +       "113:\n"                                        \
> +       _ASM_EXTABLE_REFCOUNT(112b, 113b)
> +
> +/* Trigger refcount exception if refcount result is negative. */
> +#define REFCOUNT_CHECK_LT_ZERO                         \
> +       "js 111f\n\t"                                   \
> +       _REFCOUNT_EXCEPTION
> +
> +/* Trigger refcount exception if refcount result is zero or negative. */
> +#define REFCOUNT_CHECK_LE_ZERO                         \
> +       "jz 111f\n\t"                                   \
> +       REFCOUNT_CHECK_LT_ZERO
> +
> +/* Trigger refcount exception unconditionally. */
> +#define REFCOUNT_ERROR                                 \
> +       "jmp 111f\n\t"                                  \
> +       _REFCOUNT_EXCEPTION
>
>  static __always_inline void refcount_add(unsigned int i, refcount_t *r)
>  {
>         asm volatile(LOCK_PREFIX "addl %1,%0\n\t"
> -               "REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
> -               : [counter] "+m" (r->refs.counter)
> +               REFCOUNT_CHECK_LT_ZERO
> +               : [var] "+m" (r->refs.counter)
>                 : "ir" (i)
>                 : "cc", "cx");
>  }
> @@ -63,32 +51,31 @@ static __always_inline void refcount_add(unsigned int i, refcount_t *r)
>  static __always_inline void refcount_inc(refcount_t *r)
>  {
>         asm volatile(LOCK_PREFIX "incl %0\n\t"
> -               "REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
> -               : [counter] "+m" (r->refs.counter)
> +               REFCOUNT_CHECK_LT_ZERO
> +               : [var] "+m" (r->refs.counter)
>                 : : "cc", "cx");
>  }
>
>  static __always_inline void refcount_dec(refcount_t *r)
>  {
>         asm volatile(LOCK_PREFIX "decl %0\n\t"
> -               "REFCOUNT_CHECK_LE_ZERO counter=\"%[counter]\""
> -               : [counter] "+m" (r->refs.counter)
> +               REFCOUNT_CHECK_LE_ZERO
> +               : [var] "+m" (r->refs.counter)
>                 : : "cc", "cx");
>  }
>
>  static __always_inline __must_check
>  bool refcount_sub_and_test(unsigned int i, refcount_t *r)
>  {
> -
>         return GEN_BINARY_SUFFIXED_RMWcc(LOCK_PREFIX "subl",
> -                                        "REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
> +                                        REFCOUNT_CHECK_LT_ZERO,
>                                          r->refs.counter, e, "er", i, "cx");
>  }
>
>  static __always_inline __must_check bool refcount_dec_and_test(refcount_t *r)
>  {
>         return GEN_UNARY_SUFFIXED_RMWcc(LOCK_PREFIX "decl",
> -                                       "REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
> +                                       REFCOUNT_CHECK_LT_ZERO,
>                                         r->refs.counter, e, "cx");
>  }
>
> @@ -106,8 +93,8 @@ bool refcount_add_not_zero(unsigned int i, refcount_t *r)
>
>                 /* Did we try to increment from/to an undesirable state? */
>                 if (unlikely(c < 0 || c == INT_MAX || result < c)) {
> -                       asm volatile("REFCOUNT_ERROR counter=\"%[counter]\""
> -                                    : : [counter] "m" (r->refs.counter)
> +                       asm volatile(REFCOUNT_ERROR
> +                                    : : [var] "m" (r->refs.counter)
>                                      : "cc", "cx");
>                         break;
>                 }
> @@ -122,6 +109,4 @@ static __always_inline __must_check bool refcount_inc_not_zero(refcount_t *r)
>         return refcount_add_not_zero(1, r);
>  }
>
> -#endif /* __ASSEMBLY__ */
> -
>  #endif
> diff --git a/arch/x86/kernel/macros.S b/arch/x86/kernel/macros.S
> deleted file mode 100644
> index 161c950..0000000
> --- a/arch/x86/kernel/macros.S
> +++ /dev/null
> @@ -1,16 +0,0 @@
> -/* SPDX-License-Identifier: GPL-2.0 */
> -
> -/*
> - * This file includes headers whose assembly part includes macros which are
> - * commonly used. The macros are precompiled into assmebly file which is later
> - * assembled together with each compiled file.
> - */
> -
> -#include <linux/compiler.h>
> -#include <asm/refcount.h>
> -#include <asm/alternative-asm.h>
> -#include <asm/bug.h>
> -#include <asm/paravirt.h>
> -#include <asm/asm.h>
> -#include <asm/cpufeature.h>
> -#include <asm/jump_label.h>
> diff --git a/include/asm-generic/bug.h b/include/asm-generic/bug.h
> index cdafa5e..20561a6 100644
> --- a/include/asm-generic/bug.h
> +++ b/include/asm-generic/bug.h
> @@ -17,8 +17,10 @@
>  #ifndef __ASSEMBLY__
>  #include <linux/kernel.h>
>
> -struct bug_entry {
> +#ifdef CONFIG_BUG
> +
>  #ifdef CONFIG_GENERIC_BUG
> +struct bug_entry {
>  #ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS
>         unsigned long   bug_addr;
>  #else
> @@ -33,10 +35,8 @@ struct bug_entry {
>         unsigned short  line;
>  #endif
>         unsigned short  flags;
> -#endif /* CONFIG_GENERIC_BUG */
>  };
> -
> -#ifdef CONFIG_BUG
> +#endif /* CONFIG_GENERIC_BUG */
>
>  /*
>   * Don't use BUG() or BUG_ON() unless there's really no way out; one
> diff --git a/include/linux/compiler.h b/include/linux/compiler.h
> index 06396c1..fc5004a 100644
> --- a/include/linux/compiler.h
> +++ b/include/linux/compiler.h
> @@ -99,13 +99,22 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
>   * unique, to convince GCC not to merge duplicate inline asm statements.
>   */
>  #define annotate_reachable() ({                                                \
> -       asm volatile("ANNOTATE_REACHABLE counter=%c0"                   \
> -                    : : "i" (__COUNTER__));                            \
> +       asm volatile("%c0:\n\t"                                         \
> +                    ".pushsection .discard.reachable\n\t"              \
> +                    ".long %c0b - .\n\t"                               \
> +                    ".popsection\n\t" : : "i" (__COUNTER__));          \
>  })
>  #define annotate_unreachable() ({                                      \
> -       asm volatile("ANNOTATE_UNREACHABLE counter=%c0"                 \
> -                    : : "i" (__COUNTER__));                            \
> +       asm volatile("%c0:\n\t"                                         \
> +                    ".pushsection .discard.unreachable\n\t"            \
> +                    ".long %c0b - .\n\t"                               \
> +                    ".popsection\n\t" : : "i" (__COUNTER__));          \
>  })
> +#define ASM_UNREACHABLE                                                        \
> +       "999:\n\t"                                                      \
> +       ".pushsection .discard.unreachable\n\t"                         \
> +       ".long 999b - .\n\t"                                            \
> +       ".popsection\n\t"
>  #else
>  #define annotate_reachable()
>  #define annotate_unreachable()
> @@ -293,45 +302,6 @@ static inline void *offset_to_ptr(const int *off)
>         return (void *)((unsigned long)off + *off);
>  }
>
> -#else /* __ASSEMBLY__ */
> -
> -#ifdef __KERNEL__
> -#ifndef LINKER_SCRIPT
> -
> -#ifdef CONFIG_STACK_VALIDATION
> -.macro ANNOTATE_UNREACHABLE counter:req
> -\counter:
> -       .pushsection .discard.unreachable
> -       .long \counter\()b -.
> -       .popsection
> -.endm
> -
> -.macro ANNOTATE_REACHABLE counter:req
> -\counter:
> -       .pushsection .discard.reachable
> -       .long \counter\()b -.
> -       .popsection
> -.endm
> -
> -.macro ASM_UNREACHABLE
> -999:
> -       .pushsection .discard.unreachable
> -       .long 999b - .
> -       .popsection
> -.endm
> -#else /* CONFIG_STACK_VALIDATION */
> -.macro ANNOTATE_UNREACHABLE counter:req
> -.endm
> -
> -.macro ANNOTATE_REACHABLE counter:req
> -.endm
> -
> -.macro ASM_UNREACHABLE
> -.endm
> -#endif /* CONFIG_STACK_VALIDATION */
> -
> -#endif /* LINKER_SCRIPT */
> -#endif /* __KERNEL__ */
>  #endif /* __ASSEMBLY__ */
>
>  /* Compile time object size, -1 for unknown */
> diff --git a/scripts/Kbuild.include b/scripts/Kbuild.include
> index bb01555..3d09844 100644
> --- a/scripts/Kbuild.include
> +++ b/scripts/Kbuild.include
> @@ -115,9 +115,7 @@ __cc-option = $(call try-run,\
>
>  # Do not attempt to build with gcc plugins during cc-option tests.
>  # (And this uses delayed resolution so the flags will be up to date.)
> -# In addition, do not include the asm macros which are built later.
> -CC_OPTION_FILTERED = $(GCC_PLUGINS_CFLAGS) $(ASM_MACRO_FLAGS)
> -CC_OPTION_CFLAGS = $(filter-out $(CC_OPTION_FILTERED),$(KBUILD_CFLAGS))
> +CC_OPTION_CFLAGS = $(filter-out $(GCC_PLUGINS_CFLAGS),$(KBUILD_CFLAGS))
>
>  # cc-option
>  # Usage: cflags-y += $(call cc-option,-march=winchip-c6,-march=i586)
> diff --git a/scripts/mod/Makefile b/scripts/mod/Makefile
> index a5b4af4..42c5d50 100644
> --- a/scripts/mod/Makefile
> +++ b/scripts/mod/Makefile
> @@ -4,8 +4,6 @@ OBJECT_FILES_NON_STANDARD := y
>  hostprogs-y    := modpost mk_elfconfig
>  always         := $(hostprogs-y) empty.o
>
> -CFLAGS_REMOVE_empty.o := $(ASM_MACRO_FLAGS)
> -
>  modpost-objs   := modpost.o file2alias.o sumversion.o
>
>  devicetable-offsets-file := devicetable-offsets.h
> --
> 2.7.4
>
================================================================================

From: Sedat Dilek <sedat.dilek () gmail ! com>
To: linux-kbuild
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Mon, 17 Dec 2018 09:16:32 +0000
Message-ID: <CA+icZUUnkGBp-3pJxE_s_Oc+pdNBrVc_ngYUwpv7zLEbtE_eVg () mail ! gmail ! com>
--------------------
On Thu, Dec 13, 2018 at 10:19 AM Masahiro Yamada
<yamada.masahiro@socionext.com> wrote:
>
> Revert the following commits:
>
> - 5bdcd510c2ac9efaf55c4cbd8d46421d8e2320cd
>   ("x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs")
>
> - d5a581d84ae6b8a4a740464b80d8d9cf1e7947b2
>   ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
>
> - 0474d5d9d2f7f3b11262f7bf87d0e7314ead9200.
>   ("x86/extable: Macrofy inline assembly code to work around GCC inlining bugs")
>
> - 494b5168f2de009eb80f198f668da374295098dd.
>   ("x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops")
>
> - f81f8ad56fd1c7b99b2ed1c314527f7d9ac447c6.
>   ("x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs")
>
> - 77f48ec28e4ccff94d2e5f4260a83ac27a7f3099.
>   ("x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs")
>
> - 9e1725b410594911cc5981b6c7b4cea4ec054ca8.
>   ("x86/refcount: Work around GCC inlining bug")
>   (Conflicts: arch/x86/include/asm/refcount.h)
>
> - c06c4d8090513f2974dfdbed2ac98634357ac475.
>   ("x86/objtool: Use asm macros to work around GCC inlining bugs")
>
> - 77b0bf55bc675233d22cd5df97605d516d64525e.
>   ("kbuild/Makefile: Prepare for using macros in inline assembly code to work around asm() related GCC inlining bugs")
>
> A few days after those commits applied, discussion started to solve
> the issue more elegantly on the compiler side:
>
>   https://lkml.org/lkml/2018/10/7/92
>
> The "asm inline" was implemented by Segher Boessenkool, and now queued
> up for GCC 9. (People were positive even for back-porting it to older
> compilers).
>
> Since the in-kernel workarounds merged, some issues have been reported:
> breakage of building with distcc/icecc, breakage of distro packages for
> module building. (More fundamentally, we cannot build external modules
> after 'make clean')
>
> Patching around the build system would make the code even uglier.
>
> Given that this issue will be solved in a cleaner way sooner or later,
> let's revert the in-kernel workarounds, and wait for GCC 9.
>
> Reported-by: Logan Gunthorpe <logang@deltatee.com> # distcc
> Reported-by: Sedat Dilek <sedat.dilek@gmail.com> # debian/rpm package

Hi,

I reported the issue with debian package breakage in [1].

I am not subscribed to any involved mailing-list and not following all
the discussions.
I see the situation is not easy as there is especially linux-kbuild
and linux/x86 involved and maybe other interests.
But I am interested in having a fix in v4.20 final and hope this all
still works with LLVM/Clang.

I can offer my help in testing - against Linux v4.20-rc7.
Not sure if all discussed material is in upstream or elsewhere.
What is your suggestion for me as a tester?

Will we have a solution in Linux v4.20 final?

Thanks.

With my best wishes,
- Sedat -

[1] https://marc.info/?t=154212770600037&r=1&w=2

> Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
> Cc: Nadav Amit <namit@vmware.com>
> Cc: Segher Boessenkool <segher@kernel.crashing.org>
> ---
>
> Please consider this for v4.20 release.
> Currently, distro package build is broken.
>
>
>  Makefile                               |  9 +---
>  arch/x86/Makefile                      |  7 ---
>  arch/x86/entry/calling.h               |  2 +-
>  arch/x86/include/asm/alternative-asm.h | 20 +++----
>  arch/x86/include/asm/alternative.h     | 11 +++-
>  arch/x86/include/asm/asm.h             | 53 +++++++++++-------
>  arch/x86/include/asm/bug.h             | 98 +++++++++++++++-------------------
>  arch/x86/include/asm/cpufeature.h      | 82 ++++++++++++----------------
>  arch/x86/include/asm/jump_label.h      | 72 ++++++++++++++++++-------
>  arch/x86/include/asm/paravirt_types.h  | 56 +++++++++----------
>  arch/x86/include/asm/refcount.h        | 81 ++++++++++++----------------
>  arch/x86/kernel/macros.S               | 16 ------
>  include/asm-generic/bug.h              |  8 +--
>  include/linux/compiler.h               | 56 +++++--------------
>  scripts/Kbuild.include                 |  4 +-
>  scripts/mod/Makefile                   |  2 -
>  16 files changed, 262 insertions(+), 315 deletions(-)
>  delete mode 100644 arch/x86/kernel/macros.S
>
> diff --git a/Makefile b/Makefile
> index f2c3423..4cf4c5b 100644
> --- a/Makefile
> +++ b/Makefile
> @@ -1081,7 +1081,7 @@ scripts: scripts_basic scripts_dtc asm-generic gcc-plugins $(autoksyms_h)
>  # version.h and scripts_basic is processed / created.
>
>  # Listed in dependency order
> -PHONY += prepare archprepare macroprepare prepare0 prepare1 prepare2 prepare3
> +PHONY += prepare archprepare prepare0 prepare1 prepare2 prepare3
>
>  # prepare3 is used to check if we are building in a separate output directory,
>  # and if so do:
> @@ -1104,9 +1104,7 @@ prepare2: prepare3 outputmakefile asm-generic
>  prepare1: prepare2 $(version_h) $(autoksyms_h) include/generated/utsrelease.h
>         $(cmd_crmodverdir)
>
> -macroprepare: prepare1 archmacros
> -
> -archprepare: archheaders archscripts macroprepare scripts_basic
> +archprepare: archheaders archscripts prepare1 scripts_basic
>
>  prepare0: archprepare gcc-plugins
>         $(Q)$(MAKE) $(build)=.
> @@ -1174,9 +1172,6 @@ archheaders:
>  PHONY += archscripts
>  archscripts:
>
> -PHONY += archmacros
> -archmacros:
> -
>  PHONY += __headers
>  __headers: $(version_h) scripts_basic uapi-asm-generic archheaders archscripts
>         $(Q)$(MAKE) $(build)=scripts build_unifdef
> diff --git a/arch/x86/Makefile b/arch/x86/Makefile
> index 75ef499..85a66c4 100644
> --- a/arch/x86/Makefile
> +++ b/arch/x86/Makefile
> @@ -232,13 +232,6 @@ archscripts: scripts_basic
>  archheaders:
>         $(Q)$(MAKE) $(build)=arch/x86/entry/syscalls all
>
> -archmacros:
> -       $(Q)$(MAKE) $(build)=arch/x86/kernel arch/x86/kernel/macros.s
> -
> -ASM_MACRO_FLAGS = -Wa,arch/x86/kernel/macros.s
> -export ASM_MACRO_FLAGS
> -KBUILD_CFLAGS += $(ASM_MACRO_FLAGS)
> -
>  ###
>  # Kernel objects
>
> diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h
> index 25e5a6b..20d0885 100644
> --- a/arch/x86/entry/calling.h
> +++ b/arch/x86/entry/calling.h
> @@ -352,7 +352,7 @@ For 32-bit we have the following conventions - kernel is built with
>  .macro CALL_enter_from_user_mode
>  #ifdef CONFIG_CONTEXT_TRACKING
>  #ifdef HAVE_JUMP_LABEL
> -       STATIC_BRANCH_JMP l_yes=.Lafter_call_\@, key=context_tracking_enabled, branch=1
> +       STATIC_JUMP_IF_FALSE .Lafter_call_\@, context_tracking_enabled, def=0
>  #endif
>         call enter_from_user_mode
>  .Lafter_call_\@:
> diff --git a/arch/x86/include/asm/alternative-asm.h b/arch/x86/include/asm/alternative-asm.h
> index 8e4ea39..31b627b 100644
> --- a/arch/x86/include/asm/alternative-asm.h
> +++ b/arch/x86/include/asm/alternative-asm.h
> @@ -7,24 +7,16 @@
>  #include <asm/asm.h>
>
>  #ifdef CONFIG_SMP
> -.macro LOCK_PREFIX_HERE
> +       .macro LOCK_PREFIX
> +672:   lock
>         .pushsection .smp_locks,"a"
>         .balign 4
> -       .long 671f - .          # offset
> +       .long 672b - .
>         .popsection
> -671:
> -.endm
> -
> -.macro LOCK_PREFIX insn:vararg
> -       LOCK_PREFIX_HERE
> -       lock \insn
> -.endm
> +       .endm
>  #else
> -.macro LOCK_PREFIX_HERE
> -.endm
> -
> -.macro LOCK_PREFIX insn:vararg
> -.endm
> +       .macro LOCK_PREFIX
> +       .endm
>  #endif
>
>  /*
> diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h
> index d7faa16..4cd6a3b 100644
> --- a/arch/x86/include/asm/alternative.h
> +++ b/arch/x86/include/asm/alternative.h
> @@ -31,8 +31,15 @@
>   */
>
>  #ifdef CONFIG_SMP
> -#define LOCK_PREFIX_HERE "LOCK_PREFIX_HERE\n\t"
> -#define LOCK_PREFIX "LOCK_PREFIX "
> +#define LOCK_PREFIX_HERE \
> +               ".pushsection .smp_locks,\"a\"\n"       \
> +               ".balign 4\n"                           \
> +               ".long 671f - .\n" /* offset */         \
> +               ".popsection\n"                         \
> +               "671:"
> +
> +#define LOCK_PREFIX LOCK_PREFIX_HERE "\n\tlock; "
> +
>  #else /* ! CONFIG_SMP */
>  #define LOCK_PREFIX_HERE ""
>  #define LOCK_PREFIX ""
> diff --git a/arch/x86/include/asm/asm.h b/arch/x86/include/asm/asm.h
> index 21b0867..6467757b 100644
> --- a/arch/x86/include/asm/asm.h
> +++ b/arch/x86/include/asm/asm.h
> @@ -120,25 +120,12 @@
>  /* Exception table entry */
>  #ifdef __ASSEMBLY__
>  # define _ASM_EXTABLE_HANDLE(from, to, handler)                        \
> -       ASM_EXTABLE_HANDLE from to handler
> -
> -.macro ASM_EXTABLE_HANDLE from:req to:req handler:req
> -       .pushsection "__ex_table","a"
> -       .balign 4
> -       .long (\from) - .
> -       .long (\to) - .
> -       .long (\handler) - .
> +       .pushsection "__ex_table","a" ;                         \
> +       .balign 4 ;                                             \
> +       .long (from) - . ;                                      \
> +       .long (to) - . ;                                        \
> +       .long (handler) - . ;                                   \
>         .popsection
> -.endm
> -#else /* __ASSEMBLY__ */
> -
> -# define _ASM_EXTABLE_HANDLE(from, to, handler)                        \
> -       "ASM_EXTABLE_HANDLE from=" #from " to=" #to             \
> -       " handler=\"" #handler "\"\n\t"
> -
> -/* For C file, we already have NOKPROBE_SYMBOL macro */
> -
> -#endif /* __ASSEMBLY__ */
>
>  # define _ASM_EXTABLE(from, to)                                        \
>         _ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
> @@ -161,7 +148,6 @@
>         _ASM_PTR (entry);                                       \
>         .popsection
>
> -#ifdef __ASSEMBLY__
>  .macro ALIGN_DESTINATION
>         /* check for bad alignment of destination */
>         movl %edi,%ecx
> @@ -185,7 +171,34 @@
>         _ASM_EXTABLE_UA(100b, 103b)
>         _ASM_EXTABLE_UA(101b, 103b)
>         .endm
> -#endif /* __ASSEMBLY__ */
> +
> +#else
> +# define _EXPAND_EXTABLE_HANDLE(x) #x
> +# define _ASM_EXTABLE_HANDLE(from, to, handler)                        \
> +       " .pushsection \"__ex_table\",\"a\"\n"                  \
> +       " .balign 4\n"                                          \
> +       " .long (" #from ") - .\n"                              \
> +       " .long (" #to ") - .\n"                                \
> +       " .long (" _EXPAND_EXTABLE_HANDLE(handler) ") - .\n"    \
> +       " .popsection\n"
> +
> +# define _ASM_EXTABLE(from, to)                                        \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
> +
> +# define _ASM_EXTABLE_UA(from, to)                             \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_uaccess)
> +
> +# define _ASM_EXTABLE_FAULT(from, to)                          \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_fault)
> +
> +# define _ASM_EXTABLE_EX(from, to)                             \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_ext)
> +
> +# define _ASM_EXTABLE_REFCOUNT(from, to)                       \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_refcount)
> +
> +/* For C file, we already have NOKPROBE_SYMBOL macro */
> +#endif
>
>  #ifndef __ASSEMBLY__
>  /*
> diff --git a/arch/x86/include/asm/bug.h b/arch/x86/include/asm/bug.h
> index 5090035..6804d66 100644
> --- a/arch/x86/include/asm/bug.h
> +++ b/arch/x86/include/asm/bug.h
> @@ -4,8 +4,6 @@
>
>  #include <linux/stringify.h>
>
> -#ifndef __ASSEMBLY__
> -
>  /*
>   * Despite that some emulators terminate on UD2, we use it for WARN().
>   *
> @@ -22,15 +20,53 @@
>
>  #define LEN_UD2                2
>
> +#ifdef CONFIG_GENERIC_BUG
> +
> +#ifdef CONFIG_X86_32
> +# define __BUG_REL(val)        ".long " __stringify(val)
> +#else
> +# define __BUG_REL(val)        ".long " __stringify(val) " - 2b"
> +#endif
> +
> +#ifdef CONFIG_DEBUG_BUGVERBOSE
> +
> +#define _BUG_FLAGS(ins, flags)                                         \
> +do {                                                                   \
> +       asm volatile("1:\t" ins "\n"                                    \
> +                    ".pushsection __bug_table,\"aw\"\n"                \
> +                    "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"   \
> +                    "\t"  __BUG_REL(%c0) "\t# bug_entry::file\n"       \
> +                    "\t.word %c1"        "\t# bug_entry::line\n"       \
> +                    "\t.word %c2"        "\t# bug_entry::flags\n"      \
> +                    "\t.org 2b+%c3\n"                                  \
> +                    ".popsection"                                      \
> +                    : : "i" (__FILE__), "i" (__LINE__),                \
> +                        "i" (flags),                                   \
> +                        "i" (sizeof(struct bug_entry)));               \
> +} while (0)
> +
> +#else /* !CONFIG_DEBUG_BUGVERBOSE */
> +
>  #define _BUG_FLAGS(ins, flags)                                         \
>  do {                                                                   \
> -       asm volatile("ASM_BUG ins=\"" ins "\" file=%c0 line=%c1 "       \
> -                    "flags=%c2 size=%c3"                               \
> -                    : : "i" (__FILE__), "i" (__LINE__),                \
> -                        "i" (flags),                                   \
> +       asm volatile("1:\t" ins "\n"                                    \
> +                    ".pushsection __bug_table,\"aw\"\n"                \
> +                    "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"   \
> +                    "\t.word %c0"        "\t# bug_entry::flags\n"      \
> +                    "\t.org 2b+%c1\n"                                  \
> +                    ".popsection"                                      \
> +                    : : "i" (flags),                                   \
>                          "i" (sizeof(struct bug_entry)));               \
>  } while (0)
>
> +#endif /* CONFIG_DEBUG_BUGVERBOSE */
> +
> +#else
> +
> +#define _BUG_FLAGS(ins, flags)  asm volatile(ins)
> +
> +#endif /* CONFIG_GENERIC_BUG */
> +
>  #define HAVE_ARCH_BUG
>  #define BUG()                                                  \
>  do {                                                           \
> @@ -46,54 +82,4 @@ do {                                                         \
>
>  #include <asm-generic/bug.h>
>
> -#else /* __ASSEMBLY__ */
> -
> -#ifdef CONFIG_GENERIC_BUG
> -
> -#ifdef CONFIG_X86_32
> -.macro __BUG_REL val:req
> -       .long \val
> -.endm
> -#else
> -.macro __BUG_REL val:req
> -       .long \val - 2b
> -.endm
> -#endif
> -
> -#ifdef CONFIG_DEBUG_BUGVERBOSE
> -
> -.macro ASM_BUG ins:req file:req line:req flags:req size:req
> -1:     \ins
> -       .pushsection __bug_table,"aw"
> -2:     __BUG_REL val=1b        # bug_entry::bug_addr
> -       __BUG_REL val=\file     # bug_entry::file
> -       .word \line             # bug_entry::line
> -       .word \flags            # bug_entry::flags
> -       .org 2b+\size
> -       .popsection
> -.endm
> -
> -#else /* !CONFIG_DEBUG_BUGVERBOSE */
> -
> -.macro ASM_BUG ins:req file:req line:req flags:req size:req
> -1:     \ins
> -       .pushsection __bug_table,"aw"
> -2:     __BUG_REL val=1b        # bug_entry::bug_addr
> -       .word \flags            # bug_entry::flags
> -       .org 2b+\size
> -       .popsection
> -.endm
> -
> -#endif /* CONFIG_DEBUG_BUGVERBOSE */
> -
> -#else /* CONFIG_GENERIC_BUG */
> -
> -.macro ASM_BUG ins:req file:req line:req flags:req size:req
> -       \ins
> -.endm
> -
> -#endif /* CONFIG_GENERIC_BUG */
> -
> -#endif /* __ASSEMBLY__ */
> -
>  #endif /* _ASM_X86_BUG_H */
> diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h
> index 7d44272..aced6c9 100644
> --- a/arch/x86/include/asm/cpufeature.h
> +++ b/arch/x86/include/asm/cpufeature.h
> @@ -2,10 +2,10 @@
>  #ifndef _ASM_X86_CPUFEATURE_H
>  #define _ASM_X86_CPUFEATURE_H
>
> -#ifdef __KERNEL__
> -#ifndef __ASSEMBLY__
> -
>  #include <asm/processor.h>
> +
> +#if defined(__KERNEL__) && !defined(__ASSEMBLY__)
> +
>  #include <asm/asm.h>
>  #include <linux/bitops.h>
>
> @@ -161,10 +161,37 @@ extern void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int bit);
>   */
>  static __always_inline __pure bool _static_cpu_has(u16 bit)
>  {
> -       asm_volatile_goto("STATIC_CPU_HAS bitnum=%[bitnum] "
> -                         "cap_byte=\"%[cap_byte]\" "
> -                         "feature=%P[feature] t_yes=%l[t_yes] "
> -                         "t_no=%l[t_no] always=%P[always]"
> +       asm_volatile_goto("1: jmp 6f\n"
> +                "2:\n"
> +                ".skip -(((5f-4f) - (2b-1b)) > 0) * "
> +                        "((5f-4f) - (2b-1b)),0x90\n"
> +                "3:\n"
> +                ".section .altinstructions,\"a\"\n"
> +                " .long 1b - .\n"              /* src offset */
> +                " .long 4f - .\n"              /* repl offset */
> +                " .word %P[always]\n"          /* always replace */
> +                " .byte 3b - 1b\n"             /* src len */
> +                " .byte 5f - 4f\n"             /* repl len */
> +                " .byte 3b - 2b\n"             /* pad len */
> +                ".previous\n"
> +                ".section .altinstr_replacement,\"ax\"\n"
> +                "4: jmp %l[t_no]\n"
> +                "5:\n"
> +                ".previous\n"
> +                ".section .altinstructions,\"a\"\n"
> +                " .long 1b - .\n"              /* src offset */
> +                " .long 0\n"                   /* no replacement */
> +                " .word %P[feature]\n"         /* feature bit */
> +                " .byte 3b - 1b\n"             /* src len */
> +                " .byte 0\n"                   /* repl len */
> +                " .byte 0\n"                   /* pad len */
> +                ".previous\n"
> +                ".section .altinstr_aux,\"ax\"\n"
> +                "6:\n"
> +                " testb %[bitnum],%[cap_byte]\n"
> +                " jnz %l[t_yes]\n"
> +                " jmp %l[t_no]\n"
> +                ".previous\n"
>                  : : [feature]  "i" (bit),
>                      [always]   "i" (X86_FEATURE_ALWAYS),
>                      [bitnum]   "i" (1 << (bit & 7)),
> @@ -199,44 +226,5 @@ static __always_inline __pure bool _static_cpu_has(u16 bit)
>  #define CPU_FEATURE_TYPEVAL            boot_cpu_data.x86_vendor, boot_cpu_data.x86, \
>                                         boot_cpu_data.x86_model
>
> -#else /* __ASSEMBLY__ */
> -
> -.macro STATIC_CPU_HAS bitnum:req cap_byte:req feature:req t_yes:req t_no:req always:req
> -1:
> -       jmp 6f
> -2:
> -       .skip -(((5f-4f) - (2b-1b)) > 0) * ((5f-4f) - (2b-1b)),0x90
> -3:
> -       .section .altinstructions,"a"
> -       .long 1b - .            /* src offset */
> -       .long 4f - .            /* repl offset */
> -       .word \always           /* always replace */
> -       .byte 3b - 1b           /* src len */
> -       .byte 5f - 4f           /* repl len */
> -       .byte 3b - 2b           /* pad len */
> -       .previous
> -       .section .altinstr_replacement,"ax"
> -4:
> -       jmp \t_no
> -5:
> -       .previous
> -       .section .altinstructions,"a"
> -       .long 1b - .            /* src offset */
> -       .long 0                 /* no replacement */
> -       .word \feature          /* feature bit */
> -       .byte 3b - 1b           /* src len */
> -       .byte 0                 /* repl len */
> -       .byte 0                 /* pad len */
> -       .previous
> -       .section .altinstr_aux,"ax"
> -6:
> -       testb \bitnum,\cap_byte
> -       jnz \t_yes
> -       jmp \t_no
> -       .previous
> -.endm
> -
> -#endif /* __ASSEMBLY__ */
> -
> -#endif /* __KERNEL__ */
> +#endif /* defined(__KERNEL__) && !defined(__ASSEMBLY__) */
>  #endif /* _ASM_X86_CPUFEATURE_H */
> diff --git a/arch/x86/include/asm/jump_label.h b/arch/x86/include/asm/jump_label.h
> index a5fb34f..21efc9d 100644
> --- a/arch/x86/include/asm/jump_label.h
> +++ b/arch/x86/include/asm/jump_label.h
> @@ -2,6 +2,19 @@
>  #ifndef _ASM_X86_JUMP_LABEL_H
>  #define _ASM_X86_JUMP_LABEL_H
>
> +#ifndef HAVE_JUMP_LABEL
> +/*
> + * For better or for worse, if jump labels (the gcc extension) are missing,
> + * then the entire static branch patching infrastructure is compiled out.
> + * If that happens, the code in here will malfunction.  Raise a compiler
> + * error instead.
> + *
> + * In theory, jump labels and the static branch patching infrastructure
> + * could be decoupled to fix this.
> + */
> +#error asm/jump_label.h included on a non-jump-label kernel
> +#endif
> +
>  #define JUMP_LABEL_NOP_SIZE 5
>
>  #ifdef CONFIG_X86_64
> @@ -20,9 +33,15 @@
>
>  static __always_inline bool arch_static_branch(struct static_key *key, bool branch)
>  {
> -       asm_volatile_goto("STATIC_BRANCH_NOP l_yes=\"%l[l_yes]\" key=\"%c0\" "
> -                         "branch=\"%c1\""
> -                       : :  "i" (key), "i" (branch) : : l_yes);
> +       asm_volatile_goto("1:"
> +               ".byte " __stringify(STATIC_KEY_INIT_NOP) "\n\t"
> +               ".pushsection __jump_table,  \"aw\" \n\t"
> +               _ASM_ALIGN "\n\t"
> +               ".long 1b - ., %l[l_yes] - . \n\t"
> +               _ASM_PTR "%c0 + %c1 - .\n\t"
> +               ".popsection \n\t"
> +               : :  "i" (key), "i" (branch) : : l_yes);
> +
>         return false;
>  l_yes:
>         return true;
> @@ -30,8 +49,14 @@ static __always_inline bool arch_static_branch(struct static_key *key, bool bran
>
>  static __always_inline bool arch_static_branch_jump(struct static_key *key, bool branch)
>  {
> -       asm_volatile_goto("STATIC_BRANCH_JMP l_yes=\"%l[l_yes]\" key=\"%c0\" "
> -                         "branch=\"%c1\""
> +       asm_volatile_goto("1:"
> +               ".byte 0xe9\n\t .long %l[l_yes] - 2f\n\t"
> +               "2:\n\t"
> +               ".pushsection __jump_table,  \"aw\" \n\t"
> +               _ASM_ALIGN "\n\t"
> +               ".long 1b - ., %l[l_yes] - . \n\t"
> +               _ASM_PTR "%c0 + %c1 - .\n\t"
> +               ".popsection \n\t"
>                 : :  "i" (key), "i" (branch) : : l_yes);
>
>         return false;
> @@ -41,26 +66,37 @@ static __always_inline bool arch_static_branch_jump(struct static_key *key, bool
>
>  #else  /* __ASSEMBLY__ */
>
> -.macro STATIC_BRANCH_NOP l_yes:req key:req branch:req
> -.Lstatic_branch_nop_\@:
> -       .byte STATIC_KEY_INIT_NOP
> -.Lstatic_branch_no_after_\@:
> +.macro STATIC_JUMP_IF_TRUE target, key, def
> +.Lstatic_jump_\@:
> +       .if \def
> +       /* Equivalent to "jmp.d32 \target" */
> +       .byte           0xe9
> +       .long           \target - .Lstatic_jump_after_\@
> +.Lstatic_jump_after_\@:
> +       .else
> +       .byte           STATIC_KEY_INIT_NOP
> +       .endif
>         .pushsection __jump_table, "aw"
>         _ASM_ALIGN
> -       .long           .Lstatic_branch_nop_\@ - ., \l_yes - .
> -       _ASM_PTR        \key + \branch - .
> +       .long           .Lstatic_jump_\@ - ., \target - .
> +       _ASM_PTR        \key - .
>         .popsection
>  .endm
>
> -.macro STATIC_BRANCH_JMP l_yes:req key:req branch:req
> -.Lstatic_branch_jmp_\@:
> -       .byte 0xe9
> -       .long \l_yes - .Lstatic_branch_jmp_after_\@
> -.Lstatic_branch_jmp_after_\@:
> +.macro STATIC_JUMP_IF_FALSE target, key, def
> +.Lstatic_jump_\@:
> +       .if \def
> +       .byte           STATIC_KEY_INIT_NOP
> +       .else
> +       /* Equivalent to "jmp.d32 \target" */
> +       .byte           0xe9
> +       .long           \target - .Lstatic_jump_after_\@
> +.Lstatic_jump_after_\@:
> +       .endif
>         .pushsection __jump_table, "aw"
>         _ASM_ALIGN
> -       .long           .Lstatic_branch_jmp_\@ - ., \l_yes - .
> -       _ASM_PTR        \key + \branch - .
> +       .long           .Lstatic_jump_\@ - ., \target - .
> +       _ASM_PTR        \key + 1 - .
>         .popsection
>  .endm
>
> diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h
> index 26942ad..488c596 100644
> --- a/arch/x86/include/asm/paravirt_types.h
> +++ b/arch/x86/include/asm/paravirt_types.h
> @@ -348,11 +348,23 @@ extern struct paravirt_patch_template pv_ops;
>  #define paravirt_clobber(clobber)              \
>         [paravirt_clobber] "i" (clobber)
>
> +/*
> + * Generate some code, and mark it as patchable by the
> + * apply_paravirt() alternate instruction patcher.
> + */
> +#define _paravirt_alt(insn_string, type, clobber)      \
> +       "771:\n\t" insn_string "\n" "772:\n"            \
> +       ".pushsection .parainstructions,\"a\"\n"        \
> +       _ASM_ALIGN "\n"                                 \
> +       _ASM_PTR " 771b\n"                              \
> +       "  .byte " type "\n"                            \
> +       "  .byte 772b-771b\n"                           \
> +       "  .short " clobber "\n"                        \
> +       ".popsection\n"
> +
>  /* Generate patchable code, with the default asm parameters. */
> -#define paravirt_call                                                  \
> -       "PARAVIRT_CALL type=\"%c[paravirt_typenum]\""                   \
> -       " clobber=\"%c[paravirt_clobber]\""                             \
> -       " pv_opptr=\"%c[paravirt_opptr]\";"
> +#define paravirt_alt(insn_string)                                      \
> +       _paravirt_alt(insn_string, "%c[paravirt_typenum]", "%c[paravirt_clobber]")
>
>  /* Simple instruction patching code. */
>  #define NATIVE_LABEL(a,x,b) "\n\t.globl " a #x "_" #b "\n" a #x "_" #b ":\n\t"
> @@ -373,6 +385,16 @@ unsigned native_patch(u8 type, void *ibuf, unsigned long addr, unsigned len);
>  int paravirt_disable_iospace(void);
>
>  /*
> + * This generates an indirect call based on the operation type number.
> + * The type number, computed in PARAVIRT_PATCH, is derived from the
> + * offset into the paravirt_patch_template structure, and can therefore be
> + * freely converted back into a structure offset.
> + */
> +#define PARAVIRT_CALL                                  \
> +       ANNOTATE_RETPOLINE_SAFE                         \
> +       "call *%c[paravirt_opptr];"
> +
> +/*
>   * These macros are intended to wrap calls through one of the paravirt
>   * ops structs, so that they can be later identified and patched at
>   * runtime.
> @@ -509,7 +531,7 @@ int paravirt_disable_iospace(void);
>                 /* since this condition will never hold */              \
>                 if (sizeof(rettype) > sizeof(unsigned long)) {          \
>                         asm volatile(pre                                \
> -                                    paravirt_call                      \
> +                                    paravirt_alt(PARAVIRT_CALL)        \
>                                      post                               \
>                                      : call_clbr, ASM_CALL_CONSTRAINT   \
>                                      : paravirt_type(op),               \
> @@ -519,7 +541,7 @@ int paravirt_disable_iospace(void);
>                         __ret = (rettype)((((u64)__edx) << 32) | __eax); \
>                 } else {                                                \
>                         asm volatile(pre                                \
> -                                    paravirt_call                      \
> +                                    paravirt_alt(PARAVIRT_CALL)        \
>                                      post                               \
>                                      : call_clbr, ASM_CALL_CONSTRAINT   \
>                                      : paravirt_type(op),               \
> @@ -546,7 +568,7 @@ int paravirt_disable_iospace(void);
>                 PVOP_VCALL_ARGS;                                        \
>                 PVOP_TEST_NULL(op);                                     \
>                 asm volatile(pre                                        \
> -                            paravirt_call                              \
> +                            paravirt_alt(PARAVIRT_CALL)                \
>                              post                                       \
>                              : call_clbr, ASM_CALL_CONSTRAINT           \
>                              : paravirt_type(op),                       \
> @@ -664,26 +686,6 @@ struct paravirt_patch_site {
>  extern struct paravirt_patch_site __parainstructions[],
>         __parainstructions_end[];
>
> -#else  /* __ASSEMBLY__ */
> -
> -/*
> - * This generates an indirect call based on the operation type number.
> - * The type number, computed in PARAVIRT_PATCH, is derived from the
> - * offset into the paravirt_patch_template structure, and can therefore be
> - * freely converted back into a structure offset.
> - */
> -.macro PARAVIRT_CALL type:req clobber:req pv_opptr:req
> -771:   ANNOTATE_RETPOLINE_SAFE
> -       call *\pv_opptr
> -772:   .pushsection .parainstructions,"a"
> -       _ASM_ALIGN
> -       _ASM_PTR 771b
> -       .byte \type
> -       .byte 772b-771b
> -       .short \clobber
> -       .popsection
> -.endm
> -
>  #endif /* __ASSEMBLY__ */
>
>  #endif /* _ASM_X86_PARAVIRT_TYPES_H */
> diff --git a/arch/x86/include/asm/refcount.h b/arch/x86/include/asm/refcount.h
> index a8b5e1e..dbaed55 100644
> --- a/arch/x86/include/asm/refcount.h
> +++ b/arch/x86/include/asm/refcount.h
> @@ -4,41 +4,6 @@
>   * x86-specific implementation of refcount_t. Based on PAX_REFCOUNT from
>   * PaX/grsecurity.
>   */
> -
> -#ifdef __ASSEMBLY__
> -
> -#include <asm/asm.h>
> -#include <asm/bug.h>
> -
> -.macro REFCOUNT_EXCEPTION counter:req
> -       .pushsection .text..refcount
> -111:   lea \counter, %_ASM_CX
> -112:   ud2
> -       ASM_UNREACHABLE
> -       .popsection
> -113:   _ASM_EXTABLE_REFCOUNT(112b, 113b)
> -.endm
> -
> -/* Trigger refcount exception if refcount result is negative. */
> -.macro REFCOUNT_CHECK_LT_ZERO counter:req
> -       js 111f
> -       REFCOUNT_EXCEPTION counter="\counter"
> -.endm
> -
> -/* Trigger refcount exception if refcount result is zero or negative. */
> -.macro REFCOUNT_CHECK_LE_ZERO counter:req
> -       jz 111f
> -       REFCOUNT_CHECK_LT_ZERO counter="\counter"
> -.endm
> -
> -/* Trigger refcount exception unconditionally. */
> -.macro REFCOUNT_ERROR counter:req
> -       jmp 111f
> -       REFCOUNT_EXCEPTION counter="\counter"
> -.endm
> -
> -#else /* __ASSEMBLY__ */
> -
>  #include <linux/refcount.h>
>  #include <asm/bug.h>
>
> @@ -50,12 +15,35 @@
>   * central refcount exception. The fixup address for the exception points
>   * back to the regular execution flow in .text.
>   */
> +#define _REFCOUNT_EXCEPTION                            \
> +       ".pushsection .text..refcount\n"                \
> +       "111:\tlea %[var], %%" _ASM_CX "\n"             \
> +       "112:\t" ASM_UD2 "\n"                           \
> +       ASM_UNREACHABLE                                 \
> +       ".popsection\n"                                 \
> +       "113:\n"                                        \
> +       _ASM_EXTABLE_REFCOUNT(112b, 113b)
> +
> +/* Trigger refcount exception if refcount result is negative. */
> +#define REFCOUNT_CHECK_LT_ZERO                         \
> +       "js 111f\n\t"                                   \
> +       _REFCOUNT_EXCEPTION
> +
> +/* Trigger refcount exception if refcount result is zero or negative. */
> +#define REFCOUNT_CHECK_LE_ZERO                         \
> +       "jz 111f\n\t"                                   \
> +       REFCOUNT_CHECK_LT_ZERO
> +
> +/* Trigger refcount exception unconditionally. */
> +#define REFCOUNT_ERROR                                 \
> +       "jmp 111f\n\t"                                  \
> +       _REFCOUNT_EXCEPTION
>
>  static __always_inline void refcount_add(unsigned int i, refcount_t *r)
>  {
>         asm volatile(LOCK_PREFIX "addl %1,%0\n\t"
> -               "REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
> -               : [counter] "+m" (r->refs.counter)
> +               REFCOUNT_CHECK_LT_ZERO
> +               : [var] "+m" (r->refs.counter)
>                 : "ir" (i)
>                 : "cc", "cx");
>  }
> @@ -63,32 +51,31 @@ static __always_inline void refcount_add(unsigned int i, refcount_t *r)
>  static __always_inline void refcount_inc(refcount_t *r)
>  {
>         asm volatile(LOCK_PREFIX "incl %0\n\t"
> -               "REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
> -               : [counter] "+m" (r->refs.counter)
> +               REFCOUNT_CHECK_LT_ZERO
> +               : [var] "+m" (r->refs.counter)
>                 : : "cc", "cx");
>  }
>
>  static __always_inline void refcount_dec(refcount_t *r)
>  {
>         asm volatile(LOCK_PREFIX "decl %0\n\t"
> -               "REFCOUNT_CHECK_LE_ZERO counter=\"%[counter]\""
> -               : [counter] "+m" (r->refs.counter)
> +               REFCOUNT_CHECK_LE_ZERO
> +               : [var] "+m" (r->refs.counter)
>                 : : "cc", "cx");
>  }
>
>  static __always_inline __must_check
>  bool refcount_sub_and_test(unsigned int i, refcount_t *r)
>  {
> -
>         return GEN_BINARY_SUFFIXED_RMWcc(LOCK_PREFIX "subl",
> -                                        "REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
> +                                        REFCOUNT_CHECK_LT_ZERO,
>                                          r->refs.counter, e, "er", i, "cx");
>  }
>
>  static __always_inline __must_check bool refcount_dec_and_test(refcount_t *r)
>  {
>         return GEN_UNARY_SUFFIXED_RMWcc(LOCK_PREFIX "decl",
> -                                       "REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
> +                                       REFCOUNT_CHECK_LT_ZERO,
>                                         r->refs.counter, e, "cx");
>  }
>
> @@ -106,8 +93,8 @@ bool refcount_add_not_zero(unsigned int i, refcount_t *r)
>
>                 /* Did we try to increment from/to an undesirable state? */
>                 if (unlikely(c < 0 || c == INT_MAX || result < c)) {
> -                       asm volatile("REFCOUNT_ERROR counter=\"%[counter]\""
> -                                    : : [counter] "m" (r->refs.counter)
> +                       asm volatile(REFCOUNT_ERROR
> +                                    : : [var] "m" (r->refs.counter)
>                                      : "cc", "cx");
>                         break;
>                 }
> @@ -122,6 +109,4 @@ static __always_inline __must_check bool refcount_inc_not_zero(refcount_t *r)
>         return refcount_add_not_zero(1, r);
>  }
>
> -#endif /* __ASSEMBLY__ */
> -
>  #endif
> diff --git a/arch/x86/kernel/macros.S b/arch/x86/kernel/macros.S
> deleted file mode 100644
> index 161c950..0000000
> --- a/arch/x86/kernel/macros.S
> +++ /dev/null
> @@ -1,16 +0,0 @@
> -/* SPDX-License-Identifier: GPL-2.0 */
> -
> -/*
> - * This file includes headers whose assembly part includes macros which are
> - * commonly used. The macros are precompiled into assmebly file which is later
> - * assembled together with each compiled file.
> - */
> -
> -#include <linux/compiler.h>
> -#include <asm/refcount.h>
> -#include <asm/alternative-asm.h>
> -#include <asm/bug.h>
> -#include <asm/paravirt.h>
> -#include <asm/asm.h>
> -#include <asm/cpufeature.h>
> -#include <asm/jump_label.h>
> diff --git a/include/asm-generic/bug.h b/include/asm-generic/bug.h
> index cdafa5e..20561a6 100644
> --- a/include/asm-generic/bug.h
> +++ b/include/asm-generic/bug.h
> @@ -17,8 +17,10 @@
>  #ifndef __ASSEMBLY__
>  #include <linux/kernel.h>
>
> -struct bug_entry {
> +#ifdef CONFIG_BUG
> +
>  #ifdef CONFIG_GENERIC_BUG
> +struct bug_entry {
>  #ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS
>         unsigned long   bug_addr;
>  #else
> @@ -33,10 +35,8 @@ struct bug_entry {
>         unsigned short  line;
>  #endif
>         unsigned short  flags;
> -#endif /* CONFIG_GENERIC_BUG */
>  };
> -
> -#ifdef CONFIG_BUG
> +#endif /* CONFIG_GENERIC_BUG */
>
>  /*
>   * Don't use BUG() or BUG_ON() unless there's really no way out; one
> diff --git a/include/linux/compiler.h b/include/linux/compiler.h
> index 06396c1..fc5004a 100644
> --- a/include/linux/compiler.h
> +++ b/include/linux/compiler.h
> @@ -99,13 +99,22 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
>   * unique, to convince GCC not to merge duplicate inline asm statements.
>   */
>  #define annotate_reachable() ({                                                \
> -       asm volatile("ANNOTATE_REACHABLE counter=%c0"                   \
> -                    : : "i" (__COUNTER__));                            \
> +       asm volatile("%c0:\n\t"                                         \
> +                    ".pushsection .discard.reachable\n\t"              \
> +                    ".long %c0b - .\n\t"                               \
> +                    ".popsection\n\t" : : "i" (__COUNTER__));          \
>  })
>  #define annotate_unreachable() ({                                      \
> -       asm volatile("ANNOTATE_UNREACHABLE counter=%c0"                 \
> -                    : : "i" (__COUNTER__));                            \
> +       asm volatile("%c0:\n\t"                                         \
> +                    ".pushsection .discard.unreachable\n\t"            \
> +                    ".long %c0b - .\n\t"                               \
> +                    ".popsection\n\t" : : "i" (__COUNTER__));          \
>  })
> +#define ASM_UNREACHABLE                                                        \
> +       "999:\n\t"                                                      \
> +       ".pushsection .discard.unreachable\n\t"                         \
> +       ".long 999b - .\n\t"                                            \
> +       ".popsection\n\t"
>  #else
>  #define annotate_reachable()
>  #define annotate_unreachable()
> @@ -293,45 +302,6 @@ static inline void *offset_to_ptr(const int *off)
>         return (void *)((unsigned long)off + *off);
>  }
>
> -#else /* __ASSEMBLY__ */
> -
> -#ifdef __KERNEL__
> -#ifndef LINKER_SCRIPT
> -
> -#ifdef CONFIG_STACK_VALIDATION
> -.macro ANNOTATE_UNREACHABLE counter:req
> -\counter:
> -       .pushsection .discard.unreachable
> -       .long \counter\()b -.
> -       .popsection
> -.endm
> -
> -.macro ANNOTATE_REACHABLE counter:req
> -\counter:
> -       .pushsection .discard.reachable
> -       .long \counter\()b -.
> -       .popsection
> -.endm
> -
> -.macro ASM_UNREACHABLE
> -999:
> -       .pushsection .discard.unreachable
> -       .long 999b - .
> -       .popsection
> -.endm
> -#else /* CONFIG_STACK_VALIDATION */
> -.macro ANNOTATE_UNREACHABLE counter:req
> -.endm
> -
> -.macro ANNOTATE_REACHABLE counter:req
> -.endm
> -
> -.macro ASM_UNREACHABLE
> -.endm
> -#endif /* CONFIG_STACK_VALIDATION */
> -
> -#endif /* LINKER_SCRIPT */
> -#endif /* __KERNEL__ */
>  #endif /* __ASSEMBLY__ */
>
>  /* Compile time object size, -1 for unknown */
> diff --git a/scripts/Kbuild.include b/scripts/Kbuild.include
> index bb01555..3d09844 100644
> --- a/scripts/Kbuild.include
> +++ b/scripts/Kbuild.include
> @@ -115,9 +115,7 @@ __cc-option = $(call try-run,\
>
>  # Do not attempt to build with gcc plugins during cc-option tests.
>  # (And this uses delayed resolution so the flags will be up to date.)
> -# In addition, do not include the asm macros which are built later.
> -CC_OPTION_FILTERED = $(GCC_PLUGINS_CFLAGS) $(ASM_MACRO_FLAGS)
> -CC_OPTION_CFLAGS = $(filter-out $(CC_OPTION_FILTERED),$(KBUILD_CFLAGS))
> +CC_OPTION_CFLAGS = $(filter-out $(GCC_PLUGINS_CFLAGS),$(KBUILD_CFLAGS))
>
>  # cc-option
>  # Usage: cflags-y += $(call cc-option,-march=winchip-c6,-march=i586)
> diff --git a/scripts/mod/Makefile b/scripts/mod/Makefile
> index a5b4af4..42c5d50 100644
> --- a/scripts/mod/Makefile
> +++ b/scripts/mod/Makefile
> @@ -4,8 +4,6 @@ OBJECT_FILES_NON_STANDARD := y
>  hostprogs-y    := modpost mk_elfconfig
>  always         := $(hostprogs-y) empty.o
>
> -CFLAGS_REMOVE_empty.o := $(ASM_MACRO_FLAGS)
> -
>  modpost-objs   := modpost.o file2alias.o sumversion.o
>
>  devicetable-offsets-file := devicetable-offsets.h
> --
> 2.7.4
>
================================================================================

From: Sedat Dilek <sedat.dilek () gmail ! com>
To: linux-kernel
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Mon, 17 Dec 2018 09:16:32 +0000
Message-ID: <CA+icZUUnkGBp-3pJxE_s_Oc+pdNBrVc_ngYUwpv7zLEbtE_eVg () mail ! gmail ! com>
--------------------
On Thu, Dec 13, 2018 at 10:19 AM Masahiro Yamada
<yamada.masahiro@socionext.com> wrote:
>
> Revert the following commits:
>
> - 5bdcd510c2ac9efaf55c4cbd8d46421d8e2320cd
>   ("x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs")
>
> - d5a581d84ae6b8a4a740464b80d8d9cf1e7947b2
>   ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
>
> - 0474d5d9d2f7f3b11262f7bf87d0e7314ead9200.
>   ("x86/extable: Macrofy inline assembly code to work around GCC inlining bugs")
>
> - 494b5168f2de009eb80f198f668da374295098dd.
>   ("x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops")
>
> - f81f8ad56fd1c7b99b2ed1c314527f7d9ac447c6.
>   ("x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs")
>
> - 77f48ec28e4ccff94d2e5f4260a83ac27a7f3099.
>   ("x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs")
>
> - 9e1725b410594911cc5981b6c7b4cea4ec054ca8.
>   ("x86/refcount: Work around GCC inlining bug")
>   (Conflicts: arch/x86/include/asm/refcount.h)
>
> - c06c4d8090513f2974dfdbed2ac98634357ac475.
>   ("x86/objtool: Use asm macros to work around GCC inlining bugs")
>
> - 77b0bf55bc675233d22cd5df97605d516d64525e.
>   ("kbuild/Makefile: Prepare for using macros in inline assembly code to work around asm() related GCC inlining bugs")
>
> A few days after those commits applied, discussion started to solve
> the issue more elegantly on the compiler side:
>
>   https://lkml.org/lkml/2018/10/7/92
>
> The "asm inline" was implemented by Segher Boessenkool, and now queued
> up for GCC 9. (People were positive even for back-porting it to older
> compilers).
>
> Since the in-kernel workarounds merged, some issues have been reported:
> breakage of building with distcc/icecc, breakage of distro packages for
> module building. (More fundamentally, we cannot build external modules
> after 'make clean')
>
> Patching around the build system would make the code even uglier.
>
> Given that this issue will be solved in a cleaner way sooner or later,
> let's revert the in-kernel workarounds, and wait for GCC 9.
>
> Reported-by: Logan Gunthorpe <logang@deltatee.com> # distcc
> Reported-by: Sedat Dilek <sedat.dilek@gmail.com> # debian/rpm package

Hi,

I reported the issue with debian package breakage in [1].

I am not subscribed to any involved mailing-list and not following all
the discussions.
I see the situation is not easy as there is especially linux-kbuild
and linux/x86 involved and maybe other interests.
But I am interested in having a fix in v4.20 final and hope this all
still works with LLVM/Clang.

I can offer my help in testing - against Linux v4.20-rc7.
Not sure if all discussed material is in upstream or elsewhere.
What is your suggestion for me as a tester?

Will we have a solution in Linux v4.20 final?

Thanks.

With my best wishes,
- Sedat -

[1] https://marc.info/?t=154212770600037&r=1&w=2

> Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
> Cc: Nadav Amit <namit@vmware.com>
> Cc: Segher Boessenkool <segher@kernel.crashing.org>
> ---
>
> Please consider this for v4.20 release.
> Currently, distro package build is broken.
>
>
>  Makefile                               |  9 +---
>  arch/x86/Makefile                      |  7 ---
>  arch/x86/entry/calling.h               |  2 +-
>  arch/x86/include/asm/alternative-asm.h | 20 +++----
>  arch/x86/include/asm/alternative.h     | 11 +++-
>  arch/x86/include/asm/asm.h             | 53 +++++++++++-------
>  arch/x86/include/asm/bug.h             | 98 +++++++++++++++-------------------
>  arch/x86/include/asm/cpufeature.h      | 82 ++++++++++++----------------
>  arch/x86/include/asm/jump_label.h      | 72 ++++++++++++++++++-------
>  arch/x86/include/asm/paravirt_types.h  | 56 +++++++++----------
>  arch/x86/include/asm/refcount.h        | 81 ++++++++++++----------------
>  arch/x86/kernel/macros.S               | 16 ------
>  include/asm-generic/bug.h              |  8 +--
>  include/linux/compiler.h               | 56 +++++--------------
>  scripts/Kbuild.include                 |  4 +-
>  scripts/mod/Makefile                   |  2 -
>  16 files changed, 262 insertions(+), 315 deletions(-)
>  delete mode 100644 arch/x86/kernel/macros.S
>
> diff --git a/Makefile b/Makefile
> index f2c3423..4cf4c5b 100644
> --- a/Makefile
> +++ b/Makefile
> @@ -1081,7 +1081,7 @@ scripts: scripts_basic scripts_dtc asm-generic gcc-plugins $(autoksyms_h)
>  # version.h and scripts_basic is processed / created.
>
>  # Listed in dependency order
> -PHONY += prepare archprepare macroprepare prepare0 prepare1 prepare2 prepare3
> +PHONY += prepare archprepare prepare0 prepare1 prepare2 prepare3
>
>  # prepare3 is used to check if we are building in a separate output directory,
>  # and if so do:
> @@ -1104,9 +1104,7 @@ prepare2: prepare3 outputmakefile asm-generic
>  prepare1: prepare2 $(version_h) $(autoksyms_h) include/generated/utsrelease.h
>         $(cmd_crmodverdir)
>
> -macroprepare: prepare1 archmacros
> -
> -archprepare: archheaders archscripts macroprepare scripts_basic
> +archprepare: archheaders archscripts prepare1 scripts_basic
>
>  prepare0: archprepare gcc-plugins
>         $(Q)$(MAKE) $(build)=.
> @@ -1174,9 +1172,6 @@ archheaders:
>  PHONY += archscripts
>  archscripts:
>
> -PHONY += archmacros
> -archmacros:
> -
>  PHONY += __headers
>  __headers: $(version_h) scripts_basic uapi-asm-generic archheaders archscripts
>         $(Q)$(MAKE) $(build)=scripts build_unifdef
> diff --git a/arch/x86/Makefile b/arch/x86/Makefile
> index 75ef499..85a66c4 100644
> --- a/arch/x86/Makefile
> +++ b/arch/x86/Makefile
> @@ -232,13 +232,6 @@ archscripts: scripts_basic
>  archheaders:
>         $(Q)$(MAKE) $(build)=arch/x86/entry/syscalls all
>
> -archmacros:
> -       $(Q)$(MAKE) $(build)=arch/x86/kernel arch/x86/kernel/macros.s
> -
> -ASM_MACRO_FLAGS = -Wa,arch/x86/kernel/macros.s
> -export ASM_MACRO_FLAGS
> -KBUILD_CFLAGS += $(ASM_MACRO_FLAGS)
> -
>  ###
>  # Kernel objects
>
> diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h
> index 25e5a6b..20d0885 100644
> --- a/arch/x86/entry/calling.h
> +++ b/arch/x86/entry/calling.h
> @@ -352,7 +352,7 @@ For 32-bit we have the following conventions - kernel is built with
>  .macro CALL_enter_from_user_mode
>  #ifdef CONFIG_CONTEXT_TRACKING
>  #ifdef HAVE_JUMP_LABEL
> -       STATIC_BRANCH_JMP l_yes=.Lafter_call_\@, key=context_tracking_enabled, branch=1
> +       STATIC_JUMP_IF_FALSE .Lafter_call_\@, context_tracking_enabled, def=0
>  #endif
>         call enter_from_user_mode
>  .Lafter_call_\@:
> diff --git a/arch/x86/include/asm/alternative-asm.h b/arch/x86/include/asm/alternative-asm.h
> index 8e4ea39..31b627b 100644
> --- a/arch/x86/include/asm/alternative-asm.h
> +++ b/arch/x86/include/asm/alternative-asm.h
> @@ -7,24 +7,16 @@
>  #include <asm/asm.h>
>
>  #ifdef CONFIG_SMP
> -.macro LOCK_PREFIX_HERE
> +       .macro LOCK_PREFIX
> +672:   lock
>         .pushsection .smp_locks,"a"
>         .balign 4
> -       .long 671f - .          # offset
> +       .long 672b - .
>         .popsection
> -671:
> -.endm
> -
> -.macro LOCK_PREFIX insn:vararg
> -       LOCK_PREFIX_HERE
> -       lock \insn
> -.endm
> +       .endm
>  #else
> -.macro LOCK_PREFIX_HERE
> -.endm
> -
> -.macro LOCK_PREFIX insn:vararg
> -.endm
> +       .macro LOCK_PREFIX
> +       .endm
>  #endif
>
>  /*
> diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h
> index d7faa16..4cd6a3b 100644
> --- a/arch/x86/include/asm/alternative.h
> +++ b/arch/x86/include/asm/alternative.h
> @@ -31,8 +31,15 @@
>   */
>
>  #ifdef CONFIG_SMP
> -#define LOCK_PREFIX_HERE "LOCK_PREFIX_HERE\n\t"
> -#define LOCK_PREFIX "LOCK_PREFIX "
> +#define LOCK_PREFIX_HERE \
> +               ".pushsection .smp_locks,\"a\"\n"       \
> +               ".balign 4\n"                           \
> +               ".long 671f - .\n" /* offset */         \
> +               ".popsection\n"                         \
> +               "671:"
> +
> +#define LOCK_PREFIX LOCK_PREFIX_HERE "\n\tlock; "
> +
>  #else /* ! CONFIG_SMP */
>  #define LOCK_PREFIX_HERE ""
>  #define LOCK_PREFIX ""
> diff --git a/arch/x86/include/asm/asm.h b/arch/x86/include/asm/asm.h
> index 21b0867..6467757b 100644
> --- a/arch/x86/include/asm/asm.h
> +++ b/arch/x86/include/asm/asm.h
> @@ -120,25 +120,12 @@
>  /* Exception table entry */
>  #ifdef __ASSEMBLY__
>  # define _ASM_EXTABLE_HANDLE(from, to, handler)                        \
> -       ASM_EXTABLE_HANDLE from to handler
> -
> -.macro ASM_EXTABLE_HANDLE from:req to:req handler:req
> -       .pushsection "__ex_table","a"
> -       .balign 4
> -       .long (\from) - .
> -       .long (\to) - .
> -       .long (\handler) - .
> +       .pushsection "__ex_table","a" ;                         \
> +       .balign 4 ;                                             \
> +       .long (from) - . ;                                      \
> +       .long (to) - . ;                                        \
> +       .long (handler) - . ;                                   \
>         .popsection
> -.endm
> -#else /* __ASSEMBLY__ */
> -
> -# define _ASM_EXTABLE_HANDLE(from, to, handler)                        \
> -       "ASM_EXTABLE_HANDLE from=" #from " to=" #to             \
> -       " handler=\"" #handler "\"\n\t"
> -
> -/* For C file, we already have NOKPROBE_SYMBOL macro */
> -
> -#endif /* __ASSEMBLY__ */
>
>  # define _ASM_EXTABLE(from, to)                                        \
>         _ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
> @@ -161,7 +148,6 @@
>         _ASM_PTR (entry);                                       \
>         .popsection
>
> -#ifdef __ASSEMBLY__
>  .macro ALIGN_DESTINATION
>         /* check for bad alignment of destination */
>         movl %edi,%ecx
> @@ -185,7 +171,34 @@
>         _ASM_EXTABLE_UA(100b, 103b)
>         _ASM_EXTABLE_UA(101b, 103b)
>         .endm
> -#endif /* __ASSEMBLY__ */
> +
> +#else
> +# define _EXPAND_EXTABLE_HANDLE(x) #x
> +# define _ASM_EXTABLE_HANDLE(from, to, handler)                        \
> +       " .pushsection \"__ex_table\",\"a\"\n"                  \
> +       " .balign 4\n"                                          \
> +       " .long (" #from ") - .\n"                              \
> +       " .long (" #to ") - .\n"                                \
> +       " .long (" _EXPAND_EXTABLE_HANDLE(handler) ") - .\n"    \
> +       " .popsection\n"
> +
> +# define _ASM_EXTABLE(from, to)                                        \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
> +
> +# define _ASM_EXTABLE_UA(from, to)                             \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_uaccess)
> +
> +# define _ASM_EXTABLE_FAULT(from, to)                          \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_fault)
> +
> +# define _ASM_EXTABLE_EX(from, to)                             \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_ext)
> +
> +# define _ASM_EXTABLE_REFCOUNT(from, to)                       \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_refcount)
> +
> +/* For C file, we already have NOKPROBE_SYMBOL macro */
> +#endif
>
>  #ifndef __ASSEMBLY__
>  /*
> diff --git a/arch/x86/include/asm/bug.h b/arch/x86/include/asm/bug.h
> index 5090035..6804d66 100644
> --- a/arch/x86/include/asm/bug.h
> +++ b/arch/x86/include/asm/bug.h
> @@ -4,8 +4,6 @@
>
>  #include <linux/stringify.h>
>
> -#ifndef __ASSEMBLY__
> -
>  /*
>   * Despite that some emulators terminate on UD2, we use it for WARN().
>   *
> @@ -22,15 +20,53 @@
>
>  #define LEN_UD2                2
>
> +#ifdef CONFIG_GENERIC_BUG
> +
> +#ifdef CONFIG_X86_32
> +# define __BUG_REL(val)        ".long " __stringify(val)
> +#else
> +# define __BUG_REL(val)        ".long " __stringify(val) " - 2b"
> +#endif
> +
> +#ifdef CONFIG_DEBUG_BUGVERBOSE
> +
> +#define _BUG_FLAGS(ins, flags)                                         \
> +do {                                                                   \
> +       asm volatile("1:\t" ins "\n"                                    \
> +                    ".pushsection __bug_table,\"aw\"\n"                \
> +                    "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"   \
> +                    "\t"  __BUG_REL(%c0) "\t# bug_entry::file\n"       \
> +                    "\t.word %c1"        "\t# bug_entry::line\n"       \
> +                    "\t.word %c2"        "\t# bug_entry::flags\n"      \
> +                    "\t.org 2b+%c3\n"                                  \
> +                    ".popsection"                                      \
> +                    : : "i" (__FILE__), "i" (__LINE__),                \
> +                        "i" (flags),                                   \
> +                        "i" (sizeof(struct bug_entry)));               \
> +} while (0)
> +
> +#else /* !CONFIG_DEBUG_BUGVERBOSE */
> +
>  #define _BUG_FLAGS(ins, flags)                                         \
>  do {                                                                   \
> -       asm volatile("ASM_BUG ins=\"" ins "\" file=%c0 line=%c1 "       \
> -                    "flags=%c2 size=%c3"                               \
> -                    : : "i" (__FILE__), "i" (__LINE__),                \
> -                        "i" (flags),                                   \
> +       asm volatile("1:\t" ins "\n"                                    \
> +                    ".pushsection __bug_table,\"aw\"\n"                \
> +                    "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"   \
> +                    "\t.word %c0"        "\t# bug_entry::flags\n"      \
> +                    "\t.org 2b+%c1\n"                                  \
> +                    ".popsection"                                      \
> +                    : : "i" (flags),                                   \
>                          "i" (sizeof(struct bug_entry)));               \
>  } while (0)
>
> +#endif /* CONFIG_DEBUG_BUGVERBOSE */
> +
> +#else
> +
> +#define _BUG_FLAGS(ins, flags)  asm volatile(ins)
> +
> +#endif /* CONFIG_GENERIC_BUG */
> +
>  #define HAVE_ARCH_BUG
>  #define BUG()                                                  \
>  do {                                                           \
> @@ -46,54 +82,4 @@ do {                                                         \
>
>  #include <asm-generic/bug.h>
>
> -#else /* __ASSEMBLY__ */
> -
> -#ifdef CONFIG_GENERIC_BUG
> -
> -#ifdef CONFIG_X86_32
> -.macro __BUG_REL val:req
> -       .long \val
> -.endm
> -#else
> -.macro __BUG_REL val:req
> -       .long \val - 2b
> -.endm
> -#endif
> -
> -#ifdef CONFIG_DEBUG_BUGVERBOSE
> -
> -.macro ASM_BUG ins:req file:req line:req flags:req size:req
> -1:     \ins
> -       .pushsection __bug_table,"aw"
> -2:     __BUG_REL val=1b        # bug_entry::bug_addr
> -       __BUG_REL val=\file     # bug_entry::file
> -       .word \line             # bug_entry::line
> -       .word \flags            # bug_entry::flags
> -       .org 2b+\size
> -       .popsection
> -.endm
> -
> -#else /* !CONFIG_DEBUG_BUGVERBOSE */
> -
> -.macro ASM_BUG ins:req file:req line:req flags:req size:req
> -1:     \ins
> -       .pushsection __bug_table,"aw"
> -2:     __BUG_REL val=1b        # bug_entry::bug_addr
> -       .word \flags            # bug_entry::flags
> -       .org 2b+\size
> -       .popsection
> -.endm
> -
> -#endif /* CONFIG_DEBUG_BUGVERBOSE */
> -
> -#else /* CONFIG_GENERIC_BUG */
> -
> -.macro ASM_BUG ins:req file:req line:req flags:req size:req
> -       \ins
> -.endm
> -
> -#endif /* CONFIG_GENERIC_BUG */
> -
> -#endif /* __ASSEMBLY__ */
> -
>  #endif /* _ASM_X86_BUG_H */
> diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h
> index 7d44272..aced6c9 100644
> --- a/arch/x86/include/asm/cpufeature.h
> +++ b/arch/x86/include/asm/cpufeature.h
> @@ -2,10 +2,10 @@
>  #ifndef _ASM_X86_CPUFEATURE_H
>  #define _ASM_X86_CPUFEATURE_H
>
> -#ifdef __KERNEL__
> -#ifndef __ASSEMBLY__
> -
>  #include <asm/processor.h>
> +
> +#if defined(__KERNEL__) && !defined(__ASSEMBLY__)
> +
>  #include <asm/asm.h>
>  #include <linux/bitops.h>
>
> @@ -161,10 +161,37 @@ extern void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int bit);
>   */
>  static __always_inline __pure bool _static_cpu_has(u16 bit)
>  {
> -       asm_volatile_goto("STATIC_CPU_HAS bitnum=%[bitnum] "
> -                         "cap_byte=\"%[cap_byte]\" "
> -                         "feature=%P[feature] t_yes=%l[t_yes] "
> -                         "t_no=%l[t_no] always=%P[always]"
> +       asm_volatile_goto("1: jmp 6f\n"
> +                "2:\n"
> +                ".skip -(((5f-4f) - (2b-1b)) > 0) * "
> +                        "((5f-4f) - (2b-1b)),0x90\n"
> +                "3:\n"
> +                ".section .altinstructions,\"a\"\n"
> +                " .long 1b - .\n"              /* src offset */
> +                " .long 4f - .\n"              /* repl offset */
> +                " .word %P[always]\n"          /* always replace */
> +                " .byte 3b - 1b\n"             /* src len */
> +                " .byte 5f - 4f\n"             /* repl len */
> +                " .byte 3b - 2b\n"             /* pad len */
> +                ".previous\n"
> +                ".section .altinstr_replacement,\"ax\"\n"
> +                "4: jmp %l[t_no]\n"
> +                "5:\n"
> +                ".previous\n"
> +                ".section .altinstructions,\"a\"\n"
> +                " .long 1b - .\n"              /* src offset */
> +                " .long 0\n"                   /* no replacement */
> +                " .word %P[feature]\n"         /* feature bit */
> +                " .byte 3b - 1b\n"             /* src len */
> +                " .byte 0\n"                   /* repl len */
> +                " .byte 0\n"                   /* pad len */
> +                ".previous\n"
> +                ".section .altinstr_aux,\"ax\"\n"
> +                "6:\n"
> +                " testb %[bitnum],%[cap_byte]\n"
> +                " jnz %l[t_yes]\n"
> +                " jmp %l[t_no]\n"
> +                ".previous\n"
>                  : : [feature]  "i" (bit),
>                      [always]   "i" (X86_FEATURE_ALWAYS),
>                      [bitnum]   "i" (1 << (bit & 7)),
> @@ -199,44 +226,5 @@ static __always_inline __pure bool _static_cpu_has(u16 bit)
>  #define CPU_FEATURE_TYPEVAL            boot_cpu_data.x86_vendor, boot_cpu_data.x86, \
>                                         boot_cpu_data.x86_model
>
> -#else /* __ASSEMBLY__ */
> -
> -.macro STATIC_CPU_HAS bitnum:req cap_byte:req feature:req t_yes:req t_no:req always:req
> -1:
> -       jmp 6f
> -2:
> -       .skip -(((5f-4f) - (2b-1b)) > 0) * ((5f-4f) - (2b-1b)),0x90
> -3:
> -       .section .altinstructions,"a"
> -       .long 1b - .            /* src offset */
> -       .long 4f - .            /* repl offset */
> -       .word \always           /* always replace */
> -       .byte 3b - 1b           /* src len */
> -       .byte 5f - 4f           /* repl len */
> -       .byte 3b - 2b           /* pad len */
> -       .previous
> -       .section .altinstr_replacement,"ax"
> -4:
> -       jmp \t_no
> -5:
> -       .previous
> -       .section .altinstructions,"a"
> -       .long 1b - .            /* src offset */
> -       .long 0                 /* no replacement */
> -       .word \feature          /* feature bit */
> -       .byte 3b - 1b           /* src len */
> -       .byte 0                 /* repl len */
> -       .byte 0                 /* pad len */
> -       .previous
> -       .section .altinstr_aux,"ax"
> -6:
> -       testb \bitnum,\cap_byte
> -       jnz \t_yes
> -       jmp \t_no
> -       .previous
> -.endm
> -
> -#endif /* __ASSEMBLY__ */
> -
> -#endif /* __KERNEL__ */
> +#endif /* defined(__KERNEL__) && !defined(__ASSEMBLY__) */
>  #endif /* _ASM_X86_CPUFEATURE_H */
> diff --git a/arch/x86/include/asm/jump_label.h b/arch/x86/include/asm/jump_label.h
> index a5fb34f..21efc9d 100644
> --- a/arch/x86/include/asm/jump_label.h
> +++ b/arch/x86/include/asm/jump_label.h
> @@ -2,6 +2,19 @@
>  #ifndef _ASM_X86_JUMP_LABEL_H
>  #define _ASM_X86_JUMP_LABEL_H
>
> +#ifndef HAVE_JUMP_LABEL
> +/*
> + * For better or for worse, if jump labels (the gcc extension) are missing,
> + * then the entire static branch patching infrastructure is compiled out.
> + * If that happens, the code in here will malfunction.  Raise a compiler
> + * error instead.
> + *
> + * In theory, jump labels and the static branch patching infrastructure
> + * could be decoupled to fix this.
> + */
> +#error asm/jump_label.h included on a non-jump-label kernel
> +#endif
> +
>  #define JUMP_LABEL_NOP_SIZE 5
>
>  #ifdef CONFIG_X86_64
> @@ -20,9 +33,15 @@
>
>  static __always_inline bool arch_static_branch(struct static_key *key, bool branch)
>  {
> -       asm_volatile_goto("STATIC_BRANCH_NOP l_yes=\"%l[l_yes]\" key=\"%c0\" "
> -                         "branch=\"%c1\""
> -                       : :  "i" (key), "i" (branch) : : l_yes);
> +       asm_volatile_goto("1:"
> +               ".byte " __stringify(STATIC_KEY_INIT_NOP) "\n\t"
> +               ".pushsection __jump_table,  \"aw\" \n\t"
> +               _ASM_ALIGN "\n\t"
> +               ".long 1b - ., %l[l_yes] - . \n\t"
> +               _ASM_PTR "%c0 + %c1 - .\n\t"
> +               ".popsection \n\t"
> +               : :  "i" (key), "i" (branch) : : l_yes);
> +
>         return false;
>  l_yes:
>         return true;
> @@ -30,8 +49,14 @@ static __always_inline bool arch_static_branch(struct static_key *key, bool bran
>
>  static __always_inline bool arch_static_branch_jump(struct static_key *key, bool branch)
>  {
> -       asm_volatile_goto("STATIC_BRANCH_JMP l_yes=\"%l[l_yes]\" key=\"%c0\" "
> -                         "branch=\"%c1\""
> +       asm_volatile_goto("1:"
> +               ".byte 0xe9\n\t .long %l[l_yes] - 2f\n\t"
> +               "2:\n\t"
> +               ".pushsection __jump_table,  \"aw\" \n\t"
> +               _ASM_ALIGN "\n\t"
> +               ".long 1b - ., %l[l_yes] - . \n\t"
> +               _ASM_PTR "%c0 + %c1 - .\n\t"
> +               ".popsection \n\t"
>                 : :  "i" (key), "i" (branch) : : l_yes);
>
>         return false;
> @@ -41,26 +66,37 @@ static __always_inline bool arch_static_branch_jump(struct static_key *key, bool
>
>  #else  /* __ASSEMBLY__ */
>
> -.macro STATIC_BRANCH_NOP l_yes:req key:req branch:req
> -.Lstatic_branch_nop_\@:
> -       .byte STATIC_KEY_INIT_NOP
> -.Lstatic_branch_no_after_\@:
> +.macro STATIC_JUMP_IF_TRUE target, key, def
> +.Lstatic_jump_\@:
> +       .if \def
> +       /* Equivalent to "jmp.d32 \target" */
> +       .byte           0xe9
> +       .long           \target - .Lstatic_jump_after_\@
> +.Lstatic_jump_after_\@:
> +       .else
> +       .byte           STATIC_KEY_INIT_NOP
> +       .endif
>         .pushsection __jump_table, "aw"
>         _ASM_ALIGN
> -       .long           .Lstatic_branch_nop_\@ - ., \l_yes - .
> -       _ASM_PTR        \key + \branch - .
> +       .long           .Lstatic_jump_\@ - ., \target - .
> +       _ASM_PTR        \key - .
>         .popsection
>  .endm
>
> -.macro STATIC_BRANCH_JMP l_yes:req key:req branch:req
> -.Lstatic_branch_jmp_\@:
> -       .byte 0xe9
> -       .long \l_yes - .Lstatic_branch_jmp_after_\@
> -.Lstatic_branch_jmp_after_\@:
> +.macro STATIC_JUMP_IF_FALSE target, key, def
> +.Lstatic_jump_\@:
> +       .if \def
> +       .byte           STATIC_KEY_INIT_NOP
> +       .else
> +       /* Equivalent to "jmp.d32 \target" */
> +       .byte           0xe9
> +       .long           \target - .Lstatic_jump_after_\@
> +.Lstatic_jump_after_\@:
> +       .endif
>         .pushsection __jump_table, "aw"
>         _ASM_ALIGN
> -       .long           .Lstatic_branch_jmp_\@ - ., \l_yes - .
> -       _ASM_PTR        \key + \branch - .
> +       .long           .Lstatic_jump_\@ - ., \target - .
> +       _ASM_PTR        \key + 1 - .
>         .popsection
>  .endm
>
> diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h
> index 26942ad..488c596 100644
> --- a/arch/x86/include/asm/paravirt_types.h
> +++ b/arch/x86/include/asm/paravirt_types.h
> @@ -348,11 +348,23 @@ extern struct paravirt_patch_template pv_ops;
>  #define paravirt_clobber(clobber)              \
>         [paravirt_clobber] "i" (clobber)
>
> +/*
> + * Generate some code, and mark it as patchable by the
> + * apply_paravirt() alternate instruction patcher.
> + */
> +#define _paravirt_alt(insn_string, type, clobber)      \
> +       "771:\n\t" insn_string "\n" "772:\n"            \
> +       ".pushsection .parainstructions,\"a\"\n"        \
> +       _ASM_ALIGN "\n"                                 \
> +       _ASM_PTR " 771b\n"                              \
> +       "  .byte " type "\n"                            \
> +       "  .byte 772b-771b\n"                           \
> +       "  .short " clobber "\n"                        \
> +       ".popsection\n"
> +
>  /* Generate patchable code, with the default asm parameters. */
> -#define paravirt_call                                                  \
> -       "PARAVIRT_CALL type=\"%c[paravirt_typenum]\""                   \
> -       " clobber=\"%c[paravirt_clobber]\""                             \
> -       " pv_opptr=\"%c[paravirt_opptr]\";"
> +#define paravirt_alt(insn_string)                                      \
> +       _paravirt_alt(insn_string, "%c[paravirt_typenum]", "%c[paravirt_clobber]")
>
>  /* Simple instruction patching code. */
>  #define NATIVE_LABEL(a,x,b) "\n\t.globl " a #x "_" #b "\n" a #x "_" #b ":\n\t"
> @@ -373,6 +385,16 @@ unsigned native_patch(u8 type, void *ibuf, unsigned long addr, unsigned len);
>  int paravirt_disable_iospace(void);
>
>  /*
> + * This generates an indirect call based on the operation type number.
> + * The type number, computed in PARAVIRT_PATCH, is derived from the
> + * offset into the paravirt_patch_template structure, and can therefore be
> + * freely converted back into a structure offset.
> + */
> +#define PARAVIRT_CALL                                  \
> +       ANNOTATE_RETPOLINE_SAFE                         \
> +       "call *%c[paravirt_opptr];"
> +
> +/*
>   * These macros are intended to wrap calls through one of the paravirt
>   * ops structs, so that they can be later identified and patched at
>   * runtime.
> @@ -509,7 +531,7 @@ int paravirt_disable_iospace(void);
>                 /* since this condition will never hold */              \
>                 if (sizeof(rettype) > sizeof(unsigned long)) {          \
>                         asm volatile(pre                                \
> -                                    paravirt_call                      \
> +                                    paravirt_alt(PARAVIRT_CALL)        \
>                                      post                               \
>                                      : call_clbr, ASM_CALL_CONSTRAINT   \
>                                      : paravirt_type(op),               \
> @@ -519,7 +541,7 @@ int paravirt_disable_iospace(void);
>                         __ret = (rettype)((((u64)__edx) << 32) | __eax); \
>                 } else {                                                \
>                         asm volatile(pre                                \
> -                                    paravirt_call                      \
> +                                    paravirt_alt(PARAVIRT_CALL)        \
>                                      post                               \
>                                      : call_clbr, ASM_CALL_CONSTRAINT   \
>                                      : paravirt_type(op),               \
> @@ -546,7 +568,7 @@ int paravirt_disable_iospace(void);
>                 PVOP_VCALL_ARGS;                                        \
>                 PVOP_TEST_NULL(op);                                     \
>                 asm volatile(pre                                        \
> -                            paravirt_call                              \
> +                            paravirt_alt(PARAVIRT_CALL)                \
>                              post                                       \
>                              : call_clbr, ASM_CALL_CONSTRAINT           \
>                              : paravirt_type(op),                       \
> @@ -664,26 +686,6 @@ struct paravirt_patch_site {
>  extern struct paravirt_patch_site __parainstructions[],
>         __parainstructions_end[];
>
> -#else  /* __ASSEMBLY__ */
> -
> -/*
> - * This generates an indirect call based on the operation type number.
> - * The type number, computed in PARAVIRT_PATCH, is derived from the
> - * offset into the paravirt_patch_template structure, and can therefore be
> - * freely converted back into a structure offset.
> - */
> -.macro PARAVIRT_CALL type:req clobber:req pv_opptr:req
> -771:   ANNOTATE_RETPOLINE_SAFE
> -       call *\pv_opptr
> -772:   .pushsection .parainstructions,"a"
> -       _ASM_ALIGN
> -       _ASM_PTR 771b
> -       .byte \type
> -       .byte 772b-771b
> -       .short \clobber
> -       .popsection
> -.endm
> -
>  #endif /* __ASSEMBLY__ */
>
>  #endif /* _ASM_X86_PARAVIRT_TYPES_H */
> diff --git a/arch/x86/include/asm/refcount.h b/arch/x86/include/asm/refcount.h
> index a8b5e1e..dbaed55 100644
> --- a/arch/x86/include/asm/refcount.h
> +++ b/arch/x86/include/asm/refcount.h
> @@ -4,41 +4,6 @@
>   * x86-specific implementation of refcount_t. Based on PAX_REFCOUNT from
>   * PaX/grsecurity.
>   */
> -
> -#ifdef __ASSEMBLY__
> -
> -#include <asm/asm.h>
> -#include <asm/bug.h>
> -
> -.macro REFCOUNT_EXCEPTION counter:req
> -       .pushsection .text..refcount
> -111:   lea \counter, %_ASM_CX
> -112:   ud2
> -       ASM_UNREACHABLE
> -       .popsection
> -113:   _ASM_EXTABLE_REFCOUNT(112b, 113b)
> -.endm
> -
> -/* Trigger refcount exception if refcount result is negative. */
> -.macro REFCOUNT_CHECK_LT_ZERO counter:req
> -       js 111f
> -       REFCOUNT_EXCEPTION counter="\counter"
> -.endm
> -
> -/* Trigger refcount exception if refcount result is zero or negative. */
> -.macro REFCOUNT_CHECK_LE_ZERO counter:req
> -       jz 111f
> -       REFCOUNT_CHECK_LT_ZERO counter="\counter"
> -.endm
> -
> -/* Trigger refcount exception unconditionally. */
> -.macro REFCOUNT_ERROR counter:req
> -       jmp 111f
> -       REFCOUNT_EXCEPTION counter="\counter"
> -.endm
> -
> -#else /* __ASSEMBLY__ */
> -
>  #include <linux/refcount.h>
>  #include <asm/bug.h>
>
> @@ -50,12 +15,35 @@
>   * central refcount exception. The fixup address for the exception points
>   * back to the regular execution flow in .text.
>   */
> +#define _REFCOUNT_EXCEPTION                            \
> +       ".pushsection .text..refcount\n"                \
> +       "111:\tlea %[var], %%" _ASM_CX "\n"             \
> +       "112:\t" ASM_UD2 "\n"                           \
> +       ASM_UNREACHABLE                                 \
> +       ".popsection\n"                                 \
> +       "113:\n"                                        \
> +       _ASM_EXTABLE_REFCOUNT(112b, 113b)
> +
> +/* Trigger refcount exception if refcount result is negative. */
> +#define REFCOUNT_CHECK_LT_ZERO                         \
> +       "js 111f\n\t"                                   \
> +       _REFCOUNT_EXCEPTION
> +
> +/* Trigger refcount exception if refcount result is zero or negative. */
> +#define REFCOUNT_CHECK_LE_ZERO                         \
> +       "jz 111f\n\t"                                   \
> +       REFCOUNT_CHECK_LT_ZERO
> +
> +/* Trigger refcount exception unconditionally. */
> +#define REFCOUNT_ERROR                                 \
> +       "jmp 111f\n\t"                                  \
> +       _REFCOUNT_EXCEPTION
>
>  static __always_inline void refcount_add(unsigned int i, refcount_t *r)
>  {
>         asm volatile(LOCK_PREFIX "addl %1,%0\n\t"
> -               "REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
> -               : [counter] "+m" (r->refs.counter)
> +               REFCOUNT_CHECK_LT_ZERO
> +               : [var] "+m" (r->refs.counter)
>                 : "ir" (i)
>                 : "cc", "cx");
>  }
> @@ -63,32 +51,31 @@ static __always_inline void refcount_add(unsigned int i, refcount_t *r)
>  static __always_inline void refcount_inc(refcount_t *r)
>  {
>         asm volatile(LOCK_PREFIX "incl %0\n\t"
> -               "REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
> -               : [counter] "+m" (r->refs.counter)
> +               REFCOUNT_CHECK_LT_ZERO
> +               : [var] "+m" (r->refs.counter)
>                 : : "cc", "cx");
>  }
>
>  static __always_inline void refcount_dec(refcount_t *r)
>  {
>         asm volatile(LOCK_PREFIX "decl %0\n\t"
> -               "REFCOUNT_CHECK_LE_ZERO counter=\"%[counter]\""
> -               : [counter] "+m" (r->refs.counter)
> +               REFCOUNT_CHECK_LE_ZERO
> +               : [var] "+m" (r->refs.counter)
>                 : : "cc", "cx");
>  }
>
>  static __always_inline __must_check
>  bool refcount_sub_and_test(unsigned int i, refcount_t *r)
>  {
> -
>         return GEN_BINARY_SUFFIXED_RMWcc(LOCK_PREFIX "subl",
> -                                        "REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
> +                                        REFCOUNT_CHECK_LT_ZERO,
>                                          r->refs.counter, e, "er", i, "cx");
>  }
>
>  static __always_inline __must_check bool refcount_dec_and_test(refcount_t *r)
>  {
>         return GEN_UNARY_SUFFIXED_RMWcc(LOCK_PREFIX "decl",
> -                                       "REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
> +                                       REFCOUNT_CHECK_LT_ZERO,
>                                         r->refs.counter, e, "cx");
>  }
>
> @@ -106,8 +93,8 @@ bool refcount_add_not_zero(unsigned int i, refcount_t *r)
>
>                 /* Did we try to increment from/to an undesirable state? */
>                 if (unlikely(c < 0 || c == INT_MAX || result < c)) {
> -                       asm volatile("REFCOUNT_ERROR counter=\"%[counter]\""
> -                                    : : [counter] "m" (r->refs.counter)
> +                       asm volatile(REFCOUNT_ERROR
> +                                    : : [var] "m" (r->refs.counter)
>                                      : "cc", "cx");
>                         break;
>                 }
> @@ -122,6 +109,4 @@ static __always_inline __must_check bool refcount_inc_not_zero(refcount_t *r)
>         return refcount_add_not_zero(1, r);
>  }
>
> -#endif /* __ASSEMBLY__ */
> -
>  #endif
> diff --git a/arch/x86/kernel/macros.S b/arch/x86/kernel/macros.S
> deleted file mode 100644
> index 161c950..0000000
> --- a/arch/x86/kernel/macros.S
> +++ /dev/null
> @@ -1,16 +0,0 @@
> -/* SPDX-License-Identifier: GPL-2.0 */
> -
> -/*
> - * This file includes headers whose assembly part includes macros which are
> - * commonly used. The macros are precompiled into assmebly file which is later
> - * assembled together with each compiled file.
> - */
> -
> -#include <linux/compiler.h>
> -#include <asm/refcount.h>
> -#include <asm/alternative-asm.h>
> -#include <asm/bug.h>
> -#include <asm/paravirt.h>
> -#include <asm/asm.h>
> -#include <asm/cpufeature.h>
> -#include <asm/jump_label.h>
> diff --git a/include/asm-generic/bug.h b/include/asm-generic/bug.h
> index cdafa5e..20561a6 100644
> --- a/include/asm-generic/bug.h
> +++ b/include/asm-generic/bug.h
> @@ -17,8 +17,10 @@
>  #ifndef __ASSEMBLY__
>  #include <linux/kernel.h>
>
> -struct bug_entry {
> +#ifdef CONFIG_BUG
> +
>  #ifdef CONFIG_GENERIC_BUG
> +struct bug_entry {
>  #ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS
>         unsigned long   bug_addr;
>  #else
> @@ -33,10 +35,8 @@ struct bug_entry {
>         unsigned short  line;
>  #endif
>         unsigned short  flags;
> -#endif /* CONFIG_GENERIC_BUG */
>  };
> -
> -#ifdef CONFIG_BUG
> +#endif /* CONFIG_GENERIC_BUG */
>
>  /*
>   * Don't use BUG() or BUG_ON() unless there's really no way out; one
> diff --git a/include/linux/compiler.h b/include/linux/compiler.h
> index 06396c1..fc5004a 100644
> --- a/include/linux/compiler.h
> +++ b/include/linux/compiler.h
> @@ -99,13 +99,22 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
>   * unique, to convince GCC not to merge duplicate inline asm statements.
>   */
>  #define annotate_reachable() ({                                                \
> -       asm volatile("ANNOTATE_REACHABLE counter=%c0"                   \
> -                    : : "i" (__COUNTER__));                            \
> +       asm volatile("%c0:\n\t"                                         \
> +                    ".pushsection .discard.reachable\n\t"              \
> +                    ".long %c0b - .\n\t"                               \
> +                    ".popsection\n\t" : : "i" (__COUNTER__));          \
>  })
>  #define annotate_unreachable() ({                                      \
> -       asm volatile("ANNOTATE_UNREACHABLE counter=%c0"                 \
> -                    : : "i" (__COUNTER__));                            \
> +       asm volatile("%c0:\n\t"                                         \
> +                    ".pushsection .discard.unreachable\n\t"            \
> +                    ".long %c0b - .\n\t"                               \
> +                    ".popsection\n\t" : : "i" (__COUNTER__));          \
>  })
> +#define ASM_UNREACHABLE                                                        \
> +       "999:\n\t"                                                      \
> +       ".pushsection .discard.unreachable\n\t"                         \
> +       ".long 999b - .\n\t"                                            \
> +       ".popsection\n\t"
>  #else
>  #define annotate_reachable()
>  #define annotate_unreachable()
> @@ -293,45 +302,6 @@ static inline void *offset_to_ptr(const int *off)
>         return (void *)((unsigned long)off + *off);
>  }
>
> -#else /* __ASSEMBLY__ */
> -
> -#ifdef __KERNEL__
> -#ifndef LINKER_SCRIPT
> -
> -#ifdef CONFIG_STACK_VALIDATION
> -.macro ANNOTATE_UNREACHABLE counter:req
> -\counter:
> -       .pushsection .discard.unreachable
> -       .long \counter\()b -.
> -       .popsection
> -.endm
> -
> -.macro ANNOTATE_REACHABLE counter:req
> -\counter:
> -       .pushsection .discard.reachable
> -       .long \counter\()b -.
> -       .popsection
> -.endm
> -
> -.macro ASM_UNREACHABLE
> -999:
> -       .pushsection .discard.unreachable
> -       .long 999b - .
> -       .popsection
> -.endm
> -#else /* CONFIG_STACK_VALIDATION */
> -.macro ANNOTATE_UNREACHABLE counter:req
> -.endm
> -
> -.macro ANNOTATE_REACHABLE counter:req
> -.endm
> -
> -.macro ASM_UNREACHABLE
> -.endm
> -#endif /* CONFIG_STACK_VALIDATION */
> -
> -#endif /* LINKER_SCRIPT */
> -#endif /* __KERNEL__ */
>  #endif /* __ASSEMBLY__ */
>
>  /* Compile time object size, -1 for unknown */
> diff --git a/scripts/Kbuild.include b/scripts/Kbuild.include
> index bb01555..3d09844 100644
> --- a/scripts/Kbuild.include
> +++ b/scripts/Kbuild.include
> @@ -115,9 +115,7 @@ __cc-option = $(call try-run,\
>
>  # Do not attempt to build with gcc plugins during cc-option tests.
>  # (And this uses delayed resolution so the flags will be up to date.)
> -# In addition, do not include the asm macros which are built later.
> -CC_OPTION_FILTERED = $(GCC_PLUGINS_CFLAGS) $(ASM_MACRO_FLAGS)
> -CC_OPTION_CFLAGS = $(filter-out $(CC_OPTION_FILTERED),$(KBUILD_CFLAGS))
> +CC_OPTION_CFLAGS = $(filter-out $(GCC_PLUGINS_CFLAGS),$(KBUILD_CFLAGS))
>
>  # cc-option
>  # Usage: cflags-y += $(call cc-option,-march=winchip-c6,-march=i586)
> diff --git a/scripts/mod/Makefile b/scripts/mod/Makefile
> index a5b4af4..42c5d50 100644
> --- a/scripts/mod/Makefile
> +++ b/scripts/mod/Makefile
> @@ -4,8 +4,6 @@ OBJECT_FILES_NON_STANDARD := y
>  hostprogs-y    := modpost mk_elfconfig
>  always         := $(hostprogs-y) empty.o
>
> -CFLAGS_REMOVE_empty.o := $(ASM_MACRO_FLAGS)
> -
>  modpost-objs   := modpost.o file2alias.o sumversion.o
>
>  devicetable-offsets-file := devicetable-offsets.h
> --
> 2.7.4
>
================================================================================

From: Sedat Dilek <sedat.dilek () gmail ! com>
To: linux-arch
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Mon, 17 Dec 2018 09:16:32 +0000
Message-ID: <CA+icZUUnkGBp-3pJxE_s_Oc+pdNBrVc_ngYUwpv7zLEbtE_eVg () mail ! gmail ! com>
--------------------
On Thu, Dec 13, 2018 at 10:19 AM Masahiro Yamada
<yamada.masahiro@socionext.com> wrote:
>
> Revert the following commits:
>
> - 5bdcd510c2ac9efaf55c4cbd8d46421d8e2320cd
>   ("x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs")
>
> - d5a581d84ae6b8a4a740464b80d8d9cf1e7947b2
>   ("x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs")
>
> - 0474d5d9d2f7f3b11262f7bf87d0e7314ead9200.
>   ("x86/extable: Macrofy inline assembly code to work around GCC inlining bugs")
>
> - 494b5168f2de009eb80f198f668da374295098dd.
>   ("x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops")
>
> - f81f8ad56fd1c7b99b2ed1c314527f7d9ac447c6.
>   ("x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs")
>
> - 77f48ec28e4ccff94d2e5f4260a83ac27a7f3099.
>   ("x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs")
>
> - 9e1725b410594911cc5981b6c7b4cea4ec054ca8.
>   ("x86/refcount: Work around GCC inlining bug")
>   (Conflicts: arch/x86/include/asm/refcount.h)
>
> - c06c4d8090513f2974dfdbed2ac98634357ac475.
>   ("x86/objtool: Use asm macros to work around GCC inlining bugs")
>
> - 77b0bf55bc675233d22cd5df97605d516d64525e.
>   ("kbuild/Makefile: Prepare for using macros in inline assembly code to work around asm() related GCC inlining bugs")
>
> A few days after those commits applied, discussion started to solve
> the issue more elegantly on the compiler side:
>
>   https://lkml.org/lkml/2018/10/7/92
>
> The "asm inline" was implemented by Segher Boessenkool, and now queued
> up for GCC 9. (People were positive even for back-porting it to older
> compilers).
>
> Since the in-kernel workarounds merged, some issues have been reported:
> breakage of building with distcc/icecc, breakage of distro packages for
> module building. (More fundamentally, we cannot build external modules
> after 'make clean')
>
> Patching around the build system would make the code even uglier.
>
> Given that this issue will be solved in a cleaner way sooner or later,
> let's revert the in-kernel workarounds, and wait for GCC 9.
>
> Reported-by: Logan Gunthorpe <logang@deltatee.com> # distcc
> Reported-by: Sedat Dilek <sedat.dilek@gmail.com> # debian/rpm package

Hi,

I reported the issue with debian package breakage in [1].

I am not subscribed to any involved mailing-list and not following all
the discussions.
I see the situation is not easy as there is especially linux-kbuild
and linux/x86 involved and maybe other interests.
But I am interested in having a fix in v4.20 final and hope this all
still works with LLVM/Clang.

I can offer my help in testing - against Linux v4.20-rc7.
Not sure if all discussed material is in upstream or elsewhere.
What is your suggestion for me as a tester?

Will we have a solution in Linux v4.20 final?

Thanks.

With my best wishes,
- Sedat -

[1] https://marc.info/?t=154212770600037&r=1&w=2

> Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
> Cc: Nadav Amit <namit@vmware.com>
> Cc: Segher Boessenkool <segher@kernel.crashing.org>
> ---
>
> Please consider this for v4.20 release.
> Currently, distro package build is broken.
>
>
>  Makefile                               |  9 +---
>  arch/x86/Makefile                      |  7 ---
>  arch/x86/entry/calling.h               |  2 +-
>  arch/x86/include/asm/alternative-asm.h | 20 +++----
>  arch/x86/include/asm/alternative.h     | 11 +++-
>  arch/x86/include/asm/asm.h             | 53 +++++++++++-------
>  arch/x86/include/asm/bug.h             | 98 +++++++++++++++-------------------
>  arch/x86/include/asm/cpufeature.h      | 82 ++++++++++++----------------
>  arch/x86/include/asm/jump_label.h      | 72 ++++++++++++++++++-------
>  arch/x86/include/asm/paravirt_types.h  | 56 +++++++++----------
>  arch/x86/include/asm/refcount.h        | 81 ++++++++++++----------------
>  arch/x86/kernel/macros.S               | 16 ------
>  include/asm-generic/bug.h              |  8 +--
>  include/linux/compiler.h               | 56 +++++--------------
>  scripts/Kbuild.include                 |  4 +-
>  scripts/mod/Makefile                   |  2 -
>  16 files changed, 262 insertions(+), 315 deletions(-)
>  delete mode 100644 arch/x86/kernel/macros.S
>
> diff --git a/Makefile b/Makefile
> index f2c3423..4cf4c5b 100644
> --- a/Makefile
> +++ b/Makefile
> @@ -1081,7 +1081,7 @@ scripts: scripts_basic scripts_dtc asm-generic gcc-plugins $(autoksyms_h)
>  # version.h and scripts_basic is processed / created.
>
>  # Listed in dependency order
> -PHONY += prepare archprepare macroprepare prepare0 prepare1 prepare2 prepare3
> +PHONY += prepare archprepare prepare0 prepare1 prepare2 prepare3
>
>  # prepare3 is used to check if we are building in a separate output directory,
>  # and if so do:
> @@ -1104,9 +1104,7 @@ prepare2: prepare3 outputmakefile asm-generic
>  prepare1: prepare2 $(version_h) $(autoksyms_h) include/generated/utsrelease.h
>         $(cmd_crmodverdir)
>
> -macroprepare: prepare1 archmacros
> -
> -archprepare: archheaders archscripts macroprepare scripts_basic
> +archprepare: archheaders archscripts prepare1 scripts_basic
>
>  prepare0: archprepare gcc-plugins
>         $(Q)$(MAKE) $(build)=.
> @@ -1174,9 +1172,6 @@ archheaders:
>  PHONY += archscripts
>  archscripts:
>
> -PHONY += archmacros
> -archmacros:
> -
>  PHONY += __headers
>  __headers: $(version_h) scripts_basic uapi-asm-generic archheaders archscripts
>         $(Q)$(MAKE) $(build)=scripts build_unifdef
> diff --git a/arch/x86/Makefile b/arch/x86/Makefile
> index 75ef499..85a66c4 100644
> --- a/arch/x86/Makefile
> +++ b/arch/x86/Makefile
> @@ -232,13 +232,6 @@ archscripts: scripts_basic
>  archheaders:
>         $(Q)$(MAKE) $(build)=arch/x86/entry/syscalls all
>
> -archmacros:
> -       $(Q)$(MAKE) $(build)=arch/x86/kernel arch/x86/kernel/macros.s
> -
> -ASM_MACRO_FLAGS = -Wa,arch/x86/kernel/macros.s
> -export ASM_MACRO_FLAGS
> -KBUILD_CFLAGS += $(ASM_MACRO_FLAGS)
> -
>  ###
>  # Kernel objects
>
> diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h
> index 25e5a6b..20d0885 100644
> --- a/arch/x86/entry/calling.h
> +++ b/arch/x86/entry/calling.h
> @@ -352,7 +352,7 @@ For 32-bit we have the following conventions - kernel is built with
>  .macro CALL_enter_from_user_mode
>  #ifdef CONFIG_CONTEXT_TRACKING
>  #ifdef HAVE_JUMP_LABEL
> -       STATIC_BRANCH_JMP l_yes=.Lafter_call_\@, key=context_tracking_enabled, branch=1
> +       STATIC_JUMP_IF_FALSE .Lafter_call_\@, context_tracking_enabled, def=0
>  #endif
>         call enter_from_user_mode
>  .Lafter_call_\@:
> diff --git a/arch/x86/include/asm/alternative-asm.h b/arch/x86/include/asm/alternative-asm.h
> index 8e4ea39..31b627b 100644
> --- a/arch/x86/include/asm/alternative-asm.h
> +++ b/arch/x86/include/asm/alternative-asm.h
> @@ -7,24 +7,16 @@
>  #include <asm/asm.h>
>
>  #ifdef CONFIG_SMP
> -.macro LOCK_PREFIX_HERE
> +       .macro LOCK_PREFIX
> +672:   lock
>         .pushsection .smp_locks,"a"
>         .balign 4
> -       .long 671f - .          # offset
> +       .long 672b - .
>         .popsection
> -671:
> -.endm
> -
> -.macro LOCK_PREFIX insn:vararg
> -       LOCK_PREFIX_HERE
> -       lock \insn
> -.endm
> +       .endm
>  #else
> -.macro LOCK_PREFIX_HERE
> -.endm
> -
> -.macro LOCK_PREFIX insn:vararg
> -.endm
> +       .macro LOCK_PREFIX
> +       .endm
>  #endif
>
>  /*
> diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h
> index d7faa16..4cd6a3b 100644
> --- a/arch/x86/include/asm/alternative.h
> +++ b/arch/x86/include/asm/alternative.h
> @@ -31,8 +31,15 @@
>   */
>
>  #ifdef CONFIG_SMP
> -#define LOCK_PREFIX_HERE "LOCK_PREFIX_HERE\n\t"
> -#define LOCK_PREFIX "LOCK_PREFIX "
> +#define LOCK_PREFIX_HERE \
> +               ".pushsection .smp_locks,\"a\"\n"       \
> +               ".balign 4\n"                           \
> +               ".long 671f - .\n" /* offset */         \
> +               ".popsection\n"                         \
> +               "671:"
> +
> +#define LOCK_PREFIX LOCK_PREFIX_HERE "\n\tlock; "
> +
>  #else /* ! CONFIG_SMP */
>  #define LOCK_PREFIX_HERE ""
>  #define LOCK_PREFIX ""
> diff --git a/arch/x86/include/asm/asm.h b/arch/x86/include/asm/asm.h
> index 21b0867..6467757b 100644
> --- a/arch/x86/include/asm/asm.h
> +++ b/arch/x86/include/asm/asm.h
> @@ -120,25 +120,12 @@
>  /* Exception table entry */
>  #ifdef __ASSEMBLY__
>  # define _ASM_EXTABLE_HANDLE(from, to, handler)                        \
> -       ASM_EXTABLE_HANDLE from to handler
> -
> -.macro ASM_EXTABLE_HANDLE from:req to:req handler:req
> -       .pushsection "__ex_table","a"
> -       .balign 4
> -       .long (\from) - .
> -       .long (\to) - .
> -       .long (\handler) - .
> +       .pushsection "__ex_table","a" ;                         \
> +       .balign 4 ;                                             \
> +       .long (from) - . ;                                      \
> +       .long (to) - . ;                                        \
> +       .long (handler) - . ;                                   \
>         .popsection
> -.endm
> -#else /* __ASSEMBLY__ */
> -
> -# define _ASM_EXTABLE_HANDLE(from, to, handler)                        \
> -       "ASM_EXTABLE_HANDLE from=" #from " to=" #to             \
> -       " handler=\"" #handler "\"\n\t"
> -
> -/* For C file, we already have NOKPROBE_SYMBOL macro */
> -
> -#endif /* __ASSEMBLY__ */
>
>  # define _ASM_EXTABLE(from, to)                                        \
>         _ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
> @@ -161,7 +148,6 @@
>         _ASM_PTR (entry);                                       \
>         .popsection
>
> -#ifdef __ASSEMBLY__
>  .macro ALIGN_DESTINATION
>         /* check for bad alignment of destination */
>         movl %edi,%ecx
> @@ -185,7 +171,34 @@
>         _ASM_EXTABLE_UA(100b, 103b)
>         _ASM_EXTABLE_UA(101b, 103b)
>         .endm
> -#endif /* __ASSEMBLY__ */
> +
> +#else
> +# define _EXPAND_EXTABLE_HANDLE(x) #x
> +# define _ASM_EXTABLE_HANDLE(from, to, handler)                        \
> +       " .pushsection \"__ex_table\",\"a\"\n"                  \
> +       " .balign 4\n"                                          \
> +       " .long (" #from ") - .\n"                              \
> +       " .long (" #to ") - .\n"                                \
> +       " .long (" _EXPAND_EXTABLE_HANDLE(handler) ") - .\n"    \
> +       " .popsection\n"
> +
> +# define _ASM_EXTABLE(from, to)                                        \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_default)
> +
> +# define _ASM_EXTABLE_UA(from, to)                             \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_uaccess)
> +
> +# define _ASM_EXTABLE_FAULT(from, to)                          \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_fault)
> +
> +# define _ASM_EXTABLE_EX(from, to)                             \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_ext)
> +
> +# define _ASM_EXTABLE_REFCOUNT(from, to)                       \
> +       _ASM_EXTABLE_HANDLE(from, to, ex_handler_refcount)
> +
> +/* For C file, we already have NOKPROBE_SYMBOL macro */
> +#endif
>
>  #ifndef __ASSEMBLY__
>  /*
> diff --git a/arch/x86/include/asm/bug.h b/arch/x86/include/asm/bug.h
> index 5090035..6804d66 100644
> --- a/arch/x86/include/asm/bug.h
> +++ b/arch/x86/include/asm/bug.h
> @@ -4,8 +4,6 @@
>
>  #include <linux/stringify.h>
>
> -#ifndef __ASSEMBLY__
> -
>  /*
>   * Despite that some emulators terminate on UD2, we use it for WARN().
>   *
> @@ -22,15 +20,53 @@
>
>  #define LEN_UD2                2
>
> +#ifdef CONFIG_GENERIC_BUG
> +
> +#ifdef CONFIG_X86_32
> +# define __BUG_REL(val)        ".long " __stringify(val)
> +#else
> +# define __BUG_REL(val)        ".long " __stringify(val) " - 2b"
> +#endif
> +
> +#ifdef CONFIG_DEBUG_BUGVERBOSE
> +
> +#define _BUG_FLAGS(ins, flags)                                         \
> +do {                                                                   \
> +       asm volatile("1:\t" ins "\n"                                    \
> +                    ".pushsection __bug_table,\"aw\"\n"                \
> +                    "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"   \
> +                    "\t"  __BUG_REL(%c0) "\t# bug_entry::file\n"       \
> +                    "\t.word %c1"        "\t# bug_entry::line\n"       \
> +                    "\t.word %c2"        "\t# bug_entry::flags\n"      \
> +                    "\t.org 2b+%c3\n"                                  \
> +                    ".popsection"                                      \
> +                    : : "i" (__FILE__), "i" (__LINE__),                \
> +                        "i" (flags),                                   \
> +                        "i" (sizeof(struct bug_entry)));               \
> +} while (0)
> +
> +#else /* !CONFIG_DEBUG_BUGVERBOSE */
> +
>  #define _BUG_FLAGS(ins, flags)                                         \
>  do {                                                                   \
> -       asm volatile("ASM_BUG ins=\"" ins "\" file=%c0 line=%c1 "       \
> -                    "flags=%c2 size=%c3"                               \
> -                    : : "i" (__FILE__), "i" (__LINE__),                \
> -                        "i" (flags),                                   \
> +       asm volatile("1:\t" ins "\n"                                    \
> +                    ".pushsection __bug_table,\"aw\"\n"                \
> +                    "2:\t" __BUG_REL(1b) "\t# bug_entry::bug_addr\n"   \
> +                    "\t.word %c0"        "\t# bug_entry::flags\n"      \
> +                    "\t.org 2b+%c1\n"                                  \
> +                    ".popsection"                                      \
> +                    : : "i" (flags),                                   \
>                          "i" (sizeof(struct bug_entry)));               \
>  } while (0)
>
> +#endif /* CONFIG_DEBUG_BUGVERBOSE */
> +
> +#else
> +
> +#define _BUG_FLAGS(ins, flags)  asm volatile(ins)
> +
> +#endif /* CONFIG_GENERIC_BUG */
> +
>  #define HAVE_ARCH_BUG
>  #define BUG()                                                  \
>  do {                                                           \
> @@ -46,54 +82,4 @@ do {                                                         \
>
>  #include <asm-generic/bug.h>
>
> -#else /* __ASSEMBLY__ */
> -
> -#ifdef CONFIG_GENERIC_BUG
> -
> -#ifdef CONFIG_X86_32
> -.macro __BUG_REL val:req
> -       .long \val
> -.endm
> -#else
> -.macro __BUG_REL val:req
> -       .long \val - 2b
> -.endm
> -#endif
> -
> -#ifdef CONFIG_DEBUG_BUGVERBOSE
> -
> -.macro ASM_BUG ins:req file:req line:req flags:req size:req
> -1:     \ins
> -       .pushsection __bug_table,"aw"
> -2:     __BUG_REL val=1b        # bug_entry::bug_addr
> -       __BUG_REL val=\file     # bug_entry::file
> -       .word \line             # bug_entry::line
> -       .word \flags            # bug_entry::flags
> -       .org 2b+\size
> -       .popsection
> -.endm
> -
> -#else /* !CONFIG_DEBUG_BUGVERBOSE */
> -
> -.macro ASM_BUG ins:req file:req line:req flags:req size:req
> -1:     \ins
> -       .pushsection __bug_table,"aw"
> -2:     __BUG_REL val=1b        # bug_entry::bug_addr
> -       .word \flags            # bug_entry::flags
> -       .org 2b+\size
> -       .popsection
> -.endm
> -
> -#endif /* CONFIG_DEBUG_BUGVERBOSE */
> -
> -#else /* CONFIG_GENERIC_BUG */
> -
> -.macro ASM_BUG ins:req file:req line:req flags:req size:req
> -       \ins
> -.endm
> -
> -#endif /* CONFIG_GENERIC_BUG */
> -
> -#endif /* __ASSEMBLY__ */
> -
>  #endif /* _ASM_X86_BUG_H */
> diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h
> index 7d44272..aced6c9 100644
> --- a/arch/x86/include/asm/cpufeature.h
> +++ b/arch/x86/include/asm/cpufeature.h
> @@ -2,10 +2,10 @@
>  #ifndef _ASM_X86_CPUFEATURE_H
>  #define _ASM_X86_CPUFEATURE_H
>
> -#ifdef __KERNEL__
> -#ifndef __ASSEMBLY__
> -
>  #include <asm/processor.h>
> +
> +#if defined(__KERNEL__) && !defined(__ASSEMBLY__)
> +
>  #include <asm/asm.h>
>  #include <linux/bitops.h>
>
> @@ -161,10 +161,37 @@ extern void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int bit);
>   */
>  static __always_inline __pure bool _static_cpu_has(u16 bit)
>  {
> -       asm_volatile_goto("STATIC_CPU_HAS bitnum=%[bitnum] "
> -                         "cap_byte=\"%[cap_byte]\" "
> -                         "feature=%P[feature] t_yes=%l[t_yes] "
> -                         "t_no=%l[t_no] always=%P[always]"
> +       asm_volatile_goto("1: jmp 6f\n"
> +                "2:\n"
> +                ".skip -(((5f-4f) - (2b-1b)) > 0) * "
> +                        "((5f-4f) - (2b-1b)),0x90\n"
> +                "3:\n"
> +                ".section .altinstructions,\"a\"\n"
> +                " .long 1b - .\n"              /* src offset */
> +                " .long 4f - .\n"              /* repl offset */
> +                " .word %P[always]\n"          /* always replace */
> +                " .byte 3b - 1b\n"             /* src len */
> +                " .byte 5f - 4f\n"             /* repl len */
> +                " .byte 3b - 2b\n"             /* pad len */
> +                ".previous\n"
> +                ".section .altinstr_replacement,\"ax\"\n"
> +                "4: jmp %l[t_no]\n"
> +                "5:\n"
> +                ".previous\n"
> +                ".section .altinstructions,\"a\"\n"
> +                " .long 1b - .\n"              /* src offset */
> +                " .long 0\n"                   /* no replacement */
> +                " .word %P[feature]\n"         /* feature bit */
> +                " .byte 3b - 1b\n"             /* src len */
> +                " .byte 0\n"                   /* repl len */
> +                " .byte 0\n"                   /* pad len */
> +                ".previous\n"
> +                ".section .altinstr_aux,\"ax\"\n"
> +                "6:\n"
> +                " testb %[bitnum],%[cap_byte]\n"
> +                " jnz %l[t_yes]\n"
> +                " jmp %l[t_no]\n"
> +                ".previous\n"
>                  : : [feature]  "i" (bit),
>                      [always]   "i" (X86_FEATURE_ALWAYS),
>                      [bitnum]   "i" (1 << (bit & 7)),
> @@ -199,44 +226,5 @@ static __always_inline __pure bool _static_cpu_has(u16 bit)
>  #define CPU_FEATURE_TYPEVAL            boot_cpu_data.x86_vendor, boot_cpu_data.x86, \
>                                         boot_cpu_data.x86_model
>
> -#else /* __ASSEMBLY__ */
> -
> -.macro STATIC_CPU_HAS bitnum:req cap_byte:req feature:req t_yes:req t_no:req always:req
> -1:
> -       jmp 6f
> -2:
> -       .skip -(((5f-4f) - (2b-1b)) > 0) * ((5f-4f) - (2b-1b)),0x90
> -3:
> -       .section .altinstructions,"a"
> -       .long 1b - .            /* src offset */
> -       .long 4f - .            /* repl offset */
> -       .word \always           /* always replace */
> -       .byte 3b - 1b           /* src len */
> -       .byte 5f - 4f           /* repl len */
> -       .byte 3b - 2b           /* pad len */
> -       .previous
> -       .section .altinstr_replacement,"ax"
> -4:
> -       jmp \t_no
> -5:
> -       .previous
> -       .section .altinstructions,"a"
> -       .long 1b - .            /* src offset */
> -       .long 0                 /* no replacement */
> -       .word \feature          /* feature bit */
> -       .byte 3b - 1b           /* src len */
> -       .byte 0                 /* repl len */
> -       .byte 0                 /* pad len */
> -       .previous
> -       .section .altinstr_aux,"ax"
> -6:
> -       testb \bitnum,\cap_byte
> -       jnz \t_yes
> -       jmp \t_no
> -       .previous
> -.endm
> -
> -#endif /* __ASSEMBLY__ */
> -
> -#endif /* __KERNEL__ */
> +#endif /* defined(__KERNEL__) && !defined(__ASSEMBLY__) */
>  #endif /* _ASM_X86_CPUFEATURE_H */
> diff --git a/arch/x86/include/asm/jump_label.h b/arch/x86/include/asm/jump_label.h
> index a5fb34f..21efc9d 100644
> --- a/arch/x86/include/asm/jump_label.h
> +++ b/arch/x86/include/asm/jump_label.h
> @@ -2,6 +2,19 @@
>  #ifndef _ASM_X86_JUMP_LABEL_H
>  #define _ASM_X86_JUMP_LABEL_H
>
> +#ifndef HAVE_JUMP_LABEL
> +/*
> + * For better or for worse, if jump labels (the gcc extension) are missing,
> + * then the entire static branch patching infrastructure is compiled out.
> + * If that happens, the code in here will malfunction.  Raise a compiler
> + * error instead.
> + *
> + * In theory, jump labels and the static branch patching infrastructure
> + * could be decoupled to fix this.
> + */
> +#error asm/jump_label.h included on a non-jump-label kernel
> +#endif
> +
>  #define JUMP_LABEL_NOP_SIZE 5
>
>  #ifdef CONFIG_X86_64
> @@ -20,9 +33,15 @@
>
>  static __always_inline bool arch_static_branch(struct static_key *key, bool branch)
>  {
> -       asm_volatile_goto("STATIC_BRANCH_NOP l_yes=\"%l[l_yes]\" key=\"%c0\" "
> -                         "branch=\"%c1\""
> -                       : :  "i" (key), "i" (branch) : : l_yes);
> +       asm_volatile_goto("1:"
> +               ".byte " __stringify(STATIC_KEY_INIT_NOP) "\n\t"
> +               ".pushsection __jump_table,  \"aw\" \n\t"
> +               _ASM_ALIGN "\n\t"
> +               ".long 1b - ., %l[l_yes] - . \n\t"
> +               _ASM_PTR "%c0 + %c1 - .\n\t"
> +               ".popsection \n\t"
> +               : :  "i" (key), "i" (branch) : : l_yes);
> +
>         return false;
>  l_yes:
>         return true;
> @@ -30,8 +49,14 @@ static __always_inline bool arch_static_branch(struct static_key *key, bool bran
>
>  static __always_inline bool arch_static_branch_jump(struct static_key *key, bool branch)
>  {
> -       asm_volatile_goto("STATIC_BRANCH_JMP l_yes=\"%l[l_yes]\" key=\"%c0\" "
> -                         "branch=\"%c1\""
> +       asm_volatile_goto("1:"
> +               ".byte 0xe9\n\t .long %l[l_yes] - 2f\n\t"
> +               "2:\n\t"
> +               ".pushsection __jump_table,  \"aw\" \n\t"
> +               _ASM_ALIGN "\n\t"
> +               ".long 1b - ., %l[l_yes] - . \n\t"
> +               _ASM_PTR "%c0 + %c1 - .\n\t"
> +               ".popsection \n\t"
>                 : :  "i" (key), "i" (branch) : : l_yes);
>
>         return false;
> @@ -41,26 +66,37 @@ static __always_inline bool arch_static_branch_jump(struct static_key *key, bool
>
>  #else  /* __ASSEMBLY__ */
>
> -.macro STATIC_BRANCH_NOP l_yes:req key:req branch:req
> -.Lstatic_branch_nop_\@:
> -       .byte STATIC_KEY_INIT_NOP
> -.Lstatic_branch_no_after_\@:
> +.macro STATIC_JUMP_IF_TRUE target, key, def
> +.Lstatic_jump_\@:
> +       .if \def
> +       /* Equivalent to "jmp.d32 \target" */
> +       .byte           0xe9
> +       .long           \target - .Lstatic_jump_after_\@
> +.Lstatic_jump_after_\@:
> +       .else
> +       .byte           STATIC_KEY_INIT_NOP
> +       .endif
>         .pushsection __jump_table, "aw"
>         _ASM_ALIGN
> -       .long           .Lstatic_branch_nop_\@ - ., \l_yes - .
> -       _ASM_PTR        \key + \branch - .
> +       .long           .Lstatic_jump_\@ - ., \target - .
> +       _ASM_PTR        \key - .
>         .popsection
>  .endm
>
> -.macro STATIC_BRANCH_JMP l_yes:req key:req branch:req
> -.Lstatic_branch_jmp_\@:
> -       .byte 0xe9
> -       .long \l_yes - .Lstatic_branch_jmp_after_\@
> -.Lstatic_branch_jmp_after_\@:
> +.macro STATIC_JUMP_IF_FALSE target, key, def
> +.Lstatic_jump_\@:
> +       .if \def
> +       .byte           STATIC_KEY_INIT_NOP
> +       .else
> +       /* Equivalent to "jmp.d32 \target" */
> +       .byte           0xe9
> +       .long           \target - .Lstatic_jump_after_\@
> +.Lstatic_jump_after_\@:
> +       .endif
>         .pushsection __jump_table, "aw"
>         _ASM_ALIGN
> -       .long           .Lstatic_branch_jmp_\@ - ., \l_yes - .
> -       _ASM_PTR        \key + \branch - .
> +       .long           .Lstatic_jump_\@ - ., \target - .
> +       _ASM_PTR        \key + 1 - .
>         .popsection
>  .endm
>
> diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h
> index 26942ad..488c596 100644
> --- a/arch/x86/include/asm/paravirt_types.h
> +++ b/arch/x86/include/asm/paravirt_types.h
> @@ -348,11 +348,23 @@ extern struct paravirt_patch_template pv_ops;
>  #define paravirt_clobber(clobber)              \
>         [paravirt_clobber] "i" (clobber)
>
> +/*
> + * Generate some code, and mark it as patchable by the
> + * apply_paravirt() alternate instruction patcher.
> + */
> +#define _paravirt_alt(insn_string, type, clobber)      \
> +       "771:\n\t" insn_string "\n" "772:\n"            \
> +       ".pushsection .parainstructions,\"a\"\n"        \
> +       _ASM_ALIGN "\n"                                 \
> +       _ASM_PTR " 771b\n"                              \
> +       "  .byte " type "\n"                            \
> +       "  .byte 772b-771b\n"                           \
> +       "  .short " clobber "\n"                        \
> +       ".popsection\n"
> +
>  /* Generate patchable code, with the default asm parameters. */
> -#define paravirt_call                                                  \
> -       "PARAVIRT_CALL type=\"%c[paravirt_typenum]\""                   \
> -       " clobber=\"%c[paravirt_clobber]\""                             \
> -       " pv_opptr=\"%c[paravirt_opptr]\";"
> +#define paravirt_alt(insn_string)                                      \
> +       _paravirt_alt(insn_string, "%c[paravirt_typenum]", "%c[paravirt_clobber]")
>
>  /* Simple instruction patching code. */
>  #define NATIVE_LABEL(a,x,b) "\n\t.globl " a #x "_" #b "\n" a #x "_" #b ":\n\t"
> @@ -373,6 +385,16 @@ unsigned native_patch(u8 type, void *ibuf, unsigned long addr, unsigned len);
>  int paravirt_disable_iospace(void);
>
>  /*
> + * This generates an indirect call based on the operation type number.
> + * The type number, computed in PARAVIRT_PATCH, is derived from the
> + * offset into the paravirt_patch_template structure, and can therefore be
> + * freely converted back into a structure offset.
> + */
> +#define PARAVIRT_CALL                                  \
> +       ANNOTATE_RETPOLINE_SAFE                         \
> +       "call *%c[paravirt_opptr];"
> +
> +/*
>   * These macros are intended to wrap calls through one of the paravirt
>   * ops structs, so that they can be later identified and patched at
>   * runtime.
> @@ -509,7 +531,7 @@ int paravirt_disable_iospace(void);
>                 /* since this condition will never hold */              \
>                 if (sizeof(rettype) > sizeof(unsigned long)) {          \
>                         asm volatile(pre                                \
> -                                    paravirt_call                      \
> +                                    paravirt_alt(PARAVIRT_CALL)        \
>                                      post                               \
>                                      : call_clbr, ASM_CALL_CONSTRAINT   \
>                                      : paravirt_type(op),               \
> @@ -519,7 +541,7 @@ int paravirt_disable_iospace(void);
>                         __ret = (rettype)((((u64)__edx) << 32) | __eax); \
>                 } else {                                                \
>                         asm volatile(pre                                \
> -                                    paravirt_call                      \
> +                                    paravirt_alt(PARAVIRT_CALL)        \
>                                      post                               \
>                                      : call_clbr, ASM_CALL_CONSTRAINT   \
>                                      : paravirt_type(op),               \
> @@ -546,7 +568,7 @@ int paravirt_disable_iospace(void);
>                 PVOP_VCALL_ARGS;                                        \
>                 PVOP_TEST_NULL(op);                                     \
>                 asm volatile(pre                                        \
> -                            paravirt_call                              \
> +                            paravirt_alt(PARAVIRT_CALL)                \
>                              post                                       \
>                              : call_clbr, ASM_CALL_CONSTRAINT           \
>                              : paravirt_type(op),                       \
> @@ -664,26 +686,6 @@ struct paravirt_patch_site {
>  extern struct paravirt_patch_site __parainstructions[],
>         __parainstructions_end[];
>
> -#else  /* __ASSEMBLY__ */
> -
> -/*
> - * This generates an indirect call based on the operation type number.
> - * The type number, computed in PARAVIRT_PATCH, is derived from the
> - * offset into the paravirt_patch_template structure, and can therefore be
> - * freely converted back into a structure offset.
> - */
> -.macro PARAVIRT_CALL type:req clobber:req pv_opptr:req
> -771:   ANNOTATE_RETPOLINE_SAFE
> -       call *\pv_opptr
> -772:   .pushsection .parainstructions,"a"
> -       _ASM_ALIGN
> -       _ASM_PTR 771b
> -       .byte \type
> -       .byte 772b-771b
> -       .short \clobber
> -       .popsection
> -.endm
> -
>  #endif /* __ASSEMBLY__ */
>
>  #endif /* _ASM_X86_PARAVIRT_TYPES_H */
> diff --git a/arch/x86/include/asm/refcount.h b/arch/x86/include/asm/refcount.h
> index a8b5e1e..dbaed55 100644
> --- a/arch/x86/include/asm/refcount.h
> +++ b/arch/x86/include/asm/refcount.h
> @@ -4,41 +4,6 @@
>   * x86-specific implementation of refcount_t. Based on PAX_REFCOUNT from
>   * PaX/grsecurity.
>   */
> -
> -#ifdef __ASSEMBLY__
> -
> -#include <asm/asm.h>
> -#include <asm/bug.h>
> -
> -.macro REFCOUNT_EXCEPTION counter:req
> -       .pushsection .text..refcount
> -111:   lea \counter, %_ASM_CX
> -112:   ud2
> -       ASM_UNREACHABLE
> -       .popsection
> -113:   _ASM_EXTABLE_REFCOUNT(112b, 113b)
> -.endm
> -
> -/* Trigger refcount exception if refcount result is negative. */
> -.macro REFCOUNT_CHECK_LT_ZERO counter:req
> -       js 111f
> -       REFCOUNT_EXCEPTION counter="\counter"
> -.endm
> -
> -/* Trigger refcount exception if refcount result is zero or negative. */
> -.macro REFCOUNT_CHECK_LE_ZERO counter:req
> -       jz 111f
> -       REFCOUNT_CHECK_LT_ZERO counter="\counter"
> -.endm
> -
> -/* Trigger refcount exception unconditionally. */
> -.macro REFCOUNT_ERROR counter:req
> -       jmp 111f
> -       REFCOUNT_EXCEPTION counter="\counter"
> -.endm
> -
> -#else /* __ASSEMBLY__ */
> -
>  #include <linux/refcount.h>
>  #include <asm/bug.h>
>
> @@ -50,12 +15,35 @@
>   * central refcount exception. The fixup address for the exception points
>   * back to the regular execution flow in .text.
>   */
> +#define _REFCOUNT_EXCEPTION                            \
> +       ".pushsection .text..refcount\n"                \
> +       "111:\tlea %[var], %%" _ASM_CX "\n"             \
> +       "112:\t" ASM_UD2 "\n"                           \
> +       ASM_UNREACHABLE                                 \
> +       ".popsection\n"                                 \
> +       "113:\n"                                        \
> +       _ASM_EXTABLE_REFCOUNT(112b, 113b)
> +
> +/* Trigger refcount exception if refcount result is negative. */
> +#define REFCOUNT_CHECK_LT_ZERO                         \
> +       "js 111f\n\t"                                   \
> +       _REFCOUNT_EXCEPTION
> +
> +/* Trigger refcount exception if refcount result is zero or negative. */
> +#define REFCOUNT_CHECK_LE_ZERO                         \
> +       "jz 111f\n\t"                                   \
> +       REFCOUNT_CHECK_LT_ZERO
> +
> +/* Trigger refcount exception unconditionally. */
> +#define REFCOUNT_ERROR                                 \
> +       "jmp 111f\n\t"                                  \
> +       _REFCOUNT_EXCEPTION
>
>  static __always_inline void refcount_add(unsigned int i, refcount_t *r)
>  {
>         asm volatile(LOCK_PREFIX "addl %1,%0\n\t"
> -               "REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
> -               : [counter] "+m" (r->refs.counter)
> +               REFCOUNT_CHECK_LT_ZERO
> +               : [var] "+m" (r->refs.counter)
>                 : "ir" (i)
>                 : "cc", "cx");
>  }
> @@ -63,32 +51,31 @@ static __always_inline void refcount_add(unsigned int i, refcount_t *r)
>  static __always_inline void refcount_inc(refcount_t *r)
>  {
>         asm volatile(LOCK_PREFIX "incl %0\n\t"
> -               "REFCOUNT_CHECK_LT_ZERO counter=\"%[counter]\""
> -               : [counter] "+m" (r->refs.counter)
> +               REFCOUNT_CHECK_LT_ZERO
> +               : [var] "+m" (r->refs.counter)
>                 : : "cc", "cx");
>  }
>
>  static __always_inline void refcount_dec(refcount_t *r)
>  {
>         asm volatile(LOCK_PREFIX "decl %0\n\t"
> -               "REFCOUNT_CHECK_LE_ZERO counter=\"%[counter]\""
> -               : [counter] "+m" (r->refs.counter)
> +               REFCOUNT_CHECK_LE_ZERO
> +               : [var] "+m" (r->refs.counter)
>                 : : "cc", "cx");
>  }
>
>  static __always_inline __must_check
>  bool refcount_sub_and_test(unsigned int i, refcount_t *r)
>  {
> -
>         return GEN_BINARY_SUFFIXED_RMWcc(LOCK_PREFIX "subl",
> -                                        "REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
> +                                        REFCOUNT_CHECK_LT_ZERO,
>                                          r->refs.counter, e, "er", i, "cx");
>  }
>
>  static __always_inline __must_check bool refcount_dec_and_test(refcount_t *r)
>  {
>         return GEN_UNARY_SUFFIXED_RMWcc(LOCK_PREFIX "decl",
> -                                       "REFCOUNT_CHECK_LT_ZERO counter=\"%[var]\"",
> +                                       REFCOUNT_CHECK_LT_ZERO,
>                                         r->refs.counter, e, "cx");
>  }
>
> @@ -106,8 +93,8 @@ bool refcount_add_not_zero(unsigned int i, refcount_t *r)
>
>                 /* Did we try to increment from/to an undesirable state? */
>                 if (unlikely(c < 0 || c == INT_MAX || result < c)) {
> -                       asm volatile("REFCOUNT_ERROR counter=\"%[counter]\""
> -                                    : : [counter] "m" (r->refs.counter)
> +                       asm volatile(REFCOUNT_ERROR
> +                                    : : [var] "m" (r->refs.counter)
>                                      : "cc", "cx");
>                         break;
>                 }
> @@ -122,6 +109,4 @@ static __always_inline __must_check bool refcount_inc_not_zero(refcount_t *r)
>         return refcount_add_not_zero(1, r);
>  }
>
> -#endif /* __ASSEMBLY__ */
> -
>  #endif
> diff --git a/arch/x86/kernel/macros.S b/arch/x86/kernel/macros.S
> deleted file mode 100644
> index 161c950..0000000
> --- a/arch/x86/kernel/macros.S
> +++ /dev/null
> @@ -1,16 +0,0 @@
> -/* SPDX-License-Identifier: GPL-2.0 */
> -
> -/*
> - * This file includes headers whose assembly part includes macros which are
> - * commonly used. The macros are precompiled into assmebly file which is later
> - * assembled together with each compiled file.
> - */
> -
> -#include <linux/compiler.h>
> -#include <asm/refcount.h>
> -#include <asm/alternative-asm.h>
> -#include <asm/bug.h>
> -#include <asm/paravirt.h>
> -#include <asm/asm.h>
> -#include <asm/cpufeature.h>
> -#include <asm/jump_label.h>
> diff --git a/include/asm-generic/bug.h b/include/asm-generic/bug.h
> index cdafa5e..20561a6 100644
> --- a/include/asm-generic/bug.h
> +++ b/include/asm-generic/bug.h
> @@ -17,8 +17,10 @@
>  #ifndef __ASSEMBLY__
>  #include <linux/kernel.h>
>
> -struct bug_entry {
> +#ifdef CONFIG_BUG
> +
>  #ifdef CONFIG_GENERIC_BUG
> +struct bug_entry {
>  #ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS
>         unsigned long   bug_addr;
>  #else
> @@ -33,10 +35,8 @@ struct bug_entry {
>         unsigned short  line;
>  #endif
>         unsigned short  flags;
> -#endif /* CONFIG_GENERIC_BUG */
>  };
> -
> -#ifdef CONFIG_BUG
> +#endif /* CONFIG_GENERIC_BUG */
>
>  /*
>   * Don't use BUG() or BUG_ON() unless there's really no way out; one
> diff --git a/include/linux/compiler.h b/include/linux/compiler.h
> index 06396c1..fc5004a 100644
> --- a/include/linux/compiler.h
> +++ b/include/linux/compiler.h
> @@ -99,13 +99,22 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
>   * unique, to convince GCC not to merge duplicate inline asm statements.
>   */
>  #define annotate_reachable() ({                                                \
> -       asm volatile("ANNOTATE_REACHABLE counter=%c0"                   \
> -                    : : "i" (__COUNTER__));                            \
> +       asm volatile("%c0:\n\t"                                         \
> +                    ".pushsection .discard.reachable\n\t"              \
> +                    ".long %c0b - .\n\t"                               \
> +                    ".popsection\n\t" : : "i" (__COUNTER__));          \
>  })
>  #define annotate_unreachable() ({                                      \
> -       asm volatile("ANNOTATE_UNREACHABLE counter=%c0"                 \
> -                    : : "i" (__COUNTER__));                            \
> +       asm volatile("%c0:\n\t"                                         \
> +                    ".pushsection .discard.unreachable\n\t"            \
> +                    ".long %c0b - .\n\t"                               \
> +                    ".popsection\n\t" : : "i" (__COUNTER__));          \
>  })
> +#define ASM_UNREACHABLE                                                        \
> +       "999:\n\t"                                                      \
> +       ".pushsection .discard.unreachable\n\t"                         \
> +       ".long 999b - .\n\t"                                            \
> +       ".popsection\n\t"
>  #else
>  #define annotate_reachable()
>  #define annotate_unreachable()
> @@ -293,45 +302,6 @@ static inline void *offset_to_ptr(const int *off)
>         return (void *)((unsigned long)off + *off);
>  }
>
> -#else /* __ASSEMBLY__ */
> -
> -#ifdef __KERNEL__
> -#ifndef LINKER_SCRIPT
> -
> -#ifdef CONFIG_STACK_VALIDATION
> -.macro ANNOTATE_UNREACHABLE counter:req
> -\counter:
> -       .pushsection .discard.unreachable
> -       .long \counter\()b -.
> -       .popsection
> -.endm
> -
> -.macro ANNOTATE_REACHABLE counter:req
> -\counter:
> -       .pushsection .discard.reachable
> -       .long \counter\()b -.
> -       .popsection
> -.endm
> -
> -.macro ASM_UNREACHABLE
> -999:
> -       .pushsection .discard.unreachable
> -       .long 999b - .
> -       .popsection
> -.endm
> -#else /* CONFIG_STACK_VALIDATION */
> -.macro ANNOTATE_UNREACHABLE counter:req
> -.endm
> -
> -.macro ANNOTATE_REACHABLE counter:req
> -.endm
> -
> -.macro ASM_UNREACHABLE
> -.endm
> -#endif /* CONFIG_STACK_VALIDATION */
> -
> -#endif /* LINKER_SCRIPT */
> -#endif /* __KERNEL__ */
>  #endif /* __ASSEMBLY__ */
>
>  /* Compile time object size, -1 for unknown */
> diff --git a/scripts/Kbuild.include b/scripts/Kbuild.include
> index bb01555..3d09844 100644
> --- a/scripts/Kbuild.include
> +++ b/scripts/Kbuild.include
> @@ -115,9 +115,7 @@ __cc-option = $(call try-run,\
>
>  # Do not attempt to build with gcc plugins during cc-option tests.
>  # (And this uses delayed resolution so the flags will be up to date.)
> -# In addition, do not include the asm macros which are built later.
> -CC_OPTION_FILTERED = $(GCC_PLUGINS_CFLAGS) $(ASM_MACRO_FLAGS)
> -CC_OPTION_CFLAGS = $(filter-out $(CC_OPTION_FILTERED),$(KBUILD_CFLAGS))
> +CC_OPTION_CFLAGS = $(filter-out $(GCC_PLUGINS_CFLAGS),$(KBUILD_CFLAGS))
>
>  # cc-option
>  # Usage: cflags-y += $(call cc-option,-march=winchip-c6,-march=i586)
> diff --git a/scripts/mod/Makefile b/scripts/mod/Makefile
> index a5b4af4..42c5d50 100644
> --- a/scripts/mod/Makefile
> +++ b/scripts/mod/Makefile
> @@ -4,8 +4,6 @@ OBJECT_FILES_NON_STANDARD := y
>  hostprogs-y    := modpost mk_elfconfig
>  always         := $(hostprogs-y) empty.o
>
> -CFLAGS_REMOVE_empty.o := $(ASM_MACRO_FLAGS)
> -
>  modpost-objs   := modpost.o file2alias.o sumversion.o
>
>  devicetable-offsets-file := devicetable-offsets.h
> --
> 2.7.4
>
================================================================================

From: Nadav Amit <namit () vmware ! com>
To: linux-arch
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Tue, 18 Dec 2018 19:33:07 +0000
Message-ID: <A6321F73-0084-423B-8BEE-C23BF6BD1544 () vmware ! com>
--------------------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==
================================================================================

From: Nadav Amit <namit () vmware ! com>
To: linux-kernel
Subject: Re: [PATCH] kbuild, x86: revert macros in extended asm workarounds
Date: Tue, 18 Dec 2018 19:33:07 +0000
Message-ID: <A6321F73-0084-423B-8BEE-C23BF6BD1544 () vmware ! com>
--------------------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==
================================================================================


################################################################################

=== Thread: [PATCH] kill dead OP_FADD & friends ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] kill dead OP_FADD & friends
Date: Tue, 24 Jul 2018 16:52:12 +0000
Message-ID: <20180724165212.43067-1-luc.vanoostenryck () gmail ! com>
--------------------
The floating-point binops (OP_FADD, ...) are never simplified.
They are thus absent from simplify_instruction() and as such they are
never removed if dead.

Fix this by adding them to simplify_instruction() but only do
the usual check with dead_insn().

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c | 7 +++++++
 1 file changed, 7 insertions(+)

diff --git a/simplify.c b/simplify.c
index 741b1272c..4c3b3dbc4 100644
--- a/simplify.c
+++ b/simplify.c
@@ -1250,6 +1250,13 @@ int simplify_instruction(struct instruction *insn)
 		return simplify_switch(insn);
 	case OP_RANGE:
 		return simplify_range(insn);
+	case OP_FADD:
+	case OP_FSUB:
+	case OP_FMUL:
+	case OP_FDIV:
+		if (dead_insn(insn, &insn->src1, &insn->src2, NULL))
+			return REPEAT_CSE;
+		break;
 	}
 	return 0;
 }
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] ptrlist: add ptr_list_nth_entry() ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] ptrlist: add ptr_list_nth_entry()
Date: Tue, 30 Oct 2018 11:46:07 +0000
Message-ID: <20181030114607.65645-1-luc.vanoostenryck () gmail ! com>
--------------------
Usually ptr lists are accessed iteratively via the FOR/END macros
but in few case we may need to access a given element in a list,
like for example when accessing a given argument of a function.

Create an helper doing that instead of open coding it.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 ptrlist.c | 22 ++++++++++++++++++++++
 ptrlist.h |  1 +
 2 files changed, 23 insertions(+)

diff --git a/ptrlist.c b/ptrlist.c
index 3677a347c..0fb281274 100644
--- a/ptrlist.c
+++ b/ptrlist.c
@@ -114,6 +114,28 @@ void *last_ptr_list(struct ptr_list *head)
 	return PTR_ENTRY_NOTAG(list, list->nr-1);
 }
 
+///
+// get the nth element of a ptrlist
+// @head: the head of the list
+// @return: the nth element of the list or ``NULL`` if the list is too short.
+void *ptr_list_nth_entry(struct ptr_list *list, unsigned int idx)
+{
+	struct ptr_list *head = list;
+
+	if (!head)
+		return NULL;
+
+	do {
+		unsigned int nr = list->nr;
+
+		if (idx < nr)
+			return list->list[idx];
+		else
+			idx -= nr;
+	} while ((list = list->next) != head);
+	return NULL;
+}
+
 ///
 // linearize the entries of a list
 //
diff --git a/ptrlist.h b/ptrlist.h
index 2f0234784..2411e745a 100644
--- a/ptrlist.h
+++ b/ptrlist.h
@@ -44,6 +44,7 @@ extern bool ptr_list_multiple(const struct ptr_list *head);
 extern int linearize_ptr_list(struct ptr_list *, void **, int);
 extern void *first_ptr_list(struct ptr_list *);
 extern void *last_ptr_list(struct ptr_list *);
+extern void *ptr_list_nth_entry(struct ptr_list *, unsigned int idx);
 extern void pack_ptr_list(struct ptr_list **);
 
 /*
-- 
2.19.0

================================================================================


################################################################################

=== Thread: [PATCH] remove -finline-functions from CFLAGS ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] remove -finline-functions from CFLAGS
Date: Fri, 21 Dec 2018 00:01:07 +0000
Message-ID: <20181221000107.31114-1-luc.vanoostenryck () gmail ! com>
--------------------
By default, sparse is compiled with -finline-functions
but this flag as no effect on the generated code (since
gcc's defaults at -O2 already do automatic inlining).

So, remove this flag.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 Makefile | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/Makefile b/Makefile
index e3e9bc64f..2a3e44347 100644
--- a/Makefile
+++ b/Makefile
@@ -6,7 +6,7 @@ OS = linux
 
 
 CC = gcc
-CFLAGS = -O2 -finline-functions -g
+CFLAGS = -O2 -g
 CFLAGS += -Wall -Wwrite-strings
 LD = gcc
 AR = ar
-- 
2.20.0

================================================================================


################################################################################

=== Thread: [PATCH] remove redundant check of _Bool bitsize ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] remove redundant check of _Bool bitsize
Date: Thu, 20 Dec 2018 22:52:08 +0000
Message-ID: <20181220225208.27574-1-luc.vanoostenryck () gmail ! com>
--------------------
To test if a type is a variant of _Bool it is useless
to test is_bool_type(x) *and* test if 'x->bit_size == 1'
since the first implies the second.

So, remove the test of the bitsize.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 evaluate.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/evaluate.c b/evaluate.c
index 4c8c002ee..8841a6e48 100644
--- a/evaluate.c
+++ b/evaluate.c
@@ -2207,7 +2207,7 @@ static struct symbol *evaluate_sizeof(struct expression *expr)
 		size = bits_in_char;
 	}
 
-	if (size == 1 && is_bool_type(type)) {
+	if (is_bool_type(type)) {
 		if (Wsizeof_bool)
 			warning(expr->pos, "expression using sizeof _Bool");
 		size = bits_in_char;
-- 
2.20.0

================================================================================


################################################################################

=== Thread: [PATCH] remove unneeded declarations in "compat.h" ===

From: Ramsay Jones <ramsay () ramsayjones ! plus ! com>
To: linux-sparse
Subject: Re: [PATCH] remove unneeded declarations in "compat.h"
Date: Mon, 24 Dec 2018 17:07:34 +0000
Message-ID: <aca14272-c1af-17d7-4f99-e587a4eb40fe () ramsayjones ! plus ! com>
--------------------


On 19/12/2018 07:33, Luc Van Oostenryck wrote:
> struct stream & struct stat are defined in this file
> but were only used for identical_files() which has been
> removed years ago.
> 
> So, remove these declarations.

LGTM. BTW I have been testing the 'master' branch (currently up to
version: v0.6.0-rc1-22-g2369d10), without finding any errors or
regressions. I have now upgraded to Linux Mint 19.1 (64-bit) and
fedora 29 (with newer compiler, gcc v2.8.1).

Have a good holiday.

ATB,
Ramsay Jones


> 
> Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
> ---
>  compat.h | 2 --
>  1 file changed, 2 deletions(-)
> 
> diff --git a/compat.h b/compat.h
> index 9814ae3e8..4bba47ad2 100644
> --- a/compat.h
> +++ b/compat.h
> @@ -10,8 +10,6 @@
>   *  - "string to long double" (C99 strtold())
>   *	Missing in Solaris and MinGW
>   */
> -struct stream;
> -struct stat;
>  
>  /*
>   * Our "blob" allocator works on chunks that are multiples
> 
================================================================================

From: Ramsay Jones <ramsay () ramsayjones ! plus ! com>
To: linux-sparse
Subject: Re: [PATCH] remove unneeded declarations in "compat.h"
Date: Mon, 24 Dec 2018 20:56:38 +0000
Message-ID: <109fcd19-f53b-6691-365f-2e6287b8c5a3 () ramsayjones ! plus ! com>
--------------------


On 24/12/2018 17:07, Ramsay Jones wrote:
> 
> 
> On 19/12/2018 07:33, Luc Van Oostenryck wrote:
>> struct stream & struct stat are defined in this file
>> but were only used for identical_files() which has been
>> removed years ago.
>>
>> So, remove these declarations.
> 
> LGTM. BTW I have been testing the 'master' branch (currently up to
> version: v0.6.0-rc1-22-g2369d10), without finding any errors or
> regressions. I have now upgraded to Linux Mint 19.1 (64-bit) and
> fedora 29 (with newer compiler, gcc v2.8.1).

Heh, that should be v8.2.1, of course! :-D

> Have a good holiday.

ATB,
Ramsay Jones

================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH] remove unneeded declarations in "compat.h"
Date: Tue, 25 Dec 2018 19:42:11 +0000
Message-ID: <20181225194210.ee6ngko5r3i4xrtd () ltop ! local>
--------------------
On Mon, Dec 24, 2018 at 08:56:38PM +0000, Ramsay Jones wrote:
> 
> 
> On 24/12/2018 17:07, Ramsay Jones wrote:
> > 
> > 
> > On 19/12/2018 07:33, Luc Van Oostenryck wrote:
> >> struct stream & struct stat are defined in this file
> >> but were only used for identical_files() which has been
> >> removed years ago.
> >>
> >> So, remove these declarations.
> > 
> > LGTM. BTW I have been testing the 'master' branch (currently up to
> > version: v0.6.0-rc1-22-g2369d10), without finding any errors or
> > regressions. I have now upgraded to Linux Mint 19.1 (64-bit) and
> > fedora 29 (with newer compiler, gcc v2.8.1).

Great. Thank you.
 
> Heh, that should be v8.2.1, of course! :-D

Hehe, gcc 2.8.1, that was quite some years ago!
I didn't spotted it here, I autocorrect too much.

> > Have a good holiday.

Thank you very much.
Best Wishes.

-- Luc
================================================================================


################################################################################

=== Thread: [PATCH] remove validation/tmp.o in clean target ===

From: =?UTF-8?q?Uwe=20Kleine-K=C3=B6nig?= <uwe () kleine-koenig ! org>
To: linux-sparse
Subject: [PATCH] remove validation/tmp.o in clean target
Date: Sun, 19 Jul 2015 09:57:49 +0000
Message-ID: <1437299869-3669-1-git-send-email-uwe () kleine-koenig ! org>
--------------------
To prevent validation/tmp.o being still around after

	make check && make clean

remove it as part of the latter command.
---
 Makefile | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/Makefile b/Makefile
index c7031afda00b..bc837db7c3f6 100644
--- a/Makefile
+++ b/Makefile
@@ -199,7 +199,7 @@ compat-cygwin.o: $(LIB_H)
 	$(QUIET_CC)$(CC) -o $@ -c $(ALL_CFLAGS) $<
 
 clean: clean-check
-	rm -f *.[oa] .*.d *.so $(PROGRAMS) $(SLIB_FILE) pre-process.h sparse.pc
+	rm -f *.[oa] .*.d *.so $(PROGRAMS) $(SLIB_FILE) pre-process.h sparse.pc validation/*.[oa]
 
 dist:
 	@if test "$(SPARSE_VERSION)" != "v$(VERSION)" ; then \
-- 
2.1.4

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Christopher Li <sparse () chrisli ! org>
To: linux-sparse
Subject: Re: [PATCH] remove validation/tmp.o in clean target
Date: Tue, 04 Aug 2015 06:44:34 +0000
Message-ID: <CANeU7Q=x4aF3CqrP-prJmtLj_jVz+yji5b8Sn-V7FOtnfHCM4g () mail ! gmail ! com>
--------------------
On Sun, Jul 19, 2015 at 5:57 AM, Uwe Kleine-KÃ¶nig <uwe@kleine-koenig.org> wrote:
> To prevent validation/tmp.o being still around after
>
>         make check && make clean
>
> remove it as part of the latter command.

Looks good. I would like to apply it.

Can you send me a "signed-off-by" line please?
While you are at it, you can combine the two patches into one.

Thanks

Chris
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: =?UTF-8?q?Uwe=20Kleine-K=C3=B6nig?= <uwe () kleine-koenig ! org>
To: linux-sparse
Subject: [PATCH] remove validation/tmp.o in clean target
Date: Thu, 19 Apr 2018 05:17:28 +0000
Message-ID: <20180419051728.25352-1-uwe () kleine-koenig ! org>
--------------------
To prevent validation/tmp.o being still around after

	make check && make clean

remove it as part of the latter command.

Signed-off-by: Uwe Kleine-KÃ¶nig <uwe@kleine-koenig.org>
---
 Makefile | 1 +
 1 file changed, 1 insertion(+)

diff --git a/Makefile b/Makefile
index a72802ff986a..faa6ea8d4551 100644
--- a/Makefile
+++ b/Makefile
@@ -242,4 +242,5 @@ clean-check:
 	                 -o -name "*.c.error.expected" \
 	                 -o -name "*.c.error.got" \
 	                 -o -name "*.c.error.diff" \
+			 -o -name "*.o" \
 	                 \) -exec rm {} \;
-- 
2.17.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH] remove validation/tmp.o in clean target
Date: Thu, 19 Apr 2018 07:55:17 +0000
Message-ID: <20180419075515.d6kgdbgh5pmhew72 () ltop ! local>
--------------------
On Thu, Apr 19, 2018 at 07:17:28AM +0200, Uwe Kleine-König wrote:
> To prevent validation/tmp.o being still around after
> 
> 	make check && make clean
> 
> remove it as part of the latter command.
> 
> Signed-off-by: Uwe Kleine-König <uwe@kleine-koenig.org>

Acked-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] shift: avoid simplification of ASR(LSR(x,0),N) ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] shift: avoid simplification of ASR(LSR(x,0),N)
Date: Tue, 24 Jul 2018 06:05:32 +0000
Message-ID: <20180724060532.85824-1-luc.vanoostenryck () gmail ! com>
--------------------
In most circumstances, the case where the LSR's count is zero
won't happen because it would have been optimized away before.
However there is a very small posibility it would happen:
  at the first run of the simplification loop,
  if the LSR and the ASR are in two different Basic Blocks and
  the ASR's BB is processed before the LSR's BB (rare)

Reported-by: Ramsay Jones <ramsay@ramsayjones.plus.com>
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 simplify.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/simplify.c b/simplify.c
index 322450b86..e3b789f2a 100644
--- a/simplify.c
+++ b/simplify.c
@@ -600,7 +600,7 @@ static int simplify_shift(struct instruction *insn, pseudo_t pseudo, long long v
 			if (src2->type != PSEUDO_VAL)
 				break;
 			nval = src2->value;
-			if (nval > insn->size)
+			if (nval > insn->size || nval == 0)
 				break;
 			value += nval;
 			if (def->opcode == OP_LSR)
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] sparse: stricter warning for explicit cast to ulong ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] sparse: stricter warning for explicit cast to ulong
Date: Thu, 28 Jun 2018 23:21:20 +0000
Message-ID: <20180628232119.5jaavhewv5nb6ufb () ltop ! local>
--------------------
sparse issues a warning when user pointers are casted to integer
types except to unsigned longs which are explicitly allowed.
However it may happen that we would like to also be warned
on casts to unsigned long.

Fix this by adding a new warning flag: -Wcast-from-as (to mirrors
-Wcast-to-as) which extends -Waddress-space to all casts that
remove an address space attribute (without using __force).

References: https://lore.kernel.org/lkml/20180628102741.vk6vphfinlj3lvhv@armageddon.cambridge.arm.com/
Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---

This patch is available in the Git repository at:
  git://github.com/lucvoo/sparse-dev.git warn-cast-from-as

----------------------------------------------------------------
Luc Van Oostenryck (1):
      stricter warning for explicit cast to ulong

 evaluate.c                         |  4 +--
 lib.c                              |  2 ++
 lib.h                              |  1 +
 sparse.1                           |  9 ++++++
 validation/Waddress-space-strict.c | 56 ++++++++++++++++++++++++++++++++++++++
 5 files changed, 70 insertions(+), 2 deletions(-)
 create mode 100644 validation/Waddress-space-strict.c

diff --git a/evaluate.c b/evaluate.c
index 194b97218..64e1067ce 100644
--- a/evaluate.c
+++ b/evaluate.c
@@ -2998,14 +2998,14 @@ static struct symbol *evaluate_cast(struct expression *expr)
 		}
 	}
 
-	if (ttype == &ulong_ctype)
+	if (ttype == &ulong_ctype && !Wcast_from_as)
 		tas = -1;
 	else if (tclass == TYPE_PTR) {
 		examine_pointer_target(ttype);
 		tas = ttype->ctype.as;
 	}
 
-	if (stype == &ulong_ctype)
+	if (stype == &ulong_ctype && !Wcast_from_as)
 		sas = -1;
 	else if (sclass == TYPE_PTR) {
 		examine_pointer_target(stype);
diff --git a/lib.c b/lib.c
index 308f8f699..0bb5232ab 100644
--- a/lib.c
+++ b/lib.c
@@ -248,6 +248,7 @@ static struct token *pre_buffer_end = NULL;
 int Waddress = 0;
 int Waddress_space = 1;
 int Wbitwise = 1;
+int Wcast_from_as = 0;
 int Wcast_to_as = 0;
 int Wcast_truncate = 1;
 int Wconstexpr_not_const = 0;
@@ -678,6 +679,7 @@ static const struct flag warnings[] = {
 	{ "address", &Waddress },
 	{ "address-space", &Waddress_space },
 	{ "bitwise", &Wbitwise },
+	{ "cast-from-as", &Wcast_from_as },
 	{ "cast-to-as", &Wcast_to_as },
 	{ "cast-truncate", &Wcast_truncate },
 	{ "constexpr-not-const", &Wconstexpr_not_const},
diff --git a/lib.h b/lib.h
index b0453bb6e..46e685421 100644
--- a/lib.h
+++ b/lib.h
@@ -137,6 +137,7 @@ extern int preprocess_only;
 extern int Waddress;
 extern int Waddress_space;
 extern int Wbitwise;
+extern int Wcast_from_as;
 extern int Wcast_to_as;
 extern int Wcast_truncate;
 extern int Wconstexpr_not_const;
diff --git a/sparse.1 b/sparse.1
index 806fb0cf0..62956f18b 100644
--- a/sparse.1
+++ b/sparse.1
@@ -77,6 +77,15 @@ Sparse issues these warnings by default.  To turn them off, use
 \fB\-Wno\-bitwise\fR.
 .
 .TP
+.B \-Wcast\-from\-as
+Warn about which remove an address space to a pointer type.
+
+This is similar to \fB\-Waddress\-space\fR but will also warn
+on casts to \fBunsigned long\fR.
+
+Sparse does not issues these warnings by default.
+.
+.TP
 .B \-Wcast\-to\-as
 Warn about casts which add an address space to a pointer type.
 
diff --git a/validation/Waddress-space-strict.c b/validation/Waddress-space-strict.c
new file mode 100644
index 000000000..ad23f74ae
--- /dev/null
+++ b/validation/Waddress-space-strict.c
@@ -0,0 +1,56 @@
+#define __user __attribute__((address_space(1)))
+
+typedef unsigned long ulong;
+typedef long long llong;
+typedef struct s obj_t;
+
+static void expl(int i, ulong u, llong l, void *v, obj_t *o, obj_t __user *p)
+{
+	(obj_t*)(i);
+	(obj_t __user*)(i);
+
+	(obj_t*)(u);
+	(obj_t __user*)(u);
+
+	(obj_t*)(l);
+	(obj_t __user*)(l);
+
+	(obj_t*)(v);
+	(obj_t __user*)(v);
+
+	(int)(o);
+	(ulong)(o);
+	(llong)(o);
+	(void *)(o);
+	(obj_t*)(o);
+	(obj_t __user*)(o);
+
+	(int)(p);		// w
+	(ulong)(p);		// w!
+	(llong)(p);		// w
+	(void *)(p);		// w
+	(obj_t*)(p);		// w
+	(obj_t __user*)(p);	// ok
+}
+
+/*
+ * check-name: Waddress-space-strict
+ * check-command: sparse -Wcast-from-as -Wcast-to-as $file
+ *
+ * check-error-start
+Waddress-space-strict.c:10:10: warning: cast adds address space to expression (<asn:1>)
+Waddress-space-strict.c:13:10: warning: cast adds address space to expression (<asn:1>)
+Waddress-space-strict.c:16:10: warning: cast adds address space to expression (<asn:1>)
+Waddress-space-strict.c:19:10: warning: cast adds address space to expression (<asn:1>)
+Waddress-space-strict.c:26:10: warning: cast adds address space to expression (<asn:1>)
+Waddress-space-strict.c:28:10: warning: cast removes address space of expression
+Waddress-space-strict.c:29:10: warning: cast removes address space of expression
+Waddress-space-strict.c:30:10: warning: cast removes address space of expression
+Waddress-space-strict.c:31:10: warning: cast removes address space of expression
+Waddress-space-strict.c:32:10: warning: cast removes address space of expression
+Waddress-space-strict.c:9:10: warning: non size-preserving integer to pointer cast
+Waddress-space-strict.c:10:10: warning: non size-preserving integer to pointer cast
+Waddress-space-strict.c:21:10: warning: non size-preserving pointer to integer cast
+Waddress-space-strict.c:28:10: warning: non size-preserving pointer to integer cast
+ * check-error-end
+ */
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] testcases: missing evaluation of side effects in typeof(VLA) ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] testcases: missing evaluation of side effects in typeof(VLA)
Date: Tue, 24 Jul 2018 17:00:31 +0000
Message-ID: <20180724170031.46056-1-luc.vanoostenryck () gmail ! com>
--------------------
The argument of the typeof operator is normally not evaluated
but it should be when it involves a VLA.

Add a testcase for this.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/eval-typeof-vla.c | 26 ++++++++++++++++++++++++++
 1 file changed, 26 insertions(+)
 create mode 100644 validation/eval-typeof-vla.c

diff --git a/validation/eval-typeof-vla.c b/validation/eval-typeof-vla.c
new file mode 100644
index 000000000..56a9ec201
--- /dev/null
+++ b/validation/eval-typeof-vla.c
@@ -0,0 +1,26 @@
+extern int a[1];
+
+static int foo(int n)
+{
+	int i = 0;
+	int (*p)[1] = (typeof(++i, (int (*)[n])a)) &a;
+
+	(void) p;
+
+	return i;
+}
+
+/*
+ * check-name: eval-typeof-vla
+ * check-command: test-linearize -Wno-vla $file
+ * check-known-to-fail
+ *
+ * check-output-start
+foo:
+.L0:
+	<entry-point>
+	ret.32      $1
+
+
+ * check-output-end
+ */
-- 
2.18.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [PATCH] tests: add varargs printf format tests ===

From: Ben Dooks <ben.dooks () codethink ! co ! uk>
To: linux-sparse
Subject: [PATCH] tests: add varargs printf format tests
Date: Tue, 30 Oct 2018 09:57:57 +0000
Message-ID: <20181030095757.14489-1-ben.dooks () codethink ! co ! uk>
--------------------
Add some tests for the new printf format checking code.
Note, these do not all pass yet.

Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
---
 validation/varargs-format-addrspace1.c | 57 ++++++++++++++++++++++++++
 validation/varargs-format-bad.c        | 14 ++++++-
 validation/varargs-format-prefix.c     | 18 ++++++++
 validation/varargs-type-checking.c     | 19 +++++++++
 4 files changed, 106 insertions(+), 2 deletions(-)
 create mode 100644 validation/varargs-format-addrspace1.c
 create mode 100644 validation/varargs-format-prefix.c
 create mode 100644 validation/varargs-type-checking.c

diff --git a/validation/varargs-format-addrspace1.c b/validation/varargs-format-addrspace1.c
new file mode 100644
index 0000000..55da817
--- /dev/null
+++ b/validation/varargs-format-addrspace1.c
@@ -0,0 +1,57 @@
+
+extern int variadic(char *msg, ...) __attribute__((format (printf, 1, 2)));
+extern int variadic2(char *msg, int , ...) __attribute__((format (printf, 1, 3)));
+extern int variadic3(int, char *msg,  ...) __attribute__((format (printf, 2, 3)));
+
+static void test(void) {
+	void __attribute__((noderef, address_space(1))) *a;
+	void *b;
+
+	variadic("%s\n", a);
+	variadic("%s\n", b);
+	variadic("%s %s\n", b, a);
+	variadic2("%s %s\n", 1, b, a);
+	variadic3(1, "%s %s\n", b, a);
+	variadic3(1, "%s %p\n", b, a);
+}
+
+static char __msg[] = "%s %p";
+
+static void test2(void) {
+	void __attribute__((noderef, address_space(1))) *a;
+	void *b;
+	int (*ptr)(char *msg, ...) __attribute__((format (printf, 1, 2))) = variadic;
+	int (*ptr2)(char *msg, ...) __attribute__((format (printf, 1, 2)));
+
+	variadic(__msg, a, b);
+	ptr("hello %s %s", a, b);
+	ptr2("hello %s %s", a, b);
+}
+
+/*
+ * check-name: variadic formatting test with addres-space to %s
+ *
+ * check-error-start
+varargs-format-addrspace1.c:10:26: warning: incorrect type in argument 2 (different address spaces)
+varargs-format-addrspace1.c:10:26:    expected string
+varargs-format-addrspace1.c:10:26:    got void [noderef] <asn:1>*a
+varargs-format-addrspace1.c:12:32: warning: incorrect type in argument 3 (different address spaces)
+varargs-format-addrspace1.c:12:32:    expected string
+varargs-format-addrspace1.c:12:32:    got void [noderef] <asn:1>*a
+varargs-format-addrspace1.c:13:36: warning: incorrect type in argument 4 (different address spaces)
+varargs-format-addrspace1.c:13:36:    expected string
+varargs-format-addrspace1.c:13:36:    got void [noderef] <asn:1>*a
+varargs-format-addrspace1.c:14:36: warning: incorrect type in argument 4 (different address spaces)
+varargs-format-addrspace1.c:14:36:    expected string
+varargs-format-addrspace1.c:14:36:    got void [noderef] <asn:1>*a
+varargs-format-addrspace1.c:15:36: warning: incorrect type in argument 4 (different address spaces)
+varargs-format-addrspace1.c:15:36:    expected void [noderef] *
+varargs-format-addrspace1.c:15:36:    got void [noderef] <asn:1>*a
+varargs-format-addrspace1.c:26:25: warning: incorrect type in argument 2 (different address spaces)
+varargs-format-addrspace1.c:26:25:    expected string
+varargs-format-addrspace1.c:26:25:    got void [noderef] <asn:1>*a
+varargs-format-addrspace1.c:27:25: warning: incorrect type in argument 2 (different address spaces)
+varargs-format-addrspace1.c:27:25:    expected string
+varargs-format-addrspace1.c:27:25:    got void [noderef] <asn:1>*a
+ * check-error-end
+ */
diff --git a/validation/varargs-format-bad.c b/validation/varargs-format-bad.c
index a8b9cdb..2b96aba 100644
--- a/validation/varargs-format-bad.c
+++ b/validation/varargs-format-bad.c
@@ -1,9 +1,19 @@
 
 extern int variadic(char *msg, ...) __attribute__((format (printf, 0, 0)));
 extern int variadic2(char *msg, int , ...) __attribute__((format (printf, 2, 2)));
-extern int variadic3(char *msg, int , ...) __attribute__((format (printf, 2, 3)));
+extern int variadic3(char *msg, int , ...) __attribute__((format (printf, 2, 1)));
 
 static void test(void) {
-	variadic3("test", 1);
 }
 
+/*
+ * check-name: variadic formatting test with bad formatting parameters
+ *
+ * check-error-start
+varargs-format-bad.c:2:72: warning: bad format positions
+varargs-format-bad.c:3:79: warning: bad format positions
+varargs-format-bad.c:4:79: warning: format cannot be after va_args
+* check-error-end
+ */
+
+
diff --git a/validation/varargs-format-prefix.c b/validation/varargs-format-prefix.c
new file mode 100644
index 0000000..a91d1ba
--- /dev/null
+++ b/validation/varargs-format-prefix.c
@@ -0,0 +1,18 @@
+
+extern int __attribute__((format (printf, 1, 2))) variadic(char *msg, ...);
+
+static int test(void) {
+	void __attribute__((noderef, address_space(1))) *a;
+
+	variadic("%s\n", a);
+}
+
+/*
+ * check-name: variadic formatting test prefix based __attribute__
+ *
+ * check-error-start
+varargs-format-prefix.c:7:26: warning: incorrect type in argument 2 (different address spaces)
+varargs-format-prefix.c:7:26:    expected string
+varargs-format-prefix.c:7:26:    got void [noderef] <asn:1>*a
+ * check-error-end
+ */
diff --git a/validation/varargs-type-checking.c b/validation/varargs-type-checking.c
new file mode 100644
index 0000000..1e289b2
--- /dev/null
+++ b/validation/varargs-type-checking.c
@@ -0,0 +1,19 @@
+
+extern void pf(char *msg, ...) __attribute__((format (printf, 1, 2)));
+
+static void test(void) {
+	pf("%u %lu %llu\n", 1, 1UL, 1ULL);
+	pf("%d %ld %lld\n", 1, 1L, 1LL);
+	pf("%d %lx %llx\n", 1, 1L, 1LL);
+	pf("%d %ld %lld\n", 1, 1L, 1L);
+}
+
+/*
+ * check-name: variadic formatting test type checking
+ *
+ * check-error-start
+validation/varargs-type-checking.c:8:36: warning: incorrect type in argument 4 (different base types)
+validation/varargs-type-checking.c:8:36:    expected long long
+validation/varargs-type-checking.c:8:36:    got long
+ * check-error-end
+ */
-- 
2.19.1

================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [PATCH] tests: add varargs printf format tests
Date: Tue, 30 Oct 2018 12:06:53 +0000
Message-ID: <20181030120652.2er45cajryb4ggxz () ltop ! local>
--------------------
On Tue, Oct 30, 2018 at 09:57:57AM +0000, Ben Dooks wrote:
> Add some tests for the new printf format checking code.

Good. Thank you.

> Note, these do not all pass yet.

You can use the 'tag' 'check-known-to-fail' to indicate
the files that don't pass yet.

Kind regards,
-- Luc 
================================================================================


################################################################################

=== Thread: [PATCH] testsuite: fix typo with 'test-suite format -a' ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: [PATCH] testsuite: fix typo with 'test-suite format -a'
Date: Tue, 20 Feb 2018 10:36:12 +0000
Message-ID: <20180220103612.9720-1-luc.vanoostenryck () gmail ! com>
--------------------
"append" was used instead of "$append" when formatting
a new test, with the result that the infos for the test
system were always appended to the test fiel, which is
maybe often but not always desirable.

So, add the missing '$' when using the variable.

Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
---
 validation/test-suite | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/validation/test-suite b/validation/test-suite
index 3abf69d46..e1552ed46 100755
--- a/validation/test-suite
+++ b/validation/test-suite
@@ -474,7 +474,7 @@ do_format()
 	cmd=`eval echo $default_path/$fcmd`
 	$cmd 1> $file.output.got 2> $file.error.got
 	fexit_value=$?
-	[ "append" != 0 ] && exec >> $file
+	[ $append != 0 ] && exec >> $file
 	cat <<_EOF
 
 /*
-- 
2.16.0

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: [RFC v1 0/4] static analysis of copy_to_user() ===

From: Tycho Andersen <tycho () tycho ! ws>
To: linux-sparse
Subject: [RFC v1 0/4] static analysis of copy_to_user()
Date: Thu, 20 Dec 2018 19:59:27 +0000
Message-ID: <20181220195931.20331-1-tycho () tycho ! ws>
--------------------
Hi all,

A while ago I talked with various people about whether some static
analsys of copy_to_user() could be productive in finding infoleaks.
Unfortunately, due to the various issues outlined in the patch notes, it
doesn't seem like it is. Perhaps these checks are useful to put in just
to future proof ourselves against these sorts of issues, though.

Anyway, here's the code. Thoughts welcome!

Tycho

Tycho Andersen (4):
  expression.h: update comment to include other cast types
  move name-based analysis before linearization
  add a check for copy_to_user() address spaces
  check copy_to_user() sizes

 expression.h                           |   2 +-
 sparse.c                               | 327 ++++++++++++++++++++++---
 validation/copy_to_user.c              |  31 +++
 validation/copy_to_user_sizes.c        |  53 ++++
 validation/copy_to_user_sizes_inline.c |  29 +++
 5 files changed, 405 insertions(+), 37 deletions(-)
 create mode 100644 validation/copy_to_user.c
 create mode 100644 validation/copy_to_user_sizes.c
 create mode 100644 validation/copy_to_user_sizes_inline.c

-- 
2.19.1

================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [RFC v1 0/4] static analysis of copy_to_user()
Date: Fri, 21 Dec 2018 22:47:19 +0000
Message-ID: <20181221224717.zixqkz26xujllmq4 () ltop ! local>
--------------------
On Thu, Dec 20, 2018 at 12:59:27PM -0700, Tycho Andersen wrote:
> Hi all,
> 
> A while ago I talked with various people about whether some static
> analsys of copy_to_user() could be productive in finding infoleaks.
> Unfortunately, due to the various issues outlined in the patch notes, it
> doesn't seem like it is. Perhaps these checks are useful to put in just
> to future proof ourselves against these sorts of issues, though.
> 
> Anyway, here's the code. Thoughts welcome!

Hi,

I'm taking the first patch directly but I won't be able to look
closer at the other patches until next week.

Best regards,
-- Luc 
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: kernel-hardening
Subject: Re: [RFC v1 0/4] static analysis of copy_to_user()
Date: Fri, 21 Dec 2018 22:47:19 +0000
Message-ID: <20181221224717.zixqkz26xujllmq4 () ltop ! local>
--------------------
On Thu, Dec 20, 2018 at 12:59:27PM -0700, Tycho Andersen wrote:
> Hi all,
> 
> A while ago I talked with various people about whether some static
> analsys of copy_to_user() could be productive in finding infoleaks.
> Unfortunately, due to the various issues outlined in the patch notes, it
> doesn't seem like it is. Perhaps these checks are useful to put in just
> to future proof ourselves against these sorts of issues, though.
> 
> Anyway, here's the code. Thoughts welcome!

Hi,

I'm taking the first patch directly but I won't be able to look
closer at the other patches until next week.

Best regards,
-- Luc 
================================================================================

From: Tycho Andersen <tycho () tycho ! ws>
To: linux-sparse
Subject: Re: [RFC v1 0/4] static analysis of copy_to_user()
Date: Sun, 20 Jan 2019 19:05:10 +0000
Message-ID: <20190120190510.GC3987 () cisco>
--------------------
Hey Luc,

On Fri, Dec 21, 2018 at 11:47:19PM +0100, Luc Van Oostenryck wrote:
> On Thu, Dec 20, 2018 at 12:59:27PM -0700, Tycho Andersen wrote:
> > Hi all,
> > 
> > A while ago I talked with various people about whether some static
> > analsys of copy_to_user() could be productive in finding infoleaks.
> > Unfortunately, due to the various issues outlined in the patch notes, it
> > doesn't seem like it is. Perhaps these checks are useful to put in just
> > to future proof ourselves against these sorts of issues, though.
> > 
> > Anyway, here's the code. Thoughts welcome!
> 
> Hi,
> 
> I'm taking the first patch directly but I won't be able to look
> closer at the other patches until next week.

Any chance you can take a peek at these?

Cheers,

Tycho
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: kernel-hardening
Subject: Re: [RFC v1 0/4] static analysis of copy_to_user()
Date: Mon, 21 Jan 2019 21:41:28 +0000
Message-ID: <20190121214127.t3opb6cffaz4ibp5 () ltop ! local>
--------------------
On Mon, Jan 21, 2019 at 08:05:10AM +1300, Tycho Andersen wrote:
> Hey Luc,
> 
> On Fri, Dec 21, 2018 at 11:47:19PM +0100, Luc Van Oostenryck wrote:
> > On Thu, Dec 20, 2018 at 12:59:27PM -0700, Tycho Andersen wrote:
> > > Hi all,
> > > 
> > > A while ago I talked with various people about whether some static
> > > analsys of copy_to_user() could be productive in finding infoleaks.
> > > Unfortunately, due to the various issues outlined in the patch notes, it
> > > doesn't seem like it is. Perhaps these checks are useful to put in just
> > > to future proof ourselves against these sorts of issues, though.
> > > 
> > > Anyway, here's the code. Thoughts welcome!
> > 
> > Hi,
> > 
> > I'm taking the first patch directly but I won't be able to look
> > closer at the other patches until next week.
> 
> Any chance you can take a peek at these?

Hi,

Sorry, I've had few available time the last weeks.
I had look at them shortly after you send them but
I haven't yet made my mind about them.

I'm quite reluctant to add complexity (the AST walking)
if it doesn't bring much benefit if any.

In, short the problems are:
1) duplication of the AST walking
2) unreliable type because of using void *
3) unreliable size because array to pointer degeneracy

There is some solutions, though:
1) what *could* be done is to add a method 'check'
   to struct symbol_op and call it, *for example*,
   just after op->expand() in expand_symbol_call()
   (and add a mechanism to set this method for the symbol
   corresponding to copy_to_user()).

   Otherwise, splitting the AST walking from sparse.c
   and making it something generic would be preferable.

   Another approach could be keep the check via OP_CALL
   but doing it just after linearization, before the
   optimization destroy the types (and add, if needed,
   some flag to force linearize_cast() keep absolutely
   all type info).

2) this one seems pretty hopeless

3) the current calls degenerate()/create_pointer()
   do indeed destroy the original type and (at first sight)
   no 'addressof' should exist anymore after evaluation.
   This is inconsistent with the existence of expand_addressof().
   By changing degenerate()/create_pointer() the original
   type should stay available.

Sorry again for the late reply,
-- Luc
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: [RFC v1 0/4] static analysis of copy_to_user()
Date: Mon, 21 Jan 2019 21:41:28 +0000
Message-ID: <20190121214127.t3opb6cffaz4ibp5 () ltop ! local>
--------------------
On Mon, Jan 21, 2019 at 08:05:10AM +1300, Tycho Andersen wrote:
> Hey Luc,
> 
> On Fri, Dec 21, 2018 at 11:47:19PM +0100, Luc Van Oostenryck wrote:
> > On Thu, Dec 20, 2018 at 12:59:27PM -0700, Tycho Andersen wrote:
> > > Hi all,
> > > 
> > > A while ago I talked with various people about whether some static
> > > analsys of copy_to_user() could be productive in finding infoleaks.
> > > Unfortunately, due to the various issues outlined in the patch notes, it
> > > doesn't seem like it is. Perhaps these checks are useful to put in just
> > > to future proof ourselves against these sorts of issues, though.
> > > 
> > > Anyway, here's the code. Thoughts welcome!
> > 
> > Hi,
> > 
> > I'm taking the first patch directly but I won't be able to look
> > closer at the other patches until next week.
> 
> Any chance you can take a peek at these?

Hi,

Sorry, I've had few available time the last weeks.
I had look at them shortly after you send them but
I haven't yet made my mind about them.

I'm quite reluctant to add complexity (the AST walking)
if it doesn't bring much benefit if any.

In, short the problems are:
1) duplication of the AST walking
2) unreliable type because of using void *
3) unreliable size because array to pointer degeneracy

There is some solutions, though:
1) what *could* be done is to add a method 'check'
   to struct symbol_op and call it, *for example*,
   just after op->expand() in expand_symbol_call()
   (and add a mechanism to set this method for the symbol
   corresponding to copy_to_user()).

   Otherwise, splitting the AST walking from sparse.c
   and making it something generic would be preferable.

   Another approach could be keep the check via OP_CALL
   but doing it just after linearization, before the
   optimization destroy the types (and add, if needed,
   some flag to force linearize_cast() keep absolutely
   all type info).

2) this one seems pretty hopeless

3) the current calls degenerate()/create_pointer()
   do indeed destroy the original type and (at first sight)
   no 'addressof' should exist anymore after evaluation.
   This is inconsistent with the existence of expand_addressof().
   By changing degenerate()/create_pointer() the original
   type should stay available.

Sorry again for the late reply,
-- Luc
================================================================================

From: Tycho Andersen <tycho () tycho ! ws>
To: kernel-hardening
Subject: Re: [RFC v1 0/4] static analysis of copy_to_user()
Date: Thu, 24 Jan 2019 03:15:00 +0000
Message-ID: <20190124031500.GA22711 () cisco>
--------------------
Hi Luc,

On Mon, Jan 21, 2019 at 10:41:28PM +0100, Luc Van Oostenryck wrote:
> On Mon, Jan 21, 2019 at 08:05:10AM +1300, Tycho Andersen wrote:
> > Hey Luc,
> > 
> > On Fri, Dec 21, 2018 at 11:47:19PM +0100, Luc Van Oostenryck wrote:
> > > On Thu, Dec 20, 2018 at 12:59:27PM -0700, Tycho Andersen wrote:
> > > > Hi all,
> > > > 
> > > > A while ago I talked with various people about whether some static
> > > > analsys of copy_to_user() could be productive in finding infoleaks.
> > > > Unfortunately, due to the various issues outlined in the patch notes, it
> > > > doesn't seem like it is. Perhaps these checks are useful to put in just
> > > > to future proof ourselves against these sorts of issues, though.
> > > > 
> > > > Anyway, here's the code. Thoughts welcome!
> > > 
> > > Hi,
> > > 
> > > I'm taking the first patch directly but I won't be able to look
> > > closer at the other patches until next week.
> > 
> > Any chance you can take a peek at these?
> 
> Hi,
> 
> Sorry, I've had few available time the last weeks.
> I had look at them shortly after you send them but
> I haven't yet made my mind about them.
> 
> I'm quite reluctant to add complexity (the AST walking)
> if it doesn't bring much benefit if any.

No problem :).

> In, short the problems are:
> 1) duplication of the AST walking
> 2) unreliable type because of using void *
> 3) unreliable size because array to pointer degeneracy
> 
> There is some solutions, though:
> 1) what *could* be done is to add a method 'check'
>    to struct symbol_op and call it, *for example*,
>    just after op->expand() in expand_symbol_call()
>    (and add a mechanism to set this method for the symbol
>    corresponding to copy_to_user()).
> 
>    Otherwise, splitting the AST walking from sparse.c
>    and making it something generic would be preferable.

Yeah, this sounds like a good option to me.

>    Another approach could be keep the check via OP_CALL
>    but doing it just after linearization, before the
>    optimization destroy the types (and add, if needed,
>    some flag to force linearize_cast() keep absolutely
>    all type info).
> 
> 2) this one seems pretty hopeless

I was hoping you might have some brilliant insight here. It seems like
these checks could catch real bugs at some point, so I'll give the
changes you've suggested a go over the next couple of weeks and see
about a v2.

> 3) the current calls degenerate()/create_pointer()
>    do indeed destroy the original type and (at first sight)
>    no 'addressof' should exist anymore after evaluation.
>    This is inconsistent with the existence of expand_addressof().
>    By changing degenerate()/create_pointer() the original
>    type should stay available.

Ah ha, thanks. I guess it's not necessary to change create_pointer()
for this, but degenerate() definitely looks important.

Thanks!

Tycho
================================================================================

From: Tycho Andersen <tycho () tycho ! ws>
To: linux-sparse
Subject: Re: [RFC v1 0/4] static analysis of copy_to_user()
Date: Thu, 24 Jan 2019 03:15:00 +0000
Message-ID: <20190124031500.GA22711 () cisco>
--------------------
Hi Luc,

On Mon, Jan 21, 2019 at 10:41:28PM +0100, Luc Van Oostenryck wrote:
> On Mon, Jan 21, 2019 at 08:05:10AM +1300, Tycho Andersen wrote:
> > Hey Luc,
> > 
> > On Fri, Dec 21, 2018 at 11:47:19PM +0100, Luc Van Oostenryck wrote:
> > > On Thu, Dec 20, 2018 at 12:59:27PM -0700, Tycho Andersen wrote:
> > > > Hi all,
> > > > 
> > > > A while ago I talked with various people about whether some static
> > > > analsys of copy_to_user() could be productive in finding infoleaks.
> > > > Unfortunately, due to the various issues outlined in the patch notes, it
> > > > doesn't seem like it is. Perhaps these checks are useful to put in just
> > > > to future proof ourselves against these sorts of issues, though.
> > > > 
> > > > Anyway, here's the code. Thoughts welcome!
> > > 
> > > Hi,
> > > 
> > > I'm taking the first patch directly but I won't be able to look
> > > closer at the other patches until next week.
> > 
> > Any chance you can take a peek at these?
> 
> Hi,
> 
> Sorry, I've had few available time the last weeks.
> I had look at them shortly after you send them but
> I haven't yet made my mind about them.
> 
> I'm quite reluctant to add complexity (the AST walking)
> if it doesn't bring much benefit if any.

No problem :).

> In, short the problems are:
> 1) duplication of the AST walking
> 2) unreliable type because of using void *
> 3) unreliable size because array to pointer degeneracy
> 
> There is some solutions, though:
> 1) what *could* be done is to add a method 'check'
>    to struct symbol_op and call it, *for example*,
>    just after op->expand() in expand_symbol_call()
>    (and add a mechanism to set this method for the symbol
>    corresponding to copy_to_user()).
> 
>    Otherwise, splitting the AST walking from sparse.c
>    and making it something generic would be preferable.

Yeah, this sounds like a good option to me.

>    Another approach could be keep the check via OP_CALL
>    but doing it just after linearization, before the
>    optimization destroy the types (and add, if needed,
>    some flag to force linearize_cast() keep absolutely
>    all type info).
> 
> 2) this one seems pretty hopeless

I was hoping you might have some brilliant insight here. It seems like
these checks could catch real bugs at some point, so I'll give the
changes you've suggested a go over the next couple of weeks and see
about a v2.

> 3) the current calls degenerate()/create_pointer()
>    do indeed destroy the original type and (at first sight)
>    no 'addressof' should exist anymore after evaluation.
>    This is inconsistent with the existence of expand_addressof().
>    By changing degenerate()/create_pointer() the original
>    type should stay available.

Ah ha, thanks. I guess it's not necessary to change create_pointer()
for this, but degenerate() definitely looks important.

Thanks!

Tycho
================================================================================


################################################################################

=== Thread: [RFC v1 1/4] expression.h: update comment to include other cast types ===

From: Tycho Andersen <tycho () tycho ! ws>
To: linux-sparse
Subject: [RFC v1 1/4] expression.h: update comment to include other cast types
Date: Thu, 20 Dec 2018 19:59:28 +0000
Message-ID: <20181220195931.20331-2-tycho () tycho ! ws>
--------------------
This part of the union is used with other cast types as well, so let's
include those in the comment.

Signed-off-by: Tycho Andersen <tycho@tycho.ws>
---
 expression.h | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/expression.h b/expression.h
index ba4157f..fece40d 100644
--- a/expression.h
+++ b/expression.h
@@ -187,7 +187,7 @@ struct expression {
 			struct expression *base;
 			unsigned r_bitpos, r_nrbits;
 		};
-		// EXPR_CAST and EXPR_SIZEOF
+		// EXPR_CAST, FORCE_CAST, EXPR_IMPLIED_CAST and EXPR_SIZEOF
 		struct /* cast_arg */ {
 			struct symbol *cast_type;
 			struct expression *cast_expression;
-- 
2.19.1

================================================================================


################################################################################

=== Thread: [RFC v1 3/4] add a check for copy_to_user() address spaces ===

From: Tycho Andersen <tycho () tycho ! ws>
To: linux-sparse
Subject: [RFC v1 3/4] add a check for copy_to_user() address spaces
Date: Thu, 20 Dec 2018 19:59:30 +0000
Message-ID: <20181220195931.20331-4-tycho () tycho ! ws>
--------------------
Leaking kernel pointers to userspace is a bad idea, so let's try to do some
analysis for it.

The basic idea is that every pointer copied to userspace "should be" (but
isn't necessarily) annotated with __user, and if it is not, then it's a
potential infoleak. The motivation for this is stuff like [1], which is
exactly a case of this. Based on a subsequent manual analysis of the uapi
headers, I found [2].

Unfortunately in both of these cases, there is void * (for compat) trickery
that masks them from actually turning up in this case. But hey, I wrote it
and tried it out, so perhaps the code is useful for someone :)

[1]: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/drivers/block/floppy.c?id=65eea8edc315589d6c993cf12dbb5d0e9ef1fe4e
[2]: https://lkml.org/lkml/2018/11/3/142

Signed-off-by: Tycho Andersen <tycho@tycho.ws>
---
 sparse.c                  | 93 ++++++++++++++++++++++++++++++++++++++-
 validation/copy_to_user.c | 31 +++++++++++++
 2 files changed, 123 insertions(+), 1 deletion(-)

diff --git a/sparse.c b/sparse.c
index 217a5bf..8bcbe16 100644
--- a/sparse.c
+++ b/sparse.c
@@ -171,13 +171,104 @@ static void check_memset(struct position pos, struct expression_list *args)
 	check_byte_count(pos, size);
 }
 
+static struct symbol *resolve_arg_type(struct position pos, struct expression *arg)
+{
+	struct expression *uncast;
+
+	uncast = arg;
+	switch (arg->type) {
+	case EXPR_CAST:
+	case EXPR_FORCE_CAST:
+	case EXPR_IMPLIED_CAST:
+		/*
+		 * Undo any casting done by sparse to the function's
+		 * argument type.
+		 */
+		uncast = arg->cast_expression;
+		break;
+	case EXPR_SYMBOL:
+		break;
+	case EXPR_PREOP:
+		/*
+		 * handle derefs; these are really just the type of the
+		 * resulting expression.
+		 */
+		break;
+	case EXPR_BINOP:
+		/* TODO: resolve this pointer math if possible? */
+		return NULL;
+	default:
+		warning(pos, "huh? arg not a cast or symbol? %d", arg->type);
+		return NULL;
+	}
+
+	return uncast->ctype->ctype.base_type;
+}
+
+static void check_ptr_in_other_as(struct position pos, struct symbol *sym, int this_as)
+{
+	struct ident *ident = sym->ident;
+
+	if (sym->type == SYM_NODE)
+		sym = sym->ctype.base_type;
+
+	switch (sym->type) {
+	case SYM_ARRAY:
+	case SYM_PTR: {
+		if (sym->ctype.as != this_as)
+			warning(pos, "member %s is a kernel pointer copied to userspace", show_ident(ident));
+		check_ptr_in_other_as(pos, sym->ctype.base_type, this_as);
+		break;
+	}
+	case SYM_STRUCT:
+	case SYM_UNION: {
+		struct symbol *member;
+
+		FOR_EACH_PTR(sym->symbol_list, member) {
+			check_ptr_in_other_as(pos, member, this_as);
+		} END_FOR_EACH_PTR(member);
+		break;
+	}
+	default:
+		/*
+		 * scalar types are ok
+		 * TODO: what about SYM_LABEL/PREPROCESSOR?
+		 */
+		break;
+	}
+}
+
+static void check_no_kernel_pointers(struct position pos, struct expression_list *args)
+{
+	struct expression *src = ptr_list_nth_entry((struct ptr_list *)args, 1);
+	struct symbol *base = NULL;
+
+	if (!Waddress_space)
+		return;
+
+	/* get the type of src */
+	base = resolve_arg_type(pos, src);
+
+	/*
+	 * And deref it to *src; src will *always* be a kernel pointer, and
+	 * we're really after members of structures here, not the pointers
+	 * themselves. So we do this deref at the top level.
+	 */
+	base = base->ctype.base_type;
+
+	check_ptr_in_other_as(pos, base, 1);
 }
 
 
 #define check_memcpy check_memset
-#define check_ctu check_memset
 #define check_cfu check_memset
 
+void check_ctu(struct position pos, struct expression_list *args)
+{
+	check_memset(pos, args);
+	check_no_kernel_pointers(pos, args);
+}
+
 struct checkfn {
 	struct ident *id;
 	void (*check)(struct position pos, struct expression_list *args);
diff --git a/validation/copy_to_user.c b/validation/copy_to_user.c
new file mode 100644
index 0000000..d9cded6
--- /dev/null
+++ b/validation/copy_to_user.c
@@ -0,0 +1,31 @@
+#define __user __attribute__((address_space(1)))
+
+struct bar {
+	char *bar_kptr;
+};
+
+struct foo {
+	char *foo_kptr;
+	char __user *uptr;
+	struct bar bar;
+};
+
+static void copy_to_user(void __user *to, const void *from, unsigned long n)
+{
+}
+
+static void bar(void)
+{
+	struct foo f;
+	void __user *p = (void __user *)0;
+
+	copy_to_user(p, &f, sizeof(f));
+}
+/*
+ * check-name: copy_to_user arguments
+ *
+ * check-error-start
+copy_to_user.c:22:9: warning: member foo_kptr is a kernel pointer copied to userspace
+copy_to_user.c:22:9: warning: member bar_kptr is a kernel pointer copied to userspace
+ * check-error-end
+ */
-- 
2.19.1

================================================================================


################################################################################

=== Thread: [RFC v1 4/4] check copy_to_user() sizes ===

From: Tycho Andersen <tycho () tycho ! ws>
To: linux-sparse
Subject: [RFC v1 4/4] check copy_to_user() sizes
Date: Thu, 20 Dec 2018 19:59:31 +0000
Message-ID: <20181220195931.20331-5-tycho () tycho ! ws>
--------------------
The intent here is to be a smarter version of the size checking that's just
a hard coded constant. The insight is that we know the type of the thing
that's getting passed to copy_to_user(), so if we can evaluate the size, it
should be <= sizeof(*the thing being copied).

I've run this over x86 with allyes config, and it didn't find anything, so
perhaps it isn't particularly useful. Indeed, most copy_to_user()s look
like:

        copy_to_user(p, &foo, sizeof(foo))

so perhaps this isn't a surprise.

The one potentially interesting case is the hard coded array length case:
it's possible someone might change one place but not the other. See e.g.
the first copy_to_user() in drivers/ata/libata-scsi.c, which looks like:

        copy_to_user(dst, dev->id, ATA_ID_WORDS * sizeof(u16))

id here is a u16 id[ATA_ID_WORDS], so it would be possible to change this
in one place or not the other. Unfortunately, sparse seems to unify the
type of dev->id to a u16* instead of u16[], so we can't see through this.

Anyway, there are currently no violations of this in the kernel either,
although it does emit some false positives like,

fs/xfs/xfs_ioctl.c:1813:25: warning: copy_to_user() where size (13) is larger than src (1)

due to the above. I wrote the code, so perhaps someday it will be useful to
someone, so I'm posting it :)

Signed-off-by: Tycho Andersen <tycho@tycho.ws>
---
 sparse.c                               | 45 ++++++++++++++++++++++
 validation/copy_to_user_sizes.c        | 53 ++++++++++++++++++++++++++
 validation/copy_to_user_sizes_inline.c | 29 ++++++++++++++
 3 files changed, 127 insertions(+)

diff --git a/sparse.c b/sparse.c
index 8bcbe16..eb7d5d7 100644
--- a/sparse.c
+++ b/sparse.c
@@ -31,6 +31,7 @@
 #include <ctype.h>
 #include <unistd.h>
 #include <fcntl.h>
+#include <limits.h>
 
 #include "lib.h"
 #include "allocate.h"
@@ -259,6 +260,49 @@ static void check_no_kernel_pointers(struct position pos, struct expression_list
 	check_ptr_in_other_as(pos, base, 1);
 }
 
+static void check_copy_size(struct position pos, struct expression_list *args)
+{
+	struct expression *src = ptr_list_nth_entry((struct ptr_list *)args, 1);
+	struct expression *size = ptr_list_nth_entry((struct ptr_list *)args, 2);
+	long long src_actual = LLONG_MAX;
+	long long size_actual;
+	struct symbol *base, *deref;
+
+	/* get the type of src */
+	base = resolve_arg_type(pos, src);
+
+	/* and deref it to *src */
+	deref = base->ctype.base_type;
+
+	/*
+	 * Note, this is never hit right now because sparse seems to unify
+	 * arrays that are arguments to type *.
+	 */
+	if (is_array_type(base)) {
+		long long array_size = get_expression_value_silent(base->array_size);
+		printf("%s is array type\n", show_ident(base->ident));
+
+		/* failed to evaluate */
+		if (array_size == 0)
+			return;
+		src_actual = array_size * bits_to_bytes(deref->bit_size);
+	} else {
+		src_actual = bits_to_bytes(deref->bit_size);
+	}
+
+	/* Evaluate size, if we can */
+	size_actual = get_expression_value_silent(size);
+	/*
+	 * size_actual == 0 means that get_expression_value failed; of
+	 * course we'll miss something if there is a 0 length copy, but
+	 * then nothing will leak anyway so...
+	 */
+	if (size_actual == 0)
+		return;
+
+	if (size_actual > src_actual)
+		warning(pos, "copy_to_user() where size (%lld) is larger than src (%lld)", size_actual, src_actual);
+}
 
 #define check_memcpy check_memset
 #define check_cfu check_memset
@@ -267,6 +311,7 @@ void check_ctu(struct position pos, struct expression_list *args)
 {
 	check_memset(pos, args);
 	check_no_kernel_pointers(pos, args);
+	check_copy_size(pos, args);
 }
 
 struct checkfn {
diff --git a/validation/copy_to_user_sizes.c b/validation/copy_to_user_sizes.c
new file mode 100644
index 0000000..f1f7b8e
--- /dev/null
+++ b/validation/copy_to_user_sizes.c
@@ -0,0 +1,53 @@
+static void copy_to_user(void *to, const void *from, unsigned long n)
+{
+}
+
+struct foo {
+	int bar;
+};
+
+static void main(void)
+{
+	void *p;
+	struct foo f;
+	int uninitialized;
+
+	copy_to_user(p, &f, sizeof(f));
+	copy_to_user(p, &f, sizeof(f)-1);
+	copy_to_user(p, &f, sizeof(f)+1);
+	copy_to_user(p, &f, 1);
+	copy_to_user(p, &f, 100);
+	copy_to_user(p, &f, uninitialized);
+}
+
+#if 0
+static void broken(void)
+{
+	void *p;
+	char arr[100];
+	struct foo foo_arr[2];
+
+	/*
+	 * These all fail right now, because sparse seems to unify the type of
+	 * arr/foo_arr to char * /struct foo *, instead of char[]/struct foo[].
+	 *
+	 * That's unfortunate, because it means that these generate false
+	 * positives in the kernel when using a static array length, and that
+	 * seems like a case where this type of check would be especially
+	 * useful.
+	 */
+	copy_to_user(p, arr, 100);
+	copy_to_user(p, arr, 101);
+	copy_to_user(p, foo_arr, sizeof(foo_arr));
+	copy_to_user(p, foo_arr, sizeof(foo_arr)-1);
+	copy_to_user(p, foo_arr, sizeof(foo_arr[0])*2);
+}
+#endif
+/*
+ * check-name: copy_to_user sizes
+ *
+ * check-error-start
+copy_to_user_sizes.c:17:9: warning: copy_to_user() where size (5) is larger than src (4)
+copy_to_user_sizes.c:19:9: warning: copy_to_user() where size (100) is larger than src (4)
+ * check-error-end
+ */
diff --git a/validation/copy_to_user_sizes_inline.c b/validation/copy_to_user_sizes_inline.c
new file mode 100644
index 0000000..bd3c3bd
--- /dev/null
+++ b/validation/copy_to_user_sizes_inline.c
@@ -0,0 +1,29 @@
+static inline void copy_to_user(void *to, const void *from, unsigned long n)
+{
+}
+
+struct foo {
+	int bar;
+};
+
+static void main(void)
+{
+	void *p;
+	struct foo f;
+	int uninitialized;
+
+	copy_to_user(p, &f, sizeof(f));
+	copy_to_user(p, &f, sizeof(f)-1);
+	copy_to_user(p, &f, sizeof(f)+1);
+	copy_to_user(p, &f, 1);
+	copy_to_user(p, &f, 100);
+	copy_to_user(p, &f, uninitialized);
+}
+/*
+ * check-name: copy_to_user sizes
+ *
+ * check-error-start
+copy_to_user_sizes_inline.c:17:21: warning: copy_to_user() where size (5) is larger than src (4)
+copy_to_user_sizes_inline.c:19:21: warning: copy_to_user() where size (100) is larger than src (4)
+ * check-error-end
+ */
-- 
2.19.1

================================================================================


################################################################################

=== Thread: [bug report] kernel.h: hide __is_constexpr() from sparse ===

From: Dan Carpenter <dan.carpenter () oracle ! com>
To: linux-sparse
Subject: [bug report] kernel.h: hide __is_constexpr() from sparse
Date: Tue, 27 Nov 2018 10:09:00 +0000
Message-ID: <20181127100900.6g6w32omobw2xseg () kili ! mountain>
--------------------
Hello Johannes Berg,

The patch a06ae2017b9e: "kernel.h: hide __is_constexpr() from sparse"
from Nov 17, 2018, leads to a bunch of Smatch warnings.  We stubbed out
__is_constexpr() to return always true because Sparse was complaining
but Smatch needs the __is_constexpr() to work or it can't parse
min()/max() macros.

regards,
dan carpenter
================================================================================

From: Dan Carpenter <dan.carpenter () oracle ! com>
To: linux-sparse
Subject: Re: [bug report] kernel.h: hide __is_constexpr() from sparse
Date: Tue, 27 Nov 2018 10:12:56 +0000
Message-ID: <20181127101256.GQ3088 () unbuntlaptop>
--------------------
On Tue, Nov 27, 2018 at 01:09:00PM +0300, Dan Carpenter wrote:
> Hello Johannes Berg,
> 
> The patch a06ae2017b9e: "kernel.h: hide __is_constexpr() from sparse"
> from Nov 17, 2018, leads to a bunch of Smatch warnings.  We stubbed out
> __is_constexpr() to return always true because Sparse was complaining
> but Smatch needs the __is_constexpr() to work or it can't parse
> min()/max() macros.
> 

Actually, the simple solution is to just make __is_constexpr() always
return 0.  That should work for everyone?

regards,
dan carpenter

================================================================================

From: "Berg, Johannes" <johannes.berg () intel ! com>
To: linux-sparse
Subject: RE: [bug report] kernel.h: hide __is_constexpr() from sparse
Date: Tue, 27 Nov 2018 10:15:33 +0000
Message-ID: <1DC40B07CD6EC041A66726C271A73AE68CF77F67 () IRSMSX108 ! ger ! corp ! intel ! com>
--------------------
> The patch a06ae2017b9e: "kernel.h: hide __is_constexpr() from sparse"
> from Nov 17, 2018, leads to a bunch of Smatch warnings.  We stubbed out

Hmm. I don't think it should be in linux-next. Andrew?

johannes
-- 

Intel Deutschland GmbH
Registered Address: Am Campeon 10-12, 85579 Neubiberg, Germany
Tel: +49 89 99 8853-0, www.intel.de
Managing Directors: Christin Eisenschmid
Chairperson of the Supervisory Board: Nicole Lau
Registered Office: Munich
Commercial Register: Amtsgericht Muenchen HRB 186928

================================================================================


################################################################################

=== Thread: bitwise enums ===

From: Matthew Wilcox <willy () infradead ! org>
To: linux-sparse
Subject: bitwise enums
Date: Tue, 20 Feb 2018 16:09:39 +0000
Message-ID: <20180220160939.GG21243 () bombadil ! infradead ! org>
--------------------

I have a feature request relating to bitwise and enums.  The following
does not work the way I would like it to:

typedef __attribute__((bitwise)) enum {
	FOO,
	BAR,
} my_t;

static my_t f(int a)
{
	if (a)
		return FOO;
	return BAR;
};

sparse complains:

test.c:10:16: warning: incorrect type in return expression (different base types)
test.c:10:16:    expected restricted my_t
test.c:10:16:    got int

(line 10 is, of course 'return BAR'; since FOO has value 0, it does not warn
on line 9).

I think it would be appropriate for the __attribute__((bitwise)) to
apply not just to 'my_t' but also to 'FOO' and 'BAR'.  ie parse this
as if it were written:

typedef __attribute__((bitwise)) enum {
	FOO = ((__force my_t))0,
	BAR = ((__force my_t))1,
} my_t;

... except of course you can't write that because my_t doesn't exist
until the end of the block.

sparse doesn't whinge about this:

typedef __attribute__((bitwise)) unsigned int my_t;
  
typedef enum {
        FOO = (__attribute__((force)) my_t)0,
        BAR = (__attribute__((force)) my_t)1,
} my_t;

although GCC does complain about redefining a typedef.  Maybe sparse
should also warn about that?

Oh, and I'd also like sparse to permit this:

typedef __attribute__((bitwise)) enum {
	FOO = 1,
	BAR = 2,
	BAZ = 4,
	QUUX = 8,
	WIBBLE = 0x10,
} my_t;

(if you're defining enums for a bitwise type, you're almost certainly
defining individual bits, so you want to do explicit assignment of values)

I can work around this for now by writing:

typedef __attribute__((bitwise)) unsigned int my_t;
  
enum {
        FOO = (__attribute__((force)) my_t)0,
        BAR = (__attribute__((force)) my_t)1,
};

but that's less elegant than the syntax I suggested above.
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: bitwise enums
Date: Tue, 20 Feb 2018 17:02:43 +0000
Message-ID: <20180220170241.i3i2ghhipb6ezgqj () ltop ! local>
--------------------
On Tue, Feb 20, 2018 at 08:09:39AM -0800, Matthew Wilcox wrote:
> 
> I have a feature request relating to bitwise and enums.  The following
> does not work the way I would like it to:
> 
> typedef __attribute__((bitwise)) enum {
> 	FOO,
> 	BAR,
> } my_t;
> 
> static my_t f(int a)
> {
> 	if (a)
> 		return FOO;
> 	return BAR;
> };
> 
> sparse complains:
> 
> test.c:10:16: warning: incorrect type in return expression (different base types)
> test.c:10:16:    expected restricted my_t
> test.c:10:16:    got int
> 
> (line 10 is, of course 'return BAR'; since FOO has value 0, it does not warn
> on line 9).
> 
> I think it would be appropriate for the __attribute__((bitwise)) to
> apply not just to 'my_t' but also to 'FOO' and 'BAR'.

Yes, it's reasonable and intuitive.
It would be nice to have Al Viro's option on this, though.

> sparse doesn't whinge about this:
> 
> typedef __attribute__((bitwise)) unsigned int my_t;
>   
> typedef enum {
>         FOO = (__attribute__((force)) my_t)0,
>         BAR = (__attribute__((force)) my_t)1,
> } my_t;
> 
> although GCC does complain about redefining a typedef.  Maybe sparse
> should also warn about that?

That's clearly a bug.

> Oh, and I'd also like sparse to permit this:
> 
> typedef __attribute__((bitwise)) enum {
> 	FOO = 1,
> 	BAR = 2,
> 	BAZ = 4,
> 	QUUX = 8,
> 	WIBBLE = 0x10,
> } my_t;
> 
> (if you're defining enums for a bitwise type, you're almost certainly
> defining individual bits, so you want to do explicit assignment of values)

Sure. For me, it's implied by your first request.
 
Thanks for the feedback. I'll see what I can do in the coming days.
-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Linus Torvalds <torvalds () linux-foundation ! org>
To: linux-sparse
Subject: Re: bitwise enums
Date: Tue, 20 Feb 2018 17:20:45 +0000
Message-ID: <CA+55aFyj6dTyHeniiiTwn=P0oewbwcCwsPKvYYH37paXozUWiQ () mail ! gmail ! com>
--------------------
On Tue, Feb 20, 2018 at 9:02 AM, Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
> On Tue, Feb 20, 2018 at 08:09:39AM -0800, Matthew Wilcox wrote:
>>
>> I have a feature request relating to bitwise and enums.  The following
>> does not work the way I would like it to:
>>
>> typedef __attribute__((bitwise)) enum {
>>       FOO,
>>       BAR,
>> } my_t;
>>
>> sparse complains:
>>
>> test.c:10:16: warning: incorrect type in return expression (different base types)
>> test.c:10:16:    expected restricted my_t
>> test.c:10:16:    got int

Yeah, that's garbage.

>> I think it would be appropriate for the __attribute__((bitwise)) to
>> apply not just to 'my_t' but also to 'FOO' and 'BAR'.

Definitely.

> Yes, it's reasonable and intuitive.
> It would be nice to have Al Viro's option on this, though.

I'm pretty sure Al will agree, and it's just a matter of "it's a
bug/not implemented".

>> Oh, and I'd also like sparse to permit this:
>>
>> typedef __attribute__((bitwise)) enum {
>>       FOO = 1,
>>       BAR = 2,
>>       BAZ = 4,
>>       QUUX = 8,
>>       WIBBLE = 0x10,
>> } my_t;

I agree.

I just don't have a patch to correct this behavior.

               Linus
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Linus Torvalds <torvalds () linux-foundation ! org>
To: linux-sparse
Subject: Re: bitwise enums
Date: Tue, 20 Feb 2018 22:18:28 +0000
Message-ID: <CA+55aFzVjWo_K4Zh23Q-zhQVAkuiK_jCx5SzGvXRVN0g_67mgg () mail ! gmail ! com>
--------------------
On Tue, Feb 20, 2018 at 2:00 PM, Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
>
> One detail about the syntax: my understanding of the attribute
> syntax is that in:
>         typedef __bitwise enum foobar { FOO, BAR, } my_t;
> which is essentially the same as:
>         enum foobar { FOO, BAR, };
>         typedef __bitwise enum foobar my_t;
> the attribute relate to the typedef/my_t and not to the enum type.

You are correct, and I didn't think of that wrinkle.

And it's a fundamental wrinkle. A big part of the __bitwise thing
really is that it creates a private type for each typedef, which is
important for things like "big-endian u32".

Now, arguably the typedef that actually is part of the definition of
the type could be special, but I'm getting the feeling that what Willy
really wants is "nocast", not "bitwise".

Except the sparse "nocast" is so weak as to be almost useless, and
doesn't work at all for this case.

Oh well.

But maybe we could make "nocast" work on enums. They are special enough.

           Linus
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Matthew Wilcox <willy () infradead ! org>
To: linux-sparse
Subject: Re: bitwise enums
Date: Tue, 20 Feb 2018 22:18:54 +0000
Message-ID: <20180220221854.GA17220 () bombadil ! infradead ! org>
--------------------
On Tue, Feb 20, 2018 at 11:00:37PM +0100, Luc Van Oostenryck wrote:
> On Tue, Feb 20, 2018 at 09:20:45AM -0800, Linus Torvalds wrote:
> > On Tue, Feb 20, 2018 at 9:02 AM, Luc Van Oostenryck
> > <luc.vanoostenryck@gmail.com> wrote:
> > > On Tue, Feb 20, 2018 at 08:09:39AM -0800, Matthew Wilcox wrote:
> > >> typedef __attribute__((bitwise)) enum {
> > >>       FOO,
> > >>       BAR,
> > >> } my_t;
> > >>
> 
> One detail about the syntax: my understanding of the attribute
> syntax is that in:
> 	typedef __bitwise enum foobar { FOO, BAR, } my_t;
> which is essentially the same as:
> 	enum foobar { FOO, BAR, };
> 	typedef __bitwise enum foobar my_t;
> the attribute relate to the typedef/my_t and not to the enum type.

Yes, I was worrying about that a little myself.

> It doesn't matter much later, when my_t is used, but it matters
> for the initializers, for example in:
> > >> typedef __attribute__((bitwise)) enum {
> > >>       FOO = 1,
> > >>       BAR = 2,
> > >>       BAZ = 4,
> > >>       QUUX = 8,
> > >>       WIBBLE = 0x10,
> 
> So, I propose that what we should aim for is to support:
> 	enum __bitwise foobar { FOO, BAR, };
> and, maybe:
> 	enum foobar { FOO, BAR, } __bitwise;
> Both are equivalent following GGC's attribute's syntax but in
> this case, I think the first is far better (and is easier).

I like your first option better than your second.  From my point of view,
it's just swapping two words around:

typedef __bitwise enum {

to

typedef enum __bitwise {

(I understand from a syntactic analysis point of view, it has a completely
different meaning ;-)
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Matthew Wilcox <willy () infradead ! org>
To: linux-sparse
Subject: Re: bitwise enums
Date: Tue, 20 Feb 2018 22:22:14 +0000
Message-ID: <20180220222214.GB17220 () bombadil ! infradead ! org>
--------------------
On Tue, Feb 20, 2018 at 02:18:28PM -0800, Linus Torvalds wrote:
> On Tue, Feb 20, 2018 at 2:00 PM, Luc Van Oostenryck
> <luc.vanoostenryck@gmail.com> wrote:
> >
> > One detail about the syntax: my understanding of the attribute
> > syntax is that in:
> >         typedef __bitwise enum foobar { FOO, BAR, } my_t;
> > which is essentially the same as:
> >         enum foobar { FOO, BAR, };
> >         typedef __bitwise enum foobar my_t;
> > the attribute relate to the typedef/my_t and not to the enum type.
> 
> You are correct, and I didn't think of that wrinkle.
> 
> And it's a fundamental wrinkle. A big part of the __bitwise thing
> really is that it creates a private type for each typedef, which is
> important for things like "big-endian u32".
> 
> Now, arguably the typedef that actually is part of the definition of
> the type could be special, but I'm getting the feeling that what Willy
> really wants is "nocast", not "bitwise".

Actually, I do want bitwise; I'm looking at creating vm_fault_t so that
we don't get fault handlers returning -EFOO instead of VM_FAULT_FOO.
This would also be good for gfp_t.  I don't have a particularly strong
feeling about other enums, but it would probably be good to find someone
with strong feelings so that we're not passing up an opportunity to do
something better than what I've asked for.

--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Linus Torvalds <torvalds () linux-foundation ! org>
To: linux-sparse
Subject: Re: bitwise enums
Date: Tue, 20 Feb 2018 22:27:16 +0000
Message-ID: <CA+55aFw1rGif+vQcb=0fddbgxBwGEqDZuw8_9sVV9wBsOhaQLw () mail ! gmail ! com>
--------------------
On Tue, Feb 20, 2018 at 2:22 PM, Matthew Wilcox <willy@infradead.org> wrote:
>
> Actually, I do want bitwise; I'm looking at creating vm_fault_t so that
> we don't get fault handlers returning -EFOO instead of VM_FAULT_FOO.

Fair enough, that does have bitwise semantics, not just the "don't
cast this value implicitly".

With traditional enums that really are entirely independent values and
aren't mixed together and have no arithmetic done on them, "nocast"
really should work. It doesn't.

            Linus
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: bitwise enums
Date: Tue, 20 Feb 2018 23:50:01 +0000
Message-ID: <20180220235000.esd337vwuunfqqz2 () ltop ! local>
--------------------
On Tue, Feb 20, 2018 at 02:18:54PM -0800, Matthew Wilcox wrote:
> On Tue, Feb 20, 2018 at 11:00:37PM +0100, Luc Van Oostenryck wrote:
> > 
> > So, I propose that what we should aim for is to support:
> > 	enum __bitwise foobar { FOO, BAR, };
> > and, maybe:
> > 	enum foobar { FOO, BAR, } __bitwise;
> > Both are equivalent following GGC's attribute's syntax but in
> > this case, I think the first is far better (and is easier).
> 
> I like your first option better than your second.

Yes. The second option is just because for GCC's attribute syntax
both should be equivalent. So, I'll will implement the first, but
the second only if there is a strong demand for it in the name
of compleness.

-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: bitwise enums
Date: Wed, 21 Feb 2018 00:06:16 +0000
Message-ID: <20180221000614.ssogkgasjlavmoug () ltop ! local>
--------------------
On Tue, Feb 20, 2018 at 02:18:28PM -0800, Linus Torvalds wrote:
> On Tue, Feb 20, 2018 at 2:00 PM, Luc Van Oostenryck
> <luc.vanoostenryck@gmail.com> wrote:
> >
> > One detail about the syntax: my understanding of the attribute
> > syntax is that in:
> >         typedef __bitwise enum foobar { FOO, BAR, } my_t;
> > which is essentially the same as:
> >         enum foobar { FOO, BAR, };
> >         typedef __bitwise enum foobar my_t;
> > the attribute relate to the typedef/my_t and not to the enum type.
> 
> You are correct, and I didn't think of that wrinkle.
> 
> And it's a fundamental wrinkle. A big part of the __bitwise thing
> really is that it creates a private type for each typedef, which is
> important for things like "big-endian u32".
> 
> Now, arguably the typedef that actually is part of the definition of
> the type could be special,

I'm not 100% sure I understand the problem (if problem there is).
For the typedef, yes it's how it's used for __le32 or gfp_t because
we want to combine __bitwise with int to create a new, distinct type.
But here, we don't care about the typedef because we don't need one
since we're creating a new, distinct type anyway: the enum itself.

So I think that 'enum __bitwise foobar { FOO = 1, };' should be
a restricted type and should behave as Matthew would like.
Of course, 'typedef enum __bitwise { FOO = 1, } my_t' shoudl work
as well, being just a new name for a restricted anonymous enum.

-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: bitwise enums
Date: Mon, 16 Apr 2018 08:48:59 +0000
Message-ID: <20180416084855.zwa3gwd2qs752stx () ltop ! local>
--------------------
On Tue, Feb 20, 2018 at 09:20:45AM -0800, Linus Torvalds wrote:
> On Tue, Feb 20, 2018 at 9:02 AM, Luc Van Oostenryck wrote:
> > On Tue, Feb 20, 2018 at 08:09:39AM -0800, Matthew Wilcox wrote:
> >>
> >> I have a feature request relating to bitwise and enums.  The following
> >> does not work the way I would like it to:
> >>
> >> typedef __attribute__((bitwise)) enum {
> >>       FOO,
> >>       BAR,
> >> } my_t;

Hi,

I'm back on this topic but I'm seeing some oddities related to enums
and I'm wondering if these are desired behaviour or just bugs or
implementation flaws. Any clarification is very welcome.

1) type_difference() doesn't make a distinction between an enum and an
   int (or more exactly, between an enum and its base type).
   So you can write things like:
	void foo(enum e1);
	...
	void foo(int a) { ... }

   and so on, without any warnings.
   It certainly defeats a lot of type checking related to enums.

2) if you define an enum like:
	enum e2 {
		NIL = 0,
		ONE = 1U,
		DUO = 2LL,
	};

   then the type of NIL, ONE & DUO is int, unsigned int and long long.
   2a) I expect NIL, ONE & DUO to all have the same type:
       the base type of 'enum e2' (or 'enum e2' itself but IIUC, that's
       not what the standard says).
   2b) I expect a warning (or an error) if the initializers are not all
       type compatible.


Cheers,
-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Linus Torvalds <torvalds () linux-foundation ! org>
To: linux-sparse
Subject: Re: bitwise enums
Date: Mon, 16 Apr 2018 15:27:43 +0000
Message-ID: <CA+55aFxh+QdWips2QRhG-OULC+6Nxp_jpxPCs170-rPOEF69yw () mail ! gmail ! com>
--------------------
On Mon, Apr 16, 2018 at 1:48 AM, Luc Van Oostenryck
<luc.vanoostenryck@gmail.com> wrote:
>
> I'm back on this topic but I'm seeing some oddities related to enums
> and I'm wondering if these are desired behaviour or just bugs or
> implementation flaws. Any clarification is very welcome.
>
> 1) type_difference() doesn't make a distinction between an enum and an
>    int

That's just being lazy - in standard C (ie the absence of __bitwise),
enums work like int for all type checking.

So that whole "just peel off the enum type, look at the base type" is
just because of the normal C semantics. Maybe

Note that it is wrong *even for standard C*.

You can pass a "pointer to enum", and the lazy cherck in
type_difference() means that it thinks it's the same as "pointer to
int".

So the type_difference handling is actively wrong, and much too
permissive, but nobody cared enough to ever get it right.

Sorry.

> 2) if you define an enum like:
>         enum e2 {
>                 NIL = 0,
>                 ONE = 1U,
>                 DUO = 2LL,
>         };
>
>    then the type of NIL, ONE & DUO is int, unsigned int and long long.

Yeah, I think that's wrong too, they should all be 'enum e2' as you say.

And I think it comes from the same thing above: we never were very
careful with enums. Sparse tried to be careful with types, but it was
always mainly about other things (ie notably the whole "__user"
annotation), and enums were more of a "make it not complain".

                   Linus
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: bitwise enums
Date: Tue, 17 Apr 2018 01:30:02 +0000
Message-ID: <20180417013000.zvu3rtovuckxsrks () ltop ! local>
--------------------
On Mon, Apr 16, 2018 at 08:27:43AM -0700, Linus Torvalds wrote:
> On Mon, Apr 16, 2018 at 1:48 AM, Luc Van Oostenryck wrote:
> >
> > I'm back on this topic but I'm seeing some oddities related to enums
> > and I'm wondering if these are desired behaviour or just bugs or
> > implementation flaws. Any clarification is very welcome.
> >
> > 1) type_difference() doesn't make a distinction between an enum and an
> >    int
> 
> That's just being lazy - in standard C (ie the absence of __bitwise),
> enums work like int for all type checking.
> 
> So that whole "just peel off the enum type, look at the base type" is
> just because of the normal C semantics. Maybe
> 
> Note that it is wrong *even for standard C*.
> 
> You can pass a "pointer to enum", and the lazy cherck in
> type_difference() means that it thinks it's the same as "pointer to
> int".
> 
> So the type_difference handling is actively wrong, and much too
> permissive, but nobody cared enough to ever get it right.

OK, thanks for the info.
I wanted to be sure that it wasn't so on purpose because the code
in type_difference() looks as if it could have been so.

> > 2) if you define an enum like:
> >         enum e2 {
> >                 NIL = 0,
> >                 ONE = 1U,
> >                 DUO = 2LL,
> >         };
> >
> >    then the type of NIL, ONE & DUO is int, unsigned int and long long.
> 
> Yeah, I think that's wrong too, they should all be 'enum e2' as you say.
> 
> And I think it comes from the same thing above: we never were very
> careful with enums. Sparse tried to be careful with types, but it was
> always mainly about other things (ie notably the whole "__user"
> annotation), and enums were more of a "make it not complain".

Yes, no problem. I'll look a this very soon.

Thanks again,
-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: bitwise enums
Date: Sat, 21 Apr 2018 12:21:43 +0000
Message-ID: <20180421122141.6qylqogywa5kqkti () ltop ! local>
--------------------
On Tue, Apr 17, 2018 at 03:30:02AM +0200, Luc Van Oostenryck wrote:
> On Mon, Apr 16, 2018 at 08:27:43AM -0700, Linus Torvalds wrote:
> > On Mon, Apr 16, 2018 at 1:48 AM, Luc Van Oostenryck wrote:
> > >
> > > I'm back on this topic but I'm seeing some oddities related to enums
> > > and I'm wondering if these are desired behaviour or just bugs or
> > > implementation flaws. Any clarification is very welcome.
> > >
> > > 1) type_difference() doesn't make a distinction between an enum and an
> > >    int
> > 
> > That's just being lazy - in standard C (ie the absence of __bitwise),
> > enums work like int for all type checking.
> > 
> > So that whole "just peel off the enum type, look at the base type" is
> > just because of the normal C semantics. Maybe
> > 
> > Note that it is wrong *even for standard C*.
> > 
> > You can pass a "pointer to enum", and the lazy cherck in
> > type_difference() means that it thinks it's the same as "pointer to
> > int".
> > 
> > So the type_difference handling is actively wrong, and much too
> > permissive, but nobody cared enough to ever get it right.
> 
> OK, thanks for the info.
> I wanted to be sure that it wasn't so on purpose because the code
> in type_difference() looks as if it could have been so.

I hadn't read the standard till now and it appears that the situation
is more annoying that I thought. From what I understand of it:
1) The enumeration *constants* are allways of type 'int' (C11 6.7.2.2p3).
   So, following the standard, it's impossible to have an enumeration
   constant that is not representable as an 'int' although it's OK to
   have such value for the initializer. Uh.
   GCC doesn't seems to respect that and for each enumeration uses a
   type that is large enough for all values (but I find nothing in
   GCC's doc about it).
   sparse does essentially the same as GCC.
2) Each enumeration is a distinct type (C11 6.2.5p16). But every
   enumeration type is compatible with 'char', a signed integer or an
   unsigned type (c11 6.7.2.2p4).
   So, yes having something like:
	enum e { ... };
	int foo(void);
	enum e foo(void);
   *may* be perfectly legit and GCC will accept this without any warning
   *if* 'int' is the choosen compatible type.

So, yes, I think the current code in sparse was purposely written so
to match the standard (I think it's Al that wrote this part).
OTOH, since we don't know what will be the compatible type, we shouldn't
mix up an enumeration type and some integer type and thus we would like
to have sparse issuing at least a warning when it happen.

The patch I posted a few days ago, essentially remove the part
"is compatible with one of the integer type" and shows about 200 places
in the kernel when there is some mixup between an enum type and 'int'
or 'unsigned int'.

The question now is "what do we want for sparse?", "do we want a flag
for it?" and "what do we want as default for this flag?".

> > > 2) if you define an enum like:
> > >         enum e2 {
> > >                 NIL = 0,
> > >                 ONE = 1U,
> > >                 DUO = 2LL,
> > >         };
> > >
> > >    then the type of NIL, ONE & DUO is int, unsigned int and long long.
> > 
> > Yeah, I think that's wrong too, they should all be 'enum e2' as you say.

Mmmm, 'enum e2' would be the type I think they should have but this will
be against the standard and against what GCC do (see above and C11 or C99
6.7.2.2p3 or C89 6.5.2.2).

> > And I think it comes from the same thing above: we never were very
> > careful with enums. Sparse tried to be careful with types, but it was
> > always mainly about other things (ie notably the whole "__user"
> > annotation), and enums were more of a "make it not complain".

Looking closer at the code, there some code to force all the constant
to have the same type (cast_enum_list()) but it uses cast_value() that
don't change the type, only the value. So it's just a one-liner to fix it.

-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: checking parameters to variadic printf formatted functions ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: checking parameters to variadic printf formatted functions
Date: Fri, 26 Oct 2018 09:36:41 +0000
Message-ID: <20181026093640.intkhvf7srjjiosv () ltop ! local>
--------------------
On Fri, Oct 26, 2018 at 09:51:06AM +0100, Ben Dooks wrote:
> I have been looking into testing the address_space() pointers in
> the current Linux kernel and came up with a case where I am not
> sure how this should work (the kernel's __user attribute).
> 
> We tested the following cases, In this case sparse correctly
> identified the incorrect address space/no-dereference. The first
> was not identified:
> 
> # define __user  __attribute__((noderef, address_space(1)))
> 
> void test(char __user *ptr)
> {
> 	printk("%s\n", ptr);
> 	printk("%c\n", ptr[0]);
> }

Hi,

Yes, but everything here is as it should:
* the 2nd line should trigger a message like
	test.c:8:27: warning: dereference of noderef expression
  since the pointer is dereferenced (before being passed to printk().
* no warning for the 1st line since there is no dereference and no
  pointer assignment to a pointer of a different address space..

> I know that there is a gcc printf format attribute to allow
> specifying the positions of the format and start of variadic
> arguments.
> 
> I have had a quick look at the code but have not tried to make
> any modifications yet, and would like some feedback or help
> before making any:
> 
> - Should we automatically warn on possible de-ref (or address space)
>   on passing to a variadic function (I think the simplest)

I think this would be wrong.
For example, there is no problems if the corresponding address is
simply displayed (via "%p" or "%lu") but it would if the pointer
is dereferenced (for example, via "%s").

> - Is adding support  for __attribute__((format(printf,a,b))) a
>   good idea (and if so is anyone looking into this already)

Yes, I think it's a good idea.
I don't think anyone is looking to add this to sparse already
(it's something I've added in my nice-to-have list but this list
is quite long ...).

> - If adding printf format a good idea, then should we extend it
>   to adding a kernel printk style one too (modern kernels seem
>   to love extending the printf formatting)

I really think so, yes.
 
> Any help would be appreciated.

You're welcome,
-- Luc Van Oostenryck
================================================================================


################################################################################

=== Thread: information required ===

From: info () 3guneurope ! eu
To: linux-sparse
Subject: information required
Date: Wed, 21 Feb 2018 06:56:57 +0000
Message-ID: <201802210703.w1L5v5xx008023 () ncompany ! narasoft ! com>
--------------------
Thanks for your last email response to me.
The information required should include the following-:
Your full names
Your address
Telephone number
Your private email
Occupation
Age
This is to enable my further discussion with you in confidence.
Best regards and wishes to you.
Mohammad Amir Khadov

NB: Please reply to:

uk4uk@postaxte.com


uk3uk@postaxte.com
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: recent sparse ssa patches ===

From: Ramsay Jones <ramsay () ramsayjones ! plus ! com>
To: linux-sparse
Subject: Re: recent sparse ssa patches
Date: Thu, 30 Aug 2018 00:02:44 +0000
Message-ID: <cf9b8318-53a4-f2d9-00b3-36c2731049f3 () ramsayjones ! plus ! com>
--------------------


On 29/08/18 22:57, Luc Van Oostenryck wrote:
> On Wed, Aug 29, 2018 at 03:55:20AM +0100, Ramsay Jones wrote:
>> Hi Luc,
>>
>> Just a quick note about the recent SSA commits on your sparse.git
>> master branch (commits d46b7b2..96f388f). I haven't looked at them
>> in detail, but they cause the 'make selfcheck' target to complain:
> 
> I've spend months on testing and retesting it but I forgot to
> use selfcheck! I'm even more surprise that I haven't include
> ssa.h in ssa.c ... :(
>  
>> The patch below fixes up the warnings. I assume the idf_dump() function
>> is useful to have around to debug the dominance frontier while in the
>> debugger (making it static would cause gcc to complain).
> 
> It was certainly useful at the beginning, now much less but who
> knows it can still one day.

Yeah, I felt that it may come in handy, hence the extern
declaration. However, if you feel that it has outlived its
usefulness, then maybe you could remove it.

> 
>> I would normally look to see which specific commits the individual
>> hunks should be applied to, but its late here (3-50am), so I'll
>> leave that to you. :-D
> 
> I hope you hadn't to get up too early!

Well, early enough that I had to think about whether it was
worth going to bed! ;-)

> 
> Thank you for looking at this.
> I would like to apply your patch but you haven't added your signoff.
> May I add it?

Well, it wasn't really a patch; I had anticipated that you
would squash the hunks into the relevant commits (hence the
comment). Yes, it is generally not a good idea to re-wind
a published branch to make such changes, but that kinda
depends on how many people will be affected ...

However, if you would rather not rewind the branch and want to
apply the 'patch' on top, then that is fine by me as well, so:

Signed-off-by: Ramsay Jones <ramsay@ramsayjones.plus.com>

> 
> Also, I see that this email was not send to the mailing list.
> I don't know if it was on purpose or not, so this reply is also
> not public but feel free to reply to the ML.

No, it was just late ... :-D

ATB,
Ramsay Jones

================================================================================


################################################################################

=== Thread: regressions on HEAD ===

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: Re: regressions on HEAD
Date: Wed, 28 Feb 2018 20:24:52 +0000
Message-ID: <CACXZuxe1yM1rU+rvZ+fMbpGXgGWwsX4LHzAVLK0N3HwKmL-_CA () mail ! gmail ! com>
--------------------
On 28 February 2018 at 19:28, Christopher Li <sparse@chrisli.org> wrote:
> On Wed, Feb 28, 2018 at 10:59 AM, Dibyendu Majumdar
> <mobile@majumdar.org.uk> wrote:
>>
>> Okay, so I am trying to understand where in the simplification phase
>> the size of the value pseudo used. This was the only place I could
>> find. Although now multiple value pseudos are created - I guess the
>> actual value is still the same, and if the size of the value pseudo
>> does not play a part in simplification then sure this change cannot
>> cause a change in the simplification process? Or am I missing
>> something?
>
> It is in theory possible in the following case, you have two instruction all the
> same except two operand size:
>
> %r10 <- op.32  %r11, $10 (size 8)
> %r13 <- op.32  %r11, $10 (size 32)
>
> In that case, CSE will not able to see the $10 as the same, because they
> have size difference and reference by two different pseudo value.
>
> If that ever happen, that is a strange situation to get into in the first place.
> We might want to look a closer look if that ever happen. How can same size
> instruction have different size operand.
>
> The fix I think just need to change the first $10(size 8) into the $10(size 32)
> version. It is a lot simpler than change the type. I am not sure this example
> is actually trigger able yet. Not big deal.
>

Okay thank you for the explanation - might be worth trying to create a
test case that triggers this scenario.

Regards
Dibyendu
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: sparse considers enums as signed ===

From: "Jason A. Donenfeld" <Jason () zx2c4 ! com>
To: linux-sparse
Subject: sparse considers enums as signed
Date: Fri, 26 Oct 2018 02:39:41 +0000
Message-ID: <CAHmME9oZk7Afns=W=xM=6waqvVfV+iAdAcB6731xzUUcm4P0zA () mail ! gmail ! com>
--------------------
Hi,

The C spec says that an enum becomes whatever integral type is
sufficient for holding its contents. Sparse seems to assume that only
applies to signed numbers:

zx2c4@thinkpad /tmp $ cat sparse.c
enum { sparse_does_not_like_this = 0x8000000000000003ULL };
zx2c4@thinkpad /tmp $ sparse sparse.c
sparse.c:1:36: warning: cast truncates bits from constant value
(8000000000000000 becomes 3)

Both gcc and clang are fine with that and do the right thing. AFAICT,
it's valid C too.

Thanks,
Jason
================================================================================


################################################################################

=== Thread: sparse maintenance pace? ===

From: "Jason A. Donenfeld" <Jason () zx2c4 ! com>
To: linux-sparse
Subject: sparse maintenance pace?
Date: Thu, 22 Feb 2018 22:28:21 +0000
Message-ID: <CAHmME9o=f2GOnrLDh9AvybUz8bYvXYv+JcCxsuSM_T98fu0y0Q () mail ! gmail ! com>
--------------------
Hey Chris & Sparse ML,

I've seen that Luc (CC'd) has a boat load of well tested patches, and
is also actively working on new features and improvements. Meanwhile
the sparse master branch on kernel.org seems like a bit of an
afterthought. What would it take to enable Luc to really take the
reins and start merging his stuff upstream and pulling things forward?
Sparse is a special project -- being lightweight yet having so many
neat capabilities -- and I'd really hate to see it languish in the age
of all-things-clang. (More importantly, is that even something that
appeals to you, Luc?)

Regards,
Jason
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Josh Triplett <josh () joshtriplett ! org>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Fri, 23 Feb 2018 11:17:41 +0000
Message-ID: <20180223111740.GA6528 () localhost>
--------------------
On Thu, Feb 22, 2018 at 11:28:21PM +0100, Jason A. Donenfeld wrote:
> Hey Chris & Sparse ML,
> 
> I've seen that Luc (CC'd) has a boat load of well tested patches, and
> is also actively working on new features and improvements. Meanwhile
> the sparse master branch on kernel.org seems like a bit of an
> afterthought. What would it take to enable Luc to really take the
> reins and start merging his stuff upstream and pulling things forward?
> Sparse is a special project -- being lightweight yet having so many
> neat capabilities -- and I'd really hate to see it languish in the age
> of all-things-clang.

Sparse is the kind of project that needs careful curation, and
personally, I think it's quite reasonable and important to be cautious
about direction and patch acceptance. I'd be concerned about changing
that, personally.

That said, I do think the ambiguity between the master branch and
development branches is a bit problematic, and it would help if the
master branch stayed more up to date with current development. That
would probably help address such concerns.
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Randy Dunlap <rdunlap () infradead ! org>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Fri, 23 Feb 2018 18:13:47 +0000
Message-ID: <7b2e108e-714a-10a7-d425-d35a6c044252 () infradead ! org>
--------------------
On 02/23/2018 03:17 AM, Josh Triplett wrote:
> On Thu, Feb 22, 2018 at 11:28:21PM +0100, Jason A. Donenfeld wrote:
>> Hey Chris & Sparse ML,
>>
>> I've seen that Luc (CC'd) has a boat load of well tested patches, and
>> is also actively working on new features and improvements. Meanwhile
>> the sparse master branch on kernel.org seems like a bit of an
>> afterthought. What would it take to enable Luc to really take the
>> reins and start merging his stuff upstream and pulling things forward?
>> Sparse is a special project -- being lightweight yet having so many
>> neat capabilities -- and I'd really hate to see it languish in the age
>> of all-things-clang.
> 
> Sparse is the kind of project that needs careful curation, and
> personally, I think it's quite reasonable and important to be cautious
> about direction and patch acceptance. I'd be concerned about changing
> that, personally.
> 
> That said, I do think the ambiguity between the master branch and
> development branches is a bit problematic, and it would help if the
> master branch stayed more up to date with current development. That
> would probably help address such concerns.
> --

I would add that:

(a) I would prefer to see more "released versions"
(see https://sparse.wiki.kernel.org/index.php/Main_Page and
https://www.kernel.org/pub/software/devel/sparse/dist/)

(b) If there are not going to be more released versions on kernel.org due to
the past security problem (and signing required etc.), then the sparse wiki
page should be updated to say that there won't be more "released versions" on
kernel.org.

thanks,
-- 
~Randy
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: "Jason A. Donenfeld" <Jason () zx2c4 ! com>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Fri, 23 Feb 2018 19:28:52 +0000
Message-ID: <CAHmME9p_5GHsWKaSKeiJy8Xu6rEPExMy5H4OvnfufGjxQ=7UUA () mail ! gmail ! com>
--------------------
On Fri, Feb 23, 2018 at 12:17 PM, Josh Triplett <josh@joshtriplett.org> wrote:
> Sparse is the kind of project that needs careful curation, and
> personally, I think it's quite reasonable and important to be cautious
> about direction and patch acceptance. I'd be concerned about changing
> that, personally.

I agree. And generally being a maintainer of a sensitive project means
saying no much more often than saying yes. But, there's a difference
between putting in the energy to consider things carefully enough to
say no, and simply not putting time in at all.

I don't know what the best solution is precisely -- whether its
installing Luc or rearranging things in some other way -- but it seems
like something needs to change to get things moving.
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Fri, 23 Feb 2018 19:45:11 +0000
Message-ID: <CACXZuxduR1G9iumM57bFP9OB_BH1SAV2z6aYN=ynwM1pt4EdTg () mail ! gmail ! com>
--------------------
On 23 February 2018 at 19:28, Jason A. Donenfeld <Jason@zx2c4.com> wrote:
> I don't know what the best solution is precisely -- whether its
> installing Luc or rearranging things in some other way -- but it seems
> like something needs to change to get things moving.

Hi, Just out of interest - how do you use Sparse currently and are
there specific patches you need?

Regards
Dibyendu
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Christopher Li <sparse () chrisli ! org>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Sun, 25 Feb 2018 13:08:14 +0000
Message-ID: <CANeU7QnPa1J57Eycfr9+OtOx7Tck1GJnMTtr_kd3U5jwJPhh9Q () mail ! gmail ! com>
--------------------
On Fri, Feb 23, 2018 at 10:13 AM, Randy Dunlap <rdunlap@infradead.org> wrote:
>
> I would add that:
>
> (a) I would prefer to see more "released versions"
> (see https://sparse.wiki.kernel.org/index.php/Main_Page and
> https://www.kernel.org/pub/software/devel/sparse/dist/)
>
> (b) If there are not going to be more released versions on kernel.org due to
> the past security problem (and signing required etc.), then the sparse wiki
> page should be updated to say that there won't be more "released versions" on
> kernel.org.

Actually there is a tool to upload tgz release to kernel.org. I keep forgetting
this task. It is my bad.

Chris
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Christopher Li <sparse () chrisli ! org>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Sun, 25 Feb 2018 19:14:24 +0000
Message-ID: <CANeU7QmMwEQkKGRhnT0f8E-kkMYV2_CsLd23qpKD43xABmJpKg () mail ! gmail ! com>
--------------------
On Fri, Feb 23, 2018 at 10:13 AM, Randy Dunlap <rdunlap@infradead.org> wrote:
> (a) I would prefer to see more "released versions"
> (see https://sparse.wiki.kernel.org/index.php/Main_Page and
> https://www.kernel.org/pub/software/devel/sparse/dist/)

I have upload the sparse-0.5.1.tar.gz into
https://www.kernel.org/pub/software/devel/sparse/dist/

I also update the wiki for the release page of 0.5.1
https://sparse.wiki.kernel.org/index.php/Sparse_0.5.1_released

The main page haven't pick up the news yet. May be it
take a while for the news category to be indexed.

Chris
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Randy Dunlap <rdunlap () infradead ! org>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Tue, 27 Feb 2018 17:51:25 +0000
Message-ID: <9e519675-11b0-8998-125f-f41e8a75f251 () infradead ! org>
--------------------
On 02/25/2018 11:14 AM, Christopher Li wrote:
> On Fri, Feb 23, 2018 at 10:13 AM, Randy Dunlap <rdunlap@infradead.org> wrote:
>> (a) I would prefer to see more "released versions"
>> (see https://sparse.wiki.kernel.org/index.php/Main_Page and
>> https://www.kernel.org/pub/software/devel/sparse/dist/)
> 
> I have upload the sparse-0.5.1.tar.gz into
> https://www.kernel.org/pub/software/devel/sparse/dist/

Yes, that one is there. Thanks.

> I also update the wiki for the release page of 0.5.1
> https://sparse.wiki.kernel.org/index.php/Sparse_0.5.1_released
> 
> The main page haven't pick up the news yet. May be it
> take a while for the news category to be indexed.

The wiki page still has not been updated. I wonder what is going on
with it.

-- 
~Randy
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Christopher Li <sparse () chrisli ! org>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Tue, 27 Feb 2018 18:25:51 +0000
Message-ID: <CANeU7QkGMXzLn=9XOJbbqMFQZPQxDZMUcs5Gkyzmhig1q=qtkA () mail ! gmail ! com>
--------------------
On Tue, Feb 27, 2018 at 9:51 AM, Randy Dunlap <rdunlap@infradead.org> wrote:
>> The main page haven't pick up the news yet. May be it
>> take a while for the news category to be indexed.
>
> The wiki page still has not been updated. I wonder what is going on
> with it.

The release  0.5.1 page is already there.
https://sparse.wiki.kernel.org/index.php/Sparse_0.5.1_released

The News Category is tagged in the end of the 0.5.1 release page source.

The wiki thinks that page is belong to the News, see the
the news category page. The last entry is the 0.5.1 release.

https://sparse.wiki.kernel.org/index.php/Category:News

However, I don't know why it is not picking up in the Main_Page.
Maybe there is a limit on how many entry is going to show by using category
that way.

Instead of using the wiki category auto aggregating of the news.
We can manually edit the news sections to list the news entry.

Chris
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Tue, 27 Feb 2018 20:04:59 +0000
Message-ID: <20180227200457.4dx2wpv3qbipwwf2 () ltop ! local>
--------------------
On Tue, Feb 27, 2018 at 09:51:25AM -0800, Randy Dunlap wrote:
> On 02/25/2018 11:14 AM, Christopher Li wrote:
> > I also update the wiki for the release page of 0.5.1
> > https://sparse.wiki.kernel.org/index.php/Sparse_0.5.1_released
> > 
> > The main page haven't pick up the news yet. May be it
> > take a while for the news category to be indexed.
> 
> The wiki page still has not been updated. I wonder what is going on
> with it.

The page https://sparse.wiki.kernel.org/index.php/Sparse_0.5.1_released
is tagged as belonging to the Category:News but the main page show the
content of Category:News itself. In other words, the content of the page
https://sparse.wiki.kernel.org/index.php/Category:News still need to be
updated manually (as can be seen by looking at its history).

-- Luc Van Oostenryck
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Randy Dunlap <rdunlap () infradead ! org>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Tue, 27 Feb 2018 20:25:16 +0000
Message-ID: <9556f77d-b8c9-5183-972d-a7664d53c5e9 () infradead ! org>
--------------------
On 02/27/2018 10:25 AM, Christopher Li wrote:
> On Tue, Feb 27, 2018 at 9:51 AM, Randy Dunlap <rdunlap@infradead.org> wrote:
>>> The main page haven't pick up the news yet. May be it
>>> take a while for the news category to be indexed.
>>
>> The wiki page still has not been updated. I wonder what is going on
>> with it.
> 
> The release  0.5.1 page is already there.
> https://sparse.wiki.kernel.org/index.php/Sparse_0.5.1_released

hmph, I would never know that just by looking at
https://sparse.wiki.kernel.org/index.php/Main_Page

Hopefully it all gets fixed one day. :)

> The News Category is tagged in the end of the 0.5.1 release page source.
> 
> The wiki thinks that page is belong to the News, see the
> the news category page. The last entry is the 0.5.1 release.
> 
> https://sparse.wiki.kernel.org/index.php/Category:News
> 
> However, I don't know why it is not picking up in the Main_Page.
> Maybe there is a limit on how many entry is going to show by using category
> that way.
> 
> Instead of using the wiki category auto aggregating of the news.
> We can manually edit the news sections to list the news entry.

thanks,
-- 
~Randy
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Linus Torvalds <torvalds () linux-foundation ! org>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Wed, 28 Feb 2018 20:22:58 +0000
Message-ID: <CA+55aFyNxOgcn6XK=KghiyocTBrnkuFChbTg1OrULEejQ2rNgA () mail ! gmail ! com>
--------------------
On Wed, Feb 28, 2018 at 12:04 PM, Dibyendu Majumdar
<mobile@majumdar.org.uk> wrote:
>
> Sometimes I wish the upstream projects were frozen
> forever - as it is so painful merging changes.

The solution to that tends to be

 (a) merge often enough that the pain is minimized

 (b) try to not diverge (you already apparently do that) by organizing
your own changes so that they are as independent as possible

 (c) try to send patches upstream that help your project - at least
for merge purposes

Note that (c) can be a huge deal, but it requires that you make sure
your patches make sense in the context of upstream, not just in your
own context.

It's often painful. In the kernel, some projects have been very good
at it. The RT people spent a lot of effort on (c), and it paid off for
them, to the point where not only did merges get easier, but most of
their code ended up being upstream too.

Other projects haven't tried as hard.

         Linus
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Wed, 28 Feb 2018 20:31:08 +0000
Message-ID: <CACXZuxdpR=F6j_33hMx5J355BWoFg+_gHdsD2gaNyQFF_SJqjg () mail ! gmail ! com>
--------------------
On 28 February 2018 at 20:22, Linus Torvalds
<torvalds@linux-foundation.org> wrote:
> On Wed, Feb 28, 2018 at 12:04 PM, Dibyendu Majumdar
> <mobile@majumdar.org.uk> wrote:
>>
>> Sometimes I wish the upstream projects were frozen
>> forever - as it is so painful merging changes.
>
> The solution to that tends to be
>
>  (a) merge often enough that the pain is minimized
>
>  (b) try to not diverge (you already apparently do that) by organizing
> your own changes so that they are as independent as possible
>
>  (c) try to send patches upstream that help your project - at least
> for merge purposes
>
> Note that (c) can be a huge deal, but it requires that you make sure
> your patches make sense in the context of upstream, not just in your
> own context.
>
> It's often painful. In the kernel, some projects have been very good
> at it. The RT people spent a lot of effort on (c), and it paid off for
> them, to the point where not only did merges get easier, but most of
> their code ended up being upstream too.
>
> Other projects haven't tried as hard.
>

I would love to submit patches - and hoping to do this for the LLVM
backend in Sparse, and after that hopefully all the tests. My other
changes are to do with making Sparse a robust library - avoid
polluting global namespace, avoid global / static variables, graceful
error handling etc. The problem is that these changes can't be
submitted piecemeal - and if I submitted a huge patch that touches all
files - it will probably not be palatable!

Regards
Dibyendu
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Linus Torvalds <torvalds () linux-foundation ! org>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Wed, 28 Feb 2018 20:55:17 +0000
Message-ID: <CA+55aFxKuekjpCD9tUW1QfOY0Fqzbx8xYaagzcS9zTQXnAaC5Q () mail ! gmail ! com>
--------------------
On Wed, Feb 28, 2018 at 12:31 PM, Jason A. Donenfeld <Jason@zx2c4.com> wrote:
>
> So far as I've been told, it takes many many months for Luc to get a
> response on things he sends upstream, then gets some stylistic
> feedback, fixes it, resubmits, and then has to wait another couple of
> months. For this reason, people get fed up and are then inclined to
> diverge. This is a bummer, as it'd be preferable to have quick review
> cycles and thus continuous merging.

Oh, I agree. I think sparse maintenance does need to be more responsive.


================================================================================

From: Dibyendu Majumdar <mobile () majumdar ! org ! uk>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Wed, 28 Feb 2018 20:56:08 +0000
Message-ID: <CACXZuxc+HQYv8UuoH3keyWZZXECXR==os24fZfMP2qm3hcpEZQ () mail ! gmail ! com>
--------------------
On 28 February 2018 at 20:31, Jason A. Donenfeld <Jason@zx2c4.com> wrote:
> On Wed, Feb 28, 2018 at 9:22 PM, Linus Torvalds
> <torvalds@linux-foundation.org> wrote:
>>  (c) try to send patches upstream that help your project - at least
>> for merge purposes
>>
>> Note that (c) can be a huge deal, but it requires that you make sure
>> your patches make sense in the context of upstream, not just in your
>> own context.
>
> So far as I've been told, it takes many many months for Luc to get a
> response on things he sends upstream, then gets some stylistic
> feedback, fixes it, resubmits, and then has to wait another couple of
> months. For this reason, people get fed up and are then inclined to
> diverge. This is a bummer, as it'd be preferable to have quick review
> cycles and thus continuous merging.

I think it is somewhat more complex than that.

Firstly some changes are directional - e.g. SSA implementation, or the
debate about pseudo value size versus a pseudo OP code. And sometimes
these changes can block other changes. In my view a solution can be
found if people look at things more from a technical standpoint and
avoid hurling personal abuse / insults. Also if a technical change is
blocking other changes then submitters can perhaps omit the
controversial change and move forward with the rest - but there has
been a lack of flexibility in my opinion.

Secondly Sparse lacks a robust test framework that can be used to
verify the output - I do not mean that just for the traditional use
case of Sparse as a code checker - but as a basis for code generation.
This is something I am hoping to address by contributing a working
LLVM backend and test cases. Having a robust test framework will
enable more rapid changes as it will give confidence that a change
does not break things.

There is also a lack of reviewers - so that is I guess a bandwidth
issue. But on the whole it is better to not skip the review and hurry
with changes - especially due to above. Sometimes changes are quite
large and it is hard to verify that they don't break things. I
certainly found a number of changes in the last year that broke things
- and several revisions were needed to fix issues - and in some cases
I have not yet merged those changes. Again this makes applying large
series of changes very hard.

In my case I have not had issues - I got good responses on all the
problems I reported. Although I haven't submitted patches yet I hope
to start soon.

Regards
Dibyendu
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Christopher Li <sparse () chrisli ! org>
To: linux-sparse
Subject: Re: sparse maintenance pace?
Date: Fri, 02 Mar 2018 09:27:07 +0000
Message-ID: <CANeU7Qm5ox4Ah_LcMdZ9eX7BsvtN3xDgkc4WGtBVJVjO8BzM3A () mail ! gmail ! com>
--------------------
On Wed, Feb 28, 2018 at 12:55 PM, Linus Torvalds
<torvalds@linux-foundation.org> wrote:
> On Wed, Feb 28, 2018 at 12:31 PM, Jason A. Donenfeld <Jason@zx2c4.com> wrote:
>>
>> So far as I've been told, it takes many many months for Luc to get a
>> response on things he sends upstream, then gets some stylistic
>> feedback, fixes it, resubmits, and then has to wait another couple of
>> months. For this reason, people get fed up and are then inclined to
>> diverge. This is a bummer, as it'd be preferable to have quick review
>> cycles and thus continuous merging.
>
> Oh, I agree. I think sparse maintenance does need to be more responsive.
>
> From personal experience, I can say that can be pretty hard to start
> trusting others enough to just apply patches and take pull requests.
> But it's important.
>
> The maintainer does need to be a source of quality control, but at the
> same time, it does very much require "trust others to just do the
> right thing" too. The quality control may be about finding a quality
> person, not about each patch.
>
> Otherwise maintenance ends up being a huge bottleneck.

Thanks for the very insightful feed back. That is super helpful.

> So I do think Chris should take patches from Luc in particular more
> aggressively - and preferably just pull his tree. Or even have shared
> maintenance of the whole tree.
>
> Because Luc has definitely been around long enough that we know he
> fixes any issues he has introduced.

You are right. Let me start merging Luc's master tree.

There will be conflicts, I will post my result on a different thread.

Chris
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: sparse test failures & PATH_MAX ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: sparse test failures & PATH_MAX
Date: Fri, 27 Apr 2018 07:33:00 +0000
Message-ID: <20180427073258.ga6i5maalktyukxu () ltop ! local>
--------------------
On Fri, Apr 27, 2018 at 07:56:38AM +0200, Uwe Kleine-König wrote:
> Hello,
> 
> On 08/30/2017 06:14 PM, Uwe Kleine-König wrote:
> > Antoine Beaupre (on Cc:) noticed that sparse doesn't work on some not so
> > common architectures like ppc32le, s390x, ppc64 and sparc64[1]. This is
> > nicely catched by the testsuite, e.g.:
> >
> > [..]
> 
> Just a heads up: I uploaded 0.5.2 to Debian and there are problems left
> on hurd-i386 (where PATH_MAX isn't defined[1])
> ...
> [1]
> https://buildd.debian.org/status/fetch.php?pkg=sparse&arch=hurd-i386&ver=0.5.2-1&stamp=1524168405&raw=0

Thanks for the repport.
I'll see what can be done.

-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: sparse test failures & PATH_MAX
Date: Fri, 27 Apr 2018 16:11:11 +0000
Message-ID: <20180427161110.3apj6vx4q2imbm3e () ltop ! local>
--------------------
On Fri, Apr 27, 2018 at 09:33:55AM +0200, Uwe Kleine-König wrote:
> On 04/27/2018 09:33 AM, Luc Van Oostenryck wrote:
> > On Fri, Apr 27, 2018 at 07:56:38AM +0200, Uwe Kleine-König wrote:
> >> Hello,
> >>
> >>> [..]
> >>
> >> Just a heads up: I uploaded 0.5.2 to Debian and there are problems left
> >> on hurd-i386 (where PATH_MAX isn't defined[1])
> >> ...
> >> [1]
> >> https://buildd.debian.org/status/fetch.php?pkg=sparse&arch=hurd-i386&ver=0.5.2-1&stamp=1524168405&raw=0
> > 
> > Thanks for the repport.
> > I'll see what can be done.
> 
> I think the default idiom is:
> 
> #ifndef PATH_MAX
> #define PATH_MAX 4096
> #endif

Yes.
I had hoped to avoid this together with removing a memcpy() but things are
more annoying than I had first thought.

Best regards,
-- Luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: sparse test failures on ppc32le (and other not so common archs) ===

From: =?UTF-8?Q?Uwe_Kleine-K=c3=b6nig?= <uwe () kleine-koenig ! org>
To: linux-sparse
Subject: Re: sparse test failures on ppc32le (and other not so common archs)
Date: Fri, 27 Apr 2018 05:56:38 +0000
Message-ID: <779a85fe-0691-0f8b-c70a-85df449003bc () kleine-koenig ! org>
--------------------
This is an OpenPGP/MIME signed message (RFC 4880 and 3156)
--WLXmqMhZeba6Qul5ZUkRV91CafJW6Xy3r
Content-Type: multipart/mixed; boundary="tsP6BUa2VIKFdNzk4p3BcdmG7zdZL89V8";
 protected-headers="v1"
From: =?UTF-8?Q?Uwe_Kleine-K=c3=b6nig?= <uwe@kleine-koenig.org>
To: linux-sparse@vger.kernel.org
Cc: 873508@bugs.debian.org, Antoine Beaupre <anarcat@debian.org>
Message-ID: <779a85fe-0691-0f8b-c70a-85df449003bc@kleine-koenig.org>
Subject: Re: sparse test failures on ppc32le (and other not so common archs)
References: <20170830161435.krq44m5zub4mq43x@taurus.defre.kleine-koenig.org>
In-Reply-To: <20170830161435.krq44m5zub4mq43x@taurus.defre.kleine-koenig.org>

--tsP6BUa2VIKFdNzk4p3BcdmG7zdZL89V8
Content-Type: text/plain; charset=windows-1252
Content-Language: en-US
Content-Transfer-Encoding: quoted-printable

Hello,

On 08/30/2017 06:14 PM, Uwe Kleine-K=F6nig wrote:
> Antoine Beaupre (on Cc:) noticed that sparse doesn't work on some not s=
o
> common architectures like ppc32le, s390x, ppc64 and sparc64[1]. This is=

> nicely catched by the testsuite, e.g.:
>
> [..]

Just a heads up: I uploaded 0.5.2 to Debian and there are problems left
on hurd-i386 (where PATH_MAX isn't defined[1]) and x32 where I get:

	env CHECK=3D./sparse ./cgcc -no-compile memops.c
	/usr/include/x86_64-linux-gnux32/gnu/stubs.h:10:12: error: unable to
open 'gnu/stubs-64.h'

The stubs.h file looks as follows:

#if !defined __x86_64__
# include <gnu/stubs-32.h>
#endif
#if defined __x86_64__ && defined __LP64__
# include <gnu/stubs-64.h>
#endif
#if defined __x86_64__ && defined __ILP32__
# include <gnu/stubs-x32.h>
#endif

Given that libc6-dev only provides stubs-x32.h from these three, I guess
we must not define __LP64__ in this case. I don't have a x32 machine
handy, but the complete build log of the auto builder can be found at

	https://buildd.debian.org/status/fetch.php?pkg=3Dsparse&arch=3Dx32&ver=3D=
0.5.2-1&stamp=3D1524169455&raw=3D0

Best regards
Uwe

[1]
https://buildd.debian.org/status/fetch.php?pkg=3Dsparse&arch=3Dhurd-i386&=
ver=3D0.5.2-1&stamp=3D1524168405&raw=3D0


--tsP6BUa2VIKFdNzk4p3BcdmG7zdZL89V8--

--WLXmqMhZeba6Qul5ZUkRV91CafJW6Xy3r
Content-Type: application/pgp-signature; name="signature.asc"
Content-Description: OpenPGP digital signature
Content-Disposition: attachment; filename="signature.asc"

-----BEGIN PGP SIGNATURE-----

iQEzBAEBCgAdFiEEfnIqFpAYrP8+dKQLwfwUeK3K7AkFAlriu5oACgkQwfwUeK3K
7AmZbQgAj3Lz4vAS4R0U3MOcnssT2J7L5ILB8oNGusTKFCZ9FnsB5T1nSRaEYRcp
3VlnvBCwaZHYvTQrJzrgzSldz+8hCH2pq2jFMPzRn4UpISDeDRnCGwa4mT3kkedB
ad6T/uqO/fLaSmaulzreorNXBBZjj4TQjWvmrE5ouYOL8TxGUDy3vqTbL8AmofhW
bollSehIo02bWhRahgO6dw9rsQvjUM/NHsVI5rHT6NxDVk0aaqufqJJv6rZR1bI6
KZbz6Oh7kWTzCGsiN5ewzUk5dCfuiNSQzSlfUkgQYrksb8fNGtqzXD5Wd/tYfCG6
irh3NsQHWKnZTy051DKAI1QUfiCGFg==
=izKD
-----END PGP SIGNATURE-----

--WLXmqMhZeba6Qul5ZUkRV91CafJW6Xy3r--
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

=== Thread: sparse test failures on x32 ===

From: Luc Van Oostenryck <luc.vanoostenryck () gmail ! com>
To: linux-sparse
Subject: Re: sparse test failures on x32
Date: Fri, 27 Apr 2018 16:08:04 +0000
Message-ID: <20180427160803.4z62asb7g4eevbjp () ltop ! local>
--------------------
On Fri, Apr 27, 2018 at 09:48:31AM +0200, Uwe Kleine-König wrote:
> Hello Luc,
> 
> On 04/27/2018 09:43 AM, Luc Van Oostenryck wrote:
> >> I don't have a x32 machine
> > 
> > But can you launch a test build for sparse easily enough?
> 
> Hmm, well, I can upload a new version of sparse and then it will be
> build on all archs and then made available for all unstable users. So I
> don't want to schedule builds to debug x32 builds.

Yes, it's understandable.
 
> > I guess that there is no install CD available yet?
> 
> I think it's not too hard to setup a x32 chroot on amd64.
> 
> See https://wiki.debian.org/X32Port .

I'll see what's the easiest for me.

The main issue in sparse itself is very trivial but some changes
are also needed for cgcc and this will need more tests.

Best regards,
-- luc
--
To unsubscribe from this list: send the line "unsubscribe linux-sparse" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
================================================================================


################################################################################

